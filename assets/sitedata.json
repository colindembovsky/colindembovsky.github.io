


{
  "pages": [
    {
      
      
      
      "content": "\n",
      "url": "/404.html"
    },{
      "image": "/assets/img/blog/hydejack-9.jpg",
      
      
      "content": "\n    \n\n\n\nColin Dembovsky is a Field Solution Engineer for GitHub. He is a DevOpsologist and ALM MVP and is based in Spring, TX (formerly from South Africa). After completing an MSc in Computer Science at Rhodes University, he worked as a developer (first in Linux using C++ and moving to .NET and C#) and later systems architect. He left development work to start ALM consulting, and has been an ALM MVP since 2011. He was a DevOps practice lead before joining GitHub.\n\nColin is passionate about helping teams improve the quality of their software, and do it more efficiently and securely. Colin is a frequent guest speaker and regularly blogs at http://colinsalmcorner.com and can be found on twitter at @colindembovsky and on GitHub.com/colindembovsky.\n\nWhen he is not working, he is playing guitar, mandolin or drums, 3D printing on his Ender3, boardgaming, or entertaining his wife and 2 kids.\n",
      "url": "/about/"
    },{
      
      "title": "Blog",
      "description": "The official Hydejack blog. Version updates, example content and how-to guides on how to blog with Jekyll.\n",
      "content": "\n",
      "url": "/blog/"
    },{
      
      "title": "Posts",
      
      "content": "\n",
      "url": "/"
    },{
      
      
      
      "content": "\n",
      "url": "/offline.html"
    },{
      
      "title": "Posts",
      "description": "This is the list layout for showing blog posts, which shows just the title and groups them by year of publication. Check out the blog layout for comparison.\n",
      "content": "\n",
      "url": "/posts/"
    },{
      
      "title": "Résumé*",
      "description": "This is the description of your resume page, as it will be seen by search engines. You’ll probably want to modify it in resume.md, and maybe set hide_description to true in the front matter.\n",
      "content": "\n",
      "url": "/resume/"
    },{
      
      "title": "Tags",
      
      "content": "\n",
      "url": "/tags/"
    },{
      
      "title": "Blog",
      "description": "The official Hydejack blog. Version updates, example content and how-to guides on how to blog with Jekyll.\n",
      "content": "\n",
      "url": "/blog/2/"
    },{
      
      "title": "Blog",
      "description": "The official Hydejack blog. Version updates, example content and how-to guides on how to blog with Jekyll.\n",
      "content": "\n",
      "url": "/blog/3/"
    },{
      
      "title": "Blog",
      "description": "The official Hydejack blog. Version updates, example content and how-to guides on how to blog with Jekyll.\n",
      "content": "\n",
      "url": "/blog/4/"
    },{
      
      "title": "Blog",
      "description": "The official Hydejack blog. Version updates, example content and how-to guides on how to blog with Jekyll.\n",
      "content": "\n",
      "url": "/blog/5/"
    },{
      
      "title": "Blog",
      "description": "The official Hydejack blog. Version updates, example content and how-to guides on how to blog with Jekyll.\n",
      "content": "\n",
      "url": "/blog/6/"
    },{
      
      "title": "Blog",
      "description": "The official Hydejack blog. Version updates, example content and how-to guides on how to blog with Jekyll.\n",
      "content": "\n",
      "url": "/blog/7/"
    },{
      
      "title": "Blog",
      "description": "The official Hydejack blog. Version updates, example content and how-to guides on how to blog with Jekyll.\n",
      "content": "\n",
      "url": "/blog/8/"
    },{
      
      "title": "Blog",
      "description": "The official Hydejack blog. Version updates, example content and how-to guides on how to blog with Jekyll.\n",
      "content": "\n",
      "url": "/blog/9/"
    },{
      
      "title": "Blog",
      "description": "The official Hydejack blog. Version updates, example content and how-to guides on how to blog with Jekyll.\n",
      "content": "\n",
      "url": "/blog/10/"
    },{
      
      "title": "Blog",
      "description": "The official Hydejack blog. Version updates, example content and how-to guides on how to blog with Jekyll.\n",
      "content": "\n",
      "url": "/blog/11/"
    },{
      
      "title": "Blog",
      "description": "The official Hydejack blog. Version updates, example content and how-to guides on how to blog with Jekyll.\n",
      "content": "\n",
      "url": "/blog/12/"
    },{
      
      "title": "Blog",
      "description": "The official Hydejack blog. Version updates, example content and how-to guides on how to blog with Jekyll.\n",
      "content": "\n",
      "url": "/blog/13/"
    },{
      
      "title": "Blog",
      "description": "The official Hydejack blog. Version updates, example content and how-to guides on how to blog with Jekyll.\n",
      "content": "\n",
      "url": "/blog/14/"
    },{
      
      "title": "Blog",
      "description": "The official Hydejack blog. Version updates, example content and how-to guides on how to blog with Jekyll.\n",
      "content": "\n",
      "url": "/blog/15/"
    },{
      
      "title": "Blog",
      "description": "The official Hydejack blog. Version updates, example content and how-to guides on how to blog with Jekyll.\n",
      "content": "\n",
      "url": "/blog/16/"
    },{
      
      "title": "Blog",
      "description": "The official Hydejack blog. Version updates, example content and how-to guides on how to blog with Jekyll.\n",
      "content": "\n",
      "url": "/blog/17/"
    },{
      
      "title": "Blog",
      "description": "The official Hydejack blog. Version updates, example content and how-to guides on how to blog with Jekyll.\n",
      "content": "\n",
      "url": "/blog/18/"
    },{
      
      "title": "Blog",
      "description": "The official Hydejack blog. Version updates, example content and how-to guides on how to blog with Jekyll.\n",
      "content": "\n",
      "url": "/blog/19/"
    },{
      
      "title": "Blog",
      "description": "The official Hydejack blog. Version updates, example content and how-to guides on how to blog with Jekyll.\n",
      "content": "\n",
      "url": "/blog/20/"
    }
  ], 
  "documents": [
    {
      
      "title": "Running DB Data Generation and Unit Tests in TeamBuild 2010",
      "date": "2010-10-13 23:38:00 +0000",
      
      "content": "Using “Data Dude” (or VS DB Professional) in VS 2010 allows you to bring Agile to your database development. You can import a database schema and add it to source control – so you get linking to work items as well as branching and labelling etc. Furthermore, you can use TeamBuild to build a .schema file, and then use vsdbcmd.exe to deploy schema changes (incrementally) to databases. You can also create unit tests and data generation plans.\n\nImporting the schema and building the schema in TeamBuild is straightforward. However, if you want to add running data generation and unit tests in your build, you have to jump through a few hoops.\n\nLet’s go through an example.\n\nFirst, we’ll import the schema into a database project using the SQL 2008 Database Wizard project template.\n\n\n\n\nNext, right click the Data Generation Plans folder and add a new Data Generation plan. The details are not important for this example, so just accept the defaults. Press F5 to generate data to the database.\n\n\n\n\nNow go to the schema view and select a stored proc. Right click and select “Create Unit Tests”.\n\n\n\n\nCreate a new test project and fill in some tests. Again, the exact details here are not critical – just make sure you’ve got some tests that pass when you run them from Visual Studio.\n\n\n\n\nNext you’ll have to create a build. Check you solution into source control. Then right click the builds node of the project and create a new build definition. Choose the workspace and the drop location, and leave everything else defaulted. This generates a standard build that will compile the database project and then run the unit tests.\n\nUnfortunately, this build will only partially succeed – the unit tests will fail. This is due to the fact that the paths to the .dbproj and .dgen files are different during a TeamBuild than when we build in VS. So we have to modify the configs.\n\n\n\n\nRight click the app.config  of the Test Project. Copy and paste it so that you have a copy of the config. Rename this to buildserver.dbunittest.config where buildserver is the name of your build server (this could also be the name of the user running the build). The .dbunittest is important since TeamBuild looks for *.dbunittest.config if we override the default config.\n\nNow remove all the tags except the  and its contents. Make sure that the very 1st character of the file is the &lt;, else TeamBuild won’t read this config correctly. Add “..\\Sources\\” into the paths to the .dbproj file and .dgen files (just after the ....).\n\n&lt;DatabaseUnitTesting&gt;\n    &lt;DatabaseDeployment DatabaseProjectFileName=”......\\Sources\\TailspinToys\\TailspinToys.dbproj”\n        Configuration=”Debug” /&gt;\n    &lt;DataGeneration DataGenerationFileName=”......\\Sources\\TailspinToys\\Data Generation Plans\\UnitTest.dgen”\n        ClearDatabase=”true” /&gt;\n    &lt;ExecutionContext Provider=”System.Data.SqlClient” ConnectionString=”Data Source=TRAIN-TFS;Initial Catalog=TailspinToys;Integrated Security=True;Pooling=False”\n        CommandTimeout=”30” /&gt;\n    &lt;PrivilegedContext Provider=”System.Data.SqlClient” ConnectionString=”Data Source=TRAIN-TFS;Initial Catalog=TailspinToys;Integrated Security=True;Pooling=False”\n        CommandTimeout=”30” /&gt;\nDatabaseUnitTesting&gt;\n\nOpen up the app.config in the Test project. Add AllowConfigurationOverride=”true” to the  tag. This prompts TeamBuild to look for the config we created in the previous step.\n\n&lt;DatabaseUnitTesting AllowConfigurationOverride=”true”&gt;\n\n\n\n\nFinally we need to create a new test settings file. Right click the Solution Items folder and add a new item – select the Test Settings item. You can name this whatever you want to and set whatever you want for your settings. The important bit here is to go to the “Deployment” tag and check the “Enable Deployment”. Then click the “Add” button and browse to the buildserver.dbunittest.config file (you’ll have to change the file-type drop-down to *.*). Check in your solution.\n\n\n\n\nCheck your solution into source control.\n\nFinally we’ll modify the build. Edit the build definition that you created before. In the Process tab, click the ellipses after the Automated Test Settings to get the test settings dialogue. Set the Test Settings File to be the test settings we just created. If you can’t see the file, it may be that you haven’t checked it into source control – this browser browses source control, not your local drive.\n\n\n\n\nNow queue a build and check that your tests are passing!\n\n\n\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/running-db-data-generation-and-unit-tests-in-teambuild-2010/"
    },{
      
      "title": "Running Data-Driven Unit Tests with SpreadSheets on Sharepoint",
      "date": "2010-11-09 22:38:00 +0000",
      
      "content": "Data-driven unit tests are a great way to run lots of tests. You define a test harness (or method) and tie it to a spreadsheet in an Excel file to “drive” the test – each row in the spreadsheet is one iteration of the test case. I am not going to go into any more detail here – there are plenty of examples about how to set these up. Another advantage of using Excel for data-driven tests is that testers love Excel. They can open and edit the spreadsheet and add / edit / remove test cases at will.\n\n\n  \n    \n      When a developer creates a test method, they can specify the spreadsheet to connect to using the [DataSource] attribute. Usually, you’d have a spreadsheet in your solution and set it to deploy in the test settings file (or by marking its properties to copy to output on a build) and then use the\n      DataDirectory\n      placeholder in the [DataSource] attribute to tell the test to look in the build output folder for the Excel file to open.\n    \n  \n\n\nThere are some drawbacks to doing this if your test team is maintaining the spreadsheets. They’ll need to have access to the source folder (out of Visual Studio) in order to change the spreadsheet. This is hardly ideal. A much better approach would be to set up a document library on a fileshare. Going one step better, set up a document library on Sharepoint – that way you get version history on the test data spreadsheets.\n\nSo if you decide to put the spreadsheets in a Sharepoint document library, you’ll have to decide on which site you want to store your docs on and then jump through a few hoops to hook up the tests to the Sharepoint documents. It makes sense to use the Team Project Portal for these sorts of documents, but any Sharepoint site will do.\n\nOnce you’ve uploaded your Excel files to a document library (don’t forget to enable Versioning if you want to track changes to these files), you have to do 2 things to get the test methods to work.\n\nFirstly, you’ll have to map a network drive to the document library in the [ClassInitialize] method of your test class. This maps the document library of sharepoint to a local drive letter. You need to do this programmatically since the mapping won’t persist after you log out. Make sure that the account that is running the build controller has at least read access to the document library! I downloaded (and tweaked) this project from CodeProject to get some code for mapping network drives. I then created a method that map the drive:\n\n\npublic static void MapNetworkDrive(string localDrive, string\n\n\n serverDrive){    var drive = new NetworkDrive\n\n\n();    drive.PromptForCredentials = false\n\n\n;    drive.ShareName = serverDrive;    drive.LocalDrive = localDrive;    drive.Force = true;    drive.MapDrive();}\n\n\nI’m telling the library no to prompt for credentials and setting Force to true will unmap if the mapping exists already. You can then call this method from the [ClassInitialize] method:\n\n&lt;font face=\"Consolas\"&gt;&lt;font size=\"2\"&gt;&lt;font color=\"#000000\"&gt;[&lt;/font&gt;&lt;span&gt;&lt;font color=\"#2b91af\"&gt;ClassInitialize&lt;/font&gt;&lt;/span&gt;&lt;/font&gt;&lt;/font&gt;&lt;font face=\"Consolas\"&gt;&lt;font size=\"2\"&gt;&lt;font color=\"#000000\"&gt;]&lt;br&gt;&lt;/font&gt;&lt;span&gt;&lt;font color=\"#0000ff\"&gt;public&lt;/font&gt;&lt;/span&gt;&lt;font color=\"#000000\"&gt;&amp;nbsp;&lt;/font&gt;&lt;span&gt;&lt;font color=\"#0000ff\"&gt;static&lt;/font&gt;&lt;/span&gt;&lt;font color=\"#000000\"&gt;&amp;nbsp;&lt;/font&gt;&lt;span&gt;&lt;font color=\"#0000ff\"&gt;void&lt;/font&gt;&lt;/span&gt;&lt;font color=\"#000000\"&gt; Setup(&lt;/font&gt;&lt;span&gt;&lt;font color=\"#2b91af\"&gt;TestContext&lt;/font&gt;&lt;/span&gt;&lt;/font&gt;&lt;/font&gt;&lt;font face=\"Consolas\"&gt;&lt;font size=\"2\"&gt;&lt;font color=\"#000000\"&gt; context)&lt;br&gt;{&lt;br&gt;&lt;/font&gt;&lt;span&gt;&lt;font color=\"#2b91af\"&gt; NetworkDrive&lt;/font&gt;&lt;/span&gt;&lt;font color=\"#000000\"&gt;.MapNetworkDrive(&lt;/font&gt;&lt;span&gt;&lt;font color=\"#a31515\"&gt;@\"z:\\\"&lt;/font&gt;&lt;/span&gt;&lt;font color=\"#000000\"&gt;, &lt;/font&gt;&lt;span&gt;&lt;font color=\"#a31515\"&gt;@\"\\\\server\\sites\\DefaultCollection\\Project\\TestMatrices\"&lt;/font&gt;&lt;/span&gt;&lt;/font&gt;&lt;font color=\"#000000\" size=\"1\"&gt;&lt;font size=\"2\"&gt;);&lt;br&gt;}&lt;/font&gt;\t&lt;/font&gt;&lt;/font&gt;\n\n\nHere I am mapping the z: to the root of the test document library on the Sharepoint portal for my Project. Once this is done, the [DataSource] attribute can just reference any spreadsheet from within this folder:\n\n&lt;font face=\"Consolas\"&gt;&lt;font size=\"2\"&gt;&lt;font color=\"#000000\"&gt;[&lt;/font&gt;&lt;span&gt;&lt;font color=\"#2b91af\"&gt;TestMethod&lt;/font&gt;&lt;/span&gt;&lt;/font&gt;&lt;/font&gt;&lt;font face=\"Consolas\"&gt;&lt;font size=\"2\"&gt;&lt;font color=\"#000000\"&gt;()]&lt;br&gt;[&lt;/font&gt;&lt;span&gt;&lt;font color=\"#2b91af\"&gt;Description&lt;/font&gt;&lt;/span&gt;&lt;font color=\"#000000\"&gt;(&lt;/font&gt;&lt;span&gt;&lt;font color=\"#a31515\"&gt;\"Testing database component to add error information\"&lt;/font&gt;&lt;/span&gt;&lt;font color=\"#000000\"&gt;)]&lt;/font&gt;&lt;/font&gt;&lt;/font&gt;&lt;font face=\"Consolas\"&gt;&lt;font color=\"#000000\"&gt;&lt;br&gt;&lt;font size=\"2\"&gt;[&lt;/font&gt;&lt;/font&gt;&lt;font size=\"2\"&gt;&lt;span&gt;&lt;font color=\"#2b91af\"&gt;DataSource&lt;/font&gt;&lt;/span&gt;&lt;font color=\"#000000\"&gt;(&lt;/font&gt;&lt;span&gt;&lt;font color=\"#a31515\"&gt;\"System.Data.Odbc\"&lt;/font&gt;&lt;/span&gt;&lt;font color=\"#000000\"&gt;, &lt;/font&gt;&lt;span&gt;&lt;font color=\"#a31515\"&gt;@\"Dsn=Excel Files;dbq=z:\\Error\\ULP_Error_ErrorManager_Add.xlsx;defaultdir=.;driverid=1046;maxbuffersize=2048;pagetimeout=5\"&lt;/font&gt;&lt;/span&gt;&lt;font color=\"#000000\"&gt;, &lt;/font&gt;&lt;span&gt;&lt;font color=\"#a31515\"&gt;\"ErrorAdd$\"&lt;/font&gt;&lt;/span&gt;&lt;font color=\"#000000\"&gt;, &lt;/font&gt;&lt;span&gt;&lt;font color=\"#2b91af\"&gt;DataAccessMethod&lt;/font&gt;&lt;/span&gt;&lt;/font&gt;&lt;/font&gt;&lt;font face=\"Consolas\"&gt;&lt;font size=\"2\"&gt;&lt;font color=\"#000000\"&gt;.Sequential)]&lt;br&gt;&lt;/font&gt;&lt;span&gt;&lt;font color=\"#0000ff\"&gt;public&lt;/font&gt;&lt;/span&gt;&lt;font color=\"#000000\"&gt;&amp;nbsp;&lt;/font&gt;&lt;span&gt;&lt;font color=\"#0000ff\"&gt;void&lt;/font&gt;&lt;/span&gt;&lt;/font&gt;&lt;font color=\"#000000\" size=\"2\"&gt; AddErrorTest()&lt;br&gt;{&lt;/font&gt;&lt;/font&gt;\n\n&lt;font color=\"#000000\" size=\"2\" face=\"Consolas\"&gt; ...&lt;/font&gt;\n\n&lt;font color=\"#000000\" size=\"2\" face=\"Consolas\"&gt;}&lt;/font&gt;\n\n&lt;font face=\"Arial\"&gt;There’s one other gotcha that you might see – the test run on the build fails saying that the data source could not be opened. In this case, log into your build server as your build service (tfsbuild or whatever identity you’re using) and open any Excel file – make sure that you are not being prompted for your name and initials (a popup that appears the very first time you use Excel). This will break the build until you’ve dismissed this dialogue once.&lt;/font&gt;\n\n&lt;font face=\"Arial\"&gt;A last aside: when trying to set up the document library for the spreadsheets, we could not get the Explorer View working in Sharepoint – we just got a “Web page could not be opened”. After using Fiddler, I could see an HTTP 405 error. Googling a bit, I found that in order for Sharepoint’s own WebDAV to work (that’s what the Sharepoint Explorer View uses), you need to uninstall IIS’s WebDAV feature! Go figure…&lt;/font&gt;\n\n&lt;font face=\"Arial\"&gt;Happy testing!&lt;/font&gt;\n\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/running-data-driven-unit-tests-with-spreadsheets-on-sharepoint/"
    },{
      
      "title": "ClickOnce Versioning and Deployment in a TeamBuild",
      "date": "2011-02-06 23:49:00 +0000",
      
      "content": "TeamBuild in TFS 2010 is an incredibly powerful engine – it’s based on Windows Workflow 4 (WF4). Once you get over the initial learning curve of WF4, you can get your team builds to do some impressive stuff.\n\nOne common scenario is supporting a ClickOnce deployment from a build. Essentially, you need to add an MSBuild activity into your build, tweaking the command line parameters that are passed to MSBuild. That’s not too difficult in and of itself – but I wanted to go further and tie the ClickOnce version into the build version.\n\nTo do that, I used a custom build activity from Jim Lamb’s post on versioning assemblies. However, I had to tweak the “UpdateVersionInfo” activity to accept 2 regex patterns instead of Jim’s 1. Jim uses the regex pattern to extract version info from the build number (so if your build is ‘MyBuild_4.0.0.2’ he extracts 4.0.0.2). He uses the same pattern to find/replace in the AssemblyInfo.* files (or whatever files you specify). Usually this is fine.\n\nCreating the Custom Assembly\n\nHowever, the assembly version in the project file for a ClickOnce app can have a different format – if you select “increment revision whenever I deploy” in the Publish properties of a ClickOnce project, the version places a * in the revision number – making the version 1.0.0.* (or if you look in the project file, 1.0.0.%2a). So I needed to separate the “source” regex pattern (the pattern used to extract the version from the build number) and the “target” regex – the pattern used for the search/replace in the target files.\n\nI opened the UpdateVersionInfo.xaml and added a new argument (called “SourceRegeEx”). I then changed the “Extract Version Info” assignment activity to use SourceRegex instead of Regex.\n\n\n\n\nIn the custom build workflow (make sure you follow Jim’s method of copying and then branching your custom workflow) I used two UpdateVersionInfo activities – one to update the version info in the AssemblyInfo.* files and one to update the ClickOnce vcproj file (this is where the ClickOnce version resides).\n\nCustomizing the Default Build Template\n\nI stared from the DefaultTemplate.xaml added a number of arguments (I also updated the metadata to make the arguments prettier when you create builds):\n\n\n  VersioningFileSpec – the file search pattern to update assembly versions (AssemblyInfo.* by default)\n  VersioningRegularExpression – the regex pattern to use to update assembly versions (“\\d+.\\d+.\\d+.\\d+” by default)\n  ClickOnceDeployIfTestsFail – true to deploy even if tests fail, false otherwise\n  ClickOncePublishDir – the UNC share to publish to\n  ClickOncePublishURL – the ClickOnce URL (can be the same as the PublishDir)\n  ClickOnceCertThumbprint – the thumbprint of the certificate for signing manifests (note: this certificate needs to be installed on any build agent machines – you can copy the pfx file (if you made a temporary certificate in VS) to the build machine and import it into the store by double clicking it – then use mmc to view the certificate and get its SHA1 for this field when you create a build)\n  ClickOnceSolutionPath – the source control path of the solution that contains the ClickOnce project\n  ClickOnceVersionFileSpec – the file search pattern to update ClickOnce version – usually the name of the csproj file that you want to publish – e.g. ClickOnceProject.csproj)\n  ClickOnceVersionRegEx – the regex target pattern for doing ClickOnce version replacements (usually either “\\d+.\\d+.\\d+.%2a” or “\\d+.\\d+.\\d+.\\d+”)\n\n\nI then added two UpdateVersionInfo activities – one right after the “GetWorkspace Activity” in the Process-&gt;Overall Build Process-&gt;Run On Agent-&gt;Initialize Workspace sequence. I used the VersioningFileSpec and RegularExpression arguments and hardcoded the source regex to \\d+.\\d+.\\d+.\\d+”.\n\n\n\n\nThe next step was the ClickOnce versioning customizations. I located the step “If TestStatus = Unknown” in the “Try Compile and Test” sequence. Right after that activity I added a sequence activity called “ClickOnce Deployment”.\n\n\n\n\nHere is the sequence and explanation:\n\n\n\n\n  Add an “If” activity\n  Condition: ClickOnceDeployIfTestsFail Or BuildDetail.TestStatus = BuildPhaseStatus.Succeeded\n  This will ensure that the “deploy even if failed” condition is obeyed\n  Add a “WriteBuildMessage” to the Else side of the If Activity to just log that tests failed and the deployment will be skipped\n  Add a sequence to the “Then” side of the If\n  Add an UpdateVersionInfo Activity (this will apply the build version to the ClickOnce deployment) and set the following:\n  SourcesDirectory = SourcesDirectory\n  FileSpec = ClickOnceVersionFileSpec\n  SourceRegEx = “\\d+.\\d+.\\d+.\\d+”\n  RegularExpression = ClickOnceVersionRegEx\n  Add a ConvertWorkspaceItem (to convert the source path of the click once solution to a local path) and set:\n  Direction = ServerToLocal\n  Input = ClickOnceSolutionPath\n  Result = localSolution (you’ll need to add this as a string variable on the sequence)\n  Workspace = Workspace\n  Add an MSBuild Activity to publish\n  The only argument worthy of note here is the CommandLineArguments which should be set to:String.Format(“/p:SkipInvalidConfigurations=true {0} /Target:Publish /property:PublishDir=”“{1}”” /property:PublishUrl=”“{2}”” /property:InstallUrl=”“{2}”” /property:SignManifests=true /property:ManifestCertificateThumbprint=”“{3}”””, MSBuildArguments, ClickOncePublishDir, ClickOncePublishURL, ClickOnceCertThumbprint)\n  Add a WriteBuildMessage to say that the deployment was executed\n\n\nNow you need to compile the custom activity project and put the dll into source control for your controller to find (again refer to Jim’s blog post).\n\nFinally, you can set up a build:\n\n\n\n\nNOTE: If you have different configurations (for Production and Staging for example) use Vishal Joshi’s blog to enable config transforms on your app.config file – then target the config you want in the build.\n\nDownload the project (including the DefaultClickOnce.xaml template – see the test project’s Templates folder) from my skydrive.\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/clickonce-versioning-and-deployment-in-a-teambuild/"
    },{
      
      "title": "Using Indexed Published Symbols from TeamBuilds",
      "date": "2011-02-08 15:05:00 +0000",
      
      "content": "You’ve seen the indexing and symbol publishing options when you set up a Team Build – but have you ever tried to debug and application that has symbols? How do you get VS to use the published symbols? What are these settings for anyway?\nSource indexing and published symbols make possible to debug an application without shipping symbols. Source indexing produces pdb files during a build – essentially mapping the binary to the source code. During a team build, extra information is wrapped inside the pdbs – like the Team Foundation Server URL where they were built and version control paths to and versions of source code. This allows VS to download source code for debugging – and since there’s security involved (to access the TFS server) – source server support is disabled for debugging by default.\nFortunately enabling it is quite easy – especially if you read your Kindle copy of Inside the MS Build Engine (2nd edition) on the plane!\nOpen VS and go to Tools-&gt;Options-&gt;Debugging-&gt;General settings and tick the “Enable source server support”.\n\n\n\n\nThe DefaultTemplate.xaml for default builds supports symbol publishing. When you set up a Team Build, expand the “Basic Section”, set “Index Sources” to True and enter a UNC for the “Path to Publish Symbols” – try to stick to one global symbol store – TeamBuild will organise the symbols within this folder. The only time you’d really want multiple stores is for concurrency – only one build can publish at a time (that’s the way the Default Template is designed), so if you have lots of build going all the time and the indexing is slowing them, then you may want to add another store or two.\n\n\n\n\nThe final step is to tell VS where to find the published symbols. To add a symbol store in VS, go to Tools-&gt;Options-&gt;Debugging-&gt;Symbols and add the UNC to the store.\n\n\n\n\nHappy debugging!\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/using-indexed-published-symbols-from-teambuilds/"
    },{
      
      "title": "Using Data Generation Plans to create repeatable data for code unit tests",
      "date": "2011-02-13 03:25:00 +0000",
      
      "content": "The Database Professional tools in VS 2010 allow you to create a project that encapsulates the schema of a SQL database. You can also use this project to create unit tests for stored procs or functions – and create test data for these database unit tests.\n\nHowever, what if just want repeatable data for normal code unit tests as part of a TeamBuild? I was recently at a client where this scenario came up. I tried to get the data generation plan to run in the [AssemblyInitialize] method of the unit test project, but just got the error “Could not deploy database” or “Could not run data generation plan” – not very helpful.\n\nSo we figured out a workaround – essentially we use MSBuild to run the data generation plan in the solution just before unit testing.\n\nHere are the high-level steps you need to follow:\n\n\n  Create a DBPro project with a data generation plan\n  Create an MSBuild project file that can run the data generation plan\n  Customize your build template\n  Add 2 arguments – RunDataGeneration and PathToMSBuildProjFile\n  Add a couple of build activities to invoke MSBuild to run the data generation plan\n\n\nThe MSBuild Project File\n\nOnce you’ve created the database project (I won’t cover it in this post, but there are plenty of blogs and articles on the web about how to do this), create a data generation plan for your test data. Now create a new xml file in the root of your database project – the one I created is called datagen.proj. Here’s the file contents:\n\n&lt;span style=\"color: blue\"&gt;&amp;lt;&lt;/span&gt;&lt;span style=\"color: #a31515\"&gt;Project &lt;/span&gt;&lt;span style=\"color: red\"&gt;DefaultTargets&lt;/span&gt;&lt;span style=\"color: blue\"&gt;=&lt;/span&gt;\"&lt;span style=\"color: blue\"&gt;DataGen&lt;/span&gt;\" &lt;span style=\"color: red\"&gt;xmlns&lt;/span&gt;&lt;span style=\"color: blue\"&gt;=&lt;/span&gt;\"&lt;span style=\"color: blue\"&gt;&lt;a href=\"http://schemas.microsoft.com/developer/msbuild/2003\"&gt;\"&amp;gt;http://schemas.microsoft.com/developer/msbuild/2003&lt;/a&gt;&lt;/span&gt;&lt;a href=\"http://schemas.microsoft.com/developer/msbuild/2003\"&gt;\"&lt;span style=\"color: blue\"&gt;&amp;gt;&lt;br&gt;&lt;/span&gt;&lt;/a&gt;&lt;br&gt;&amp;lt;&lt;span style=\"color: #a31515\"&gt;Import &lt;/span&gt;&lt;span style=\"color: red\"&gt;Project&lt;/span&gt;&lt;span style=\"color: blue\"&gt;=&lt;/span&gt;\"&lt;span style=\"color: blue\"&gt;$(MSBuildExtensionsPath)\\Microsoft\\VisualStudio\\v10.0\\TeamData\\Microsoft.Data.Schema.Common.targets&lt;/span&gt;\" &lt;span style=\"color: blue\"&gt;/&amp;gt; &lt;br&gt;&lt;br&gt;&amp;lt;&lt;/span&gt;&lt;span style=\"color: #a31515\"&gt;Target &lt;/span&gt;&lt;span style=\"color: red\"&gt;Name&lt;/span&gt;&lt;span style=\"color: blue\"&gt;=&lt;/span&gt;\"&lt;span style=\"color: blue\"&gt;DataGen&lt;/span&gt;\"&lt;span style=\"color: blue\"&gt;&amp;gt;&lt;br&gt;&lt;br&gt;&amp;lt;&lt;/span&gt;&lt;span style=\"color: #a31515\"&gt;DataGeneratorTask&lt;br&gt;&lt;br&gt;&lt;/span&gt;&lt;span style=\"color: red\"&gt;ConnectionString&lt;/span&gt;&lt;span style=\"color: blue\"&gt;=&lt;/span&gt;\"&lt;span style=\"color: blue\"&gt;Data Source=dataserver;Initial Catalog=AutomatedTesting;Integrated Security=True;Pooling=False&lt;/span&gt;\"&lt;br&gt;&lt;br&gt;&lt;span style=\"color: red\"&gt;SourceFile&lt;/span&gt;&lt;span style=\"color: blue\"&gt;=&lt;/span&gt;\"&lt;span style=\"color: blue\"&gt;.\\Data Generation Plans\\UnitTestPlan.dgen&lt;/span&gt;\"&lt;br&gt;&lt;br&gt;&lt;span style=\"color: red\"&gt;PurgeTablesBeforePopulate&lt;/span&gt;&lt;span style=\"color: blue\"&gt;=&lt;/span&gt;\"&lt;span style=\"color: blue\"&gt;True&lt;/span&gt;\"&lt;span style=\"color: blue\"&gt;/&amp;gt;&lt;br&gt;&lt;br&gt;&lt;!--&lt;/span--&gt;&lt;span style=\"color: #a31515\"&gt;Target&lt;/span&gt;&lt;span style=\"color: blue\"&gt;&amp;gt;&lt;br&gt;&lt;br&gt;&lt;!--&lt;/span--&gt;&lt;span style=\"color: #a31515\"&gt;Project&lt;/span&gt;&lt;span style=\"color: blue\"&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;\n\n\nOf course, you’ll need to change the connection string appropriately and set the “PurgeTablesBeforePopulate” to false if you don’t want to blow away the existing data – though since the data generation is creating test data, you should blow away existing data anyway. Set the sourcefile to the dgen file – the path is relative to the root of the database project.\n\nYou can test the project file by opening a VS command console, navigating to the folder containing the proj file and typing “msbuild datagen.proj”. Make sure this step succeeds before you continue.\n\nCustomizing the Build Workflow\n\nI added two arguments to the workflow – a Boolean called “RunDataGeneration” and a string called DataGenMSBuildProjSourcePath. I also added metadata to “prettify” the arguments when builds are created.\n\nNow it’s time to customize the build workflow to create the test data before unit testing. I used the DefaultTemplate.xaml and navigated into the “Try, Compile and Test” sequence – then went deeper and found the “If Not TestSpecs = Emtpy” activity. A little further, just before the tasks that actually run the tests, I inserted an “If” activity (setting the condition to “RunDataGeneration = True”). The “else” branch is empty and I show the “Then” branch below:\n\n\n\n\nThere are just two activities – a ConvertWorkSpace and an MSBuild activity. The ConvertWorkspace task is set to ServerToLocal and converts the DataGenMSBuildProjSourcePath to a local path (set the workspace parameter to “Workspace”). You’ll need to create a local variable scoped to the sequence called “localProjectPath” for the out parameter.\n\nNext, set the MSBuild task’s project to localProjectPath. The only other thing that needs mentioning here is that you MUST set the ToolPlatform of the MSBuild activity to X86 – the data generation seems to only work if called from the x86 msbuild and not from the x64 one.\n\nCheck in your template and create the build – make sure you set “RunDataGeneration” to true and set the DataGenMSBuildProjSourcePath to the full source control path to the proj file (starting with $/…).\n\nHappy testing!\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/using-data-generation-plans-to-create-repeatable-data-for-code-unit-tests/"
    },{
      
      "title": "Build-Deploy-Test Workflow for Physical Environments",
      "date": "2011-02-19 21:12:00 +0000",
      
      "content": "This week, Darshan Desai published a post about a build-deploy-test workflow for Physical environments. The solution is entirely XAML based – you don’t need any custom assemblies. However, the design-time experience is not as rich as the wizard that you get when you do a Lab workflow for build-deploy-test using the LabDefault.xaml template.\n\nI’ve been working on building a workflow that includes a wizard similar to that of the Lab workflow. Seeing Darshan’s solution allowed me to iron out a few kinks in my solution, as well as overcome some of his solution’s limitations. Obviously, since this is a scenario for a Physical environment, there is no ability to do snapshots or restores - you’ll have to make sure you have some clean up scripts to run pre-deployment to get to a “clean-ish” state.\n\nNotions Physical Build-Deploy-Test Solution\n\nSetting up the environment is the same for my solution as it is for Darshan – you need to install and configure both a Build agent (or workflow agent, as it’s called in the Lab scenario) as well as a Test agent. Both agents need to be connected to controllers in your TFS environment. The name of the Build agent is important, since this is the way that you configure where deployment scripts are run.\n\nIf you’re going to test, you need to create a test plan with a test suite that contains tests that have automation associate with them. Also, you’ll need a “regular” build that can compile (and optionally unit test) the code as well as the dll’s that contain the automated tests (call this the “Source build”). Then you’ll need to create some automated test settings for your test plan. This test setup is exactly the same setup you’d need if you’re using the Lab workflow or Darshan’s workflow.\n\nTo use our workflow, you need the PhysicalDefaultWorkflow in source control somewhere, as well as a custom assembly. You’re controller needs to be configured to point to the folder in source control that contains this custom assembly. In contrast, Darshan’s workflow doesn’t require a custom assembly.\nHere’s a walkthrough of what my workflow looks like once you’ve configured the physical environment, the Source build and the test plan.\n\n\n  \n    Create a new Build Definition and set the workspace, drop location, trigger and retention policy just as you would for any other build.\n  \n  \n    Change the Build Process Template on the Process tab to the PhysicalDefaultTemplate.xaml.\n  \n  \n    You’ll see the familiar “Click here to edit details…” for the Workflow Process Details argument. Clicking the button with the ellipsis will launch the Physical Workflow Parameters wizard. You’ll see a welcome screen – click next to start configuring the build.\n  \n  \n    On the “Select Environment” screen, select the Physical environment that you want to deploy to\n  \n\n\n\n\n\n\n  On the “Configure Build” screen, configure the Source build. Here I overcome some of Darshan’s build’s limitations – you can choose a custom drop location, queue a new build or select the latest available build for your build definition.\n\n\n\n\n\n\n  \n    The next screen is “Deployment Scripts”. Here you can configure scripts for the deployment. This screen is slightly different from the same screen in the Lab Workflow. Instead of “Role” for the deployment script, you need to target the Build (Workflow) agent to run the script on (you may see agents that are not part of the environment in this list, so make sure you select the correct agents). You can use $(BuildLocation) as a parameter for the build drop folder of the Source build. I’ve also created a Machine_ parameter that’s similar to the Computer_ (and InternalComputer_) parameters of the Lab workflow. You use $(Machine_AgentName) as a variable for the physical machine name that the agent with name AgentName resides on. For example, if you have an agent called “MyAgent” on a machine called “MyMachine”, then you can use $(Machine_MyAgent) as the variable and when the build runs, this variable will be expanded to “MyMachine”.\n  \n  \n    You’ll need to configure an account to perform the deployment under. In the Lab Workflow, this is the account that the Lab Agent is configured with. This can be any account as long as it has permissions to execute the script and to the Source build drop folder. Warning: the password is not stored securely!\n  \n\n\n\n\n\n\n  Finally, configure testing on the “Configure Testing” screen. This screen is exactly the same as the “Configure Testing” screen in the Lab Workflow.\n\n\n\n\n\nNow you can run your build! Here’s the output of one of my builds:\n\n\n\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/build-deploy-test-workflow-for-physical-environments/"
    },{
      
      "title": "Default Team Build to Invoke Instead of Build",
      "date": "2011-03-10 05:37:00 +0000",
      
      "content": "Sometimes you need to do a “deployment” that doesn’t involve a build of source code – for example, I have been working at a customer in gorgeous Durban and they’ve got some Dynamix AX scripts that they need to source control – no problem for TFS. The “deployment” involves simply copying the entire source folder to a secure share.\n\nSo I sat down with Dave Pike, one of their senior developers / architects and we customized the default team build template. Essentially, we ripped out the MSBuild task that compiles, the testing tasks and some of the arguments that you specify on a default build. We still wanted the build to create the workspace, check out some folder (or folders), label source for the build and associate changesets and work items. Where the MSBuild task used to be, we wanted to invoke a script.\n\nFinally, we changed the “copy binaries to drop folder” task to copy the sources to the drop location – this gives you the “build output” in the drop location.\n\nSource Control Structure\n\nIn our case, we wanted to check out a folder from source control and invoke a command within that folder to do an xcopy. Here’s how we structured the folders:\n\n\n\n\nThe tree in source control that we set as the workspace for the build (and hence the root SourcesDirectory on the build agent when the build is running) was $/TeamProject/DEV/Scripts. Within that folder, we had some other folders and the Deploy.cmd. When setting up the build definition, we specified the “Source Control Path to Script” as $/TeamProject/DEV/Scripts/Deploy.cmd – the build was able to invoke the script from there. The script is executed using the Sources directory (the root of the workspace mapping on the build agent) so you can work in relative paths from there if you’re copying or manipulating files in the workspace. In this case the working folder would be the local mapping of $/TeamProject/DEV/Scripts.\n\nThe Deploy.cmd File\n\nYou’ll have to create a script (a Powershell script or a bat file or any other script you can invoke) that does the “deployment”. In our case, we wanted to xcopy the entire sources directory (the entire workspace that we set up when we create a build) to a UNC, so we just created a cmd file. We added this to the source control folder that held the scripts we wanted to deploy so that it would be checked out when the build runs. The source control path to this script is the only argument that you really need to specify when you set up one of these builds.\n\nFor example,\n\nHere’s a stub for a generic deploy.cmd file that reports errors back to the build (so that the whole build is failed if the script fails):\n\n&lt;font color=\"#0000ff\"&gt;@echo off&lt;br&gt; &lt;br&gt;&lt;font color=\"#ff0000\"&gt;rem =============&lt;br&gt;rem do stuff here&lt;br&gt;rem =============&lt;/font&gt;&lt;br&gt; &lt;br&gt;IF %ERRORLEVEL% NEQ 0 GOTO Err&lt;br&gt;GOTO End&lt;br&gt; &lt;br&gt;:Err&lt;br&gt;@echo An error occurred&lt;br&gt;exit /B 1&lt;br&gt; &lt;br&gt;:End&lt;br&gt;@echo Done!&lt;/font&gt;\n\n&lt;font face=\"Arial\"&gt;Obviously you’ll replace the actual “work” that you want this script to perform where the red “rems” are. That was where we put our xcopy.&lt;/font&gt;\n\n\nGotchas For Associating Changesets and Work Items\n\nWhen we had dropped in an Invoke build activity, we ran the build. The builds worked fine, but weren’t calculating the changesets or work items associated to this build. We opened the template again and for a while we were stumped – we had the AssociateChangeSetsAndWorkItems activity there and hadn’t messed with it at all – so why were we not seeing the associated changesets and work items?\n\nWe examined the build log of a few of the builds and realised that the AssociateChangeSetsAndWorkItems activity was logging a warning that said, “Cannot find label ‘’”. Reflecting the task revealed that the activity calculates that “last good label” for this build and then using dates, works out all the changesets and associated work items from the date of the last good build label to now and associates them with this current build. We could see that the logs were labelling the builds and we could see the labels in source control, but still the activity couldn’t seem to work out the “last good build label”. Then we realised that in the sequence of the Default Template where the MSBuild activity is executed, the compilation and test statuses are set. We’d ripped out that sequence when we added the Invoke, so we weren’t setting the compilation or test statuses at all. We simply added a SetBuildDetail activity just after our Invoke activity and set compilation, test and overall status on the BuildDetail to Successful – and voila, the builds were now able to work out the changesets and work items since the “last good build” and associate them with this build.\n\nI suppose that makes sense – TFS only sets the build as “good” if the compilation and test status are both successful – we set that after our Invoke (if the Invoke fails because of error in the script, the status is not set to Success and the build is not classified as “good”).\n\nThe Workflow\n\nHere’s the Workflow “summary”:\n\n\n\n\nThis is the detail of the parallel activity inside the “Try Invoke and Associate Changesets and Work Items:\n\n\n\n\nFinally, here’s the sequence that does the deployment:\n\n\n\n\nYou can get the template from my skydrive.\n\nHappy building!\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/default-team-build-to-invoke-instead-of-build/"
    },{
      
      "title": "TFS 2010 is Complete ALM",
      "date": "2011-03-18 21:36:00 +0000",
      
      "content": "\n\n(This slide shows the capabilities of TFS 2010 in 1 page. The orange blocks are TFS 2010 Features)\n\nSome products in the Application Lifecycle Management (ALM) space provide only slices of ALM capabilities. For example, Subversion (SVN) is a good source repository. Atlassian’s Jira provides issue tracking (and some other modules that bolt on). Other products (like Collabnet and the Jazz platform) are more comprehensive ALM offerings.\n\nI’m not going to dig too deeply into why I think TFS 2010 outshines all of these products, but I do want to focus on two main aspects that I think put TFS ahead: comprehensiveness and integration.\n\nTFS 2010 is Complete (or, “TFS, You Complete Me”)\n\nTFS isn’t just Source Control. It isn’t just Work Item Tracking. It isn’t just Lab Management, Automated Builds, Code Analysis, Automated UI Tests, Requirements Management and Project Management, Reporting and Dashboards, Performance Profiling and Load Testing, Manual Testing and Code Coverage. It is, in fact, all of these (and more).\n\nWhich leads to the second major benefit of using TFS 2010 for ALM:\n\nTFS 2010 is Integrated (or, “TFS, You’re One For All, And All in One”)\n\nUnlike many other “comprehensive” ALM products, TFS was designed to provide all the ALM functionality you could want from the ground up. It’s not just a hodge-podge of different pieces stitched together with fragile “bridges” and “connectors” (though TFS is extensible). That’s why you can link check-ins to work items and have builds report what changesets and what work items are included in a build. That’s why you can pull a report that shows your requirements with a breakdown of tasks used to implement the requirement (and a roll-up of completed and remaining work) as well as testing effort against that requirement as well as bugs against that requirement. That’s why you can get a tester to log a bug using just a title and have the system automatically attach video of the test session, historical debug information (IntelliTrace), event logs and other diagnostic data in seconds – your developers will never close another bug with “no repro” again. And so on, and so on.\n\n\n\n\n(This is a screen-shot of the “Stories Overview” report – this shows your requirements (stories) together with a roll up of development effort, test progress and bug status – all in one place)\n\nThis ability to tie work items and check-ins and builds and tests and bugs, almost effortlessly, is made possible by the fact that TFS 2010 provides all these features from a centralized repository. All source code, all work items, all test results, all reports live in a single place – meaning you can link almost anything to almost anything else – and it means you can analyse your entire ALM process from start to end (using the centralized data warehouse, of course).\n\nThis all means immense productivity gains for your development team. Add to that portals and wiki’s, ad-hoc reporting, event notifications, the Microsoft Test Manager tool for manual testing and Lab Management, integration into Visual Studio (and even other IDE’s like Eclipse via Team Explorer Everywhere) and a host of other great process tools, and you can concentrate on creating applications quickly, continually improving on quality and enhance your process along the way.\n",
      "categories": [],
      "tags": ["alm"],
      
      "collection": "posts",
      "url": "/tfs-2010-is-complete-alm/"
    },{
      
      "title": "Monitoring Check-in Policy Overrides",
      "date": "2011-03-28 16:06:00 +0000",
      
      "content": "I’m often asked by customers why there is no way to prevent check-in policy overrides in TFS. Usually I respond along the lines of, “Well, it should be a really rare occurrence (otherwise why have the policy?) and besides, every action against TFS is tracked, so you can monitor overrides and beat ‘chat nicely’ to the developer who is overriding the policies”. Which is all well and fine, but exactly how do you monitor policy overrides?\n\nThis weekend I checked Amazon to see if I could find Professional Team Foundation Server 2010 for my Kindle – and to my surprise, there it was! Ed Blankenship, one of the authors, is a colleague at Notion Solutions and is an absolute genius on all things TFS. The other authors are, of course, TFS legends too! I instantly purchased the book and while going through it, found a section in Chapter 7 that speaks about monitoring check-in policy overrides.\n\nThere are 2 main ways of doing so – via email alerts and via the warehouse.\n\nEmail Notification\n\nInstall the latest version of the Team Foundation Power Tools – anyone using TFS should always have this installed! One of the features that gets installed with these tools is a richer notifications manger – the Alerts Explorer.\n\nIn VS, go to Team-&gt;Alerts Explorer and add a “Check-In to specific folder with policy overriden” alert (under the Check-in alerts section).\n\n\n\n\nNow you’ll get an email whenever someone does something nefarious and overrides the check-in policy you’ve so meticulously planned and put in place…\n\nMonitoring via the Warehouse\n\nThe second method of monitoring check-in overrides is to query the warehouse. In the warehouse database (called Tfs_Warehouse by default) there is a table called\n\n\nDimChangeset\n\n\n. One of the columns on this table is\n\n\nPolicyOverrideComment\n\n\n, which is null whenever there is no override and which contains the policy override comment (which is mandatory when overriding) otherwise. So you can easily craft a query (and turn it into a report) that looks for check-ins with a\n\n\nPolicyOverrideComment\n\n\n. Here’s the T-SQL:\n\n&lt;span style=\"color: blue\"&gt;SELECT&lt;br&gt; &lt;/span&gt;&lt;span style=\"color: teal\"&gt;[ChangesetSK]&lt;/span&gt;&lt;span style=\"color: gray\"&gt;,&lt;br&gt; &lt;/span&gt;&lt;span style=\"color: teal\"&gt;[ChangesetID]&lt;/span&gt;&lt;span style=\"color: gray\"&gt;,&lt;br&gt; &lt;/span&gt;&lt;span style=\"color: teal\"&gt;[ChangesetTitle]&lt;/span&gt;&lt;span style=\"color: gray\"&gt;,&lt;br&gt; &lt;/span&gt;&lt;span style=\"color: teal\"&gt;[PolicyOverrideComment]&lt;/span&gt;&lt;span style=\"color: gray\"&gt;,&lt;br&gt; &lt;/span&gt;&lt;span style=\"color: teal\"&gt;[LastUpdatedDateTime]&lt;/span&gt;&lt;span style=\"color: gray\"&gt;,&lt;br&gt; &lt;/span&gt;&lt;span style=\"color: teal\"&gt;[TeamProjectCollectionSK]&lt;/span&gt;&lt;span style=\"color: gray\"&gt;,&lt;br&gt; &lt;/span&gt;&lt;span style=\"color: teal\"&gt;[CheckedInBySK]&lt;br&gt;&lt;/span&gt;&lt;span style=\"color: blue\"&gt;FROM&lt;br&gt; &lt;/span&gt;&lt;span style=\"color: teal\"&gt;[Tfs_Warehouse]&lt;/span&gt;&lt;span style=\"color: gray\"&gt;.&lt;/span&gt;&lt;span style=\"color: teal\"&gt;[dbo]&lt;/span&gt;&lt;span style=\"color: gray\"&gt;.&lt;/span&gt;&lt;span style=\"color: teal\"&gt;[DimChangeset]&lt;br&gt;&lt;/span&gt;&lt;span style=\"color: blue\"&gt;WHERE&lt;br&gt; &lt;/span&gt;&lt;span style=\"color: teal\"&gt;[PolicyOverrideComment] &lt;/span&gt;&lt;span style=\"color: gray\"&gt;IS NOT NULL&lt;br&gt;&lt;/span&gt;\n\n\nHappy monitoring!\n",
      "categories": [],
      "tags": ["alm"],
      
      "collection": "posts",
      "url": "/monitoring-check-in-policy-overrides/"
    },{
      
      "title": "Lab management: The Dreaded “Unknown Error: 0x8033811e”",
      "date": "2011-04-14 17:42:00 +0000",
      
      "content": "I have been setting up a TFS for demos and web training. Since TFS installation is now really easy, I got it up quickly and configured Lab Management. Everything was plain sailing until I tried to deploy a stored environment. The SCVMM job failed and provided this rather unhelpful message (where host.com is my host server):\n\nA Hardware Management error has occurred trying to contact server host.com (Unknown error 0x8033811e). Check that WinRM is installed and running on the server host.com.\n\nWell I did check WinRM – no problems there. So I started to search for other solutions – found one about ip listening ports and a whole bunch of other nothing. After sifting through some obscure sites, I managed to narrow the solution to one of two things:\n\n\n  BITS port conflicts\n  Delegation issues\n\n\nChanging the BITS Port\n\nBy default, BITS uses port 443 to transfer files – so if you have anything using SSL or firewalls blocking the port, you’ll have issues with BITS. I ignored this at first since the SCVMM job seemed to get past copying the VHD over BITS. Anyway, this turned out to be the solution to my problem. Here’s how you fix it:\n\n\n  Open the registry of the VMM server machine\n  Find HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Microsoft System Center Virtual Machine Manager Server\\Settings\n  Edit the BITSTcpPort to something other than 443 (I used 8050)\n  Restart the “Virtual Machine Manager” service\n\n\nFixing Delegation Issues\n\nThe other possibility is delegation issues. Follow these steps:\n\n\n  Go to the Domain Controller and open “Active Directory Users and Computers”\n  In the Computers node, find the host machine and open its properties\n  Go to the Delegation tab and check “Trust this computer for delegation to specified services only”\n  Select “Use any authentication protocol”\n  In the list of services, click “Add” and find the VMM server computer\n  When you click OK, a list of services is shown – find the “cifs” service, select it and click ok\n\n\nHappy labbing!\n",
      "categories": [],
      "tags": ["labmanagement"],
      
      "collection": "posts",
      "url": "/lab-management-the-dreaded-unknown-error-0x8033811e/"
    },{
      
      "title": "Automating Office Tasks in a TeamBuild",
      "date": "2011-04-29 21:01:00 +0000",
      
      "content": "I was working on a TeamBuild that doesn’t compile code – this build checks out a number of MS Word documents from source control, converts them to PDF and then uses PDFSharp to merge them all into one PDF document. I started with the DefaultTemplate.xaml and ripped out the compile / test tasks. I then created a Powershell script that would do the heavy lifting. So all the build does is check out the doc files in the workspace, invoke the powershell script and then copy the final file to the drop folder. Seems pretty simple, right?\n\nWell, in theory it was. I modified the workflow and created the Powershell script. I tested the Powershell script “manually” by calling it from a command prompt. I then got the workflow to call the script, expecting goodness. However, that’s where Simple ended and Frustration started – the script wouldn’t work when invoked from the workflow.\n\nI tried to do several things to figure out the problem. Here’s the snippet of the code that was not working (the line in red):\n\n\n$word = new-object -ComObject \"word.application\"    \n\n\n    $missing = [System.Reflection.Missing]::Value    # open read only    $doc = $word.documents.open($source, $missing, $true)    if ($doc -eq $null) {        Throw \"Could not open $source\"    }\n\n\nWhenever the script was invoked from within the build, the open() method to open the Word doc would return null and the script would Throw. For some reason, it couldn’t open the file. At first I thought it was an “interactive / non-interactive” problem, so I set the Visible property on the $word ComObject to false. No luck. I checked permissions. I checked the readonly attribute on the Word doc. Nothing helped.\n\nThe Solution – the SystemProfile Desktop directory\n\nI eventually stumbled onto some websites talking about a similar problem when doing Office automation from an ASP.NET site. They did all sorts of things with the identities and impersonation and so on, but that wouldn’t apply here since there’s no ASP.NET site.\n\nThere was one other piece of advice that turned out to solve the problem: the Desktop folder for the system profile. I was incredulous at first, but then decided what the heck and tried it.\n\nAll I had to do was create a folder called “Desktop” in the systemProfile folder (which is different for 64 bit and 32 bit machines). Once that folder existed, there was no more problem and the build worked like a charm. So I added in a sequence to check that the folder exists and to create it if it doesn’t. Here’s the activity:\n\n\n\n\nI added an If with the condition set to:\n\nDirectory.Exists(“C:\\Windows\\SysWOW64\\config”)\n\nIn the “Then”, I assign “C:\\Windows\\SysWOW64\\config\\systemProfile\\Desktop” to a variable called profileFolder, and in the Else I assign the profileFolder the value “C:\\Windows\\System32\\config\\systemProfile\\Desktop”.\n\nNext I added a “CreateDirectory” activity and pass in profileFolder as the directory to create.\n\nI placed this sequence before any activity that performs Office automation and voila, the build works.\n\nHappy building!\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/automating-office-tasks-in-a-teambuild/"
    },{
      
      "title": "Presenting @ DevDays SA 2011: Cape Town and Johannesburg",
      "date": "2011-05-10 22:28:00 +0000",
      
      "content": "I am going to be co-presenting with Ahmed Salijee (a Development Solution Specialist) from Microsoft South Africa at DevDays later this month. We’re going to be showing off a ton of TFS and VS 2010 capabilities around testing applications. Here’s the official blurb for our sessions:\n\nVisual Studio 2010 offers a wide range of software testing capabilities: manual testing, automated UI testing, database testing, low-level unit testing and even performance testing tools capable of simulating thousands of active users. It also provides support for test case management, defect tracking as well as configuring and running your tests in virtual and physical environments. But how do you know which tools to choose for your project? This session will, via a demo scenario, walk you through the various testing capabilities to assist you with the most effective use of Visual Studio 2010’s testing capabilities. This session will be useful to developers as well as various tester roles including functional, performance and automation.\n\nWe’re going to start off with a simple web application: IBuySpy. This is a reference application for an ASP.NET store. We’re going to do some refactoring and then showcase:\n\n\n  Unit Testing\n  Database Testing\n  Test Driven Development (TDD) using “Generate from Usage”\n  Moles and Pex for isolation and automated code coverage discovery\n  Functional Testing\n  Web and Load Testing\n  Coded UI Testing\n  Running Automated Tests during Builds\n  Lab Management Build-Deploy-Test Workflow for Automated Testing\n\n\nHope to see you there!\n",
      "categories": [],
      "tags": ["news"],
      
      "collection": "posts",
      "url": "/presenting-devdays-sa-2011-cape-town-and-johannesburg/"
    },{
      
      "title": "Running Load Tests in Lab Management: Perfmon Issues",
      "date": "2011-05-16 23:59:00 +0000",
      
      "content": "I am preparing for the demo at DevDays, and we wanted to run a Load Test using our Lab environment. However, when trying to access the performance counters on the lab machines, I got “Access Denied” errors. That lead to some searching on why this is the case.\n\nTo access performance counters on the lab machines, you have to have 2 things in place:\n\n\n  \n    The Remote Registry service must be running on each lab machine you want to monitor\n  \n  \n    The user that you’re running VS out of (or the identity of the Test Controller) needs to be an administrator on the lab machines.\n  \n\n\nRemote Registry Service\n\nThis one’s easy – open the services snap-in and start the service (set it to start automatically if you want to).\n\nPermissions\n\nThis one was a little harder. In my lab, I have the computers connecting to a workgroup, so I couldn’t just add notiontraining\\colind as an administrator (since the lab machines aren’t on the notiontraining domain, which is the domain I am running on). So I created a local user on each lab machine with the name “colind” and the same password as notiontraining\\colind. I then added the local colind user into the local admin group.\n\nNow I can fire up VS on my box (logged in as notiontraining\\colind) and can add the performance counters to each lab machine without any issues.\n\nHappy Load Testing!\n",
      "categories": [],
      "tags": ["labmanagement"],
      
      "collection": "posts",
      "url": "/running-load-tests-in-lab-management-perfmon-issues/"
    },{
      
      "title": "Running Lab Management with XP SP3 Machines",
      "date": "2011-06-16 20:24:00 +0000",
      
      "content": "If you’re running Lab Management with lab machines with XP SP3, you may run into a problem where the Test Agent is never ready for running tests.\n\n\n\n\nThe environment was starting up fine and configuring both network isolation as well as the workflow agent. However, the Test Capability was continually not ready – the event logs had a message like this:\n\nUnable to connect to the controller on ‘TFSMachine.Domain:6901’. The agent can connect to the controller but the controller cannot connect to the agent because of following reason: An error occurred while processing the request on the server: System.IO.IOException: The write operation failed, see inner exception. —&gt; System.ComponentModel.Win32Exception: The message or signature supplied for verification has been altered.\n\nScratching around led me to this rather helpful blog site: Troubleshooting Guide for Visual Studio Test Controller and Agent. The problem is mentioned there – it’s a problem with a Windows hotfix (KB968389) which you simply need to uninstall from the XP Lab machine. Reboot and you’re good to go.\n\nHappy Labbing!\n",
      "categories": [],
      "tags": ["labmanagement"],
      
      "collection": "posts",
      "url": "/running-lab-management-with-xp-sp3-machines/"
    },{
      
      "title": "Using Powershell to Replace Config Settings in Lab Management",
      "date": "2011-06-16 20:49:00 +0000",
      
      "content": "So you’ve got your virtual environment running and you’re deploying your application using the Lab Default Workflow. Only, you have a config file and you need to update a connection string or something in the config file for automated tests to run properly.\n\nNow if you’re deploying websites, for goodness sake use WebDeploy  and the Web.Config transforms within Visual Studio to configure your web.configs as part of the WebDeploy packaging. This is by far the easiest way to deploy web applications into lab environments.\n\nHowever, if you’ve got some other configs that need to be twiddled as part of the deployment, then create a PowerShell script to do your replacements.\n\nGiven an XML that looks like this:\n\n&lt;span style=\"color: blue\"&gt;&amp;lt;&lt;/span&gt;&lt;span style=\"color: #a31515\"&gt;configuration&lt;/span&gt;&lt;span style=\"color: blue\"&gt;&amp;gt;&lt;br&gt; &amp;lt;&lt;/span&gt;&lt;span style=\"color: #a31515\"&gt;settings &lt;/span&gt;&lt;span style=\"color: red\"&gt;LogTraceInfo&lt;/span&gt;&lt;span style=\"color: blue\"&gt;=&lt;/span&gt;\"&lt;span style=\"color: blue\"&gt;False&lt;/span&gt;\" &lt;span style=\"color: red\"&gt;KeepConnectionAlive&lt;/span&gt;&lt;span style=\"color: blue\"&gt;=&lt;/span&gt;\"&lt;span style=\"color: blue\"&gt;True&lt;/span&gt;\"&lt;span style=\"color: blue\"&gt;&amp;gt;&lt;br&gt; &amp;lt;&lt;/span&gt;&lt;span style=\"color: #a31515\"&gt;sql &lt;/span&gt;&lt;span style=\"color: red\"&gt;server&lt;/span&gt;&lt;span style=\"color: blue\"&gt;=&lt;/span&gt;\"&lt;span style=\"color: blue\"&gt;dbserver&lt;/span&gt;\" &lt;span style=\"color: red\"&gt;database&lt;/span&gt;&lt;span style=\"color: blue\"&gt;=&lt;/span&gt;\"&lt;span style=\"color: blue\"&gt;Live&lt;/span&gt;\" &lt;span style=\"color: blue\"&gt;/&amp;gt;&lt;br&gt; &lt;!--&lt;/span--&gt;&lt;span style=\"color: #a31515\"&gt;settings&lt;/span&gt;&lt;span style=\"color: blue\"&gt;&amp;gt;&lt;br&gt;&lt;!--&lt;/span--&gt;&lt;span style=\"color: #a31515\"&gt;configuration&lt;/span&gt;&lt;span style=\"color: blue\"&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;\n\n\nLet’s say you want to replace the database property. Here’s the PS to do this:\n\n######################################################################################\n#\nUsage: ReplaceDatabaseInConfig.ps1 -configPath “path to config” -database “new database name”\n#\n######################################################################################\nparam($configPath, $database)\n\nWrite-Host “Replacing database name in config file…”\n\nGet the content of the config file and cast it to XML\n$xml = xml\n$root = $xml.get_DocumentElement();\n\nchange the db name\n$root.settings.sql.database = $database\n\n#save\n$xml.Save($configPath)\n\nWrite-Host “Done!”\n\n&lt;span style=\"color: green\"&gt;###########################################################################&lt;/span&gt;\n\n\nCheck this script into Source Control – make sure that you get it into your drop folder somehow (add it to a solution and mark it’s action to “Copy Always” or something along those lines).\n\nThen it’s a simple matter of executing the script as part of your deployment. If you set the working directory to the folder where your deployment copies the script, then you can invoke it as follows:\n\n\ncmd /c powershell .\\ReplaceDatabaseInConfig.ps1 -configPath \"AppCustom.config\" -database \"Live\"\n\n\nIn your build log, you’ll see the following:\n\n\n\n\nHappy testing!\n",
      "categories": [],
      "tags": ["labmanagement"],
      
      "collection": "posts",
      "url": "/using-powershell-to-replace-config-settings-in-lab-management/"
    },{
      
      "title": "Lab Management: Use Database Backups During Deployment for Repeatable Automated Tests",
      "date": "2011-06-17 23:02:00 +0000",
      
      "content": "I was at a customer recently who have a small test database – around 120MB in size. I had encouraged them to use the DB Professional tools in VS 2010 to track their schema – that way they could deploy the database (as well as the latest schema) and some test data (through INSERT statements or even better, data generation plans). However, their developers didn’t have the time or skills (yet) to do this.\n\nSo we initially created the lab with a dependency on a database outside the Lab environment – an external database if you will. However, we were using Network Isolation and had numerous instances of the Lab environments – so contention on the external database was a risk.\n\n\n\n\nThe Lab with the External DB Dependency\n\nI suggested that we install SQLExpress within the Lab environment (since we didn’t have a database VM in the environment, we just installed it on the Client VM where the tests are running). We could have restored a backup of the external database into the Lab as part of the “Clean” snapshot (allowing the Lab to start with a known database), but this would mean any changes to the database schema or lookup data would require someone re-create the snapshot.\n\nSo we came up with another solution: we created a shared folder on the network and placed a backup of the database into this folder. Then in the deployment section of the Lab Workflow, we added a script to copy the backup to a folder on the Lab machine (in this case, c:\\data). The last step is to use sqlcmd to restore the database as follows (paths and dbnames depend on your environment, of course):\n\n\ncmd /c sqlcmd -S .\\SqlExpress -Q \"RESTORE DATABASE Demo FROM Disk='c:\\data\\Lab DB.bak' WITH MOVE 'DataLogicalName' TO 'c:\\data\\Lab DB.mdf', MOVE 'LogLogicalName' TO 'c:\\data\\Lab DB.ldf'\"\n\n\nwhere DataLogicalName and LogLogicalName are the logical names of the data and log files within the backup.\n\nThis approach allows you to “deploy” schema changes by refreshing the backup on the shared location. If you add a new table or something that the code requires, simply backup the new database to the shared location and you’re done.\n\n\n\n\nThe Lab at the “Clean” Snapshot\n\n\n\n\nThe Lab after deployment\n\nHappy testing!\n",
      "categories": [],
      "tags": ["labmanagement"],
      
      "collection": "posts",
      "url": "/lab-management-use-database-backups-during-deployment-for-repeatable-automated-tests/"
    },{
      
      "title": "Imaginet (Notion Solutions) wins Microsoft Partner of the Year in the Application Lifecycle Management Category",
      "date": "2011-06-24 15:09:00 +0000",
      
      "content": "Notion Solutions is part of Imaginet, and last week Microsoft announced that Imaginet had won the Microsoft Partner of the Year in the inaugural Application Lifecycle Management category.\n\nWe were selected from over 3000 other worldwide entrants in various categories. The award honours partners who have exhibited excellence in providing services that increase the speed of deployment or implementation of Microsoft Visual Studio.\n\nHear our CEO, Rod Giesbrecht, talk about what this award means to Imaginet.\n",
      "categories": [],
      "tags": ["news"],
      
      "collection": "posts",
      "url": "/imaginet-notion-solutions-wins-microsoft-partner-of-the-year-in-the-application-lifecycle-management-category/"
    },{
      
      "title": "QFE Fixes VS 2010 SP1 Bug on Test Agents",
      "date": "2011-06-24 15:15:00 +0000",
      
      "content": "Last week I was helping a customer get up and running with Lab Management. In an effort to make sure they’re up to date, I installed TFS and VS 2010 SP1 on their TFS machine as well as in all the Lab machines.\n\nEverything was going smoothly until we started running automated tests. The tests would return as “not executed” and the error was:\n\n\n\"Attempted to access an unloaded AppDomain. (Exception from HRESULT: 0x80131014)\"\n\n\nThe strange thing was that if we turned off all the diagnostic adaptors, the tests ran successfully\n\nAfter some investigation, we found out that this was a known bug in SP1. Yesterday, however, a QFE was released that fixes this issue. If you’ve installed TFS and VS 2010 SP1 on your TFS and in your lab machines, make sure you apply this QFE if you want to run automated tests with diagnostic data adaptors!\n\nHappy testing!\n",
      "categories": [],
      "tags": ["labmanagement"],
      
      "collection": "posts",
      "url": "/qfe-fixes-vs-2010-sp1-bug-on-test-agents/"
    },{
      
      "title": "Links to this series:",
      "date": "2011-07-07 20:09:00 +0000",
      
      "content": "\n  Part 2 – Setup\n  Part 3 – Configuration\n  Part 4 – Synchronizing Hierarchies from TFS\n\n\nDevelopment teams often work in conjunction with a Project Management Office (PMO). A common scenario is the PMO creating high level requirements and the Dev team keeping the Project plans up to date manually in order to report progress back to the PMO. This process means duplication – the PMO task and the TFS requirement. The problem is even worse if the PMO is tracking detailed tasks. Another complication is having to update tasks in both systems.\n\nThat’s where the TFS Project Server integration comes into play. This integration keeps TFS and Project Server up to date via 2 way synchronization. You can kick the tires a bit with this virtual machine and labs that demo the capabilities. You can download the integrator from your MSDN subscription downloads.\n\nI recently worked on an integration for a customer – and these posts are going to detail some of the gotchas that I ran across. This post will focus on the some of the limitations of the integration.\n\nSupported Scenarios\n\nThere are 2 scenario’s that are supported by the Integration: High Level Task Roll-up and Detailed Task Breakdown.\n\nIn the High Level Task Roll-up, the PMO creates high level tasks (in Project) that map to requirements in TFS. Only the requirements in TFS are sync’d to Project Server.\n\nIn the Detailed Task Breakdown, the requirements are created on the Project Server and then broken down into tasks on TFS. The tasks are also sync’d to Project Server.\n\nThis table compares the 2 approaches:\n\nHigh Level Task Roll-up  Detailed Task Breakdown  Only requirements are sync’d Requirements and tasks are sync’d  PMO only gets high level progress PMO can do detailed resource planning  Best when mapped to Agile Template Best when mapped to CMMI Template\n\nLimitations\n\nThere are 2 limitations that you need to be aware of when doing the integration.\n\nTime Tracking\n\nSince TFS has no notion of when work was completed (it tracks only the total work completed and remaining work), you can’t use this synchronization to perform time tracking from the TFS side. If you don’t care about that, then you haven’t got a problem. If you care, then your developers will have to track time in the PWA timesheet.\n\nOn a related note, though you can assign multiple resources to a task in Project, you can’t on TFS – so make sure your PMO understands this!\n\nHierarchy Sync\n\nThis is more of an irritation than a limitation. Project Server has some sort of limitation that it requires parent tasks to exist and be sync’d before the child tasks can be sync’d.\n\nOnce you’ve connected a Team Project (in TFS) to an Enterprise Project (in Project Server), you’ll need to tell TFS which work items you want to sync to the Project Server. You do this by setting the “Sync to Enterprise” field on your work item to true and selecting the Enterprise Project you want to sync the work item to.\n\nHere’s the gotcha: if you create a hierarchy in TFS, you need to sync level by level. Start by setting the top level items of the hierarchy to sync – then wait until they are sync’d. Then set the next level of items to sync. Wait for them to sync before next level and so on and so on. Once the items are sync’d you can create child items (and set them to sync) without further problems.\n\nNames and Permissions\n\nIt’s best to use Active Directory for the Enterprise Resources. The sync engine matches Enterprise Resources to TFS users using the display name – if you’re not using Active Directory Sync in the PWA, then make sure the display name of the Enterprise Resource matched the Display Name of the AD User exactly.\n\nIf your PMO has customized permissions for Enterprise users in the PWA, then you may run into issues. For example, if your PMO does not grant the “Create Tasks” permission (if they don’t want everyone creating tasks in the Project) then the sync engine won’t be able to sync from TFS to Project Server – the engine uses the “Created By” identity to create tasks on the Enterprise Project.\n\nIn the next post, I’ll talk about setup and configuration of the sync engine.\n",
      "categories": [],
      "tags": ["alm"],
      
      "collection": "posts",
      "url": "/tfs-and-project-server-integration-tips-from-the-trenches-part-1/"
    },{
      
      "title": "Links to this series:",
      "date": "2011-07-07 20:14:00 +0000",
      
      "content": "\n  Part 1 – Considerations\n  Part 3 – Configuration\n  Part 4 – Synchronizing Hierarchies from TFS\n\n\nPart 2: Setup and Configuration\n\nThe best way to do this is to follow the instructions from the “Configuration Quick Reference” page in the Integration help file. In this post I’ll add some summaries and screen shots to augment the manual a bit.\n\nRequired Accounts\n\n\n  An account that will be used to configure the integration globally – or at least at the Project Collection&lt;-&gt;PWA level. (I used the tfssetup account that was used to install and configure TFS). (For the rest of this post, this account will be referred to as tfssetup)\n  Accounts that will be used to manage mappings per project – at the TeamProject&lt;-&gt;Enterprise Project level (these can be Project Admins or Team Leads on the TFS side). (This can also be the tfssetup account if you want – for this post, I will use the colind account)\n  The TFS Service account ( tfsservice for this post).\n\n\nConfiguring Permissions on TFS\n\n\n  Make sure the global config account ( tfssetup ) is a in the Team Foundation Administrator group\n  Open the TFS Admin Console and click on the Application Tier node\n  Click on the “Group Membership” link in the Application Tier Summary section\n  Add the user (tfssetup) to the Team Foundation Administrator group\n\n  Make sure each account that will be managing mapping at the project level ( colind ) has “Administer Project Server integration” permissions.\n  Open the TFS Admin Console and click on the “Team Project Collections” node\n  Click on the Project Collection you want to configure\n  Click on “Administer Security”\n  Select the group that the user is in (or add the user explicitly) and make sure the “Administer Project Server integration” permission is allowed\n\n  Make sure the resources you are going to use on the project are part of some group on the TFS Team Project that you will be mapping to Project Server (usually just place them into the Contributors group).\n\n\nConfiguring Permissions on Project Server 2010\n\n\n  Configure service account permissions on the Project Server Application in Sharepoint Central Admin\n  Open the Sharepoint central admin console and click on the “Manage Service Applications” link\n\n  Look for the “Project Server Service Application” and click next to the name (don’t click the hyperlink – you just want to select the row, not follow the link – see the red ‘x’ in the picture below). This highlights the row and activates some buttons in the ribbon at the top.\n\n  Click on the “Permissions” button in the ribbon.\n  Add the TFSService account and make sure it has full control checked in the permissions section.\n\n    Configuring Permissions in PWA\n  \n\n\nTo configure the integration, you’ll need to create 2 user accounts on the PWA – one for the TFSService account and one for the account you’ll use to register the PWA (TFSSetup). You can add them to the Administrator group on PWA or assign the accounts the minimum permissions as described in the Integration help file.\n\nEach user that is going to have tasks must be created as an enterprise resource. Make sure the display name matches the display name for the user in Active Directory – and it’s recommended that you let Project server sync users from Active Directory.\n\nThe resources will need to be part of the Team Members group in Project Server (or have the same permission set).\n\nOnce you’ve created the Enterprise Project, make sure that you select “Build Team” and assign the enterprise resources to the project.\n\nSo now you’ve got the permissions set and the components installed – next we’ll configure the bits.\n",
      "categories": [],
      "tags": ["alm"],
      
      "collection": "posts",
      "url": "/tfs-and-project-server-integration-tips-from-the-trenches-part-2/"
    },{
      
      "title": "Links to this series:",
      "date": "2011-07-07 20:15:00 +0000",
      
      "content": "\n  Part 1 – Considerations\n  Part 2 – Setup\n  Part 4 – Synchronizing Hierarchies from TFS\n\n\nPart 3: Configuring the Bits\n\nNow that you’ve installed the Integrator, you need to configure it. This happens at various levels:\n\n\n  Register the PWA with TFS\n  Associate a Project Collection with a registered PWA\n  Specify mappings (or use the default mapping)\n  Associate a Team Project with one (or more) Enterprise Projects\n  Mark Tasks for synchronization\n\n\nYou only need to configure the Project Collection &lt;-&gt; PWA once – but for each Team Project that you want to map to an Enterprise Project, you’ll need to configure a mapping (again, usually just once).\n\nI logged into a client machine with VS 2010 SP1 installed (that’s where tfsadmin.exe comes from) as TFSSetup (my admin account for the integration).\n\nRegistering a PWA\n\nTo register a PWA, open a VS command prompt and use the following command:\n\n\ntfsadmin ProjectServer /RegisterPWA /pwa:pwaUrl /tfs:tfsUrl\n\n\nwhere\n\n\n  pwaUrl is the url to your PWA – like http://server/pwa\n  tfsUrl is the url to your TFS services – like http://server:8080/tfs\n\n\nTF244079 when trying RegisterPWA\n\nIf you get the following (helpful!) error:\n\n\nTF244079: An error occurred while retrieving the URL for shared services\n\n\nthen you probably haven’t installed the latest cumulative updates for Project Server (Project 2010 SP1 is also available – if you install SP1, make sure you run the Sharepoint Configuration Wizard after installing the SP).\n\nAssociate a Project Collection to a Registered PWA\n\nTo map a Project Collection to a PWA, open a VS command prompt and use the following command:\n\n\ntfsadmin ProjectServer /MapPWAToCollection /pwa:pwaUrl /collection:tpcUrl\n\n\nwhere\n\n\n  pwaUrl is the url to your PWA – like http://server/pwa\n  tpcUrl is the url to the Project collection – like http://server:8080/tfs/DefaultCollection\n\n\nSpecifying Mappings\n\nYou can customize the mappings yourself if you want to deviate from the default mapping – consult the help file if you want to do this. For this example, I used the default mapping.\n\nTo specify the default mapping, use the following command:\n\n\ntfsadmin ProjectServer /UploadFieldMapping /collection:tpcUrl /useDefaultFieldMappings\n\n\nwhere tpcUrl is the url to the Project collection – like http://server:8080/tfs/DefaultCollection\n\nMapping a Team Project to an Enterprise Project\n\nNow you can map a Team Project to an Enterprise Project. Use the following command:\n\n\ntfsadmin ProjectServer /MapPlanToTeamProject /collection:tpcUrl /enterpriseProject:epmProjectName /teamProject:teamProjectName /workItemTypes:typeList /nofixedwork\n\n\nwhere\n\n\n  tpcUrl is the url to the Project collection – like http://server:8080/tfs/DefaultCollection\n  epmProjectName is the name of the Project Plan on Project Server (use “” to enclose the name if it contains spaces)\n  teamProjectName is the name of the Team Project in TFS (use “” to enclose the name if it contains spaces)\n  typeList is the list of types you want to sync – if you specify multiple types, use “” to enclose the comma-separated list without spaces after the commas (for example, “Requirement,Task” or “User Story,Task”)\n  nofixedwork specifies no fixed work in Project (consult the Integration help file for more info on this switch).\n\n\nIn the next post, we’ll look at how to synchronize work items.\n",
      "categories": [],
      "tags": ["alm"],
      
      "collection": "posts",
      "url": "/tfs-and-project-server-integration-tips-from-the-trenches-part-3/"
    },{
      
      "title": "Links to this series:",
      "date": "2011-07-07 20:17:00 +0000",
      
      "content": "\n  Part 1 – Considerations\n  Part 2 – Setup\n  Part 3 – Configuration\n\n\nPart 4: Synchronizing Hierarchies from TFS to Project\n\nOne limitation of the synchronization is that you need to sync parent items before child items. One way of getting work items into TFS and Project is to use Excel. Open Excel, go to the Team Tab and click “New List”. In the dialogue, select the Team Project that is associated and select “Input List” to get some default columns. In the ribbon at the top, select “Choose Columns” and add “Project Server Submit” and “Project Server Enterprise Project”. Finally, press the “Add Tree Level” button to add a level for each level in your hierarchy.\n\nNow you can enter your work items.\n\nWhen you’re ready to submit to TFS, change the Project Server Submit value on only the 1st level of the hierarchy to “Yes” and select the Enterprise Project in the Project Server Enterprise Project column (there will be a value here for each Enterprise Project that is associated to this Team Project). In the image below, you can see that I’ve only set the High Level Requirements to sync.\n\n\n\n\nHit the Publish button in the ribbon to publish to TFS. The Integration engine will kick in and sync the high level items to Project Server. You can check this by waiting a short while (a minute or two) and then checking the history field of the work items that you’ve sync’d.\n\nTF285010: Not a valid Project Server Resource\n\nOften when you’re doing synchronization, you’ll come across this error message:\n\n\nTF285010: The following user is not a valid Project Server resource: Colin Dembovsky. Add the Team Foundation user to the enterprise resource pool.\n\n\nThis is actually a generic error message (which could mean that the user is not an enterprise resource or has not been added to the resource pool for the Enterprise Project) or it could be a permissions issue. Check that the TFSService account (the integration identity) has the correct permissions.\n\nYou must also make sure that you publish the Enterprise Project. Synchronization only works with published projects.\n\n(Note: To get the sync engine to kick in again, I usually make a change to the title field of the work item).\n\nMake sure you can see the following message in the history of the high level work items:\n\n\nProject Server Sync: Successfully submitted the request to Project Server.\n\nApprovals\n\nSo now the changes have been submitted to the Enterprise Project – if required, the Project Manager (or Timesheet approver) will need to approve the changes and publish the project.\n\n\n\n\nIf you log onto your PWA, you’ll see some task approvals waiting (in this case it’s a New Task Request). Approve the requests (you can preview if you want to and you can add comments when approving). Once you’ve approved the tasks, you’ll get approvals again. This time there are “edits” to the tasks – hours worked, work item type and other fields. Again, you can preview if you want to – make sure you approve this 2nd approval!\n\nOnce you have approved the tasks (each one needs 2 approvals) you must publish the project! Open it in Project Professional and select File-&gt;Publish. Once you’ve done that, you’ll be able to see the work items in the Project Plan:\n\n\n\n\nNow if you look at the history of Work Item in TFS, you’ll see a couple more edits from the sync engine. Note the approval that comes in (as well as the comment) are from the 2nd approval on the PWA.\n\nPublishing the Next Level\n\nNow you can go back to your spreadsheet and change the Project Server Sync to yes on the next level of the hierarchy. I also added remaining work to the tasks at the same time. Once again, make sure you see the “Successfully submitted to Project Server” message in the history. Then do the double approval on the PWA and don’t forget to publish the plan!\n\nYou can also go ahead and enter predecessor information on the Project Plan to make sure that your resources are levelled and so on. Once you publish, you’ll see the changes (again the 2nd approval) on the work items in TFS. You’ll also notice the start dates coming in from the Project Plan as well as the padlock on the hierarchy – when you sync a hierarchy to Project Server, the links are locked within TFS. If you want to change the parenting, do it from the Project Plan.\n\n\n\n",
      "categories": [],
      "tags": ["alm"],
      
      "collection": "posts",
      "url": "/tfs-and-project-server-integration-tips-from-the-trenches-part-4/"
    },{
      
      "title": "Load Balancing Application Tiers on VMWare",
      "date": "2011-08-16 04:18:00 +0000",
      
      "content": "Most posts about load balancing TFS Application Tiers using NLB use either physical servers or Hyper-V virtual servers. So you would think that you can do the same using VMWare for the Application Tiers, right?\n\nWrong. NLB doesn’t play nicely with VMWare out-of-the-box – actually, it’s the virtual switches that are problematic. The details have to do with port-flooding and RARP packets when you configure NLB in unicast mode – the details are not that important, but the solution is. There are two solutions to the problem: configure the VMWare virtual switch and set up NLB in unicast mode, or set up NLB in multicast mode.\n\nAt a customer that I had to work at recently, we couldn’t tinker with the virtual switch because there were other servers connected to it, so we had to take the multicast route. To do this, you need to set up a static ARP route once you’ve set up the NLB.\n\nWhen setting up the NLB, make a note of the MAC address that the cluster is assigned – you’ll need both the IP address and the MAC address of the cluster to set up the static ARP entry.\n\n\n\n\nOn your router, configure the static ARP entry:\n\n\narp [ip] [cluster multicast mac] ARPA\n\n\nOnce that’s done, you can configure all the nodes in the NLB cluster with an application tier only install. Configure the friendly name of the TFS url to the “Full Internet Name” from the above dialog (of course you’ll need an A record that points the friendly name to the IP address of the cluster).\n\nOnce that’s done, you’re good to go!\n",
      "categories": [],
      "tags": ["tfsconfig"],
      
      "collection": "posts",
      "url": "/load-balancing-application-tiers-on-vmware/"
    },{
      
      "title": "TF50299: The value named ‘xxx’ was not found when evaluating a condition",
      "date": "2011-08-16 04:31:00 +0000",
      
      "content": "Recently while working at a customer, we configured mail alerts for TFS. We checked that the SMTP server was correct and that we could send mail from the application tiers – everything looked correct, but still there were no mails.\n\nIn the event log, we found this “helpful” error (note the sarcasm):\n\n\nResultMessage : Job extension had an unhandled error: System.Exception: TF50299: The value named '070001' was not found when evaluating a condition.\n\n\nUnfortunately, there was woefully little information about this error.\n\nThe Team Project Collection was upgraded from a TFS2008 server, so it inherited a lot of “old” notifications. We eventually scanned the notifications, and found one that had a rather strange clause in it:\n\n\n… AND ‘070001’ = ‘MyProject’…\n\n\nIt seemed strange to me that there appeared to be value parameters on the left and the right of the = operator – more usual conditions look like\n\n\n… AND TeamProject = ‘MyProject’…\n\n\nSo we deleted this notification, and suddenly all the mails started coming through. It seems like this exception crashes the notification job completely – it would be nice if it just skipped this notification clause and then continued with the other notifications!\n\nMoral of the story – make sure your notifications have properly formed condition clauses!\n",
      "categories": [],
      "tags": ["tfsconfig"],
      
      "collection": "posts",
      "url": "/tf50299-the-value-named-xxx-was-not-found-when-evaluating-a-condition/"
    },{
      
      "title": "Presenting at TechEd Africa 2011",
      "date": "2011-09-27 22:44:00 +0000",
      
      "content": "It’s a little over 3 weeks until Tech Ed Africa 2011 starts (it runs from 17 to 20 October). I’ll be presenting a two topics on the Development Track\n\n\n  Demystifying Debugging with Visual Studio 2010 and IntelliTrace\n  Understanding Your Systems with Architectural Discovery in Microsoft Visual Studio 2010\n\n\nI’ll also be doing a session with Ahmed Salijee from MS South Africa on the Virtualization Track covering Lab Management.\n\nThe other presenters in the ALM sessions include my boss, Chris Menegay, as well as Brian Keller from MS in the US.\n\nIf you haven’t registered yet, make sure you do! Feel free to catch me at any time if you have questions about ALM using TFS and VS 2010 (or 2011, for that matter!)\n",
      "categories": [],
      "tags": ["news"],
      
      "collection": "posts",
      "url": "/presenting-at-teched-africa-2011/"
    },{
      
      "title": "I’ve been made an MVP",
      "date": "2011-10-06 18:30:00 +0000",
      
      "content": "This week I got an email congratulating me on becoming an MVP for Application Lifecycle Management. This is a huge honour! Thanks to all involved in this – including the community!\n\nI thought the stats on where MVPs are globally was interesting – here’s an image showing the distribution geographically.\n\n\n\n\nDon’t forget to catch me at TechEd Africa!\n",
      "categories": [],
      "tags": ["news"],
      
      "collection": "posts",
      "url": "/ive-been-made-an-mvp/"
    },{
      
      "title": "RunQuery won’t work for Hierarchical Queries",
      "date": "2011-10-10 18:37:00 +0000",
      
      "content": "Using the TFS API to display results of a flat query is fairly straightforward – once you have the WIQL you just execute the RunQuery() method and voila – a nice WorkItemCollection for you to enumerate over. However, if you try to execute RunQuery() on a tree or one-hop WIQL, you’ll see this error message:\n\nTF248021: You have specified a query string that is not valid when you use the query method for a flat list of work items. You cannot specify a parameterized query or a query string for linked work items with the query method you specified.\n\n\nI was working on a proof-of-concept and needed to display the result of a Work Item Query in a WPF form. Once I saw the TF248012 error, I googled binged a bit to see if I could work out how to run a hierarchical query. That brought me to the RunLinkQuery() method, which returns a WorkItemLinkInfo array. But that’s where my google-fu ran out of steam – there didn’t seem to be much info about how to proceed once you’ve got this array. So here I’ll show you how I used this array to enumerate the work item hierarchy.\n\nI’ve uploaded the code to my skydrive if you want to download it. (Note: If you want to copy code from the snippets below, just double click in the code area and Cntrl-C).\n\nPhase 1 – Setting up the QueryRunner Class\n\nIn order to work with hierarchical results, you need to execute the query first! Here’s how I did it:\n\nclass QueryRunner{ public WorkItemStore WorkItemStore { get; private set; } public string TeamProjectName { get; private set; } public string CurrentUserDisplayName { get; private set; } public QueryRunner(string tpcUrl, string teamProjectName) { var tpc = TfsTeamProjectCollectionFactory.GetTeamProjectCollection(new Uri(tpcUrl)); WorkItemStore = tpc.GetService&lt;workitemstore&gt;(); TeamProjectName = teamProjectName; }}&lt;/workitemstore&gt;\n\n\nI defined a class called QueryRunner that has a couple of properties:\n\n\n  WorkItemStore – the WorkItemStore service\n  TeamProjectName – the name of the team project that contains the stored query we want to run\n  CurrentUserDisplayName – the display name of the current user (more on this later)\n\n\nYou’ll need references to Microsoft.TeamFoundation, Microsoft.TeamFoundation.Client and Microsoft.TeamFoundation.WorkItemTracking.Client, all of which can be found in the .NET tab of the Add References dialogue.\n\nThe constructor takes in the Url of the Team Project Collection and the Team Project Name. It then initializes the WorkItemStore.\n\nTwo supporting methods you’ll need are the following:\n\nprivate void ResolveCurrentUserDisplayName(){ var securityService = WorkItemStore.TeamProjectCollection.GetService&lt;igroupsecurityservice&gt;(); var accountName = string.Format(\"{0}\\\\{1}\", Environment.UserDomainName, Environment.UserName); var memberInfo = securityService.ReadIdentity(SearchFactor.AccountName, accountName, QueryMembership.None); if (memberInfo != null) { CurrentUserDisplayName = memberInfo.DisplayName; } else { CurrentUserDisplayName = Environment.UserName; }}private IDictionary GetParamsDictionary(){ return new Dictionary&lt;string string=\"\" ,=\"\"&gt;() { { \"project\", TeamProjectName }, { \"me\", CurrentUserDisplayName } };}&lt;/string&gt;&lt;/igroupsecurityservice&gt;\n\n\nResolveCurrentUserDisplayName uses the IGroupSecurityService to resolve the current user’s display name. This is needed if your WIQL contains the “@Me” macro. You can see how it’s used in the GetParamsDictionary method.\n\nPhase 2 – Define a Hierarchical Data Structure\n\nNow we’re almost ready to execute queries – we just need a data structure to hold the results. Here’s what I ended up defining:\n\nclass PropertyChangingBase : INotifyPropertyChanged{ public event PropertyChangedEventHandler PropertyChanged; protected void OnPropertyChanged(string propertyName) { if (PropertyChanged != null) { PropertyChanged(this, new PropertyChangedEventArgs(propertyName)); } }}class WorkItemNode : PropertyChangingBase{ private WorkItem workItem; public WorkItem WorkItem { get { return workItem; } set { workItem = value; OnPropertyChanged(\"WorkItem\"); } } private string relationshipToParent; public string RelationshipToParent { get { return relationshipToParent; } set { relationshipToParent = value; OnPropertyChanged(\"RelationshipToParent\"); } } private List&lt;workitemnode&gt; children; public List&lt;workitemnode&gt; Children { get { return children; } set { children = value; OnPropertyChanged(\"Children\"); } }}&lt;/workitemnode&gt;&lt;/workitemnode&gt;\n\n\nPhase 3 – Run the Query\n\nNow we’re ready to run the query.\n\npublic List&lt;workitemnode&gt; RunQuery(Guid queryGuid){ // get the query var queryDef = WorkItemStore.GetQueryDefinition(queryGuid); var query = new Query(WorkItemStore, queryDef.QueryText, GetParamsDictionary()); // get the link types var linkTypes = new List&lt;workitemlinktype&gt;(WorkItemStore.WorkItemLinkTypes); // run the query var list = new List&lt;workitemnode&gt;(); if (queryDef.QueryType == QueryType.List) { foreach (WorkItem wi in query.RunQuery()) { list.Add(new WorkItemNode() { WorkItem = wi, RelationshipToParent = \"\" }); } } else { var workItemLinks = query.RunLinkQuery().ToList(); list = WalkLinks(workItemLinks, linkTypes, null); } return list;}&lt;/workitemnode&gt;&lt;/workitemlinktype&gt;&lt;/workitemnode&gt;\n\n\nIn this method, we take in the Guid of the query we want to run (how to get that Guid is another discussion – you can see the Guids of stored queries by looking at the TeamProject.QueryHierarchy property). Also, this method is using the WIQL from the stored query, but you could just as well pass in raw WIQL too.\n\nWe construct a Query object passing in the WorkItemStore, the WIQL and the parameter dictionary for “@Project” and “@Me” macros. Next we get a list of all the WorkItemLinkTypes in the WorkItemStore. We’ll use these when we enumerate the work item links.\n\nFinally, we decide on whether or not to run a flat query or a hierarchical query based on the stored query type. If you’re passing in WIQL instead, you’ll have to decide some other way. For flat queries, just construct a list of nodes. For hierarchical queries, get the WorkItemLinkInfo array and walk it using the following (recursive) method:\n\nprivate List&lt;workitemnode&gt; WalkLinks(List&lt;workitemlinkinfo&gt; workItemLinks, List&lt;workitemlinktype&gt; linkTypes, WorkItemNode current){ var currentId = 0; if (current != null) { currentId = current.WorkItem.Id; } var workItems = (from linkInfo in workItemLinks where linkInfo.SourceId == currentId select new WorkItemNode() { WorkItem = WorkItemStore.GetWorkItem(linkInfo.TargetId), RelationshipToParent = linkInfo.LinkTypeId == 0 ? \"Parent\" : linkTypes.Single(lt =&amp;gt; lt.ForwardEnd.Id == linkInfo.LinkTypeId).ForwardEnd.Name }).ToList(); workItems.ForEach(w =&amp;gt; w.Children = WalkLinks(workItemLinks, linkTypes, w)); return workItems;}&lt;/workitemlinktype&gt;&lt;/workitemlinkinfo&gt;&lt;/workitemnode&gt;\n\n\nThis method walks the array, starting with links that have a source ID of 0 (these are top level work items in the hierarchy). For each of those work items, create a WorkItemNode and then populate the children using the current node as the “current” for the next level of recursion. When we go to the next level, the name of the WorkItemLinkType to the parent can be found using the LinkTypeId property of the WorkItemLinkInfo and finding the corresponding ForwardEnd Id in the list of WorkItemLinkTypes.\n\nEpilogue: Displaying the Result in WPF\n\nOnce you’ve got the tree of WorkItemNodes, displaying them in WPF is really easy. Here’s the XAML for the TreeView:\n\n&lt;treeview horizontalalignment=\"Stretch\" verticalalignment=\"Stretch\" itemssource=\"{Binding}\"&gt; &lt;treeview.itemtemplate&gt; &lt;hierarchicaldatatemplate itemssource=\"{Binding Children}\"&gt; &lt;stackpanel orientation=\"Horizontal\"&gt; &lt;textblock text=\"{Binding WorkItem.Id, StringFormat=F0}\" fontweight=\"Bold\"&gt; &lt;textblock text=\"{Binding WorkItem.Title}\" padding=\"3, 0, 0, 0\"&gt; &lt;textblock text=\"{Binding RelationshipToParent, StringFormat=({0})}\" padding=\"3, 0, 0, 0\" fontstyle=\"Italic\"&gt; &lt;/textblock&gt;&lt;/textblock&gt;&lt;/textblock&gt;&lt;/stackpanel&gt; &lt;/hierarchicaldatatemplate&gt; &lt;/treeview.itemtemplate&gt;&lt;/treeview&gt;\n\n\nWe specify that the ItemTemplate for the TreeView contains hierarchical data. For each node in the tree, display the Id, Title and RelationshipToParent. Then use the “Children” property to display the next level in the hierarchy. In the code-behind for this XAML, we simply set the DataContext to the QueryRunner.RunQuery() results. Here are some screenshots of the results for a one-hop and a tree query respectively.\n\n\n\n\nHappy querying!\n",
      "categories": [],
      "tags": ["tfsapi"],
      
      "collection": "posts",
      "url": "/using-the-tfs-api-to-display-results-of-a-hierarchical-work-item-query/"
    },{
      
      "title": "GenericAutomationPeer – Helping the Coded UI Framework Find Your Custom Controls",
      "date": "2011-11-03 22:21:00 +0000",
      
      "content": "Sometimes you’ll write an WPF application that has some sort of “dynamic” way of loading portions of the UI (think: Prism). Sometimes entire frameworks are too much, so you’d prefer something a bit simpler – like, say, a TabControl with a data template. Bind the ItemsSource of your TabControl to an ObservableCollection (where T is some model) and you’ve got a “dynamic” interface.\n\nSo you’ve written your killer dynamic interface app – and, since you’re a good developer, you try to add some coded UI tests.\n\nProblem: The UI Test framework doesn’t “see” any of the controls that were loaded dynamically! What gives?\n\n\n\n\nThe figure above shows the “deepest” level that the UI Framework can see – none of the child controls (the comboboxes or table) exist as far as the coded UI Test framework is concerned.\n\nWhat’s happening here is that the AutomationPeer of the TabControl doesn’t know anything about the controls within it, since they’re loaded dynamically. You have to help your little TabControl a bit. You have to let the Automation framework know about all the little controls that you load. But what if each tab loads a completely different UserControl? This sounds like a lot of work…\n\nThe Solution: GenericAutomationPeer\n\nFortunately, you can walk the child controls and just get them to give their “default” AutomationPeers to you (most primitive WPF controls – like TextBoxes, ComboBoxes, Buttons and so on – have built in AutomationPeers). So we need 2 things: a way to walk the child controls and a hook into the Automation Framework.\n\n  public class GenericAutomationPeer : UIElementAutomationPeer {\n    public GenericAutomationPeer(UIElement owner) : base(owner) { }\n    \n    protected override List&lt;automationpeer&gt; GetChildrenCore() {\n      var list = base.GetChildrenCore();\n      list.AddRange(GetChildPeers(Owner));\n      return list;\n    }\n    \n    private List&lt;automationpeer&gt; GetChildPeers(UIElement element) {\n      var list = new List&lt;automationpeer&gt;();\n      for (int i = 0; i &amp;lt; VisualTreeHelper.GetChildrenCount(element); i++) {\n        var child = VisualTreeHelper.GetChild(element, i) as UIElement;\n        if (child != null) {\n          var childPeer = UIElementAutomationPeer.CreatePeerForElement(child);\n          if (childPeer != null) {\n            list.Add(childPeer);\n          } else { \n            list.AddRange(GetChildPeers(child));\n          }\n        }\n      }\n      return list;\n      }\n    }\n  &lt;/automationpeer&gt;&lt;/automationpeer&gt;&lt;/automationpeer&gt;\n\n\n&gt;p&gt;This class inherits from UIElementAutomationPeer, so you just need to override the GetChildrenCore() method. Inside that, just use the VisualTreeHelper to walk the child controls. For each child control, see if it has an AutomationPeer by calling the static method UIElementAutomationPeer.CreatePeerForElement(). If it has an element, add it to the list of AutomationPeers. If it doesn’t, then recursively call to see if it’s children have AutomationPeers.\n\nSo we’ve got our GenericAutomationPeer: now we just need a hook in to use it. In this example, the “lowest” control visible to the Automation Framework was the TabControl – so that’s where we’ll get our hook in.\n\npublic class CustomTabControl : TabControl{ protected override AutomationPeer OnCreateAutomationPeer() { return new GenericAutomationPeer(this); }}\n\n\nWe create a CustomTabControl that inherits from TabControl and simply overrides its OnCreateAutomationPeer. Inside the override, simple new up a GenericAutomationPeer, and you’re done. Don’t forget to change the XAML from TabControl to local:CustomTableControl (where local is your imported namespace).\n\nOther Tools for your Toolbox\n\n\n  UISpy – this lets you inspect exactly what the Automation Framework can “see”\n  CUITE – Coded UI Test Enhancements on Codeplex – useful coded UI Extension library\n\n\nHappy (coded UI) testing!\n\nUpdate: This post also had a solution specifically for TabControls that avoids having to implement AutomationPeers entirely.\n",
      "categories": [],
      "tags": ["testing"],
      
      "collection": "posts",
      "url": "/genericautomationpeer-helping-the-coded-ui-framework-find-your-custom-controls/"
    },{
      
      "title": "WpfCell – The Key to Coded UI Automation of DataGrids",
      "date": "2011-11-04 17:52:00 +0000",
      
      "content": "As a follow on from my post yesterday about some of the intricacies of using Coded UI testing on WPF applications, I wanted to give some details about using the WpfCell class to automate validation and updates into DataGrids.\n\nThe Problem: When you bind data to a datagrid, the test recorder often complains that the rows (and / or) cells don’t have any “good identification property”. This prevents both validating cell contents as well as recording actions (such as updates) onto the cell.\n\nThe reason for this is that the cells don’t have any good automation properties – they all look the same as far as the UIMap is concerned. If you have a DataGrid with a cell that has the text “135” in it, and you find the control using the coded UI Test builder, you’ll see the Name of the cell is “135”. But what if the data isn’t always 135? What if the cell value that displays is dependent on the current record? The framework won’t be able to find that cell and the test will fail.\n\n\n\nThe Solution: WpfCell\n\nUsing WpfCell (from the Microsoft.VisualStudio.TestTools.UITesting.WpfControls namespace) lets us access cells logically (instead of visually). Add the following code to the UIMap.cs file in your coded UI test project:\n\ninternal void ValidateCell(int row, int column, string value)&lt;br&gt;{&lt;br&gt; var cell = FindCell(row, column);&lt;br&gt; Assert.AreEqual(value, cell.Value);&lt;br&gt;}&lt;br&gt;&lt;br&gt;internal void UpdateCell(int row, int column, string value)&lt;br&gt;{&lt;br&gt; var cell = FindCell(row, column);&lt;br&gt; cell.Value = value;&lt;br&gt; Keyboard.SendKeys(\"\\t\");&lt;br&gt;}&lt;br&gt;&lt;br&gt;private WpfCell FindCell(int row, int column)&lt;br&gt;{&lt;br&gt; var cell = new WpfCell(DemoWindow.UIItem.Table);&lt;br&gt; cell.SearchProperties.Add(WpfCell.PropertyNames.RowIndex, row.ToString());&lt;br&gt; cell.SearchProperties.Add(WpfCell.PropertyNames.ColumnIndex, column.ToString());&lt;br&gt; return cell;&lt;br&gt;}&lt;br&gt;\n\n\nYou’ll have to change the Table that is used in the FindCell method – you’ll find the table class generated into the UIMap if you look at the controls (you may have to record a click in the table or something to get this control into the map). The only bit that was interesting here was the Keyboard.SendKeys(“\\t”) which presses tab – this actually confirms the update of the value into the cell (otherwise the cell is still “dirty” and the update won’t actually have happened).\n\nNow from within your coded UI Test, you can add code like this:\n\nUIMap.ValidateCell(0, 1, \"135\");&lt;br&gt;UIMap.UpdateCell(0, 1, \"140\");&lt;br&gt;UIMap.ValidateCell(0, 1, \"140\");&lt;br&gt;\n\n\nHappy (DataGrid) testing!\n",
      "categories": [],
      "tags": ["testing"],
      
      "collection": "posts",
      "url": "/wpfcell-the-key-to-coded-ui-automation-of-datagrids/"
    },{
      
      "title": "ISubscriber: Getting the TFS Url for Client Operations",
      "date": "2011-12-30 05:03:00 +0000",
      
      "content": "Responding to TFS events can be done in (at least) 2 ways: create a SOAP webservice and register with bissubscribe (this works in a “client” fashion) or implement the ISubscriber interface (in the Microsoft.TeamFoundation.Framework.Server namespace).\n\nThe advantage to the ISubscriber interface implementation is that the plugin can be installed on the TFS server and can also be used to allow or disallow a change (such as a check-in policy). One disadvantage of ISubscribers is that you only get access to a TeamFoundationRequestContext object, not a TfsTeamProjectCollection object. This can limit what operations you can perform.\n\nI was working with an ISubscriber and wanted to update a Global List, so I needed a reference to the WorkItemStore object. Unfortunately, there was no obvious way to do this from the TeamFoundationRequestContext. I could have hard-coded a TFS url into a config file, but this felt like a cop out to me. So I dug around a little bit more and came up with a solution: the TeamFoundationLocationService (see this article about TFS services).\n\nThe TeamFoundationLocationService\n\nYou can easily get the TeamFoundationLocationService from the TeamFoundationRequestContext. You can then query the location service to get the url of the TFS server and collection. Once you have that, you can then instantiate a TfsTeamProjectCollection object and use that to get the WorkItemStore. Here’s the code:\n\nprivate Uri GetTFSUri(TeamFoundationRequestContext requestContext)&lt;br&gt;{&lt;br&gt; var locationService = requestContext.GetService&lt;teamfoundationlocationservice&gt;();&lt;br&gt; return new Uri(locationService.ServerAccessMapping.AccessPoint + \"/\" + requestContext.ServiceHost.Name);&lt;br&gt;}&lt;br&gt;&lt;br&gt;private WorkItemStore GetWorkItemStore(TeamFoundationRequestContext requestContext)&lt;br&gt;{&lt;br&gt; var uri = GetTFSUri(requestContext);&lt;br&gt; var tpc = TfsTeamProjectCollectionFactory.GetTeamProjectCollection(uri);&lt;br&gt; return tpc.GetService&lt;workitemstore&gt;();&lt;br&gt;}&lt;br&gt;&lt;/workitemstore&gt;&lt;/teamfoundationlocationservice&gt;\n\n\nHappy subscribing!\n",
      "categories": [],
      "tags": ["alm"],
      
      "collection": "posts",
      "url": "/isubscriber-getting-the-tfs-url-for-client-operations/"
    },{
      
      "title": "Workspaces: Updating the owner",
      "date": "2012-01-17 19:19:00 +0000",
      
      "content": "Recently we changed our internal domain. It was a little bit of a pain, since I had to migrate my setting and preferences. One “snag” I hit was reconnecting to TFS once I had changed my primary login from the old domain to the new domain.\n\nIf you change your username (or domain), open Visual Studio and go to the Source Control Explorer, you’ll see that (of course) you don’t have any mappings in your default workspace. If you try to create a mapping to an existing folder on your hard-drive, you’ll get an error stating that the folder is mapped in another workspace:\n\n\n\n\nSo you drop into the Visual Studio command prompt and use the tf.exe command to list the workspaces:\n\n\ntf workspaces\n\n\n\n\nHowever (and this may be a bug, I’m not sure), this only brings back workspaces for the current user (on the new domain). So if you want to see all the workspaces (for any user) you’ll have to enter this command:\n\n\ntf workspaces /collection:tpcUrl /owner:*\n\n\n(Of course tpcUrl is the url of your team project collection).\n\n\n\n\nNow that you can see all the workspaces, you’ll want to update the owner of the old workspace.\n\n\ntf workspace /name:workspacename;workspaceowner /collection:tpcUrl\n\n\nThis launches the dialogue for the workspace properties. So I tried to just update the name in the owner field, but there were pending changes in the workspace, so I got an error message:\n\n\n\n\nBack to the console. To see the pending changes in the old workspace change dir to the root of your workspace mapping and run the following command:\n\n\ntf status /collection:tpcUrl /workspace:workspacename;workspaceowner\n\n\nThis lists all the pending changes. Now from the list of changes, I could see some that I wanted to keep and others that I didn’t – but I didn’t want to think about sorting the changes out now. So I decided to follow the previous error log’s advice and shelve the pending changes.\n\nTo “select” the old workspace from the command prompt, run the tf workspace command:\n\n\ntf workspace /name:workspacename;workspaceowner /collection:tpcUrl\n\n\nWhen the dialogue opens, change the “Permissions” to Public (if it’s not set already) to make sure that the new user can shelve in the old user’s workspace and press the OK button.\n\n\n\n\nNow you can run the shelve command:\n\n\ntf shelve /recursive name;owner *.* /move\n\n\n(Here name is the name of the shelveset and owner is the domain\\username of the new user)\n\nThe shelve dialogue opens, so you can review the shelveset. Press Shelve to complete the operation.\n\nNow run the workspace command again, and change the owner:\n\n\n\n\n(I had to rename the new workspace since it had the same name as the old one before this worked).\n\nFinally, to get the pending changes back, you need to unshelve. Open the Pending Changes window (View-&gt;Other Windows-&gt;Pending Changes in VS) and press the Unshelve button. Find the shelveset that you saved the pending changes in and select it. Then hit Unshelve and your pending changes are back.\n\nNow your workspace is updated to the new user, and you can get back to work!\n",
      "categories": [],
      "tags": ["sourcecontrol"],
      
      "collection": "posts",
      "url": "/workspaces-updating-the-owner/"
    },{
      
      "title": "Using TFS ALM for Sharepoint Development (The Easy Way – with Lab Management)",
      "date": "2012-02-10 20:46:00 +0000",
      
      "content": "Visual Studio 2010 has some amazing features for Sharepoint development, like project templates, server explorers, feature and package GUIs to name a few. So you’re tasked with creating a WebPart or a Workflow – no problem, fire up VS, create a new project and you’re coding.\nHowever, just because you’re up and coding quickly, doesn’t mean you’re being productive (necessarily). What about requirements management? Testing? Source control? And if there’s more than 1 of you coding, what Sharepoint site do you code against? Oh wait, I forgot to mention that you need to install Sharepoint on the same machine that you have VS on to actually get the Sharepoint projects to work.\n\nThe ChallengesThere are a few challenges that you’ll need to overcome if you want to be a good ALM citizen while doing Sharepoint dev:\n\n\n  Sharepoint and VS need to be on the same machine\n  Source code needs to go somewhere other than your hard drive\n  Build Automation\n  Deployment\n  Automated TestingFortunately, without too much work, you can address all of these challenges using TFS, and specifically Lab Management. Of course TFS covers the other ALM concerns too (like requirements management and bug tracking). I won’t focus on these too much in this article; I’ll concentrate more on the “technical challenges” bulleted above.\n\n\nSetting up a Dev Environment using a VMInstalling Sharepoint on your local machine will allow you to at least get your applications compiling, but it’s not a sustainable solution to the SP/VS-on-the-same-machine problem. I decided to use a VM. Here are the steps I followed to set up the VM for development:\n\n\n  Install OS and join domain\n  Install and configure Sharepoint\n  Install VS and connect to TFS\n  SnapshotNow I have a VM for Sharepoint development. I can easily duplicate this VM for other team members as they need development environments.\nNote: Because this VM is going to also become a test environment, I was careful to add my solution to Source Control. That way if I restored to a previous snapshot I don’t lose any code!\n\n\nBuild AutomationBuilding the code in TeamBuild is easy – but you’ll have to jump through some extra hoops if you want to create a WSP that can be deployed to your test Sharepoint.\nChris O’Brien (from the Sharepoint Dev Team) wrote a series of blogs about Continuous Integration with TFS and Sharepoint (supposedly there are 5 posts, but I only found 3: one, three and five). They’re certainly detailed – but it seemed like a lot of work to go through. I think that my Lab Management solution overcomes some of the complexities that Chris had to work through (most of which revolved around remote deployment of WSP’s).\nTo get automated builds working, start off using the DefaultTemplate and get the build to compile your solution. Once you get a passed build, it’s time to create the WSP package. So open up the build template and add\n/p:IsPackage=true\nto the MSBuild arguments parameter.\n\n\n\n\nHowever, your build is now going to fail (unless you have Sharepoint installed on your build server – which you shouldn’t!). The Sharepoint packaging requires some build targets as well as some dll’s that only get installed with Sharepoint.\nOne of the most useful bits of info in Chris O’Brien’s articles is the mention of this Powershell script that will “harvest” the Sharepoint dll’s and build targets and then “install” them on your build server. I ran the script (1st on the Sharepoint dev VM), copied over the folder it created to the build server and ran the script again to install. Easy as pie. Trigger another build, and you’ll see in the drop folder a shiny new WSP package!\n\nDeploymentDeploying your WSP to the Sharepoint site is arguable the most difficult challenge that you’ll face. That’s why Chris O’Brien’s article gets quite complicated. Enter Lab Management.\nSince we’re working off a VM anyway, let’s bring it into Lab Management. The workflow capability (provided by the TeamBuild agent) will allow us to automate deployment not from a remote perspective, but “locally” as it were (we’re going to execute the deployment from the Sharepoint machine, not remotely from the build server). This is a massive simplification.\nThese are the steps I followed to enable deployment:\n\n\n  Install the TFS Build, Lab and Test agents on the Sharepoint Dev VM\n  I configured the Build and Test agents, hooking them up to existing Build and Test controllers in my TFS environment\n  Compose a new Virtual Environment in Lab Management\n  \n    Bring in the Sharepoint VM and enable Deployment and Testing capabilities\n\n  \n  Use VS to deploy the package to the Sharepoint site  by debugging the project\n  \n    In the Sharepoint Tab of the Sharepoint project properties, make sure you turn off the “Auto-retract after debug” option\n\n  \n  Create a custom page to add the WebPart in (this is only necessary for visual development, like WebParts)\n  Create a Powershell script that can update the WSP (here’s mine – it’s pretty generic, so should just work)\n  Add this to the Sharepoint project and set its “Copy to output directory” property to “Copy always” (this ensures that it ends up in the drop folder)\n\n\nCoded UI TestingI’m not going to go into too much detail here – I created a test plan and a test suite. I then created a test case and executed the case (with Action Recording enabled). I then added a Test Project to my Sharepoint solution and generated a coded UI test from the Test Case action recording. Then I associated the test method to the Test Case (in the Associated Automation tab of the test case). Voila – one automated test case ready to fire.\nYou’ll need to open MTM and create Test Settings (automated in this case) for your Sharepoint Lab environment.\n\nLab Management WorkflowThe final step is hooking it all up in the Lab Management workflow. Create a new build and change the template to the LabDefaultTemplate. Click the ellipsis to launch the Lab Workflow Wizard.\n\n\n  Environments Tab: Select the Sharepoint environment from the list of environments\n  Build Tab: Select the build that you created using the DefaultTemplate and that builds your WSP package\n  Note: The coded UI test project must be part of this solution too, so that the dll’s end up in the drop folder\n  Deploy Tab: check the “Deploy the build” checkbox\n  Add the following 2 scripts (make sure you create the c:\\deployment folder on the VM):\n  cmd /c xcopy /Y $(BuildLocation)*.* c:\\deployment\n  \n    cmd /c powershell c:\\deployment\\deployWSP.ps1 $(BuildLocation)\n\n  \n  Test Tab: Select the test plan, suite, configuration and settings\n\n\nConclusionUsing Lab Management greatly simplifies the ALM aspects of Sharepoint development – automated build, deployment and testing specifically.\nSummary of Steps:\n\n\n  Create a VM and install Sharepoint and VS 2010\n  Create your SP solution and check into Source control!\n  Install and configure TFS Build, Test and Lab agents\n  Compose a new Environment using the VM\n  Create automated test settings for the environment\n  Deploy the package by debugging the project (turn off auto-retract after debugging)\n  Create test cases in a test plan\n  Execute them using the Action Recording\n  Turn the action recordings into Coded UI Tests\n  Associate the test methods to the Test Cases\n  Customize the build to produce a WSP package\n  This includes “installing” Sharepoint dlls and build targets onto your build server\n  Create a powershell script that can update the deployed Package from a WSP file\n  Hook it all up using the Lab Management Workflow\n\n\n\nHappy SP dev’ing!\n",
      "categories": [],
      "tags": ["alm"],
      
      "collection": "posts",
      "url": "/using-tfs-alm-for-sharepoint-development-the-easy-way-with-lab-management/"
    },{
      
      "title": "Code Monkey?",
      "date": "2012-03-03 04:12:00 +0000",
      
      "content": "I’ve just spent 4 days in Seattle at my first Global MVP Summit – it’s been great meeting a lot of the other ALM MVPs and putting faces to email addresses! It’s also been great getting an “inside scope” on some of the strategic directions that the TFS and VS product teams are taking.\n\nOn Tuesday, Chuck Sterling did a presentation and “snagged me” to be his live Code Monkey demonstrator (he spoke and I clicked around). Tiago Pascoal later joked about a t-shirt, and I whipped out my Photoshop and created a Code Monkey t-shirt, which I sent to Chuck. He blogged about it in this post.\n",
      "categories": [],
      "tags": ["news"],
      
      "collection": "posts",
      "url": "/code-monkey/"
    },{
      
      "title": "Using the Fakes Framework to Test TFS API Code (Part 1 of 2)",
      "date": "2012-03-20 20:20:00 +0000",
      
      "content": "(Here’s the link to Part 2)\n\nIf you’ve ever written a utility to do TFS “stuff” using the TFS API, you probably tested by hitting F5 and stepping through a bit before letting any large loops do their thing. So what happened to all the goodness is expected of good developers – for example unit testing?\n\nWell, it turns out it’s insanely difficult to test any code that depends on the TFS APIs. You could craft tests that hit an actual TFS server, but then you’d have to worry about clean up so that your tests could be repeated. And of course you’d *never* do this sort of testing against a Live server (or a Live project), right?\n\nBesides, unit tests, at least in theory, are supposed to have no dependencies on external systems. So how do you go about unit testing code that uses the TFS API?\n\nScenario: Copy Work Items\n\nLet’s imagine you’ve written a console app to copy work items (obtained from executing a stored query) to a target iteration. Here’s the code for the WorkItemCopyer class and Program.cs:\n\nclass WorkItemCopyer&lt;br&gt;{&lt;br&gt; public WorkItemCollection WorkItems { get; private set; }&lt;br&gt; public QueryHierarchy QueryHierarchy { get; private set; }&lt;br&gt;&lt;br&gt; public WorkItemStore Store { get; private set; }&lt;br&gt; public TfsTeamProjectCollection TPC { get; private set; }&lt;br&gt; public string TeamProjectName { get; private set; }&lt;br&gt;&lt;br&gt; public WorkItemCopyer(TfsTeamProjectCollection tpc, string teamProjectName)&lt;br&gt; {&lt;br&gt; TPC = tpc;&lt;br&gt; TeamProjectName = teamProjectName;&lt;br&gt; Store = TPC.GetService&lt;workitemstore&gt;();&lt;br&gt; QueryHierarchy = Store.Projects[TeamProjectName].QueryHierarchy;&lt;br&gt; }&lt;br&gt;&lt;br&gt; public void RunQuery(QueryDefinition queryDef)&lt;br&gt; {&lt;br&gt; var dict = new Dictionary&lt;string, string=\"\"&gt;() &lt;br&gt; {&lt;br&gt; { \"project\", TeamProjectName }, &lt;br&gt; { \"me\", GetCurrentUserDisplayName() } &lt;br&gt; };&lt;br&gt;&lt;br&gt; var query = new Query(Store, queryDef.QueryText, dict);&lt;br&gt; WorkItems = query.RunQuery();&lt;br&gt; }&lt;br&gt;&lt;br&gt; private string GetCurrentUserDisplayName()&lt;br&gt; {&lt;br&gt; var securityService = TPC.GetService&lt;igroupsecurityservice&gt;();&lt;br&gt; var accountName = string.Format(\"{0}\\\\{1}\", Environment.UserDomainName, Environment.UserName);&lt;br&gt; var memberInfo = securityService.ReadIdentity(SearchFactor.AccountName, accountName, QueryMembership.None);&lt;br&gt; if (memberInfo != null)&lt;br&gt; {&lt;br&gt; return memberInfo.DisplayName;&lt;br&gt; }&lt;br&gt; return Environment.UserName;&lt;br&gt; }&lt;br&gt;&lt;br&gt; public int CopyWorkItems(string targetIterationPath)&lt;br&gt; {&lt;br&gt; foreach (WorkItem workItem in WorkItems)&lt;br&gt; {&lt;br&gt; var copy = workItem.Copy();&lt;br&gt; copy.IterationPath = targetIterationPath;&lt;br&gt; copy.Save();&lt;br&gt; }&lt;br&gt; return WorkItems.Count;&lt;br&gt; }&lt;br&gt;&lt;br&gt; public QueryDefinition FindQuery(string queryName)&lt;br&gt; {&lt;br&gt; return FindQueryInFolder(QueryHierarchy, queryName);&lt;br&gt; }&lt;br&gt;&lt;br&gt; private QueryDefinition FindQueryInFolder(QueryFolder folder, string queryName)&lt;br&gt; {&lt;br&gt; foreach (var query in folder.OfType&lt;querydefinition&gt;())&lt;br&gt; {&lt;br&gt; if (query.Name == queryName)&lt;br&gt; {&lt;br&gt; return query;&lt;br&gt; }&lt;br&gt; }&lt;br&gt; QueryDefinition subQuery = null;&lt;br&gt; foreach (var subFolder in folder.OfType&lt;queryfolder&gt;())&lt;br&gt; {&lt;br&gt; subQuery = FindQueryInFolder(subFolder, queryName);&lt;br&gt; if (subQuery != null)&lt;br&gt; {&lt;br&gt; return subQuery;&lt;br&gt; }&lt;br&gt; }&lt;br&gt; return null;&lt;br&gt; }&lt;br&gt;}&lt;br&gt;&lt;br&gt;class Program&lt;br&gt;{&lt;br&gt; static void Main(string[] args)&lt;br&gt; {&lt;br&gt; var tpcUrl = args[0];&lt;br&gt; var teamProjectName = args[1];&lt;br&gt; var queryName = args[2];&lt;br&gt; var targetIterationPath = args[3];&lt;br&gt;&lt;br&gt; var tpc = TfsTeamProjectCollectionFactory.GetTeamProjectCollection(new Uri(tpcUrl));&lt;br&gt; var copyer = new WorkItemCopyer(tpc, teamProjectName);&lt;br&gt;&lt;br&gt; var query = copyer.FindQuery(queryName);&lt;br&gt; copyer.RunQuery(query);&lt;br&gt; var count = copyer.CopyWorkItems(targetIterationPath);&lt;br&gt;&lt;br&gt; Console.WriteLine(\"Successfully copied {0} work items\", count);&lt;br&gt; Console.WriteLine(\"Press &lt;enter&gt; to quit...\");&lt;br&gt; Console.ReadLine();&lt;br&gt; }&lt;br&gt;}&lt;/enter&gt;&lt;/queryfolder&gt;&lt;/querydefinition&gt;&lt;/igroupsecurityservice&gt;&lt;/string,&gt;&lt;/workitemstore&gt;\n\n\nNow we get to the interesting part: unit testing. Let’s start off assuming we have a test project that we can run the tests against. A CopyTest would look something like this:\n\n[TestMethod]&lt;br&gt;public void TestCopyWithDependencies()&lt;br&gt;{&lt;br&gt; var tpc = TfsTeamProjectCollectionFactory.GetTeamProjectCollection(new Uri(\"http://localhost:8080/tfs/defaultcollection\"));&lt;br&gt; var target = new WorkItemCopyer(tpc, \"Code11\");&lt;br&gt;&lt;br&gt; // test finding the query&lt;br&gt; var query = target.FindQuery(\"Tasks_Release1_Sprint1\");&lt;br&gt; Assert.IsNotNull(query);&lt;br&gt;&lt;br&gt; // test running the query&lt;br&gt; target.RunQuery(query);&lt;br&gt; Assert.IsTrue(target.WorkItems.Count &amp;gt; 0);&lt;br&gt;&lt;br&gt; // test copy&lt;br&gt; var count = target.CopyWorkItems(@\"Code11\\Release 1\\Sprint 2\");&lt;br&gt; Assert.AreEqual(target.WorkItems.Count, count);&lt;br&gt;}\n\n\nBut there’s a problem here: if we run these tests and the server is down, they’ll fail. If someone renames the query, the tests will fail. If for some reason the query return 0 work items, the test will at best be inconclusive. So we clearly need to isolate the test from the TFS server. Enter the Fakes framework.\n\nFakes\n\nThe Fakes Framework came out of the Moles Framework from the MS RiSE team. It allows you to isolate your test code from almost anything using an interception mechanism.\n\nTo use Fakes, you’ll first need to create the Fake assemblies. We’ll right click each TeamFoundation dll reference, select “Create Fakes” and we’ll be ready to go.\n\n\n\n\nYou’ll see the Fakes assemblies in your References now.\n\n\n\n\nFaking enough to Instantiate a WorkItemCopyer\n\nThe first thing you need to know about fakes is that they only work inside a ShimsContext, which you can wrap into a using. In order to construct the WorkItemCopyer, we’re going to need a TeamProjectCollection object. So let’s see if we can fake it:\n\n[TestMethod]&lt;br&gt;public void TestCopyerInstantiate()&lt;br&gt;{&lt;br&gt; using (ShimsContext.Create())&lt;br&gt; {&lt;br&gt; // set up the fakes&lt;br&gt; var fakeTPC = new ShimTfsTeamProjectCollection();&lt;br&gt; &lt;br&gt; var target = new WorkItemCopyer(fakeTPC, \"Code11\");&lt;br&gt; Assert.IsNotNull(target);&lt;br&gt; }&lt;br&gt;}\n\n\nIf you run this code, you’ll get an error in the WorkItemCopyer constructor:\n\n\n\n\nThe GetService method seems to be confused. We’ll need to fake that call to return a fake WorkItemStore. So we create a fake store (newing up a ShimWorkItemStore). But now how do we fix the GetService call? You’ll notice it’s fake counterpart is not on the ShimTeamProjectCollection class. This is because the method doesn’t exist on the TeamProjectCollection class, but on it’s base class, TfsConnection. Now according to the MSDN Fakes documentation (which talks about faking methods in base classes), I would have expected this code to work:\n\nvar fakeStore = new ShimWorkItemStore();&lt;br&gt;var fakeTPC = new ShimTfsTeamProjectCollection();&lt;br&gt;var fakeBase = new ShimTfsConnection(fakeTPC);&lt;br&gt;fakeBase.GetServiceOf1&lt;workitemstore&gt;(() =&amp;gt; fakeStore);&lt;/workitemstore&gt;\n\n\nBut for some reason, this doesn’t work. So we’ll do the next best thing is to fake the GetService method for all instances of TfsConnection (which will include and TeamProjectCollection object as well). Here’s the code now:\n\n[TestMethod]&lt;br&gt;public void TestCopyerInstantiate()&lt;br&gt;{&lt;br&gt; using (ShimsContext.Create())&lt;br&gt; {&lt;br&gt; // set up the fakes&lt;br&gt; var fakeStore = new ShimWorkItemStore();&lt;br&gt; var fakeTPC = new ShimTfsTeamProjectCollection();&lt;br&gt; ShimTfsConnection.AllInstances.GetServiceOf1&lt;workitemstore&gt;((t) =&amp;gt; fakeStore);&lt;br&gt; &lt;br&gt; var target = new WorkItemCopyer(fakeTPC, \"Code11\");&lt;br&gt; Assert.IsNotNull(target);&lt;br&gt; }&lt;br&gt;}&lt;/workitemstore&gt;\n\n\nNow we get a bit further – we get to the code that initializes the QueryHierarchy property and then we get a ShimNotImplementedException:\n\n\n\n\nThis is because the getter method Projects on our fake store is not faked. So we’ll change the code to return a fake TeamProjectCollection, which in turn needs a fake string indexer method to get a TeamProject which in turn needs a fake QueryHierarchy getter method… to save time, I’ll show you the completed code:\n\n[TestMethod]&lt;br&gt;public void TestCopyerInstantiate()&lt;br&gt;{&lt;br&gt; using (ShimsContext.Create())&lt;br&gt; {&lt;br&gt; // set up the fakes&lt;br&gt; var fakeHierarchy = new ShimQueryHierarchy();&lt;br&gt; var fakeProject = new ShimProject()&lt;br&gt; {&lt;br&gt; NameGet = () =&amp;gt; \"TestProject\",&lt;br&gt; QueryHierarchyGet = () =&amp;gt; fakeHierarchy&lt;br&gt; };&lt;br&gt; var fakeProjectCollection = new ShimProjectCollection()&lt;br&gt; {&lt;br&gt; ItemGetString = (projectName) =&amp;gt; fakeProject&lt;br&gt; };&lt;br&gt; var fakeStore = new ShimWorkItemStore()&lt;br&gt; {&lt;br&gt; ProjectsGet = () =&amp;gt; fakeProjectCollection&lt;br&gt; };&lt;br&gt; var fakeTPC = new ShimTfsTeamProjectCollection();&lt;br&gt; ShimTfsConnection.AllInstances.GetServiceOf1&lt;workitemstore&gt;((t) =&amp;gt; fakeStore); &lt;br&gt;&lt;br&gt; // test&lt;br&gt; var target = new WorkItemCopyer(fakeTPC, \"Code11\");&lt;br&gt; Assert.IsNotNull(target);&lt;br&gt; }&lt;br&gt;}&lt;/workitemstore&gt;\n\n\nSo far so good: we can at least instantiate the WorkItemCopyer. In the Part 2 post we’ll fake some Query folders and Query objects as well as some WorkItems so that we can complete a test for copying work items.\n\nHappy faking!\n",
      "categories": [],
      "tags": ["tfsapi","testing"],
      
      "collection": "posts",
      "url": "/using-the-fakes-framework-to-test-tfs-api-code-part-1-of-2/"
    },{
      
      "title": "Using the Fakes Framework to Test TFS API Code (Part 2 of 2)",
      "date": "2012-03-20 20:22:00 +0000",
      
      "content": "In Part 1, we started faking some TFS objects. We got as far as faking the TeamProjectCollection and WorkItemStore. In this post, we’ll complete the test for copying work items by providing a fake QueryHierarchy and a fake list of WorkItems.\n\nBinding IEnumerables\n\nSince the QueryHierarchy is an IEnumerable, we’ll need to either fake the GetEnumerator() method, or find a way of making the fake QueryHierarchy enumerable! The problem with trying to fake the GetEnumerator() method is that you end up in an infinite loop – if you’re trying to get the enumerator for sub nodes of the hierarchy, you’ll also need to call GetEnumerator() which will call GetEnumerator() and so on and so on… so we’ll see if we can make the fake QueryHiearchy behave like an enumerable object. Which is really quite easy when you know how!\n\nWe create a list of QueryFolders (outside the ShimsContext, since we want these object to be real and not fake) and then call the Bind() method on the fake QueryHierarchy:\n\nvar myQueries = new QueryFolder(\"My Queries\");&lt;br&gt;var sharedQueries = new QueryFolder(\"Team Queries\");&lt;br&gt;sharedQueries.Add(new QueryDefinition(\"My Tasks\", \"SELECT System.Id FROM WorkItems WHERE System.WorkItemType = 'Task'\"));&lt;br&gt;sharedQueries.Add(new QueryDefinition(\"My Bugs\", \"SELECT System.Id FROM WorkItems WHERE System.WorkItemType = 'Bug'\"));&lt;br&gt;var topQueries = new List&lt;queryitem&gt;() { myQueries, sharedQueries };&lt;br&gt;&lt;br&gt;using (ShimsContext.Create())&lt;br&gt;{&lt;br&gt; // set up the fakes&lt;br&gt; var fakeHierarchy = new ShimQueryHierarchy();&lt;br&gt; fakeHierarchy.Bind(topQueries);&lt;br&gt;&lt;/queryitem&gt;\n\n\nNow when we run the code, the QueryHierarchy behaves like an enumeration and a call to find the “My Tasks” query will result in a QueryDefinition being returned!\n\nFaking an Interface\n\nThe next thing we’ll need to fake is the RunQuery() method on the Query. The first snag we’ll hit is that the RunQuery() method in the WorkItemCopyer uses the IGroupSecurityService to resolve the current user’s display name for any @me macros in the query. So we’ll first need to fake that. Hang on – how do you fake an interface?? Enter Stubs.\n\nSo far we’ve only used Shims. For faking interfaces, we’ll need to switch to Stubs. The only method on the IGroupSecurityService interface that we want to fake is the ReadIdentity() method. So we can fake that and return a fake Identity object. Again, we’ll run into the problem of wanting a real object in a fake world (i.e. the ShimsContext) so we’ll use a sneaky way of working around that. Here’s the code for the fake IGroupSecurityService:\n\nvar fakeSecurityService = new StubIGroupSecurityService()&lt;br&gt;{&lt;br&gt; ReadIdentitySearchFactorStringQueryMembership = (searchFactor, criteria, identity) =&amp;gt; ShimsContext.ExecuteWithoutShims&lt;identity&gt;(() =&amp;gt; new Identity() { DisplayName = \"Bob\" })&lt;br&gt;};&lt;br&gt;ShimTfsConnection.AllInstances.GetServiceOf1&lt;igroupsecurityservice&gt;((t) =&amp;gt; fakeSecurityService);&lt;br&gt;&lt;/igroupsecurityservice&gt;&lt;/identity&gt;\n\n\nThis will create the Identity object “outside” the current ShimsContext. Of course, we need to tell the TfsConnection GetService call to return the fake security service when asked for an IGroupSecurityService (which is done in the last line above).\n\nFaking Constructors\n\nThe next thing that the WorkItemCopyer code does is instantiate a Query object in order to execute the Work Item Query. We’ll need to fake the constructor in order to return a fake Query. The method we particularly want to fake out on the Query object is the RunQuery() method, which is going to return a WorkItemCollection. We’ll want to create a list of Work Items and bind a fake WorkItemCollection to the list so that we can enumerate the fake results. There is also a call to the IterationPath setter and a couple of getters on the WorkItem itself, so we’ll fake that at the same time. Here’s the code:\n\nShimQuery.ConstructorWorkItemStoreStringIDictionary = (q, store, text, dict) =&amp;gt;&lt;br&gt;{&lt;br&gt; new ShimQuery(q)&lt;br&gt; {&lt;br&gt; RunQuery = () =&amp;gt;&lt;br&gt; {&lt;br&gt; var workItemType = new ShimWorkItemType()&lt;br&gt; {&lt;br&gt; NameGet = () =&amp;gt; \"Task\"&lt;br&gt; };&lt;br&gt; var list = new List&lt;workitem&gt;()&lt;br&gt; {&lt;br&gt; new ShimWorkItem()&lt;br&gt; {&lt;br&gt; IdGet = () =&amp;gt; 12,&lt;br&gt; TitleGet = () =&amp;gt; \"Some Work Item\",&lt;br&gt; TypeGet = () =&amp;gt; workItemType,&lt;br&gt; IterationPathSetString = (path) =&amp;gt; { },&lt;br&gt; }&lt;br&gt; };&lt;br&gt; var workItems = new ShimWorkItemCollection()&lt;br&gt; {&lt;br&gt; CountGet = () =&amp;gt; list.Count&lt;br&gt; };&lt;br&gt; workItems.Bind(list);&lt;br&gt; return workItems;&lt;br&gt; }&lt;br&gt; };&lt;br&gt;};&lt;br&gt;&lt;/workitem&gt;\n\n\nFinally, we’ll create a couple of counters to make sure that our code calls the copy and the save methods of the work items for each work item.\n\nvar copyCount = 0;&lt;br&gt;ShimWorkItem.AllInstances.Copy = (w) =&amp;gt;&lt;br&gt;{&lt;br&gt; copyCount++;&lt;br&gt; return new ShimWorkItem(w);&lt;br&gt;};&lt;br&gt;var saveCount = 0;&lt;br&gt;ShimWorkItem.AllInstances.Save = (_) =&amp;gt; saveCount++;&lt;br&gt;&lt;br&gt;// test instantiate&lt;br&gt;var target = new WorkItemCopyer(fakeTPC, \"Code11\");&lt;br&gt;Assert.IsNotNull(target);&lt;br&gt;&lt;br&gt;// test find query&lt;br&gt;var query = target.FindQuery(\"My Tasks\");&lt;br&gt;Assert.IsNotNull(query);&lt;br&gt;&lt;br&gt;// test run query&lt;br&gt;target.RunQuery(query);&lt;br&gt;Assert.AreEqual(1, target.WorkItems.Count);&lt;br&gt;&lt;br&gt;// test copy work items&lt;br&gt;var count = target.CopyWorkItems(\"Test Iteration\");&lt;br&gt;Assert.AreEqual(1, count);&lt;br&gt;Assert.AreEqual(1, copyCount);&lt;br&gt;Assert.AreEqual(1, saveCount);&lt;br&gt;\n\n\nVoila! We’ve been able to test our code without actually connecting to a real TFS service. And coverage? Well, it’s up to 97% for the WorkItemCopyer class. Not too bad!\n\nThe completed solution is available on my skydrive.\n\n(More) Happy Faking!\n",
      "categories": [],
      "tags": ["tfsapi","testing"],
      
      "collection": "posts",
      "url": "/using-the-fakes-framework-to-test-tfs-api-code-part-2-of-2/"
    },{
      
      "title": "More on Fakes – the beta has issues",
      "date": "2012-04-04 18:34:00 +0000",
      
      "content": "Thanks to Peter Provost for helping answer a couple of questions I had about Fakes – you can look at some of my code in my previous posts about using Fakes with the TFS API.\n\nThere were two scenarios that I hit snags with. The first was faking methods in a class that are actually defined on the class’s base class. The second was faking an ObservableCollection.\n\nFaking Base Class Methods\n\nConsider this code:\n\n[TestMethod]&lt;br&gt;public void BaseMethodWontWorkInBeta()&lt;br&gt;{&lt;br&gt; using (ShimsContext.Create())&lt;br&gt; {&lt;br&gt; var shimWorkItemStore = new ShimWorkItemStore();&lt;br&gt; var shimTfsTeamProjectCollection = new ShimTfsTeamProjectCollection();&lt;br&gt; var shimTfsConnection = new ShimTfsConnection(shimTfsTeamProjectCollection);&lt;br&gt; shimTfsConnection.GetServiceOf1&lt;workitemstore&gt;(() =&amp;gt; shimWorkItemStore);&lt;br&gt;&lt;br&gt; var workItemStore = shimTfsTeamProjectCollection.Instance.GetService&lt;workitemstore&gt;();&lt;br&gt;&lt;br&gt; Assert.AreSame(shimWorkItemStore.Instance, workItemStore);&lt;br&gt; }&lt;br&gt;}&lt;/workitemstore&gt;&lt;/workitemstore&gt;\n\n\nThe exercise here is to try to fake the TfsTeamProjectCollection.GetService method. However, we can’t do it directly on the TfsTeamProjectCollection object, since the method is defined in its base class, TfsConnection.\n\nIn line 8, we’re using the ShimTfsConnection constructor that takes in an instance of its child class (the TfsTeamProjectCollection) in order to override the method in the child class. This should work, but Peter told me that this doesn’t work in the Beta – it’ll work when the next release of fakes comes out. The workaround is to use the ShimTfsConnection.AllInstances class to override the GetService method (see my earlier posts where I show how to do this).\n\nFaking ObservableCollection\n\nLet’s look at some more code:\n\n[TestMethod]&lt;br&gt;public void BindWontWorkBecauseOfObservableCollectionInBeta()&lt;br&gt;{&lt;br&gt; using (ShimsContext.Create())&lt;br&gt; {&lt;br&gt; var ss = new StubISharedStep();&lt;br&gt; var sharedStepReference = new StubISharedStepReference&lt;br&gt; {&lt;br&gt; FindSharedStep = () =&amp;gt; ss&lt;br&gt; };&lt;br&gt;&lt;br&gt; var actions = new List&lt;itestaction&gt;&lt;br&gt; {&lt;br&gt; sharedStepReference&lt;br&gt; };&lt;br&gt;&lt;br&gt; var fakeTestActions = new ShimTestActionCollection();&lt;br&gt; fakeTestActions.Bind((IList&lt;itestaction&gt;)actions);&lt;br&gt;&lt;br&gt; Assert.AreEqual(1, fakeTestActions.Instance.Count);&lt;br&gt;&lt;br&gt; var ssr = (ISharedStepReference)(fakeTestActions.Instance[0]);&lt;br&gt; Assert.AreSame(sharedStepReference, ssr);&lt;br&gt;&lt;br&gt; Assert.AreSame(ss, ssr.FindSharedStep());&lt;br&gt; }&lt;br&gt;}&lt;/itestaction&gt;&lt;/itestaction&gt;\n\n\nThe point of this code is to try to fake a TestActionCollection, which is an ObservableCollection. I had successfully managed to fake other collections that were not ObservableCollections (like a WorkItemCollection), and to do that I used the Bind() method of the Shim to bind the fake collection to a list of items. However, I couldn’t get the code to work on the ObservableCollection. (In the above code, I get a NullReferenceException on line 18 when calling the Bind() method.\n\nPeter confirmed that the reason for this is twofold: one, I hadn’t added Fakes for System, which is where the ObservableCollection class resides. This is a performance optimization that the Fakes creation employs so that they don’t create fakes for classes that you’ll never use.\n\nSecondly, and rather unfortunately, even adding that Fake doesn’t work in the Beta. The Fakes creation uses a white-list to generate fakes in the System namespace, and the ObservableCollection isn’t on the white-list in the Beta. In the next release, the System white-list will be customizable, so I am looking forward to getting my grubby paws on that!\n\nUnfortunately, this scenario has no work-around in the Beta, so if you’re trying to Fake an ObservableCollection, you’ll have to wait for the next release of Fakes.\n\nI really like the Fakes framework and what it can do for testing, so I am looking forward to seeing what else comes out of the woodwork.\n\nHappy faking!\n",
      "categories": [],
      "tags": ["testing"],
      
      "collection": "posts",
      "url": "/more-on-fakes-the-beta-has-issues/"
    },{
      
      "title": "VS 2012 RC – Fakes Bugs Fixed",
      "date": "2012-06-20 19:27:00 +0000",
      
      "content": "In a previous post about the MS Fakes framework, I made mention of some bugs that appeared in the Beta. I finally had some time to test out the same code in the RC, and I am pleased to tell you that the bugs have been fixed (well, the ones I found anyway!).\n\nUpgrading Gotcha\n\nThe only gotcha I came across appears to be a snag when you open in the RC a test project that was created in the Beta. I initially simply opened up the code that I had from the Beta, updated references to TeamFoundation dlls from version 10 to version 11, regenerated the fakes and tried to run the tests. FAIL – the tests were failing with “ShimNotSupportedExceptions”. After trying various things, I noticed some text in the Output window from the “Tests” section (the Show output from dropdown):\n\n\n \nFailed to configure settings for runsettings plugin ‘Fakes’ as it threw following exception:‘Object reference not set to an instance of an object’Please contact the plugin author.\n\n\n\n\n\nFor some reason, the Fakes framework wasn’t generating fakes for the v 11 assemblies correctly. I eventually created a new Test project in VS 2012 RC, copied across the code and re-added references and fakes, and voila, everything is working great!\n\nI’m not sure if this is a bug or not, so if you’re getting this error, my suggestion is to just create a new Test project in the RC and copy the code across.\n\nNow back to (fake) testing…\n",
      "categories": [],
      "tags": ["development"],
      
      "collection": "posts",
      "url": "/vs-2012-rc-fakes-bugs-fixed/"
    },{
      
      "title": "Microsoft Fakes – Customizing the System Whitelist (or, enabling Fakes for classes you’ve always wanted to fake, like WebClient)",
      "date": "2012-06-25 20:35:00 +0000",
      
      "content": "So you’re sitting down planning some tests for your shiny new code, only to find that your code uses WebClient to download a file. No problem – you’ve been reading about Microsoft’s new Fakes framework, so you just right-click the System reference in your test project and select “Create Fakes” and you get a bunch of cool fakes to work with.\n\nBut going a bit deeper into the Rabbit Hole, you realize that there are no fakes for System.Net classes. What gives?\n\nThe White-List\n\nIt turns out that when you right-click a reference and select “Add Fakes” a fakes file is created for that assembly in the Fakes folder. When you add a Fakes lib for System, you in fact get 2 fakes files: one for mscorlib and one for System.\n\n\n\n\nSince System is a large library, the Fakes framework doesn’t automatically generate you a fake for every System class. One of the classes that doesn’t have a fake created by default is System.Net.WebClient. In order to force a fake Shim for this class, you’ll need to override the default “white-list” of classes. Fortunately this is relatively straight-forward to do.\n\nThe Fakes Config File\n\nDouble-click System.fakes to open it in the editor – it’s just an XML file. It does come with an XSD, so as you type in this file IntelliSense will guide you (just like the Force, Luke). Within the  tag, add a  tag and within that an  tag and you’re good to go. Here’s my Fakes file for adding a WebClient fake:\n\n&lt;fakes xmlns=\"http://schemas.microsoft.com/fakes/2011/\"&gt;&lt;br&gt; &lt;assembly name=\"System\" version=\"4.0.0.0\"&gt;&lt;br&gt; &lt;shimgeneration&gt;&lt;br&gt; &lt;add fullname=\"System.Net.WebClient\"&gt;&lt;br&gt; &lt;/add&gt;&lt;/shimgeneration&gt;&lt;br&gt;&lt;/assembly&gt;&lt;/fakes&gt;&lt;br&gt;\n\n\nRemember to recompile!\n\nHere’s some example code using the WebClient fake:\n\npublic class HttpFunctions&lt;br&gt;{&lt;br&gt; public void Download(string url, string fileName)&lt;br&gt; {&lt;br&gt; var w = new WebClient();&lt;br&gt; w.DownloadFile(url, fileName);&lt;br&gt; &lt;br&gt; }&lt;br&gt;}&lt;br&gt;&lt;br&gt;[TestClass]&lt;br&gt;public class UnitTest1&lt;br&gt;{&lt;br&gt; [TestMethod]&lt;br&gt; public void TestFakeWebClient()&lt;br&gt; {&lt;br&gt; using (ShimsContext.Create())&lt;br&gt; {&lt;br&gt; ShimWebClient.AllInstances.DownloadFileStringString = (w, url, fileName) =&amp;gt;&lt;br&gt; {&lt;br&gt; File.WriteAllText(fileName, url);&lt;br&gt; };&lt;br&gt;&lt;br&gt; var c = new HttpFunctions();&lt;br&gt; string guidFileName = Guid.NewGuid().ToString() + \".html\";&lt;br&gt; c.Download(\"http://www.bing.com\", guidFileName);&lt;br&gt;&lt;br&gt; Assert.IsTrue(File.Exists(guidFileName));&lt;br&gt; string contents = File.ReadAllText(guidFileName);&lt;br&gt; Assert.AreEqual(\"http://www.bing.com\", contents);&lt;br&gt; File.Delete(guidFileName);&lt;br&gt; }&lt;br&gt; }&lt;br&gt;}&lt;br&gt;\n\n\nHappy faking!\n",
      "categories": [],
      "tags": ["testing"],
      
      "collection": "posts",
      "url": "/microsoft-fakes-customizing-the-system-whitelist-or-enabling-fakes-for-classes-youve-always-wanted-to-fake-like-webclient/"
    },{
      
      "title": "Build with a Hosted Build Controller: A First Attempt",
      "date": "2012-07-03 17:48:00 +0000",
      
      "content": "I am working on some code for the TFS Tester Power Tool with my colleague Anna Russo (who just got her first MVP award!) and we’re using TFS Preview for source control and work item tracking. From the start I wanted to get some unit tests and builds up and running. The challenge for the unit testing side was that the tool works against a Team Foundation Server, so testing required some sort of mocking or faking.\n\nAt around the same time I started looking at the unit tests, Microsoft released a beta of the Fakes Framework. I liked the look of the framework (especially since I had done some stuff with its predecessor, Moles), and decided to write the tests using it. However, I hit a snag with a bug in the beta. Fortunately, the RC fixed the issue, and so I was able to get the tests running.\n\nSo once Anna had granted me EditBuildDefinition rights on the hosted Team Project, I was ready to spin my first hosted build.\n\nHere are my experiences.\n\nCloaked Build Drops folder\n\n\n\nI never check-out my entire source control folder for a build (and neither should you!). When creating a build, make sure the workspace is the minimum codebase that you need. Since I didn’t have a solution open, the workspace defaulted to the root of the Team Project, and you can see that it adds a cloak for the source controlled drops folder. At first I was a little puzzled at this, and then I realised that if you do check out your entire repository for your build, you probably don’t need the previous build outputs, so this default makes sense. However, when I changed the root folder to a subfolder a bit deeper in, and didn’t delete the cloak of the drops folder – the build failed since the cloak didn’t have an active parent folder. So I just deleted the cloaked entry.\n\nSymbols\n\n\n\nI like that you can copy the output of the build to a source controlled folder – this makes sense for TFS in the cloud. However, I tried to then add a source control folder as the destination for the symbols, and of course the build failed since the symbols target must be a valid UNC (I wonder if there’s a way to get the symbols into Source Control…).\n\nHosted Build Queue Summary\n\nI like the Queue Summary page that comes up when you queue a 2012 build – it tells you how long the average build time for this build definition is and where your build is in the queue.\n\n\n\nEnabling Code Coverage\n\nI think that if you have unit tests, you may as well enable code coverage. Enabling code coverage in a 2012 build is a little different than it was in 2010 (where you did it via a runsettings file). In a 2012 build (this applies to on-premises as well as hosted), you need to edit the “Automated Tests” parameter of the build (press the ellipsis button in the right-hand column for this parameter). In the “General” tab, you’ll see a drop-down under “Options” that you can select to enable code coverage.\n\n\n\nParameter name: Input (Value cannot be null)\n\nSo I ran the build again and I bumped into a strange error.\n\n\n\n\nThe error text wasn’t too helpful, so I queued the build again and changed the logging level to “Diagnostic”. That wasn’t too helpful either, except that I could see the error was happening in the catch of the Try-Catch around the testing activities.\n\nAfter digging around in my ChampsList mail (one of the advantages of being an MVP) I came across Neno Loje’s mail about the same issue. Turns out there was a bug in the workflow which was fixed – make sure you use the DefaultTemplate11.1.xaml (as opposed to DefaultTemplate11.xaml).\n\nCode Coverage and Fakes – not friends\n\nSo now I noticed that the tests failed – even though I’ve run them on a local on-premises TeamBuild with no issues. I disabled Code Coverage, and the tests pass! So it seems at this point in time I can get the tests to pass, as long as I don’t enable Code Coverage. I’ve mailed the ChampsList, so hopefully I’ll get a resolution to this soon.\n\nUpdate: see the follow up to this in this post.\n\nSummary\n\nAll in all, it wasn’t too different getting a hosted build to work than getting an on-premises build to work. Once I get some resolution to the code coverage issue, I’ll get exactly the same functionality (I just have to figure out how to handle debug symbols elegantly…).\n\nHappy building!\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/build-with-a-hosted-build-controller-a-first-attempt/"
    },{
      
      "title": "2012 Lab Management Standard Environment: Configuring UI Tests Agent Identity Problem",
      "date": "2012-07-12 21:58:00 +0000",
      
      "content": "I am a huge fan of Lab Management. Being able to manage test rigs centrally (and, if you’re using Hyper-V, self-provisioning and self-servicing) is a huge productivity bonus.\n\nI wanted to test out a Standard Environment (what used to be called Physical Environment in TFS 2010). I am using Brian Keller’s RC VM and another VM (WebTest) for my test machine, both of which are workgrouped (and not connected to any domain).\n\nWhen configuring the environment, I came across a rather strange error. One of the wizard screens when you create a Standard Environment allows you to configure the environment for UI testing (meaning that the test agent in your rig will run in interactive mode). The dialog requests that you specify the username and password for the test agent. I assumed that this would require an account on the test machine, so I specified a local account (i.e. a WebTest account).\n\n\n\n\nUnfortunately, that failed verification with the following error:\n\n\nVerify that the machines are accessible using the user name and password provided. The credentials provided for configuring the test agent to run as process are not valid. Provide valid credentials.\n\nThat didn’t make much sense to me, since I was providing credentials that I was absolutely positive worked on the test machine.\n\nAfter trying various things and searching online, I realized that you need the account for the Test Agent to be a shadow account of the one on TFS (i.e. same name and password on both machines) when you’re on a workgroup. If both machines are on a domain, you just need to make sure the domain user for the Test Agent is an admin on the test machine. Oh, and if you’re going to use the build-deploy-test workflow, make sure that identity also has read-access onto your drops folder!\n\nHappy testing!\n",
      "categories": [],
      "tags": ["labmanagement"],
      
      "collection": "posts",
      "url": "/2012-lab-management-standard-environment-configuring-ui-tests-agent-identity-problem/"
    },{
      
      "title": "Code Coverage doesn’t work with Fakes on Hosted Build",
      "date": "2012-07-13 19:01:00 +0000",
      
      "content": "In my post about hosted build, I discovered that if you enable code coverage on unit tests that use the Fakes framework, the unit tests fail (even though the tests pass without code coverage turned on). The error is a “ShimNotSupportedException”.\n\nI mailed the ChampsList, and it turns out that there is a problem with the hosted build servers for this scenario.In short, the problem has to do with the mix of RC and RTM on the build agent machines, which are running VS 2012 RC, and TfsPreview, that is running a later build (closer to RTM) of TFS.\n\nWhen VS goes to RTM and the build agents are upgraded, this problem should go away. Until then, you’ll have to build the code on a local build server if you need code coverage and use the Fakes framework in your tests.\n\nHappy building!\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/code-coverage-doesnt-work-with-fakes-on-hosted-build/"
    },{
      
      "title": "Unit Testing Javascript in VS 2012",
      "date": "2012-07-23 19:24:00 +0000",
      
      "content": "You’re a responsible developer – you write code, and then you write tests (or, perhaps you even write tests and then write code?). You also love good, solid frameworks that separate concerns and utilize dependency injection and inversion of control and all of that good stuff – you’re using MVC for your web applications.\n\nBut you have a little problem – your pages use Javascript, and that’s difficult to unit test, right? Not any more.\n\nThe Sample Solution\n\nFor this post I’ll use a sample MVC app – of course, the principles apply to any client side app using Javascript, but this is an easy example to use.\n\nCreate a new MVC project in VS, and then add this script to the Scripts folder. This script contains some Javascript for populating a country / state dropdown combo. This is the script that we’re going to be testing.\n\nChutzpah and QUnit for MVC via NuGet\n\nYou’re going to need a Javascript testing framework, and then the ability to run your tests. There are a few frameworks out there, but one I enjoy using is QUnit. But that only gives you the infrastructure – you want to be able to run the tests easily too. Fortunately, VS 2012’s Test Explorer is extensible, so you can add your own “test adapters”. And there’s already one by Matthew Manela for Javascript testing with QUnit called Chutzpah (there is a test runner and a Test Explorer extension – we’ll just need the extension for now).\n\nLet’s create a unit test project and use NuGet to install the QUnit package.\n\n\n  Create a new C# Unit Test project. Delete the UnitTest1.cs file.\n  Open the Package Manager Console (NuGet) and make sure the test project is set to the default project.\n  Type “Install-Package QUnit-MVC” to install QUnit for MVC (this can be used to test any javascript, not just MVC).\n  In the Extension Manager, search for Chutzpah and install the Test Adapter extension. Remember to restart VS once you’ve installed the extension.\n\n    Writing Tests\n  \n\n\nLet’s create a test stub for our unit test. Create a new Javascript file in the test project called “StateDropDownTests.js”. Put in the following code:\n\n/// &lt;reference path=\"../MvcApplication/Scripts/jQuery-1.6.2.js\"&gt;&lt;br&gt;/// &lt;reference path=\"../MvcApplication/Scripts/StateDropDown.js\"&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;module(\"State Dropdown Tests:\")&lt;br&gt;&lt;br&gt;test(\"test States\", function () {&lt;br&gt;&lt;br&gt;});&lt;br&gt;&lt;/reference&gt;&lt;/reference&gt;\n\n\nNow open the Test Explorer and click “Run All”. You should see the test come up in the list of tests and it should fail (since nothing was tested).\n\n\n\n\nThe Real Test Code\n\nSince the StateDropdown script we’re testing accesses objects from the DOM, you’re going to need to add some basic elements. Fortunately QUnit has hooks for adding elements into a test fixture. Add the following code to the module to create the DOM element we need for testing:\n\nmodule(\"State Dropdown Tests:\", {&lt;br&gt; setup: function () {&lt;br&gt; $(\"#qunit-fixture\").append(&lt;br&gt; '&lt;select id=\"countrySelect\" name=\"country\"&gt;' + '&lt;option value=\"USA\"&gt;USA&lt;/option&gt;' + '&lt;/select&gt;' +&lt;br&gt; '&lt;select id=\"stateSelect\" name=\"state\"&gt;' + '&lt;/select&gt;'&lt;br&gt; );&lt;br&gt; }&lt;br&gt;});&lt;br&gt;\n\n\nNow add the following code into your test:\n\ntest(\"test States\", function () {&lt;br&gt; postCountry = '';&lt;br&gt; postState = '';&lt;br&gt;&lt;br&gt; var stateSelect = document.getElementById('stateSelect');&lt;br&gt;&lt;br&gt; // the dropdown includes all the states PLUS a 'Select State' option&lt;br&gt; initCountry('US');&lt;br&gt; equal(stateSelect.options.length, 62 + 1);&lt;br&gt;&lt;br&gt; initCountry('UK');&lt;br&gt; equal(stateSelect.options.length, 43 + 1);&lt;br&gt;&lt;br&gt; initCountry('CA');&lt;br&gt; equal(stateSelect.options.length, 15 + 1);&lt;br&gt;&lt;br&gt; initCountry('ZA');&lt;br&gt; equal(stateSelect.options.length, 0 + 1);&lt;br&gt;});&lt;br&gt;\n\n\nRun the test – and you should see lots of green! Our test is working.\n\nI was going to write a follow on post about getting the tests to run in TeamBuild, but Mathew Aniyan from the MS ALM team beat me to it – here’s a link to his post about\n\n\nJavascript Unit Tests on Team Foundation Service with Chutzpah\n\n\nHappy Javascript testing!\n",
      "categories": [],
      "tags": ["development"],
      
      "collection": "posts",
      "url": "/unit-testing-javascript-in-vs-2012/"
    },{
      
      "title": "Upgrading MSF Agile 5.0 to MSF Agile 6.0: Why does my velocity chart not work?",
      "date": "2012-08-28 16:42:00 +0000",
      
      "content": "So you’ve just upgraded your TFS 2010 server to TFS 2012. And you’ve been using the MSF Agile 5.0 process template. When you open the Web Access webpage, you get a message saying that some features need to be enabled, and you click the link and it “upgrades” your process template so that the Backlogs and Boards work in Web Access. All looks good.\n\nThen you’re doing some Product Backlog planning, and you add some User Stories and you notice that your velocity chart doesn’t show any “active” story points. What’s up?\n\nThe Problem: New States\n\nThe “problem” here is that there are new states for the User Story and Task work items in the MSF Agile 6 template. The velocity chart shows any User Story that is “In Progress” (blue) or “Completed” (green).\n\n\n\n\nIn the figure above, Iteration 1 has 4 story points “delivered” and 13 “in progress” while Iteration 2 has 10 story points “in progress”. However, for this chart to work, you need to put your User Story into the “Resolved” state (which makes it go blue) or “Complete” (which makes it go green). And that’s not ideal, since the User Story is still “active” – it’s not yet actually resolved!\n\nTFS 2012 Process Templates now include two new configuration files: an Agile Process Configuration (for configuring what work items and columns appear on the Backlogs) and a Common Process Configuration for configuring mappings from the boards to work items (among other things like categories). When you click the helpful “enable features” link when you first log into Web Access for the project, TFS creates both the Agile and the Common process config files for you (as well as creating ne work item types like Code Review Request and so on). Let’s export the Common config to see what the “upgrade” process does.\n\nOpen a “Developer Command Prompt” (this gets installed with VS 2012 and has a bunch of TFS and VS programs put into the path) and type the following command:\n\n\nwitamdin exportcommonprocessconfig /collection:http://server:8080/tfs/collection /p:Project /f:ProjectCommonConfig.xml\n\n\n(where server, collection and project are your sever, your collection and your MSF Agile 5 project)\n\nIf you then open the Common Process Configuration file for your “upgraded” MSF Agile 5.0 Template, you’ll see the following mapping:\n\n&lt;requirementworkitems category=\"Microsoft.RequirementCategory\" plural=\"Stories\"&gt; &lt;states&gt; &lt;state type=\"Proposed\" value=\"Active\"&gt; &lt;state type=\"InProgress\" value=\"Resolved\"&gt; &lt;state type=\"Complete\" value=\"Closed\"&gt; &lt;/state&gt;&lt;/state&gt;&lt;/state&gt;&lt;/states&gt;&lt;/requirementworkitems&gt;\n\n\nYou’ll notice that the “Proposed” board state maps to the “Active” work item state, that the board state “InProgress” maps to “Resolved” and “Complete” maps to “Closed”. For the velocity chart, any stories in “Proposed” don’t show, any that are “InProgress” are blue and any that are “Complete” are green. And there lies the problem – the User Story from MSF Agile 5.0 doesn’t have enough states for this to work nicely. It would make more sense to add a “New” state and update the mapping.\n\nThe Solution: Add States\n\nWhat you need to do to “fix” this is to update the User Story and Task work item definitions (essentially adding the “New” state for both work item types) and then update the CommonConfig. Here’s the process for doing this:\n\n\n  Go to Team Explorer in VS and connect to your TFS server. On the Home page, click “Settings” and then “Process Template Manager” and export the MSF Agile 6 template to your hard drive.\n  If you did not customize the User Story or Task work item types AT ALL, then skip this step. Otherwise, use witadmin exportwitd to export your User Story and Task work items to file. Then “merge” the User Story and Task definition files (so port over any customizations you did to the type definition in MSF Agile 5 to the MSF Agile 6 type definition). Make sure you end up with the New state (at least) for both work item types.\n  Use witadmin importwitd to import the new User Story and Task definition files from step 2.\n  If you have no state customizations, then simply use witadmin importcommonprocessconfig to upload the MSF Agile 6 common config to your project. If you have other states, make sure you’ve mapped them correctly (for both RequirementWorkItems and TaskWorkItems sections) in  before you import.\n\n\nYou’re done! Now you can add User Stories and Tasks (both will go into the “New” state). Then you’ll be able to “commit” to User Stories by transitioning them to Active (when they’ll appear in Blue on your velocity chart). And then you’re good to go!\n\nHappy Agile Planning!\n",
      "categories": [],
      "tags": ["alm"],
      
      "collection": "posts",
      "url": "/upgrading-msf-agile-50-to-msf-agile-60-why-does-my-velocity-chart-not-work/"
    },{
      
      "title": "Lab Management: Configuring Workgroup Lab Machines to a TFS on a Domain",
      "date": "2012-08-30 04:45:00 +0000",
      
      "content": "I’ve installed Windows 8 on my laptop, enabled HyperV and upgraded my TFS from 2010 to 2012. Since Lab Management 2012 introduced Standard Environments, I can use my HyperV machines in lab environments right out of Windows 8. However, while I was configuring my lab, I ran into a problem. My laptop is on my work domain, and my lab machines (running on HyperV) are not joined to the domain – they’re in a workgroup.\n\nHaving the TFS on a domain and the lab machines in a workgroup can cause the following symptoms:\n\n\n  When adding the lab machine to an environment, TFS cannot push the test agent to the lab machine. The environment stalls in the “Preparing to install agent”.\n  If you install the test agent on the lab machine manually, you cannot connect to the controller on the TFS machine. The Test Agent Config Tool complains that the test controller is not accessible or the service is not running, even though you’ve verified that it is.\n\n\nHere are a couple of things you can check – once I had done all of these, the environment came online without any issues.\n\nShadow Accounts\n\nSince you’re dealing with a workgroup, you’re going to need to create “shadow” accounts. These are local accounts on each machine that have the same name and password. I created a local account on my TFS machine (note: this is not a domain account – it’s a local account) and did the same on the lab machines, using the same password each time. Then make sure you add the accounts into the local administrators group.\n\nWhen you configure the Test Controller on the TFS machine, use “.\\username” for the logon account and the “lab service” account, where username is the name of the shadow account you created. In the figure below, you’ll see how I’ve done that using my shadow account, “labservice”.\n\n\n\nNetwork Settings on the Lab Machine\n\nSince the Lab machine is on a workgroup, you have to make sure you can successfully communicate with the TFS machine which is on the domain. The first thing you’ll need to do is make sure that you enable File and Printer sharing on the Lab machine. If you right-click the network icon in the Taskbar, select “Open Network and Sharing Center”. Then click the link on the left, “Change advanced sharing settings”. Check that you have File and Printer sharing turned on for your “current” network profile (in the case of my lab machines, since they’re on HyperV, it’s the network connected to my virtual network).\n\n\n\n\nYou’ll also probably need to add an entry to the hosts file on the lab machine. The trick here is to add an entry using the FQDN of the TFS machine, not simply the machine name. On your TFS machine, open a command prompt and ping yourself: if your machine’s name is “mytfs” then type “ping mytfs”. This should show you the FQDN of the TFS machine – something like “mytfs.domain.local” or something. That’s the name you’ll need for the hosts entry on the lab machine. Go to the lab machine and open c:\\windows\\system32\\drivers\\etc\\hosts and add an entry for the TFS machine, using the form IP  FQDN. Here is an example from one of my lab machines:\n\n\n\n\nThat should be it – you can now repair your environment, and you should be good to go!\n\nHappy testing!\n",
      "categories": [],
      "tags": ["labmanagement"],
      
      "collection": "posts",
      "url": "/lab-management-configuring-workgroup-lab-machines-to-a-tfs-on-a-domain/"
    },{
      
      "title": "TechDays and Jhb Training Class",
      "date": "2012-10-02 00:14:00 +0000",
      
      "content": "I will be co-presenting with Ahmed Salijee at TechDays in Durban, Johannesburg and Cape Town (12, 16 and 19 Oct respectively). Check out the website for details on exact dates and venues – there is tons of new stuff to show off, so this is going to be a really exciting event!\n\nHere’s the blurb for the demo Ahmed and I will be doing:\n\nEnd to End Application Lifecycle Management with Visual Studio and Team Foundation Server 2012\n\nMicrosoft’s application lifecycle management tooling is all about enabling teams to deliver great software. In this demo-packed session, learn how to engage early and often with your project’s stakeholders to ensure that your team is building the right software for them. Discover tooling to help you more effectively plan and track work by using the new web-based project management tools. Learn how to bridge the divide between development and operations by utilizing IntelliTrace in your production environments and integrating Microsoft System Center with Microsoft Visual Studio Team Foundation Server. And, developers can stay on-task and “in the zone” with the new “My Work” and code review features. In addition to making your team more productive, we show you how you can boost your overall code quality with new features such as code clone and an overhauled unit testing story in Visual Studio 2012. We will walk you through an end to end scenario from conception to production, leveraging capabilities across the define, build and operate phases. Feature covered include storyboarding, manual and automated testing, build, source control, code review, intellitrace and lab management.\n\nTraining Course\n\nI am also going to be running a 2 day overview class in Johannesburg at Bytes in Midrand on 19/20 November. This course will quickly upgrade you from VS/TFS 2010 to VS/TFS 2012 so that you can get going quickly. The cost is R3500 per seat. You can get the course flyer here.\n\nHope to see you at TechDays or at the training course!\n",
      "categories": [],
      "tags": ["news"],
      
      "collection": "posts",
      "url": "/techdays-and-jhb-training-class/"
    },{
      
      "title": "MVP Status Renewed",
      "date": "2012-10-02 00:20:00 +0000",
      
      "content": "I am pleased to announce that my MVP status was renewed! A large portion of that is due to this blog and all the hits I’ve gotten – so if you keep reading, I’ll keep posting!\n\n\n\n\nThanks also to Microsoft – I have really enjoyed being an MVP so far – having access to the other MVPs and the VS and TFS product groups has made my job much more enjoyable, and made me look good too!\n\nI’ll be here all year, and remember to tip your MVPs…\n",
      "categories": [],
      "tags": ["news"],
      
      "collection": "posts",
      "url": "/mvp-status-renewed/"
    },{
      
      "title": "Hybrid Lab Workflow: Standard Lab Environment with Snapshots",
      "date": "2012-11-01 04:19:00 +0000",
      
      "content": "Arguably one of the best features of TFS is Lab Management. I loved Lab Management in TFS 2010, even though it was a real pain to set up. There were a lot of moving parts and setup was tricky, but once you got SCVMM configured the rest was magic.\n\nTFS 2012 improved Lab Management in three very significant ways:\n\n\n  If you don’t have SCVMM, you can still benefit from Lab Management by using Standard Environments without having to configure anything – just install and configure (or upgrade to) TFS and you can immediately start creating Standard Environments.\n  The Lab, Build and Test Agents were consolidated into one agent that Lab Management pushes to the test machines, so you don’t have to set up any agents manually.\n  You can now run the Build-Deploy-Test workflow against Standard Environments (without snapshots) and not just against Virtual Environments (what are now called SCVMM environments in TFS 2012).\n\n\nLab Management now becomes not only powerful, but much easier.\n\nStandard Environments\n\nStandard Environments allow you to configure Labs using either physical machines or “non SCVMM” virtual machines. These could be VMWare or HyperV without SCVMM. However, one of the things I miss about the old Virtual Environment (now called SCVMM Environments) is the ability to snapshot the test machines. That way you can create a “clean” snapshot that has your test machines configured before any deployments. When the Lab Workflow kicks in, you can just restore to this clean point so that any current deployments or environment changes get reversed.\n\nYou can also create another snapshot after deployment (but before testing) so that you can re-test after any automated tests without having to redeploy or manually reset the environment to the “post-deploy-pre-test” state. For example, if you have a test that adds a Customer record, you can only run it once, since the 2nd time you run the test you’re likely to get “Duplicate” errors.\n\nIn my own day-to-day work, I use HyperV on Windows 8 and so I have to create Standard Environments (since SCVMM doesn’t support Client HyperV hosts). But I often longed for the ability to be able to restore to a clean snapshot and take a post-deployment snapshot in the Lab workflow – I am using VMs after all!\n\nEnter Hybrid Lab Workflow\n\nSo I started tinkering around with controlling HyperV from C# and found that you could do it via WMI. Once I was able to enumerate VMs and their snapshots, create and apply snapshots, I was ready to take the experience I had from creating a TFS 2010 physical environment Build-Deploy-Test workflow and incorporate the snapshot bits and Standard Environments. The Hybrid Lab workflow was born!\n\nHere’s the “simplified” workflow state diagram:\n\n\n\n\nThe blue blocks are what you get with the out-of-the-box workflow for Standard Environments. The orange blocks are what you get if you have an SCVMM environment, and what I added in to get the Hybrid Lab workflow.\n\nDisclaimer\n\nI’ve only tested this on HyperV Client (HyperV on Windows 8). So here is my “get out of jail free” image:\n\n\n\n\nAlso note that I may sporadically tinker around with this code a little, but don’t expect large scale support. The source code is on hybridlabworkflow.codeplex.com, so if you really want to change things, go ahead!\n\nSetting up the Hybrid Lab Workflow\n\nTo set up the Hybrid Lab Workflow, download the latest binaries from hybridlabworkflow.codeplex.com. You should have a few dll’s, PsKill.exe and PsExec.exe and the HybidLabTemplate.xaml. Here are the steps to follow for installation:\n\n\n  Copy the HybridLabTemplate.xaml to the BuildProcessTemplates folder of your Team Project (where the DefaultTemplate, UpgradeTemplate and LabDefaultTemplate.xaml files live).\n  Copy all the other files to a folder under source control.\n  Check in the files (the XAML and the dll’s and exe’s).\n\n  Configure your build controller to point to the folder that contains the Hybrid Lab Workflow binaries (the folder from step 2). You will want to restart Visual Studio at this point so that the wizard can launch properly.\n\n  IMPORTANT : Log into the build machine as the build service and run both PsExec.exe and PsKill.exe. You’ll see a EULA popup appear – this only happens on the machine for the login once, so you only need to do this once. If you don’t do this, the Hybrid Lab Workflow will hang. I’ll explain in my next post why the PsExec and PsKill are necessary at all.\n\n\nPrerequisites for creating A Hybrid Lab Workflow Build\n\nNow you’re ready to create a build. The same “pre workflow” steps for the out-of-the-box workflow apply here, as well as a couple of Hybrid Lab Workflow specific ones:\n\n\n  You need to create a standard environment for the workflow out of VMs. The VMs all need to be in HyperV and on the same host. (I’ve made the Lab Workflow extensible for other virtualization platforms like VMWare, but I’ve only implemented HyperV so far – more in another post).\n  For a “clean” snapshot, snapshot the VMs AFTER the lab setup and configuration has complete (i.e. when the Environment is in the Ready state in MTM)\n  You’ll need to have a build that produces bins that you want to deploy to the environment (this is called the Code build)\n  You’ll need to know how to deploy (either have your commands ready or include scripts into the Code build\n  If you want to run automated tests, you’ll have to have created them, linked them to test cases in a test plan and created automated test settings\n  You’ll need to have an admin username and password for the VM HyperV host machine\n\n\nCreating the Build\n\nClick on “New Build Definition” in the Builds pane of the Team Explorer in VS 2012. Name your workflow on the General tab. On the “Build Defaults” tab set the Staging Location to “This build does not copy output files to a drop location”. Then click on the Process tab.\n\nChange the process template to HybridLabTemplate.xaml (if it’s not in the drop down, then click New, then “Select an existing XAML file” and browse to the source control location of the HybridLabTemplate.xaml).\n\n\n\n\nNext you’ll want to click the ellipses that appear next to “Click here to edit details” of the Lab Process Settings parameter. This will launch the Hybrid Lab Workflow Wizard.\n\nThe Welcome page reminds you of some of the prerequisites for the workflow. Click Next.\n\nThe Environment Page\n\nHere you’ll select your environment from the dropdown. You’ll also see the “Restore Snapshots” checkbox. Select this if you want to restore to the “clean” state. If you do, you’ll have to specify which machines and which snapshots you want to apply.\n\nInitially the Snapshot Information will be empty and you’ll have to connect to your virtual machine host. Click the “…” button next to the “Virtual Host Machine” textbox.\n\n\n\n\nThis will open the “Connect to Virtual Host” dialog. Select a host type (at present you’ll only see HyperV). If you’re running this on the host machine, enter “localhost” for the host name and leave the rest of the boxes empty (and make sure you’re running VS as administrator). If the host is another machine, enter the host name, username, domain and password to connect to the host machine. This account must be an administrator on the host machine. Click OK.\n\n\n\n\nIf all is well, you’ll see the VMs from the host that are in the Lab Environment populated in the grid. Select which snapshot you want to apply (remember this is pre-deployment) for each machine. Leave the snapshot on &lt;&gt; if you don’t want to apply a snapshot on that machine.\n\nThe final bit of information you’ll need here applies only if you plan to run coded UI tests and one of your lab machines is configured to run interactively. Enter the password for the interactive agent account (I’ll explain why you need this in a later post) – this is the same password that you used when you configured the Lab in MTM.\n\n\n\nThe Rest of the Wizard\n\nThe “Configure Build” and “Configure Testing” pages of the wizard work exactly the same as in the Lab Default template. However, there is one more bit of info in the “Deployment Scripts” tab: the “Take a post deployment snapshot” setting. Select this if you want one, and optionally enter a prefix for the snapshot name. Note that all the VMs in the lab will get the same snapshot name when the workflow runs.\n\n\n\n\nClick Finish to close the wizard, and you’re ready to rip.\n\nSample Output\n\nHere’s a screenshot of a successful run, as well as a look at the VMs in my HyperV. You’ll see that the PostDeployment snapshot name is in the build report.\n\n\n\n\nHappy (Hybrid) Lab Workflow-ing!\n",
      "categories": [],
      "tags": ["build","testing","labmanagement"],
      
      "collection": "posts",
      "url": "/hybrid-lab-workflow-standard-lab-environment-with-snapshots/"
    },{
      
      "title": "Developing the Hybrid Lab Workflow",
      "date": "2012-11-01 19:12:00 +0000",
      
      "content": "In my previous post I talked about my Hybrid Lab Workflow – this workflow allows you to do a Build-Deploy-Test workflow against a TFS 2012 Standard Environment, and as long as the environment is composed of VMs and you’re able to connect to the VM Host, then you can apply pre-deployment snapshots and take post-deployment snapshots.\n\nWhy the Wizard needs the Interactive Agent Password\n\nThe trickiest bit of this workflow was getting the Environment ready for Deployment and Testing after a snapshot was applied. I discovered that if you took a snapshot of each the VMs in the Lab after you created the Environment, then if you applied the snapshot the Lab would look ready, but when you tried to Deploy into the Lab (specifically the machine that is running it’s lab agent as an interactive process for Coded UI tests) the deployment task failed.\n\nI discovered (by chance) that if you restart the Interactive Agent on the test machine, the Deployment and Test worked as expected.\n\n\n\n\nThe problem was then achieving this in the workflow, since there is no API for doing this. I tried to achieve this from the controller side too (since if you “offline” and then “online” the agent from the controller in MTM, the lab worked too). Again, no luck, since there’s no API.\n\n\n\n\nSo that’s why I had to add some nastiness to the Wizard and to the workflow. In the Wizard when you create a workflow, you need to provide the password for the Test Agent user – I can get the username from the Lab API.\n\n\n\n\nOnce I have that, I can use PsKill to remotely stop the Test Agent and then PsExec to remotely start the Agent again – that wires it up nicely and the rest of the workflow continues normally.\n\nThere was another alternative – I could invoke the LabEnvironment.Repair() method and specify that the agent needs to be installed on the test machine. This worked, but it took about 15 minutes for the Lab infrastructure to figure out that the Agent was already installed and then to wire it up. Resetting the Agent remotely only takes about 30 seconds, so I decided to go with that instead. I would have preferred a way to do this without a password or PsKill and PsExec, but since there’s no API for this, I went with the “quicker, dirtier” method.\n\nAnyway, if you really don’t like that you can leave the password blank. The bit of the workflow that uses PsKill and PsExec will be bypassed – but then your Agent may be in some weird limbo state because of the snapshot. Maybe this only happens in my scenario and you don’t need to do it.\n\nHappy workflowing!\n",
      "categories": [],
      "tags": ["build","labmanagement"],
      
      "collection": "posts",
      "url": "/developing-the-hybrid-lab-workflow/"
    },{
      
      "title": "Extending Hybrid Lab Workflow Virtual Hosts",
      "date": "2012-11-01 22:08:00 +0000",
      
      "content": "In an earlier post I talked about my Hybrid Lab Workflow – this workflow allows you to do a Build-Deploy-Test workflow against a TFS 2012 Standard Environment, and as long as the environment is composed of VMs and you’re able to connect to the VM Host, then you can apply pre-deployment snapshots and take post-deployment snapshots. I also blogged about the “nastiness” of PsKill and PsExec for getting the Lab into a workable state after snapshots were applied. In this post I’ll talk about how you could use exactly the same workflow for another Virtual Host – say VMWare or something else.\n\nThe Magic – MEF\n\nWhen I first thought of this workflow, I immediately thought that I could make the Virtual machine and Virtual Hosts interfaces in the workflow, so that you could hook into any virtualization platform that you want. Lab Management itself if oblivious to the virtualization platform you use (well, in Standard Environments anyway) since it treats the machines as if they are physical (not through the Virtual Host, which is what happens in SCVMM Environments).\n\nSo I created an IVirtualHost interface and an IVirtualMachine interface. I then created an enumeration of VirtualHostTypes and a VirtualHostFactory that could instantiate an IVirtualHost concrete class based on the enumeration you fed in. I soon realised that if you wanted to add a Host Type, you’d need to recompile the factory and update the workflow – it would be a bit messy. I ideally wanted something a little more “dynamic”. So I thought of the Managed Extensibility Framework (MEF) that is now baked into .NET. It’s designed exactly for this sort of problem.\n\nI created a VirtualHostContainer that could dynamically load assemblies and find IVirtualHost implementations. Then I created a static class that uses the container to enumerate the HostTypes available and get you one when you need it.\n\nVirtualHost Container\n\nnamespace HybridLab.Virtual.Interfaces&lt;br&gt;{&lt;br&gt; internal class VirtualHostContainer&lt;br&gt; {&lt;br&gt; [ImportMany(typeof(IVirtualHost))]&lt;br&gt; private IEnumerable&lt;ivirtualhost&gt; VirtualHosts;&lt;br&gt;&lt;br&gt; internal VirtualHostContainer()&lt;br&gt; {&lt;br&gt; var path = Path.GetDirectoryName(Assembly.GetExecutingAssembly().Location);&lt;br&gt; var dlls = new DirectoryInfo(path).GetFileSystemInfos(\"*.dll\");&lt;br&gt; var catalog = new AggregateCatalog();&lt;br&gt; foreach (var dll in dlls.Where(d =&amp;gt; !(d.Name.StartsWith(\"System\") || d.Name.StartsWith(\"Microsoft\"))))&lt;br&gt; {&lt;br&gt; try&lt;br&gt; {&lt;br&gt; var asm = Assembly.LoadFrom(dll.FullName);&lt;br&gt; var assemblyCatalog = new AssemblyCatalog(asm);&lt;br&gt; if (assemblyCatalog.Parts.Count() &amp;gt; 0)&lt;br&gt; {&lt;br&gt; catalog.Catalogs.Add(assemblyCatalog);&lt;br&gt; }&lt;br&gt; }&lt;br&gt; catch (Exception)&lt;br&gt; {&lt;br&gt; }&lt;br&gt; }&lt;br&gt; new CompositionContainer(catalog).SatisfyImportsOnce(this);&lt;br&gt; }&lt;br&gt;&lt;br&gt; internal List&lt;string&gt; GetHostTypes()&lt;br&gt; {&lt;br&gt; return (from h in VirtualHosts&lt;br&gt; select h.HostType).ToList();&lt;br&gt; }&lt;br&gt;&lt;br&gt; internal IVirtualHost GetHost(string hostType, string hostName, string domain = \"\", string userName = \"\", string password = \"\")&lt;br&gt; {&lt;br&gt; var host = VirtualHosts.Single(h =&amp;gt; h.HostType == hostType);&lt;br&gt; host.Connect(hostName, domain, userName, password);&lt;br&gt; return host;&lt;br&gt; }&lt;br&gt; }&lt;br&gt;}&lt;br&gt;&lt;/string&gt;&lt;/ivirtualhost&gt;\n\n\nAbove is the container code. The VirtualHosts property is attributed with an [ImportMany] attribute. The MEF framework will inject any classes that are attributed with the matching [Export] attribute into this property. This is done on line 28 in the call to SatisfyImportsOnce().\n\nOnce that property is populated, the GetHostTypes() and GetHost() methods are trivial. What’s a little trickier is dynamically loading the assemblies that may (or may not) contain IVirtualHost implementations. For that I just get the current assembly’s location and then try to load each dll (that doesn’t start with Microsoft or System). I create a new AssemblyCatalog from the assembly, and if there are any MEF exports (parts) in the assembly, I add the catalog to an AggregateCatalog. Once I’ve got all the AssemblyCatalogs into the AggregateCatalog, I create a CompositionContainer and tell it to make any hook ups using the SatisfyImportsOnce call on the VirtualHostContainer itself.\n\nVirtual Host Catalog\n\nI’ve wrapped this class into a static class that you can call to get VirtualHosts. Here it is:\n\nnamespace HybridLab.Virtual.Interfaces&lt;br&gt;{&lt;br&gt; public sealed class VirtualHostCatalog&lt;br&gt; {&lt;br&gt; public static List&lt;string&gt; GetHostTypes()&lt;br&gt; {&lt;br&gt; return new VirtualHostContainer().GetHostTypes();&lt;br&gt; }&lt;br&gt;&lt;br&gt; public static IVirtualHost GetHost(string hostType, string hostName, string domain = \"\", string userName = \"\", string password = \"\")&lt;br&gt; {&lt;br&gt; return new VirtualHostContainer().GetHost(hostType, hostName, domain, userName, password);&lt;br&gt; }&lt;br&gt; }&lt;br&gt;}&lt;br&gt;&lt;br&gt;&lt;/string&gt;\n\n\nImplementing IVirtualHost and IVirtualMachine\n\nThese interfaces are really simple (like all good interfaces). When I was implementing them for HyperV (on Windows 8) I realized that you can have a simple interface and a complicated implementation. I suppose that’s the power of interfaces – the interface consumer doesn’t have to care.\n\nnamespace HybridLab.Virtual.Interfaces&lt;br&gt;{&lt;br&gt; public interface IVirtualHost&lt;br&gt; {&lt;br&gt; string HostName { get; }&lt;br&gt; string Domain { get; }&lt;br&gt; string UserName { get; }&lt;br&gt; string Password { get; }&lt;br&gt; string HostType { get; }&lt;br&gt; &lt;br&gt; void Connect(string hostName, string domain, string userName, string password);&lt;br&gt;&lt;br&gt; List&lt;ivirtualmachine&gt; GetVMs();&lt;br&gt; }&lt;br&gt;}&lt;br&gt;&lt;/ivirtualmachine&gt;\n\n\nThe IVirtualHost has a couple of properties – the host type is used dynamically to discover host types. The rest are connection properties – the host name and admin credentials to the host machine. The Connect() method gets called in the VirtualHostContainer, so you don’t actually ever need to call it yourself. The final method is the method to get the list of VMs on the host.\n\nI’ve also supplied an abstract HostBase class to get you started:\n\nnamespace HybridLab.Virtual.Interfaces&lt;br&gt;{&lt;br&gt; public abstract class HostBase : IVirtualHost&lt;br&gt; {&lt;br&gt; public string HostName { get; protected set; }&lt;br&gt; public string Domain { get; protected set; }&lt;br&gt; public string UserName { get; protected set; }&lt;br&gt; public string Password { get; protected set; }&lt;br&gt;&lt;br&gt; public virtual string HostType { get; protected set; }&lt;br&gt;&lt;br&gt; public virtual void Connect(string hostName, string domain, string userName, string password)&lt;br&gt; {&lt;br&gt; HostName = hostName;&lt;br&gt; Domain = domain;&lt;br&gt; UserName = userName;&lt;br&gt; Password = password;&lt;br&gt; }&lt;br&gt;&lt;br&gt; public virtual List&lt;ivirtualmachine&gt; GetVMs()&lt;br&gt; {&lt;br&gt; throw new NotImplementedException();&lt;br&gt; }&lt;br&gt; }&lt;br&gt;}&lt;br&gt;&lt;/ivirtualmachine&gt;\n\n\nThe IVirtualMachine interface is also fairly simple:\n\nnamespace HybridLab.Virtual.Interfaces&lt;br&gt;{&lt;br&gt; public interface IVirtualMachine&lt;br&gt; {&lt;br&gt; void ApplySnapshot(string snapshotName);&lt;br&gt;&lt;br&gt; void CreateSnapshot(string snapshotPrefix);&lt;br&gt;&lt;br&gt; IVirtualHost Host { get; }&lt;br&gt;&lt;br&gt; string Name { get; }&lt;br&gt;&lt;br&gt; string DnsName { get; }&lt;br&gt;&lt;br&gt; List&lt;string&gt; Snapshots { get; }&lt;br&gt; }&lt;br&gt;}&lt;br&gt;&lt;/string&gt;\n\n\nThere’s a reference to the VMs IVirtualHost, a name, a property that returns all the snapshots and the DnsName of the guest OS. Then there are ApplySnapshot() and CreateSnapshot() for doing those operations.\n\nOnce you’ve implemented an IVirtualHost and IVirtualMachine, simply add the MEF Export attribute onto your IVirtualHost implementation and drop the assembly into source control along with the interface and other assemblies for the Hybrid Lab Workflow. You won’t need to change the workflow at all.\n\nHere’s an example TestHost that I implemented for testing the VirtualHostCatalog class:\n\nnamespace HybridLab.Virtual.TestHost&lt;br&gt;{&lt;br&gt; [Export(typeof(IVirtualHost))]&lt;br&gt; public class TestHost : HostBase&lt;br&gt; {&lt;br&gt; public override string HostType&lt;br&gt; {&lt;br&gt; get&lt;br&gt; {&lt;br&gt; return \"Test\";&lt;br&gt; }&lt;br&gt; }&lt;br&gt; }&lt;br&gt;}&lt;br&gt;\n\n\nNote the [Export] attribute decorating the class. Of course you’ll need some more code for a real host implementation!\n\nThat’s all there is to it. If you’re going to be implementing a host and need some help, let me know! Also, I can add it to the Hybrid Lab Workflow project on Codeplex.\n\nHappy implementing!\n",
      "categories": [],
      "tags": ["build","labmanagement"],
      
      "collection": "posts",
      "url": "/extending-hybrid-lab-workflow-virtual-hosts/"
    },{
      
      "title": "Custom Code Review Checkin Policy",
      "date": "2012-12-21 07:33:00 +0000",
      
      "content": "Over the last couple of months, I’ve been doing a lot of VS / TFS 2012 demos, as well as installing / configuring / customizing TFS at customers. Everyone loves the Code Review Workflow, but inevitably the question gets asked, “Can I enforce code review on check-in”? Out of the box you can’t, but I created a custom policy for this.\n\nYou can get the policy from the VS Gallery.\n\nNote: this policy only works with “out of the box” Code Review Request and Response work item types in TFS 2012 and for VS 2012.\n\nYou can also configure how “strict” the review policy should be:\n\n\n  The policy will fail if the Code Review Request is not Closed\n  The policy will fail if any response result is ‘Needs Work’\n  The policy will pass if all response results are ‘Looks Good’ or\n  the policy will pass if all response results are ‘Looks Good’ or ‘With Comments’\n\n\n\nIf the policy fails, you’ll get a friendly message reminding you to get a review!\n\n\n\n\nHappy reviewing!\n",
      "categories": [],
      "tags": ["alm"],
      
      "collection": "posts",
      "url": "/custom-code-review-checkin-policy/"
    },{
      
      "title": "Subtle MSF Agile Enhancement: Adding Bugs to the Backlogs",
      "date": "2013-01-18 20:08:00 +0000",
      
      "content": "TFS’s Work Item Tracking system is amazing – the flexibility and customizability of the system is fantastic. It also allows your tool to enforce your processes – which is always a good thing for efficiency in any team!\n\nWhen I install / configure / customize TFS at my customers, I usually suggest that they start with one of the three out-of-the-box Process Templates – CMMI, Scrum or Agile. Then after having run a couple of sprints or iterations using these templates, decide what changes are required and implement them slowly and over time.\n\nSo how do you decide which template to start off with? At a very high level, CMMI is the most formal of the templates. Scrum and Agile are very similar in most respects, and I only recommend Scrum for Scrum purists. My template of choice for most teams is the MSF Agile Template.\n\nScrum vs Agile\n\nScrum sort-of-noses-ahead-slightly-early-on because:\n\n\n  You can see Bugs along with Product Backlog items\n\n\nAgile comes-from-behind-and-smashes-Scrum-to-smithereens because:\n\n\n  The dashboarding and reporting are more comprehensive out-the-box\n  The bug States make more sense\n  In Scrum, Bugs are Approved and Committed – what does that really mean? I prefer Active-Resolved-Closed\n  Scrum Bugs can’t be “resolved” during Check-in because there is no resolved state\n  Scrum Bugs can’t be verified in MTM, since there is no resolved state\n\n\nSo it’s really the dashboards and Bug lifecycle that pushes me to recommending Agile over Scrum.\n\nBugs in Planning\n\nOne of the pain points in the Agile template is planning around bugs. I see two challenges:\n\n\n  You have a Bug that is known, and you plan to fix it in a future Iteration. To do this (including planning time and resources to do the work) you would have to create a User Story that is a “copy” of the Bug so that the User Story can be part of the backlog and you can log tasks against the bug. This does mean some duplication, and in the latest Agile template (6.1) there is a Reason when you transition from “Active” to “Resolved” called “Copied to Backlog” that is designed for this scenario.\n  You find a Bug during the Sprint and you need to fix it. Again, you’d need to “copy” the Bug to a User Story and add this into the backlog. This will surface as unplanned work, which is what you want.\n\n\nOf course, you can just fix the bug without the User Story, but then you’ll have to decide where to log the tasks so that they actually appear in the backlogs.\n\nThe Solution: Modify Agile Bugs to make them more “Scrummy”\n\nAt one of my customers, we made some minor changes to the Agile template that mitigate the “copy Bug to User Story” pain. It allows you to bring the Bugs into the backlogs (just like the Scrum template) so you end up with the best of both Agile and Scrum templates. The only down side is that if you have a lot of bugs, you can end up cluttering your backlogs, but this is a problem you’d have in the Scrum template anyway.\n\nTo do this you need to add a New state to the Bug Work Item – that will make the Bug and User Story States match more closely (New, Active, Resolved, Closed). User Stories have an extra “Removed” state, but that’s not required on the Bug. Secondly, you need to add the Bug Work Item to the Requirements Category, allowing the Product Backlog to show Bugs.\n\nBefore you start, make sure you backup as you go along (by source controlling your files) and do this on a test TFS Server before attempting this on a production server! If you get the error below, don’t panic. Check your configurations and try again. I wish there was a better description of exactly what is wrong, but unfortunately there doesn’t appear to be any more detail, so you’ll have to revert and try again.\n\n\n\nAdd Story Points and the “New” State to the Bug Work Item\n\nTo edit the Bug Work Item, open it using the Process Template editor or from the command line. I’ll show you how to do this from the command line and in the XML itself. I’m going to connect to localhost (my TFS server) to the DefaultCollection and work with a TeamProject called “Code”. Of course you’ll have to change the server, collection and team project name for your scenario.\n\nFrom a command line, export the Bug Work Item Template:\n\nwitadmin exportwitd /collection:http://localhost:8080/tfs/defaultcollection /p:Code /f:Bug.xml /n:Bug&lt;br&gt;&lt;br&gt;\n\n\nIn the  section, copy the Story Points field from the User Story work item type or just paste in the following:\n\n&lt;field name=\"Story Points\" refname=\"Microsoft.VSTS.Scheduling.StoryPoints\" type=\"Double\" reportable=\"measure\" formula=\"sum\"&gt;&lt;br&gt; &lt;helptext&gt;The size of work estimated for implementing this user story&lt;/helptext&gt;&lt;br&gt;&lt;/field&gt;&lt;br&gt;\n\n\nThen look for the “Planning” group in the  and change it to the following (to add the Story Points control):\n\n&lt;column percentwidth=\"33\"&gt;&lt;br&gt; &lt;group label=\"Planning\"&gt;&lt;br&gt; &lt;column percentwidth=\"100\"&gt;&lt;br&gt; &lt;control fieldname=\"Microsoft.VSTS.Scheduling.StoryPoints\" type=\"FieldControl\" label=\"Story Points\" labelposition=\"Left\"&gt;&lt;br&gt; &lt;control fieldname=\"Microsoft.VSTS.Common.StackRank\" type=\"FieldControl\" label=\"Stack Rank\" labelposition=\"Left\" numberformat=\"DecimalNumbers\" maxlength=\"10\" emptytext=\"&lt;None&gt;\"&gt;&lt;br&gt; &lt;control fieldname=\"Microsoft.VSTS.Common.Priority\" type=\"FieldControl\" label=\"Priority\" labelposition=\"Left\"&gt;&lt;br&gt; &lt;control fieldname=\"Microsoft.VSTS.Common.Severity\" type=\"FieldControl\" label=\"Severity\" labelposition=\"Left\"&gt;&lt;br&gt; &lt;/control&gt;&lt;/control&gt;&lt;/control&gt;&lt;/control&gt;&lt;/column&gt;&lt;br&gt; &lt;/group&gt;&lt;br&gt;&lt;/column&gt;&lt;br&gt;\n\n\nNow edit the  and  elements of the Bug. You can simply copy/paste the  and change the value to “New” to create the New state.\n\n&lt;states&gt;&lt;br&gt; &lt;state value=\"New\"&gt;&lt;br&gt; &lt;fields&gt;&lt;br&gt; ...&lt;br&gt; &lt;/fields&gt;&lt;br&gt; &lt;/state&gt;&lt;br&gt; &lt;state value=\"Active\"&gt;&lt;br&gt; ...&lt;br&gt;&lt;states&gt;&lt;br&gt;&lt;/states&gt;&lt;/state&gt;&lt;/states&gt;\n\n\nThen in  remove the  transition and add the following:\n\n&lt;transition from=\"\" to=\"New\"&gt;&lt;br&gt; &lt;reasons&gt;&lt;br&gt; &lt;defaultreason value=\"New\"&gt;&lt;br&gt; &lt;reason value=\"Build Failure\"&gt;&lt;br&gt; &lt;/reason&gt;&lt;/defaultreason&gt;&lt;/reasons&gt;&lt;br&gt;&lt;/transition&gt;&lt;br&gt;&lt;transition from=\"New\" to=\"Active\"&gt;&lt;br&gt; &lt;reasons&gt;&lt;br&gt; &lt;defaultreason value=\"Activated\"&gt;&lt;br&gt; &lt;/defaultreason&gt;&lt;/reasons&gt;&lt;br&gt; &lt;fields&gt;&lt;br&gt; &lt;field refname=\"Microsoft.VSTS.Common.ActivatedBy\"&gt;&lt;br&gt; &lt;allowexistingvalue&gt;&lt;br&gt; &lt;copy from=\"currentuser\"&gt;&lt;br&gt; &lt;validuser&gt;&lt;br&gt; &lt;required&gt;&lt;br&gt; &lt;/required&gt;&lt;/validuser&gt;&lt;/copy&gt;&lt;/allowexistingvalue&gt;&lt;/field&gt;&lt;br&gt; &lt;field refname=\"Microsoft.VSTS.Common.ActivatedDate\"&gt;&lt;br&gt; &lt;serverdefault from=\"clock\"&gt;&lt;br&gt; &lt;/serverdefault&gt;&lt;/field&gt;&lt;br&gt; &lt;/fields&gt;&lt;br&gt;&lt;/transition&gt;&lt;br&gt;&lt;transition from=\"Active\" to=\"New\"&gt;&lt;br&gt; &lt;reasons&gt;&lt;br&gt; &lt;defaultreason value=\"Deactivated\"&gt;&lt;br&gt; &lt;/defaultreason&gt;&lt;/reasons&gt;&lt;br&gt; &lt;fields&gt;&lt;br&gt; &lt;field refname=\"Microsoft.VSTS.Common.ActivatedBy\"&gt;&lt;br&gt; &lt;empty&gt;&lt;br&gt; &lt;/empty&gt;&lt;/field&gt;&lt;br&gt; &lt;field refname=\"Microsoft.VSTS.Common.ActivatedDate\"&gt;&lt;br&gt; &lt;empty&gt;&lt;br&gt; &lt;/empty&gt;&lt;/field&gt;&lt;br&gt; &lt;/fields&gt;&lt;br&gt;&lt;/transition&gt;&lt;br&gt;\n\n\nNow you can import the Work Item Type to make the change to your Bug Work Item:\n\nwitadmin importwitd /collection:http://localhost:8080/tfs/defaultcollection /p:Code /f:Bug.xml&lt;br&gt;\n\n\nExposing Bugs on the Backlogs\n\nTo do this, you need to edit the Categories and the CommonProcessConfiguration. First let’s export those:\n\nwitadmin exportcategories /collection:http://localhost:8080/tfs/defaultcollection /p:Code /f:Categories.xml&lt;br&gt;&lt;br&gt;witadmin exportcommonprocessconfig /collection:http://localhost:8080/tfs/defaultcollection /p:Code /f:CommonProcessConfig.xml&lt;br&gt;&lt;br&gt;\n\n\nOpen Categories.xml and add Bugs to the Requirements Category:\n\n&lt;category refname=\"Microsoft.RequirementCategory\" name=\"Requirement Category\"&gt;&lt;br&gt; &lt;defaultworkitemtype name=\"User Story\"&gt;&lt;br&gt; &lt;workitemtype name=\"Bug\"&gt;&lt;br&gt;&lt;/workitemtype&gt;&lt;/defaultworkitemtype&gt;&lt;/category&gt;&lt;br&gt;\n\n\nOpen CommonProcessConfig.xml and add the “New” state to the mapping:\n\n&lt;bugworkitems category=\"Microsoft.BugCategory\"&gt;&lt;br&gt; &lt;states&gt;&lt;br&gt; &lt;state type=\"Proposed\" value=\"New\"&gt;&lt;br&gt; &lt;state type=\"InProgress\" value=\"Active\"&gt;&lt;br&gt; &lt;state type=\"Complete\" value=\"Closed\"&gt;&lt;br&gt; &lt;state type=\"Resolved\" value=\"Resolved\"&gt;&lt;br&gt; &lt;/state&gt;&lt;/state&gt;&lt;/state&gt;&lt;/state&gt;&lt;/states&gt;&lt;br&gt;&lt;/bugworkitems&gt;&lt;br&gt;\n\n\nFinally, import the Categories and CommonProcessConfig files:\n\nwitadmin importcategories /collection:http://localhost:8080/tfs/defaultcollection /p:Code /f:Categories.xml&lt;br&gt;&lt;br&gt;witadmin importcommonprocessconfig /collection:http://localhost:8080/tfs/defaultcollection /p:Code /f:CommonProcessConfig.xml&lt;br&gt;&lt;br&gt;\n\n\nThe Results\n\nNow you’ll be able to add Bugs on your Product Backlog, as well as do work breakdowns for Bugs in the Sprint Backlog.\n\n\n\n\nYou’ll probably want to add Work Item Type as a Column to your Product and Iteration Backlogs (press the Column Options button shown in the figure above) or you can also edit the AgileProcessConfig file (works similarly to the CommonProcessConfig – see witadmin exportagileprocessoconfiguration for more details).\n\n\n\n\n\n\n\nThanks to my good friend Theo Kleynhans for working with me on this!\n\nHappy (product back-) logging!\n\nUPDATE: If you want to make some subtle changes to your Task Board (including showing the Type of the “Requirement” and the ID of the Work Item) and you don’t mind an unofficial “hack” – then my good friend Tiago Pascoal has a Task Board Plugin – read about it in this post.\n",
      "categories": [],
      "tags": ["alm"],
      
      "collection": "posts",
      "url": "/subtle-msf-agile-enhancement-adding-bugs-to-the-backlogs/"
    },{
      
      "title": "TFS 2012 Upgrade Bugs–again?",
      "date": "2013-01-28 21:17:00 +0000",
      
      "content": "On January 11, Brian Harry blogged about some bugs in the TFS 2012 upgrade process (as well as the link to the KB patch for fixing the bugs). I was upgrading a customer in December last year and we hit one of the “symptoms” that was fixed with the patch.\n\nI was at another customer about two weeks ago and we applied the patch before importing Team Project Collections and of course we didn’t hit any of the bugs. It was a great week for me!\n\nUnfortunately, I didn’t have a great week last week when I went to upgrade another customer from TFS 2010 to TFS 2012. I applied the patch from Brian’s blog and while the Team Project Collections imported successfully, some other very strange things started to happen.\n\nMost of the “bugs” occur around security – not being able to see collections that you have access to (or seeing collections you don’t have access to) for example. But we also had some obscure issues that were also security related, but were harder to diagnose.\n\nFor example, we had 22 Test Plans, and suddenly 4 of them were “broken”. If you tried to connect to these Test Plans, a popup appeared saying, “Team Foundation Server is currently offline. Try again later”. This of course wasn’t the case, since the server was up and running and we could access 18 other Test Plans! The event logs on the TFS server had an exception “Given key was not present in the dictionary”.\n\nWe also noticed that if you accesses a Test Plan that wasn’t “broken” and went to the Results pane, you got an “Object reference not set to an instance of an object” error message. Not cool…\n\nFortunately, being an MVP has its advantages. I mailed the champs list and within a couple of hours, members of the product team were helping me. Eventually they provided me with a script to run against the TFS database that sorted the weird issues, and my customer was able to get back to work. In fact, when the team provided me the script they told me that they had already found this bug and were including it in a second patch for the upgrade process. I don’t have a timeline on when this patch will be released – but I am confident that the product team has it in hand!\n\nUnofficial Analysis\n\nI through back to why out of the three upgrades I’ve done, two seemed to have issues while the 3rd was error free. In both cases where there were issues, changes had been made to users on the AD domain that the TFS server was on – users had been deleted after they had touched the old TFS. I suspect that something in the identities logic wasn’t catering for “missing” users when you install TFS and then import a Team Project Collection that has “old” users.\n\nQuarterly Pain?\n\nI love the fact that TFS on-premise is going to a quarterly release cycle. This is good news, especially for those of us who like new features and who like to be at the forefront. However, I hope that the product team improves the upgrade process. Of course this process is mammoth, and the number of permutations they have to cater for is staggering, so I’m not saying it’s going to be a cakewalk. However, if we’re going to be eager for four updates a year, we need to feel confident that they’ll “just work”.\n\nA big thank you to Chandru Ramakrishnan and his fellows for the speedy help!\n\nHappy upgrading!\n",
      "categories": [],
      "tags": ["alm"],
      
      "collection": "posts",
      "url": "/tfs-2012-upgrade-bugsagain/"
    },{
      
      "title": "Custom Build Task: Include Merged Changesets (and Work Items) in Build Report",
      "date": "2013-02-04 08:00:00 +0000",
      
      "content": "Update 2013-07-24 : This activity is now part of Community TFS Build Extensions.\n\nI’ve said it before and I’ll say it again – TFS is more than just source control. It’s more than just Work Item Tracking. One of the defining capabilities of TFS is integration across the ALM landscape.\n\nCurrently, if you’re using TeamBuild, you can check code in and build – and the build report will associate the checkins (and their associated work items) with the build. This works great if you don’t have branching… however, if you want to see the changesets that were merged in the build report too, you’re stuck. The information is in TFS – it’s just not easy to surface.\n\nFor example, let’s say you have a DEV-MAIN-LIVE branching hierarchy. Presumably, you’ll work on DEV and do a bunch of checkins. When you do a DEV build, you get a report of all the changes (and work items) that you’ve been working on. So far so good. Now let’s say you have a LIVE build that you merge to once in a while and it’s the LIVE build that you push to the testers. No problem – merge from DEV to MAIN, and then again from MAIN to LIVE. Run the build. Now the tester checks the build report to see what’s “in the build” and they see… nothing. Oh wait, there’s some merge or something – but no work items. What gives?\n\nThe “problem” here is that TeamBuild won’t traverse the merges for the build report. So the only change that’s associated to the LIVE build is the merge from MAIN. Not too helpful.\n\nBut wait: there’s an API that you can use to get the merge sources and so you *could* go and generate your own report. And after years of working with TFS, everyone says you *can* do that in theory, but I haven’t seen anyone actually go and do it.\n\nThe other alternative is to customize the build template to do the heavy lifting for you, so that your LIVE build report shows the merges as well as the merge sources (and their work items).\n\n\nCustom Activity\n\n\nUnfortunately this is not a trivial customization to make. I had to cuddle up around Reflector and manually “re-code” some Activities from the Microsoft.TeamFoundation.Build.Workflow.Activities namespace (the ones I wanted to use are marked internal – sigh). I’m getting quite good at taking code like this:\n\n\n\nParameterExpression expression;\nParameterExpression expression2;\nParameterExpression expression3;\n\nDelegateInArgument&lt;changeset&gt; changeset = new DelegateInArgument&lt;changeset&gt;();\n\nForEach&lt;changeset&gt; each = new ForEach&lt;changeset&gt;();\neach.Values = new InArgument&lt;ienumerable&lt;changeset&gt;&gt;(\n    Expression.Lambda&lt;func&lt;activitycontext, ienumerable&lt;changeset=\"\"&gt;&gt;&gt;(\n        Expression.Call(\n            Expression.Property(\n                Expression.Constant(this, typeof(AssociateChangesets)), \n                (MethodInfo) methodof(AssociateChangesets.get_Changesets)), \n                (MethodInfo) methodof(InArgument&lt;ienumerable&lt;changeset&gt;&gt;.Get, \n                InArgument&lt;ienumerable&lt;changeset&gt;&gt;\n            ), \n            new Expression[] \n            {\n                expression = Expression.Parameter(typeof(ActivityContext), \"env\") \n            }),\n            new ParameterExpression[] { expression }\n        )\n    );\n\nActivityAction&lt;changeset&gt; action = new ActivityAction&lt;changeset&gt;();\naction.Argument = changeset;\nSequence sequence = new Sequence();\nWriteBuildInformation&lt;associatedchangeset&gt; item = new WriteBuildInformation&lt;associatedchangeset&gt;();\nitem.ParentToBuildDetail = new InArgument&lt;bool&gt;(true);\nitem.Value = new InArgument&lt;associatedchangeset&gt;(\n    Expression.Lambda&lt;func&lt;activitycontext, associatedchangeset=\"\"&gt;&gt;(\n        Expression.MemberInit(\n            Expression.New((ConstructorInfo) methodof(AssociatedChangeset..ctor), new Expression[0]), \n            new MemberBinding[] \n            { \n                Expression.Bind((MethodInfo) methodof(AssociatedChangeset.set_ChangesetId), \n                Expression.Property(Expression.Call(Expression.Constant(changeset), \n                (MethodInfo) methodof(DelegateInArgument&lt;changeset&gt;.Get, DelegateInArgument&lt;changeset&gt;), \n                new Expression[] { expression2 = Expression.Parameter(typeof(ActivityContext), \"env\")\n            }),\n            (MethodInfo) methodof(Changeset.get_ChangesetId)\n        )\n    ),\n    Expression.Bind((MethodInfo) methodof(AssociatedChangeset.set_ChangesetUri), \n        Expression.Property(Expression.Call(Expression.Constant(changeset), \n            (MethodInfo) methodof(DelegateInArgument&lt;changeset&gt;.Get, DelegateInArgument&lt;changeset&gt;), \n            new Expression[] { expression2 }\n        ),\n        (MethodInfo) methodof(Changeset.get_ArtifactUri))\n    ),\n    Expression.Bind(\n        (MethodInfo) methodof(AssociatedChangeset.set_CheckedInBy),\n        Expression.Property(\n            Expression.Call(Expression.Constant(changeset), \n                (MethodInfo) methodof(DelegateInArgument&lt;changeset&gt;.Get, DelegateInArgument&lt;changeset&gt;), \n                new Expression[] { expression2 }\n            ),\n            (MethodInfo) methodof(Changeset.get_OwnerDisplayName)\n        )\n    ), \n    Expression.Bind((MethodInfo) methodof(AssociatedChangeset.set_Comment), \n        Expression.Property(Expression.Call(\n            Expression.Constant(changeset),\n            (MethodInfo) methodof(DelegateInArgument&lt;changeset&gt;.Get, DelegateInArgument&lt;changeset&gt;), \n            new Expression[] { expression2 }\n        ), \n        (MethodInfo) methodof(Changeset.get_Comment))\n    )\n    }\n), \nnew ParameterExpression[] { expression2 }\n));\nsequence.Activities.Add(item);\nWriteBuildMessage message = new WriteBuildMessage();\nmessage.Importance = new InArgument&lt;buildmessageimportance&gt;(BuildMessageImportance.Low);\nmessage.Message = new InArgument&lt;string&gt;(Expression.Lambda&lt;func&lt;activitycontext, string=\"\"&gt;&gt;(\n    Expression.Call(\n        null,\n        (MethodInfo) methodof(ActivitiesResources.Format), \n        new Expression[] { \n            Expression.Constant(\"BuiltChangeset\", typeof(string)),\n            Expression.NewArrayInit(typeof(object),\n            new Expression[] { \n                Expression.Convert(Expression.Property(\n                    Expression.Call(\n                        Expression.Constant(changeset),\n                        (MethodInfo) methodof(DelegateInArgument&lt;changeset&gt;.Get, DelegateInArgument&lt;changeset&gt;\n                    ), \n            new Expression[] { \n                expression3 = Expression.Parameter(typeof(ActivityContext), \"env\") \n            }\n        ),\n        (MethodInfo) methodof(Changeset.get_ChangesetId)\n        ), typeof(object))\n    })\n    }),\n     new ParameterExpression[] { expression3 }));\n\nsequence.Activities.Add(message);\naction.Handler = sequence;\neach.Body = action;\n\n\n\n\nand turning it into a Workflow Activity like this:\n\n\n\n\nAnyway, I’ve created an “AssociateMergedChangesetsAndWorkItems” activity that you can plop into your Default workflow that will give you the merged changes too. There’s a little bit of work to get the custom activity in, but it’s nothing you can’t handle!\n\n\n1. Copy your existing Default Workflow\n\n\nOf course, by Default here I mean the workflow that you use to compile-and-test-and-associate-changesets-and-workitems. You may have already added some of your own customizations. No problem.\n\nFirst, create a copy of your Default workflow (if you want to keep the original around). The easiest way to do this is to create a new build definition, then go to the Process page and expand the “Show Details” section at the top. Click the “New” button:\n\n\n\n\nNow just select the “Copy” option and type a name for the new template. In this case I’m going from the DefaultTemplate11.1.xaml which comes out-the-box in TFS 2012).\n\n\n\n\nClick OK and TFS will create a new XAML for you.\n\nOpen Source Control Explorer and go to your BuildProcessTemplates folder. Make sure you refresh to see the new workflow.\n\n\n2. Import the Custom Assembly\n\n\nNow check in ColinsALMCorner.CustomBuildTasks.dll (skip right to the bottom of this post for the attachment links) into a folder in Source Control – this folder is where any custom build assembly needs to be located. If you don’t have one, then I suggest you create it under the BuildProcessTemplates folder so that your workflows and custom activities exist together.\n\n\n\n\nNow go to team explorer and go to the Builds pane. Click on the Action dropdown (see below) and select “Manage Build Controllers…”.\n\n\n\n\nSelect the Build Controller you want to run the builds through, and click “Properties…”. In the “Version control path to custom assemblies” section, select the folder you just imported the custom dll to. Click OK.\n\n\n\n3. Create a Project for Customizing the Workflow\n\n\n(If you’re using the DefaultTemplate and want to skip, you can download this project from the bottom of this post).\n\nNow comes the painful part: in order to add custom activities in custom assemblies to workflows, you need to make the workflow part of a VS project.\n\nSo – File-&gt;New Project and select “Class Library” and give it a suitable name (I chose MergeWorkflow). Make sure you place it in a source controlled folder!\n\nDelete the “Class1.cs” file (you won’t need it) and add a reference to ColinsALMCorner.CustomBuildTasks.dll (you can even make this reference the local path for the corresponding server path you used for your build controller!). Check this solution into TFS.\n\nNow go back to Source Control Explorer and branch the Merge workflow that you created in Step 1. You’re going to branch this into your newly imported project folder:\n\n\n\n\nClick OK. Now go back to the solution explorer and click on your project (not the solution, the project). Enable “View All Files” and include the XAML file. Your solution should now look like this:\n\n\n\n\nTo get the project compiling, you’ll need to add a bunch of assemblies. These ones are from the GAC:\n\n\n\n  System.Activities\n  System.Activities.Presentation\n  PresentationFramework\n  WindowsBase\n  System.Xaml\n  System.Drawing\n  Microsoft.TeamFoundation.Client\n  Microsoft.TeamFoundation.Common\n  Microsoft.TeamFoundation.Build.Client\n  Microsoft.TeamFoundation.Build.Workflow\n  Microsoft.TeamFoundation.VersionControl.Client\n  Microsoft.TeamFoundation.VersionControl.Common\n  Microsoft.TeamFoundation.WorkItemTracking.Client\n\n\n\nThese ones you’ll have to copy from a TFS server (if you don’t have it installed on your machine). Look in c:\\Program Files\\Microsoft Team Foundation Server 11.0\\Tools as well as in the GAC folders (c:\\windows\\assembly\\GAC_MSIL):\n\n\n\n  Microsoft.TeamFoundation.TestImpact.BuildIntegration\n  Microsoft.TeamFoundation.TestImpact.Client\n\n\n\nYou should now be able to compile the solution. Double click the workflow to open the designer.\n\n\n4. Adding the Custom Activity\n\n\nNow you can add in the custom activity. In the tabs at the bottom of the workflow designer, click on “Imports”. Then click the dropdown and add ColinsALMCorner.CustomBuildTasks to import it into the workflow.\n\n\n\n\nClick on Arguments and create a new Argument (direction: In, type: Boolean) called “IncludeMerges”. Set the default value to True. You can also specify the MetaData for this argument to expose it in the Build Definition wizard later.\n\n\n\n\nThen click on Variables and add a variable called “associatedMergedChangesets” with Scope “Sequence” and type IList.\n\nYou’ll need to add the custom activity to the Toolbox. To do this, select a section in the toolbox, right click and select “Choose Items”. In the browse dialog, browse to ColinsALMCorner.CustomBuildActivities and click OK. There are a bunch of activities available, but the only one you really need is “AssociateMergedChangesetsAndWorkItems”.\n\n\n\nNow scroll down to the If called “If AssociateChangesetsAndWorkItems” (this is the default one in the workflow). Drop an “AssociateMergedChangesetsAndWorkItems” (that’s the new custom activity) into the workflow just below the “AssociateChangesetsAndWorkItems” activity. Set the properties as follows:\n\n\n\n\n  AssociatedChangesets: set to associatedChangesets (this is the Result of the previous activity)\n  AssociateMerges: set to IncludeMerges, the Argument you created earlier\n  Result: set to associatedMergedChangesets, the Variable you created earlier\n  UpdateWorkItems: set to True\n\n\n\nMake sure the project builds, and check in.\n\n\n5. Merge Back the Workflow\n\n\nOnce you’ve checked in, find the workflow XAML file in your project in Source Control Explorer. Right click, and merge it back to the template in the BuildProcessTemplates folder. Don’t forget to check in the merge!\n\n\nCreate a Build and Run it!\n\n\nYou’re now ready to run the build. Either create a new build definition or simply change the template of an existing build from the Default template to your newly customized template. Then go checkin, merge and build. Voila!\n\nSo for an example, I have User Story 43, which has a child Task 44. I work on the DEV branch and check in against Task 44 (changeset 97). I then make another DEV change, no work item (changeset 98). I then merge to MAIN (merging changes 97 and 98) in changeset 99. I then merge from MAIN to LIVE in changeset 100.\n\nNow if you had this scenario and ran the “old” template, you’d get 1 associated changeset (100) and no associated work items. However, running your new flashy “merge-aware” template, you get 4 associated changesets and 2 associated work items (see the build output below). Much better!\n\n\n\nGotchas to be aware of\n\n\nThis does mean that the work items may end up having been associated with 2 (or more) builds. Let’s say you have a DEV build (for CI). In the example above, Task 44 would have its “IntegratedIn” in set to the DEV build. Then you merge DEV to MAIN and MAIN to LIVE, and run the merge workflow on the LIVE branch. The workflow will update the “IntegratedIn” field of Task 44 to the LIVE build. You’ll still see both associations in the History, but the Task’s “IntegratedIn” field is now updated to the latest build (the LIVE build) – which may or may not be what you want. For me, it makes sense this way. Now the build report is a REAL change-log (not just a log of the merge changesets).\n\nHappy building!\n\n\nP.S. Attachments\n\n\nHere’s the link to the binaries on my skydrive. This zip file contains the dll (ColinsALMCorner.CustomBuildTasks) as well as the MergeWorkflow project. You can use this project to get started customizing your workflow. If you’re just using the default workflow without customizations, then you can use the workflow in this project instead of copying it etc. by checking the project into Source Control and then branching the template to the BuildProcessTemplates folder.\n",
      "categories": [],
      "tags": [],
      
      "collection": "posts",
      "url": "/custom-build-task-include-merged-changesets-and-work-items-in-build-report/"
    },{
      
      "title": "Failing Builds based on Code Coverage",
      "date": "2013-02-09 01:01:00 +0000",
      
      "content": "The logical next step after you start unit testing your code is to analyse code coverage. You can do this easily in TFS by enabling Code Coverage in the test settings of the default build template. But what about failing builds (or checkins) based on low code coverage?\n\nOut of the box, there’s nothing that can do that. You could write a checkin policy that inspects code coverage, but you’d have to make sure it’s deployed to all VS instances. Or, you could implement the logic in a build, and then make the build a rolling gated checkin build. That way if the build fails for low coverage, the checkin is discarded.\n\nModifying the Default Template\n\nI’ve created a CodeActivity for your toolbox that will inspect the test coverage and return the coverage percentage. You can then easily implement some logic from there.\n\nTo get started, you’ll have to import the Custom assembly (link at the bottom of this post). Follow steps 1 to 3 of my previous custom build post to get going.\n\nNow you can add the ColinsALMCorner.CustomBuildTasks in to the imports of your workflow:\n\n\n\n\nClick on the Arguments tab and add an Int32 in argument called “CoverageFailureIfBelow”. Set the metadata as follows:\n\n\n\n\nNow scroll down in the workflow until after the testing section, between “If CompilationStatus = Uknown” and “If TestingStatus = Uknown”. Add a Sequence between the two If activities called “Check Coverage”.\n\n\n\n\nClick on Variables at the bottom of the workflow window and create an Int32 variable (scoped to Check Coverage, called coverageTotal). You can double-click the sequence to “zoom in”.\n\nThe “GetCoverageTotal” activity relies on the test attachments that the test engine uploads to TFS when the tests are completed (most notably the Code Coverage attachment). Since this is done asynchronously, I introduced a Delay activity. Add a new Argument to the workflow (if you don’t want to hard-code the delay) and set its default to 10 seconds.\n\nSet the Timeout of the Delay activity to\n\n\nTimespan.FromSeconds(arg)\n\n\nwhere arg is the name of your timeout argument.\n\nDrop a “GetCoverageTotal” activity (from the custom assembly) into the sequence (NOTE: You may have to import the assembly into the Toolbox if you’ve never done that before). Set the properties as follows:\n\n\n\n\nOnce that’s done, you now have the coverage total, so you can do whatever you need to. Here’s what I did:\n\n\n\n\nI put an If that evaluates if the total coverage is too low or not. If the coverage is too low, I set the BuildDetail.TestStatus to TestStatus.Failed. I also set the overall build status to PartiallySucceeded. I then have a WriteBuildWarning (“Failing build because coverage is too low”) or WriteBuildMessage (“Coverage is acceptable”) depending on the result.\n\nNow I just queue up a build, and voila – I can now fail it for low coverage (or in this case, set the result to Partially Successful).\n\n\n\n\nYou can download the dll and the DefaultCoverageTemplate.11.1.xaml from my skydrive.\n\nHappy coverage!\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/failing-builds-based-on-code-coverage/"
    },{
      
      "title": "Test Case Manager: Customize Failure and Resolution Type",
      "date": "2013-03-20 03:54:00 +0000",
      
      "content": "In Test Case Manager, you can open a test run that has failed and set the Failure and Resolution types for the failure.\n\n\n\n\nI’ve had a lot of customers ask me if it’s possible to customize the lists. Up until now, the answer has been no. However, in the middle of a bunch of improvements released in the CTPs of QU2 (Quarterly Update 2), Brian Harry mentioned that it is now possible to customize the lists. The CTP he was referring to is NOT A GO LIVE CTP, so rather download CTP4 of the update (which IS GO LIVE) if you want to try this on a production server.\n\nThe customization is only possible via the command line. And even that is hard to figure out. So here are the 4 commands that you need.\n\nExport the Current Lists\n\nOpen a developer command prompt, and type the following commands (substituting your collection URL and team project accordingly):\n\n\ntcm fieldmapping /export /collection:collectionURL /teamproject:Project /type:FailureType /mappingFile:FailureTypes.xml\n\n\ntcm fieldmapping /export /collection:collectionURL /teamproject:Project /type:ResolutionType /mappingFile:ResolutionTypes.xml\n\n\nThis will export the two lists for you. They are pretty straight-forward and self-explanatory:\n\n&lt;!--?xml version=\"1.0\" encoding=\"utf-16\"?--&gt;&lt;br&gt;&lt;testfailuretypes&gt;&lt;br&gt; &lt;testfailuretype name=\"Regression\"&gt;&lt;br&gt; &lt;testfailuretype name=\"New Issue\"&gt;&lt;br&gt; &lt;testfailuretype name=\"Known Issue\"&gt;&lt;br&gt; &lt;testfailuretype name=\"Unknown\"&gt;&lt;br&gt;&lt;/testfailuretype&gt;&lt;/testfailuretype&gt;&lt;/testfailuretype&gt;&lt;/testfailuretype&gt;&lt;/testfailuretypes&gt;&lt;br&gt;\n\n&lt;!--?xml version=\"1.0\" encoding=\"utf-16\"?--&gt;&lt;br&gt;&lt;testresolutionstates&gt;&lt;br&gt; &lt;testresolutionstate name=\"Configuration issue\"&gt;&lt;br&gt; &lt;testresolutionstate name=\"Needs investigation\"&gt;&lt;br&gt; &lt;testresolutionstate name=\"Product issue\"&gt;&lt;br&gt; &lt;testresolutionstate name=\"Test issue\"&gt;&lt;br&gt;&lt;/testresolutionstate&gt;&lt;/testresolutionstate&gt;&lt;/testresolutionstate&gt;&lt;/testresolutionstate&gt;&lt;/testresolutionstates&gt;&lt;br&gt;\n\n\nNow edit the lists, and then use the import commands:\n\n\ntcm fieldmapping /import /collection:collectionURL /teamproject:Project /type:FailureType /mappingFile:FailureTypes.xml\n\n\n\ntcm fieldmapping /import /collection:collectionURL /teamproject:Project /type:ResolutionType /mappingFile:ResolutionTypes.xml\n\n\nThat’s it – restart (not just refresh) Test Case Manager and you’re good to go.\n\n\n\n\nOf course they new values appear in the Plan Results page, and though I haven’t tested it, I presume they’ll be in the warehouse too:\n\n\n\n\nHappy testing!\n",
      "categories": [],
      "tags": ["testing"],
      
      "collection": "posts",
      "url": "/test-case-manager-customize-failure-and-resolution-type/"
    },{
      
      "title": "Excel Sheet Showing Parent Items Whose Child Items Are All Closed",
      "date": "2013-03-22 20:32:00 +0000",
      
      "content": "I’ve often had the question from my customers – “I’ve got a bunch of Requirements. Some of them are Active, but all their Child Tasks are Closed. Can TFS automatically close the Parent items? Or can I at least query these Requirements out?”\n\nYou can do this with a server component that will listen to Work Item change events, query the work items and transition the states. Either you’ll download some code someone else wrote or you’ll write it yourself. As for a query, well unfortunately, you can’t do this sort of query in WIQL (to the best of my knowledge). While being really powerful, WIQL does lack things like aggregations and queries that rely on history (like show me all bugs that have been reactivated more than once). You can do these sorts of queries using the TFS API, but then you have to maintain an app and code – it gets messy.\n\nSo I’ve done some Excel “juggling” and enabled you to add some formulas to a work item query that will highlight Parents that need to be closed because all their child Tasks are closed. Unfortunately I can’t give you a simple way of implementing it because Excel doesn’t support copying formulas out of the “special” tables that the TFS Plugin creates for Work Item Queries.\n\nHere’s the end product, showing an “Action” column that highlight Parent items whose Child Items are all closed, or Child Items that are active whose Parent items are closed:\n\n\n\nSet up a Tree Query with ID, Work Item Type and State\n\nThe first thing you need to do is set up the query that returns the tree of work items. This can be any query that you want, as long as you return the ID, Work Item Type and State columns. The logic I’ll show you requires at least these 3 fields.\n\nNow open Excel and connect to that query to pull in all your Work Items.\n\nThe Settings Sheet\n\nNow add a new Sheet in the workbook called “Settings”. Add the following data (you can open my sample sheet and copy/paste the values in if you want):\n\n\n\n\nYou’ll have to set the data in the columns appropriately:\n\n\n  Parent WIT\n  The Work Item types that can be parents (usually the same work item types that you have in your Requirements Category)\n  Child WIT\n  The child work item type (usually the same work item types that you have in your Task category)\n  Parent Active States\n  States of the Parent work items that you consider to be “Active” states\n  Child Closed States\n  States of the Child work items that you consider to be “Closed” states\n  Column Count\n  When you pull in a Tree Query, Excel creates a column per level in the tree. This figure could therefore change is another level in the tree appears – so just count the columns from your original query and enter the count here.\n  ID Column Index\n  The 1-based column index of the ID column\n\n\nAdding in the Formulas\n\nNow go back to your query sheet. Add the following column headers to the right of your query columns (don’t worry, you’ll hide most of these anyway):\n\n\n  IsParent\n  ChildIndex\n  ParentID\n  ParentActive\n  ChildActive\n  ParentIDMunge\n  IDMunge\n  Action\n\n\nYour spreadsheet should look as follows:\n\n\n\n\nNow insert the following formulas into the columns (the 1st one goes into IsParent, the 2nd into ChildIndex and so on):\n\n=IF(COUNTIF(Setup!$A$2:$A$10,[@[Work Item Type]]), “P”, IF([@[Title 1]]=””,”C”, “P”))\n\n=IF(H3=”C”, IF(H2=”P”, 1, I2+1), “”)\n\n=IF([@IsParent]=”P”, “”, INDEX([#All],ROW()-[@ChildIndex]-1,Setup!$F$2))\n\n=IF([@IsParent]=”P”,IF(COUNTIF(Setup!$C$2:$C$10,[@State]), “Y”, “N”), INDEX([#All],ROW()-[@ChildIndex]-1,4+Setup!$E$2))\n\n=IF([@IsParent]=”C”,IF(COUNTIF(Setup!$D$2:$D$10,[@State]), “N”, “Y”), “”)\n\n=IF([@IsParent]=”P”, [@ID]&amp;(IF([@ParentActive]=”Y”, “N”, “Y”)), [@ParentId]&amp;[@ChildActive])\n\n=IF([@IsParent]=”P”, [@ID]&amp;[@ParentActive], [@ParentId]&amp;[@ChildActive])\n\n=IF(AND(COUNTIF([ParentIDMunge], [@ParentIDMunge])&gt;1, COUNTIF([IDMunge], [@IDMunge])=1), IF([@IsParent]=”P”,IF([@ParentActive]=”Y”, “Close Me (all children closed)”, “”), “”), IF(AND([@ChildActive]=”Y”, [@ParentActive]=”N”), “Close Me (Parent is closed)”,””))\n\nYour spreadsheet should look something like this:\n\n\n\nClean Up\n\nFinally, to clean up a bit, select the columns from IsParent to IDMunge and hide them, leaving only the Action column visible. You can of course filter this column to not show blanks and so on.\n\nPerhaps there’s better ways of doing this – the primary difficulty in these formulas was the fact that you’re not only querying the row you’re in, but also the row’s “parent row” (if it has one). If you can see better ways of doing this, then please add a comment!\n\nHere’s my sample sheet.\n\nHappy querying!\n",
      "categories": [],
      "tags": ["alm"],
      
      "collection": "posts",
      "url": "/excel-sheet-showing-parent-items-whose-child-items-are-all-closed/"
    },{
      
      "title": "Enable Custom IntelliTrace Web Events with a Right-Click",
      "date": "2013-04-12 15:15:00 +0000",
      
      "content": "Series Links:\n\n\n  Part 1: The Basics (this post) – also guest posted on MSDevDiv SA Blog\n  Part 2: IntelliTrace Everywhere – also guest posted on MSDevDiv SA Blog\n  Part 3: Enable Custom IntelliTrace Events with a Right-Click\n\n\nNote: This is an unsupported feature! Use at your own risk – though to be honest I can’t see what the risk really is. Just know that this is not supported by Microsoft.\n\nPreamble (TL;DR – skip to next section for the Good Stuff)\n\nI’m going to be presenting 3 deep dives at TechEd Africa next week:\n\n\n  Version Control (including Git)\n  Agile Planning Tools and Customizations\n  IntelliTrace\n\n\nWhile I was preparing for my IntelliTrace session, I was watching Larry Guger (IntelliTrace PM) do a presentation when IntelliTrace-in-Production had just launched. Right at the end of his talk, he did something which boggled my mind – he right-clicked a method and right there in the context menu was “Insert IntelliTrace Event”. He did this for a couple of methods, and then exported these events to a folder. When he ran the IntelliTrace collector for IIS using PowerShell, the custom events were present in the log file.\n\n\n\n\nThis was amazing because I know how tedious it is to create custom IntelliTrace events. I quickly fired up my VS just to see if I could do it – and couldn’t find the menu option.\n\nI then mailed Larry and after a short email conversation and some scratching around the IntelliTrace dll’s with Reflector, I was able to figure out that if you add a registry entry, you “unlock” this feature.\n\nThis is a “partially complete” feature in VS – here is the big limitation:\n\nThis only works with IntelliTrace collection for IIS applications via PowerShell.\n\nThe Good Stuff\n\nSo I’ll cut to the chase: here’s the registry key:\n\n\nWindows Registry Editor Version 5.00\n\n\n[HKEY_CURRENT_USER\\Software\\Microsoft\\VisualStudio\\11.0_Config\\TraceDebugger]\"IntelliTraceEventsEnabled\"=dword:00000001\n\n\nJust copy and paste this into a file (intelli.reg or something) and double-click it. Restart VS.\n\nNow open up a Web Application, find a method, right click, select “Insert IntelliTrace Event”. Repeat for a couple of other methods. You’ll see a glyph in the gutter indicating that you have an event for that method.\n\n\n\n\nNow go to Debug-&gt;IntelliTrace-&gt;Export IntelliTrace Events. Save this file to a folder somewhere.\n\n\n\n\nYou’ll see that the file has an .iFragment extension – this is an IntelliTrace config fragment. If you open up the file, you’ll see it has created Category, Module and Diagnostic sections – these are the same sections you’d have to manually create if you wanted some custom IntelliTrace events.\n\n\n\n\nAside: You can see the “AutomaticDataQuery” element in the  tag. This allows you to see the in/out arguments of the method in the locals window when you debug the log file later… No messing around with argument positions and stuff…\n\n\n\nUsing iFragments\n\nNow that you have a custom iFragment, you need to go to the server that you want to collect IntelliTrace events from. Go to the folder where you extracted the collector and open (or create) a folder called CustomEvents. This is where you drop your iFragments.\n\nNow fire up your collector (using the default collection plan). Collect a log. Open it in VS. Start debugging.\n\nThe first thing to note is that you have a custom category in the categories list of the IntelliTrace events window:\n\n\n\n\nSecondly, you’ll see the events by the same glyph that you saw when you added the events in VS:\n\n\n\n\nHappy logging!\n",
      "categories": [],
      "tags": ["news","development"],
      
      "collection": "posts",
      "url": "/enable-custom-intellitrace-web-events-with-a-right-click/"
    },{
      
      "title": "Build Fails: Path Limit Exceeded",
      "date": "2013-04-26 21:54:00 +0000",
      
      "content": "I had a customer who mailed me about their builds failing. The error message was\n\n\nException Message: The specified path, file name, or both are too long. The fully qualified file name must be less than 260 characters, and the directory name must be less than 248 characters.\n\n\n\n\nThe problem was the path of the source files got too long. There are 2 contributing factors for path length on a build server:\n\n\n  The Build Agent Working Directory setting\n  The Source Setting Workspace mapping\n\n\nWhen the build agent checks out code, it checks it out to (WorkingDirectory)\\Mapping for each mapping. By default, the build agent working directory is set to $(SystemDrive)\\Builds$(BuildAgentId)$(BuildDefinitionPath) – more on these macros later – but this usually defaults the working directory that the source gets checked out to something like\n\n\nc:\\Builds\\1\\FabrikamFiber\\FabrikamFiber.CallCenter MAIN\\src\n\n\n(where the team project name is “FabrikamFiber” and the build definition name is “FabrikamFiber.CallCenter MAIN”).\n\nThen, if you look at the Source Setting workspace mapping for the build, you’ll see the mappings for what source code the build is supposed to check out:\n\n\n\n\nHere, $(SourceDir) equates to the root working folder for the build agent that ends up running the build. You’ll see I have a 2 mapping here – one to $(SourceDir) and one to $(SourceDir)\\BuildLibs. That means that my BuildLibs will get checked out to:\n\n\nc:\\Builds\\1\\FabrikamFiber\\FabrikamFiber.CallCenter MAIN\\src\\BuildLibs\n\n\nwhich is already 69 characters. If I have lots of subdirectories below that, I could start hitting the 260 character path limit.\n\nCustomizing the Build Agent Working Directory\n\nSo let’s shorten the build agent working directory. In Team Explorer, click on the Build hub. Click “Actions” and select “Manage Build Controllers”.\n\n\n\n\nThen find the build agent(s) that are going to build your code and click “Properties”. You want to set the working directory to something shorter:\n\n\n\n\nThere are 4 macros you can use here (according to this page):\n\n\n  \n\n\n$(BuildAgentId): An automatically generated integer that uniquely identifies a build agent within a team project collection.\n\n\n  \n\n\n$(BuildAgentName): The Display Name of the build agent.\n\n\n  \n\n\n$(BuildDefinitionId): An automatically generated integer that uniquely identifies a build definition within a team project collection.\n\n\n  $(BuildDefinitionPath): The team project name and the build definition name, separated by a backslash.\n\n\nSo let’s change the Working Directory to:\n\n\nc:\\b\\$(BuildAgentId)\\$(BuildDefinitionId)\n\n\nThat means the root of the working directory will be something like:\n\n\nc:\\b\\1\\10\\src\n\n\nwhich is only 13 characters.\n\nShorten the Build Source Setting Workspace Mapping\n\nNothing says that the mapped folder for the build has to have the same name as the folder in source control. For example, if you have\n\n\n$/FabrikamFiber/Code In Some Really/Long Folder/That has SubFolders/Within SubFolders\n\n\nand you need to map a path to\n\n\n$/FabrikamFiber/Code In Some Really/Long Folder/Libs\n\n\nfor referencing a library, then you could map those 2 folders to\n\n\n$(SourceDir)/T/W\n\n\nand\n\n\n$(SourceDir)/Libs\n\n\nrespectively. As long as you keep the same relative path “distances”, any relative path references you have will just work (assuming you haven’t hard coded them).\n\nHappy building!\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/build-fails-path-limit-exceeded/"
    },{
      
      "title": "Improvements to Code Review Checkin Policy",
      "date": "2013-05-21 00:33:00 +0000",
      
      "content": "Late last year I uploaded my first VS Gallery contribution – Colin’s ALM Corner Checkin Policies. One of the policies in this pack is a Code Review Checkin Policy. I blogged about it in this post.\n\nOne of the pieces of feedback I got about this policy was that this is counter the “checkin early, checkin often” mantra of most development shops. Some suggestions were to only apply this policy to “junior” team members, allowing “senior” members to checkin without requiring a review. I decided however to approach this from a source control perspective as opposed to a group membership perspective.\n\nThe policy now allows you to configure which source control paths it must fire on – so if you checkin files in the “mapping”, a code review is required, otherwise no code review is necessary. This means you can make the DEV branch (or your equivalent) review-free, while enforcing reviews on MAIN or LIVE branches, according to your practices. This way you get the benefit of “checkin early, checkin often” without the hassle of reviewing every single change. When you merge a bunch of changes through to MAIN or LIVE, you can then enforce Code Review (you would perform your code review on the merge-set).\n\nHere’s what the Policy Configuration settings now look like:\n\n\n\n\nYou can see a list of “Paths to apply policy to”. Just add your source folders here and you get finer-grained control over when a Code Review is required.\n\nHappy reviewing!\n",
      "categories": [],
      "tags": ["alm"],
      
      "collection": "posts",
      "url": "/improvements-to-code-review-checkin-policy/"
    },{
      
      "title": "Testing Windows 8 Applications",
      "date": "2013-05-29 15:02:00 +0000",
      
      "content": "Windows Store applications are slowly becoming more popular. If you’re going to do any Windows Store development, you’ll need to shift a few paradigms in how you code – and how you test. There are hundreds of webcasts and blogs detailing Windows Store programming, but I want to explore testing Windows Store apps.\n\nThe Summary (or, TL;DR)\n\nThere’s ok news and bad news. The good news is you can test Windows Store apps – the bad news is, it’s hard to do it properly.\n\nFirst let’s consider unit test capabilities:\n\nJavaScript  .NET   Test Project Template QUnit for Metro has a project template Out-of-the-box using “Unit Test Library for Windows Store apps”   Run Tests in Test Explorer No Yes   Run Tests in Team Build No Using Interactive Build Agent (No code coverage)   Coded UI Tests No No\n\nHere’s a summary of Manual Testing capabilities:\n\nCapability  Yes or No?  Test on Local Machine No  Test on Remote Device Yes  Action Log Yes  Manual Code Coverage No  Event Log Yes  IntelliTrace No  Voice and Video Recording No  System Information Yes  Test Impact Analysis No  Manual Screen Captures Yes  Fast Forwarding No\n\nRead on if you want some more details and explanations!\n\nUnit Testing (.NET)\n\nIf you want to unit test your application, you’ll need to add a Unit Test project. For Windows Store apps using .NET, you get a project template out-the-box.\n\n\n\n\nWhen you add this project to your solution, you’ll see that the test project actually has a structure that is very similar to a Windows Store app – it has a certificate and an app manifest too. This is because if you’re going to use and of the Windows capabilities in your tests (like File Storage, Camera, Location and so on) you’re going to need to enable them in the app manifest, just like you would a normal app.\n\n\n\n\nYou can run tests that are in the project in the Test Explorer window, but automating them as part of a Team Build will require you to run the Build Agent in the Interactive Mode (as opposed to as a service) since the Windows capabilities require interaction with the desktop. Furthermore, you won’t get Code Coverage when running these tests either through the Test Explorer or in a Team Build.\n\nUnit Testing (JavaScript)\n\nFor JavaScript apps, you’re almost out of luck. You can install QUnit for Metro which gives similar functionality to the C# Unit Test Library. The difference is you’ll need to set the test project as your Start project and then run that to actually execute your tests – there is no “headless” way to run these tests. If you’ve installed Chutzpah Test Adapter, then you’ll see the tests in the Test Explorer Window, but they’ll fail if you try to run them. There’s no integration with Team Build, so you can’t run automated tests using this framework.\n\n\n\n\nOnce you’ve created a QUnit Test Project, you have to add links to the js files in the project you want to test. Then you write your QUnit tests, and run the application (setting the test project as the start up project). When you run that, it’ll execute your test cases:\n\n\n\nManual Testing (.NET or JavaScript)\n\nThe manual testing story for both .NET and JavaScript apps is the same. The trick here is that you can’t test on the local machine – you’ll have to have a separate Windows 8 device to run your app on. The steps you need to follow for manual testing are as follows:\n\nStep 1: Install the Remote Debugger on your remote device\n\n\n  Download the Remote Debugger for your architecture (x64, x86 or ARM)\n  Install the debugger on your remote device (amazingly, the ARM exe installs even on WinRT!)\n  Launch the “Microsoft Test Tools Adapter” application from the Start screen\n\n\n\nStep 2: Create a TFS Build to build your application\n\nStep 3: Push your application to your remote device via MTM\n\n\n  Launch MTM and go to the Test Tab. Look for the links “Modify” and “Install Windows Store App”.\n\n  Press “Modify” and connect to your remote device.\n\n  Enter the name of the remote device and press the “Test” link. You’ll probably be prompted for credentials (if you’re logged in as a different user). In my case, my development machine is on a domain and my remote device (a Surface RT) is not – so I enter my Microsoft Account details to connect to the device.\n\n  Once you press enter, MTM will attempt to connect to the remote device. Make sure your device is not locked and that the “Microsoft Test Tools Adapter” is running.\n\n  Next you’ll have to click the “Install Windows Store App” in MTM to push your app. Browse to the drop folder of your application and find the appx file for your application.\n\n  Once you click “Open”, MTM will push the app to your device. If this is the fist time you’re doing it, you’ll need to get a developer licence (on the remote device) – MTM will pause until you complete this step. Go to the remote device, and open the Desktop app if you don’t see anything happening. Once you’ve signed in with your Microsoft Account details, you’ll see a notice saying that a developer license has been installed.\n  MTM then also installs your app’s certificate. Again, you’ll have to go to the Desktop on the remote device and allow the certificate to install before MTM actually installs the app.\n\n\nStep 4: Set the build for your Test Plan\n\n\n  This ensures a tie-in from the executable you have and any bugs that you file\n\n\nStep 5: Configure Test Settings\n\n\n  You can try to collect video or IntelliTrace or do Test Impact Analysis – none of those collectors work.\n\n  What will work is the Action Log, Event Log and System Information. Just set them for the “Local” role, even though these are the collectors that will run on the remote device.\n\n\n\nStep 6: Execute Your Tests\n\n\n  Click on a Test and hit “Run” to run the test\n  When you do, you’ll see a message pop up on the Remote Device notifying you that a test session has started. All your actions are now being recorded (tap by tap, as it were).\n\n  As you complete each step, remember to mark it as “Pass or Fail” in the Test Runner that is running on your development machine\n  Press “Screenshot” to take a screenshot – this actually pulls the remote device’s screen onto the development machine and allows you to mark a rectangle to be attached to the test results.\n\n\nWhen you log a bug, you’ll get detailed info. The Action Log opens really nicely as an HTML page with links showing where you tapped and so on.\n\n\n\nExploratory Testing\n\nThis works in exactly the same manner – except you don’t have a test case to follow. You can filter the steps out when you log bugs (or test cases) as you go.\n\n\n\nCoded UI Testing\n\nI was really hoping that this would be something that you could do from Visual Studio – except, that it’s not. Here’s what happens when you generate a coded UI test from a test case action recording:\n\n\n\n\nThat’s right, sports fans – no coded UI support yet.\n\nConclusion\n\nUnfortunately it’s a bit of a bleak outlook at the moment for testing Windows Store apps. Even when you can do some testing, you’re limited, and you’re going to have to invest significant effort. I am hoping that the story will improve with future versions of TFS and VS. For now, the best value-for-time-and-money is on manual testing using a remote device. If your Windows Store App is written in .NET, you can do some unit testing, but you’ll have to make your build agent interactive to run the tests as part of a build. If you’ve got JavaScript Windows Store Apps, then try out the QUnit for Metro extension, but it feels a bit hacky and you won’t get any build integration.\n\nLinks\n\n\n  There is a great MSDN article about testing Windows Store Apps\n  Brian Keller does a great 11 minute video showing some of these capabilities\n  The support matrix for Coded UI and Action Recordings currently showing no support for Windows Store Apps\n  Christopher Bennage apparently got some unit tests working for his JavaScript Windows Store Apps, though I couldn’t seem to get the “headless” experience he claims. I also didn’t like polluting my application with tests (since his method has the tests in the App itself).\n\n\n(Un)Happy Windows Store App Testing!\n",
      "categories": [],
      "tags": ["testing"],
      
      "collection": "posts",
      "url": "/testing-windows-8-applications/"
    },{
      
      "title": "Automated Builds–Why They’re Absolutely Essential (Part 1)",
      "date": "2013-06-18 19:23:00 +0000",
      
      "content": "A couple of weeks ago I was doing a road-show where I did demos of TFS 2012 and it’s capabilities. I do a 4 hour demo that shows an end-to-end scenario, showing capabilities such as requirements management and elicitation, work management, developer tools, quality tools, testing tools, automated builds, lab management and reporting all using TFS. I visited 9 different companies, and most of them asked, “Why should we do builds?” This is something I want to address – you need to be doing builds, and you need to understand why they are so key to successful and effective software development. Builds are no longer an optional extra!\n\nGeneral Reasons to do Automated Builds\n\nBefore I dive into doing automated builds using TFS Team Build, there are two general principles that apply to doing automated builds that I’d like to unpack: Consistency and Quality. These principles transcend the choice of build engine – be it TFS Build (or TeamBuild), Hudson, Cruise Control, Jenkins or any other engine.\n\n1. Consistency\n\nHow do you deploy your code into test (or for that matter, into production)? Most teams start off using the “Publish” menu option from within Visual Studio, either publishing directly to test (or even production) environments, or publishing locally and copying to the target servers. Let me be brutally honest – this is simply immature. This “deploy from Visual Studio” is really something only amateur developers and hobbyists should be using. Why? Because there is so much risk attached to doing things this way.\n\nSome teams argue that they only do this to “test” environments and start getting “serious” only when deploying to production. I argue that you should treat even your test environments as if they were production targets, so that you get the deployment process right there before promoting to production. If you can do it right in pre-production environments, chances are you’ll be able to do it right in production.\n\nLet’s look at some of the risks of “Publish from VS” deployments:\n\n\n  You could be deploying anything\n  How do you know that you’ve only got the code that’s in Source Control? Perhaps you opened your solution and fiddled around a bit, just to try an idea. Now you’ve published untested code into production.\n  You could have dependencies on anything\n  Let’s imagine you’ve got a dependency on some 3rd party library – Enterprise Library or Entity Framework or MVC or any other library. Your target server has version X installed, but you’re a dev, so you install version Y (you want to be on the latest and greatest, right?). Now you’ve compiled and deployed against the wrong version.\n  Your “guy/gal who deploys” gets hit by a bus\n  Most times, a senior team member is doing the deployment. Now all your deployment know-how is invested in one person – what if that person is out sick or his/her machine crashes? Now you can’t deploy.\n  Even if you spread the deployment pain knowledge around, how long is the deploy-er going to spend doing this deployment? In the best case scenario, a few minutes. Generally though, this takes a lot longer. Do you really want a senior team member indisposed for hours and hours every month to do deployment?\n  You can’t “do it again” (in most cases)\n  What if your deploy-er is having a rough day and forgets to copy this folder or forgets to add that bit of new config to the existing configuration files? Manual deployments are not repeatable, so they’re error prone.\n\n\nSo what does this have to do with automated builds? Automated builds address each of the above risks. Fewer risks translates into better quality and higher productivity.\n\n\n  You’re deploying “known” code\n  Since the build engine is going to check out the latest version of source control, you know exactly what code is going to be compiled. No wondering if there are any “unintentional experiments that I forgot to delete”.\n  You control dependencies on the build server\n  You’re going to configure your build server – and that means it will have only what you install (assuming you lock it down). No rogue libraries or experimental versions – just exactly what you need to get into production.\n  Anyone (who has permission) can trigger the build\n  Since you’re setting up an automated build process, you’ll be able to trigger it with a single-click. No need for you to designate a deploy-er who has a whole lot of knowledge about what to compile. Once your build is set up, you can “just do it” again and again.\n  Also, since it’s automated, you can “fire and forget”. Even if the build takes half an hour, your team lead is free to continue coding or whatever you really pay him/her to do (as opposed to watching compilation and copying files all over the place).\n  Builds are repeatable\n  Most build engines can “re-build” – do an exact build again. Since the process is automated, you can rest assured that no step is going to be forgotten by mistake.\n\n\n2. Quality\n\nQuality is something that’s hard to measure. Let’s consider an example. If a user expects the system to save a record to the database when they click the “Save” button, and it works, then quality is high, right? Not necessarily. Perhaps the “Save” operation only works when the information is “clean” – and breaks if the data is invalid (perhaps it should warn the user that there is invalid data?).\n\nAutomated builds go some way to providing a measure of quality. How do you know that the code you are publishing from VS is good code? What measures do you have to even assess this? I argue that if you don’t have automated builds (with unit tests – something we’ll again discuss in a later post) then you’ve got no objective measure.\n\nLet’s consider some of the advantages that automated builds bring in terms of quality:\n\n\n  Packaging your binaries\n  Let’s say you want to test your code in Staging before you deploy to Production. If you publish from VS, then you’ll be doing the publish twice – once for Staging, and then once for Production. Automated builds give you the advantage of producing one package (be that a WebDeploy package or installer or whatever) that you can deploy multiple times, knowing that you’re deploying exactly the same thing every time.\n  Quality Measurement – i.e. test statistics\n  I’ll discuss unit testing in another post – but assume that you have unit tests (and you’re analysing code coverage). If you don’t have a build of some sort, how do you know that the tests all passed? You’d have to take the word of your deploy-er. With automated builds, you can look at the build reports to see test pass/fail rates as well as coverage. If there are failing tests or coverage is too low, you block deployment. Also, if your build engine is putting these measurements into some sort of database you’ll be able to track quality trends over time.\n  Removing user error\n  A good automated build process is exactly that – automated. That means that the process can’t “forget” to link to some library or to run this or that test. This means better quality.\n  Definition of Done\n  Just because a developer says it’s done, it usually means that “it’s sort-of-nicely-coded-and-works-on-my-machine”. An automated build will run unit tests (the first of a few quality gates that should be part of your process). Then you should be deploying this build out to a pre-production environment. Testers (or at least “Power” users) should then manually test the build. Only once they sign off should the build be deployed out to Production. Since an automated build has produced the package, you know that the same package (that’s now passed automated and manual tests) is going out to Production.\n  All of this means you get a standard, repeatable and consistent process that can become part of your “definition of done”. If it doesn’t pass unit tests, block it. If coverage is too low, block it. If it doesn’t pass manual testing, block it. Only once the build has passed these gates can it go to Production.\n\n\nConsider the Cost\n\nSo let’s now ask which process would let you sleep easier the night of your rollout to Production? The one where a developer claims the unit tests are passing with sufficient coverage and does a “Publish” from Visual Studio, or the one where you’ve got an automated build report showing test success and coverage, approval from testers that the build passed manual tests, and a script that’s proven itself over and over for deployment?\n\nI’d like to end with this consideration: the “earlier” in the process you find a bug, the cheaper it is to fix. Consider finding a bug while you’re coding. You fix it then-and-there: cost to company is a couple minutes of your time, so probably a few cents. On the other end of the spectrum, consider cost to company of a bug in Production: in terms of pure development, there’s the time of the person who finds the bug – then the time of the call-centre that they call, sometimes the ops team, 2nd line support, then some developer who spends a few hours trying to reproduce the issue and eventually fix it. Then it has to go through testing etc. etc. – and that’s just in terms of “direct” costs. Bugs in Production could cause other costs such as financial costs for legal errors or even reputational cost for your company.\n\nThe bottom line is this: investing some time now to automate builds (and add unit tests) will save you lots of time and money in production and operational issues. It’s a fact.\n\nIn the next post, I’ll talk about the Team Build automation engine.\n\nHappy building!\n",
      "categories": [],
      "tags": ["build","alm"],
      
      "collection": "posts",
      "url": "/automated-buildswhy-theyre-absolutely-essential-part-1/"
    },{
      
      "title": "Automated Builds–Why They’re Absolutely Essential (Part 2)",
      "date": "2013-06-18 19:27:00 +0000",
      
      "content": "In my previous post I wrote about why you should be doing automated builds in general terms. In this post I’ll show you how TFS’s automated build engine gives you consistency and quality in your build processes. There are other build engines, but if you’re using TFS for source control (and/or test management and/or lab management and/or work item tracking) then Team Build makes the most sense as a build engine since it ties so many other parts of the ALM spectrum together.\n\nTFS Team Build uses Workflow Foundation as the engine underneath the build. When you create a Team Project you get a few workflow XAML files out-the-box. For this post I’ll primarily discuss features of the DefaultTemplate11.1.xaml (the default build template).\n\nEnvironment\n\nWhen you configure a Build Agent, you install it on a build server. Ideally this is some Virtual Machine that is “clean” – the only things installed on the machine are the things that you need to compile (and test) your code. No rogue libraries or experimental settings – just a clean, controlled environment.\n\nInstalling the build agent is a snap – mount the TFS install media and install TFS. Then run the Build Configuration wizard and connect to a build controller (which can reside on the build machine if you want).\n\nLabelling Sources During a Build\n\nThe build agent checks out the latest version of source control when it starts the build. As it does so, by default it labels the code that it checks out with the build name. This means that you can get the exact point-in-time code that the build used to compile, test and package. To see the labels, open the Source Control Explorer, find the folder that your build workspace is configured to download (in the Sources section of the build workflow), right-click and select “View History”. Then click on the Labels tab.\n\n\n\n\nIn the above picture you can see the build reports on the left, and the labels for the root folder of the build workspace on the right.\n\nIf you don’t want the build to label the sources on each build, then go to the Process tab of your build definition, expand the Advanced parameters and set “Label Sources” to false.\n\nPerform Code Analysis During a Build\n\nMost of us developers know we should be doing code analysis, but few teams that I work with actually do it. Most of the time it comes down to the fact that it’s hard to monitor. However, if you include code analysis as part of your build, you’ll easily be able to track Code Analysis over time.\n\nIf you have particular projects that you care about and don’t want to run Code Analysis on all projects, then you can configure that in the Build. The default build template sets “Perform Code Analysis” to “AsConfigured” which means if a project is configured to do code analysis on build, then it does so.\n\n\n\n\nOf course you can set the Code Analysis to “Never” or “Always” too.\n\nAnd as easy as that you now have Code Analysis as part of your build process:\n\n\n\nLayer Validation\n\nIf you’ve got Visual Studio Ultimate, you’ll be able to draw Layer Diagrams. These diagrams allow you to visualize (and validate) layering within your architecture. Team Build can validate layering when building – all you have to do is right click your modelling project (that contains your layer diagrams) in the Solution Explorer, select Properties and set the “Validate Architecture” property to true.\n\n\n\n\nAs long as this project is part of the solution(s) being built, you’ll get layer validation as part of your build.\n\n\n\n\nOh dear – someone broke our layering!\n\nSymbols\n\nYou should never deploy pdb files (symbol files) to production environments. But there are times when you’ll need the symbols files – for example remote debugging, for IntelliTrace or for Manual Test Coverage (see below). Team Build effortlessly publishes your symbols to a network share and indexes them for you, so you never have to think about them or hunt for them again.\n\n\n\n\nConfiguring Symbols and indexing on a build – the build creates a folder structure, so just supply the root folder and the build takes care of the rest.\n\nUnit Testing and Code Coverage\n\nHaving a automated build without unit tests is like brushing your teeth without toothpaste. Once you’ve got a build in place, add unit tests and code coverage. This will increase the quality and consistency of your releases exponentially.\n\nSo let’s assume you have unit tests. You can easily configure Team Build to run the tests and perform code coverage. Set the automated test settings (you can have multiple) appropriately. By default the discovery filter is ***test*.dll (which is any dll with the word “test” in it in any subdirectory). Click on “Edit” to enable Code Coverage and you’re done. I’ve even configured a build engine to run QUnit js files to test JavaScript in my web projects! Of course you can add category filters too if you want to filter which tests the build should be running.\n\n\n\n\nBesides being a metric for each individual build, the pass/fail rates and coverage percentages go into the TFS Warehouse so that you can report off them and trend them.\n\nIf you’re not using the MSTest framework and you’re using nUnit or XUnit or some other framework and you have a corresponding Test Adapter in VS for running your unit tests within VS, then make sure you install the same Test Adapter on your build machine to enable it to run those tests during the build.\n\n\n\n\nThis build output shows a failed test.\n\n\n\n\nThat’s better – a 100% pass rate. Looks like the coverage is a little on the low side though…\n\nCode Coverage for Manual Tests\n\nAt present this only works for Web Applications running in IIS. Get your testers to run test cases out of Microsoft Test Manager (MTM) against your test servers, and then enable the “Code Coverage” diagnostic adapter. You’ll have to tell it where to find the symbols files (which you hopefully configured on your build anyway) and you’re good to go.\n\n\n\n\nSetting the Code Coverage diagnostic adapter (and the path to the symbols) in the Test Settings section of a Lab Environment.\n\nThe great thing about Team Build is that the manual code coverage is fed back onto the build report as testers execute their manual tests. Each time a manual test run is completed, the build report is updated.\n\n\n\n\nThis build report has been updated to show an additional test run (the manual test run) and the coverage has been merged into the total coverage (so it’s showing total unit test plus manual test coverage).\n\nBuild Reports – or what the heck is in this build?\n\nGet your developers into the habit of associating checkins with work items. By default, the build lists all associated checkins between “good builds”. (The last good build is the last build that was successful – no compiler errors or test failures). If those checkins are associated with work items, the work items get associated with the builds too. That means that you can look at the build report and quickly answer the question, “What work is included in this build?”. This works for “direct” associations, such as when a developer checks in code against a Bug, but also “indirect” – when a developer checks in against a Task, the Tasks parent Product Backlog Item (or User Story) is also associated with the build.\n\n\n\n\nHere we can see that Bug 82 was fixed in this build. We also see that Task 84 of PBI 83 is in this build.\n\nUnfortunately this won’t work out-the-box for merges. If you queue a build that has only merges as changesets, the only changesets you’ll see will be the merges themselves. Never fear though – I created a custom build task that pulls in the merged changesets and work items into the build report. You can get it here.\n\nFound In and Integrated In – Tracking Bugs Effectively\n\nIf you’ve got a build, and your testers specify that build number as the build they are testing, then any bug logged during testing has it’s “Found In” field automatically set. When you fix the bug and check it in, the Integrated In field is set so you know which build the bug was fixed in.\n\n\n\n\nThe System Tab of the default Bug work item: the “Found In” field gets populated automatically when logging a bug from MTM (where the build under test is specified) and the “Integrated In” field gets populated automatically when you resolve the bug during checkin.\n\nTest Impact Analysis\n\nLet’s say you have 100 manual tests. You run them all successfully. The developer then changes some code. Which tests should you run again? Ideally, all of them – but you may not have time to run all of them. So Team Build narrows down the list by doing test impact analysis. When you enable this diagnostic adapter in MTM, TFS builds a map of test vs code – it tracks which code is hit for each test the tester is executing. Then on the next build, for each passed test case, TFS does a lookup to see if any tests hit the code that changed since the last build. Each test that is “potentially impacted” is flagged during the build so that you can test is again to make sure the changes didn’t break the code. Of course TFS assumed you’ll re-run failed tests, so this only works against passed test cases.\n\n\n\n\nTwo tests were impacted by changes to the code – clicking on the “code changes” link opens details about what methods changed.\n\nBuild-Deploy-Test Workflow\n\nI won’t go into any details on this workflow, but you get a LabDefault.xaml template out-the-box when you create a Team Project. This build doesn’t compile code – it takes the output of another build (TFS or even another engine), allows you to specify scripts that automate deployment to a Lab Environment (that you’ve set up using MTM’s Lab Manager) and even run automated tests, which could include manual test cases that you’ve converted to Coded UI Tests.\n\nMetrics\n\nI’ve mentioned that the build data go into the TFS warehouse – so you can see test results, coverage and code churn over time. Then you can slice-and-dice and create dashboards and reports.\n\n\n\n\nThe out-of-the-box Build Summary report.\n\n\n\n\nThe out-of-the-box Build Success Over Time report.\n\n\n\n\nSome of the build-specific measures available when you pivot against the TFS Cube from Excel.\n\nSummary\n\nThere are other build engines that you can use (such as Hudson or TeamCity or Jenkins). Where they cannot compete with TFS is in the rich integration you get into work items, source control, lab management, testing and reporting. And you get most of it for free – out-the-box. In short, if you want to take a quantum leap in consistency and quality, you need to get building! The small investment up-front will be well worth it in the long run. And you’ll be able to sleep at night…\n\nHappy building!\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/automated-buildswhy-theyre-absolutely-essential-part-2/"
    },{
      
      "title": "IntelliTrace Tips and Tricks: The Basics",
      "date": "2013-06-18 21:33:00 +0000",
      
      "content": "Series Links:\n\n\n  Part 1: The Basics (this post) – also guest posted on MSDevDiv SA Blog\n  Part 2: IntelliTrace Everywhere – also guest posted on MSDevDiv SA Blog\n  Part 3: Enable Custom IntelliTrace Events with a Right-Click\n\n\nThis brief series of blog posts will show you how to get the most out of IntelliTrace – a historical debugger that allows you to record (and replay) program execution.\n\nUsing IntelliTrace\n\nYou can use IntelliTrace in the following scenarios:\n\n\n  During debugging in VS using F5\n  In Test Manager (enabled as a Data Diagnostic Adapter)\n  Anywhere (using the standalone collector)\n\n\nUnderstanding IntelliTrace “Modes”\n\nOut of the box, IntelliTrace has 2 broad modes – “Events Only” and “Events and Call Information”. Events Only is much more lightweight, and allows you to record “events” that occur when your program is running. These “events” are “interesting occurrences” – like ASP.NET calls or ADO.NET calls – or exceptions. The product team tried to record events which would help you understand how your code works so that you can understand long “cause and effect” chains.\n\nEvents and Call Information turns the dial up a lot – it collects not only all of the events from the Events Only mode, but also method calls (including in and out arguments). There are some limits placed on what data is collected – otherwise the logs would become even more enormous than they are using the default settings.\n\nF5 IntelliTrace\n\nUse this when you’re coding. It means you can debug and move back and forwards in your debug session without having to stop your application and add breakpoints. To turn it on, go to Tools-&gt;Options and go to IntelliTrace settings. Here you’ll see the 2 modes:\n\n\n\n\nOnce you’ve turned it on (in this case I’ve got the Events and Call Information mode on) you can start debugging. Click around your app, and then click the “Break All” link in VS in the IntelliTrace window:\n\n\n\n\n(Here I am debugging my Calculator application – click “Break All” to go to the IntelliTrace log).\n\nWhen you look at the log, you’ll initially see all the events that occurred. In this case, button clicks are “Gesture” events, so that’s what I see:\n\n\n\n\nLet’s say that I remember something weird happened when I clicked “=” when multiplying 2 numbers – well, I don’t have to restart the application, I can simply click on the “=” gesture after the “*” button click (in the above log, I’ve clicked 3 x 6 =). So I can click on that event and “rewind” to that point.\n\n\n\n\nWhat I can now do is click “Calls View” to start walking through the log from that point.\n\n\n\n\nDouble-click on the call to btnEqual_Click to “zoom” to that point of the execution. You’ll see the IntelliTrace glyphs in the gutter of the source window:\n\n\n\n\nIf you mouse-over the icons, you’ll get a tooltip. The functions (in the order they appear) are:\n\n\n  return to calling method\n  step back\n  step into\n  step over\n  return to Live Debugging\n\n\n(Just a handy tip: If you press the bottom button “return to Live Debugging” remember that this now puts you at the current debugger point – you’ll need to press F5 again if you want to start running the application again.)\n\nIf you advance using F11, the current pointer will advance. Pressing twice from the previous image gets me to the point where the program enters the switch statement:\n\n\n\n\nNotice that I don’t need to guess which case statement was selected – the log records exactly what the program did. I’ve also pinned the mouse-overs for val1 and val2 – you can see their values. Not all values are collected, but since these are primitives and are being passed into (or out of) a method, IntelliTrace dutifully collects their values.\n\nI’ll press F11 again to step into the Multiply() method, and then F11 again to advance one more event:\n\n\n\n\nThat puts me onto the closing curly brace of the Multiply method. If I look in the Autos window, I’ll see that the method in parameters (val1 and val2) were 3 and 6 respectively, and that the return value was 9.\n\nVoila – with 3 or 4 clicks we found the bug in our program. And we didn’t even have to set a breakpoint!\n\nFiltering Events\n\nAs you use IntelliTrace, you’ll start seeing large amount of events – to make the logs easier to navigate, you can use the category, thread and search box at the top of the events window to filter the events.\n\n\n\n\nFor example, expanding the dropdown with “All Categories” in it I can filter just exceptions:\n\n\n\nSearch For This Line…\n\nAnother useful tip is the “Search For this line” or “Search For This Method”. You suspect that a method was hit sometime during your debug session – but the log is quite long. No problem – right click a line (or method) and select “Search For This Line / Method in IntelliTrace”.\n\n\n\n\nOnce you do, you’ll see the search results in a bar at the top of the current code window – press the arrows to go to the previous or next instance of that line or method in the log file:\n\n\n\n\nHere I’ll click on the arrow icon directly after the work “Multiply” to go to the first call to this method in the log, and I can start “debugging” from there.\n\n\n\n\nIn the next post, I’ll show you how to run IntelliTrace anywhere and everywhere.\n\nHappy debugging!\n",
      "categories": [],
      "tags": ["development"],
      
      "collection": "posts",
      "url": "/intellitrace-tips-and-tricks-the-basics/"
    },{
      
      "title": "IntelliTrace Tips and Tricks: IntelliTrace Everywhere",
      "date": "2013-06-18 21:35:00 +0000",
      
      "content": "Series Links:\n\n\n  Part 1: The Basics (this post) – also guest posted on MSDevDiv SA Blog\n  Part 2: IntelliTrace Everywhere – also guest posted on MSDevDiv SA Blog\n  Part 3: Enable Custom IntelliTrace Events with a Right-Click\n\n\nIn my previous post I showed you how to enable IntelliTrace for debugging – F5 IntelliTrace. That’s all well and good, but what about getting IntelliTrace logs from your test environments? Or from production?\n\nHere’s where you can use IntelliTrace:\n\n\n  .NET 2.0 and above managed code (note: the collector requires .NET 3.5, so you’ll need that on the target server even if you app is in .NET 2.0)\n  Enable the IntelliTrace diagnostic adapter in Test Manager to collect logs during test runs\n  IIS: Use PowerShell to attach to an application pool and collect logs\n  Desktop Apps: Use IntelliTraceSC.exe to launch an app and collect logs\n  Windows Services: This one is tough, but possible. Involves some registry tweaking. Read more here.\n\n\nHere’s where you can’t collect IntelliTrace:\n\n\n  Silverlight applications\n  Windows Phone applications\n  .NET 1 applications\n  Native code applications\n\n\nThe IntelliTrace Standalone Collector\n\nYou can get this from the Visual Studio installation folder, or you can download it here. I recommend downloading it, since this will be the latest and greatest collector available. The page has a link to instructions about how to extract the cab file. Once you’ve expanded the cab, you’ll have the PowerShell module as well as the IntelliTraceSC.exe for collecting application data.\n\nSymbols\n\nBefore we look at how to collect logs, let’s talk about symbols. In order to open up code from the iTrace logs, you’re going to need to supply symbols. Ever seen a pdb file when you compile your apps? The pdbs map source code to compiled code. But of course no self-respecting developer ever deploys pdbs, right? So if you’re not deploying your pdbs (and you shouldn’t be) then where do you put them? You get the build to publish them to a shared folder, or Symbol Server. (If you don’t use Team Build, you can simply keep your pdbs somewhere – when you open an iTrace file, you can provide the location of your pdbs).\n\nHere’s an image showing where in the DefaultTemplate you can set the symbols location:\n\n\n\n\nOne more thing – you’ll need to set this location in the Debugging options of VS in order for it to look there for the symbols. In VS, go to the Tools-&gt;Options dialog. Then expand the Debugging-&gt;Symbols section. There are some buttons in the top right of the dialog – click the “New Location” icon (between the yellow warning icon and the ‘x’ button) and type in the same directory that you used in the Build Process settings (above).\n\n\n\n\nYou’ll also need to open the “General” tab under Debugging and set the following checkboxes:\n\n\n\n\nNow you’re ready to open the logs – let’s see how you can collect them.\n\nCollect IntelliTrace from Web Applications\n\nIf you’re developing and deploying web applications, you get a lot of love from IntelliTrace. Here are the steps you’ll need to follow to start logging:\n\n\n  Download the IntelliTrace collector and expand it.\n  Find the name (and identity) of the application pool that your web app is running under.\n  Create a log folder. Make sure the app pool identity has write access to this folder.\n  Open a PowerShell prompt. Go to the IntelliTrace folder. Run “Import-Module Microsoft.VisualStudio.IntellITrace.PowerShell.dll”\n  To list the commands, type “Get-Command *IntelliTrace*”. This will list the 5 IntelliTrace cmdlets.\n  To start logging, type\nStart-IntelliTraceCollection –ApplicationPool AppPoolName –CollectionPlan plan.xml –OutputPath logPath\n\n\nwhere\n\n\n  AppPoolName is the name of the app pool your application is running under\n  plan.xml is the collection plan (more on this in the next paragraph)\n  logPath is the path you want the log files dropped into\n\n\nThe plan.xml is the settings file for what IntelliTrace events you want to collect (and if you’re running in Events Only or Events and Call Information mode). You’ll see 2 xml files in the IntelliTrace folder that ship with IntelliTrace – the lightweight collection_plan.ASP.NET.default.xml and the diagnostic collection_plan.ASP.NET.trace.xml. I recommend starting with the default plan (Events Only) and if you get stuck then dial it up to the trace plan (Events and Call Information). In the next post, I’ll show you how to customize the collection to get fine-grained control over the events.\n\nBe aware that when you run Start-IntelliTraceCollection, IntelliTrace will attach itself to the app pool, but part of that will require a recycle.\n\nDon’t leave this on too long – especially if you’re using the trace plan. One the collector is running, you can use the following commands when you want to grab the log and open it:\n\n\n  Stop-IntelliTraceCollection – which stops the collector entirely\n  Checkpoint-IntelliTraceCollection – which unlocks the log file and starts logging to a new log file\n\n\nUse checkpoint when you want to open the log and leave the collector running (when the collector is running the file is locked by the collection process, so you won’t be able to open it).\n\nCollect IntelliTrace from Desktop Applications\n\nCollecting IntelliTrace from Desktop apps gets some love – not as much as the IIS scenario. Instead of launching your application directly, go to the IntelliTrace collector folder and run the following command:\n\n\nIntelliTraceSC.exe help launch\n\n\nto see the help on how to launch. Here’s the basic launch command:\n\n\nIntelliTraceSC.exe launch /cp:plan.xml /f:pathToLogFile application\n\n\nwhere\n\n\n  plan.xml is the collection plan – use the same ones as the web collection command – don’t worry that it’s called ASP.NET – it’ll work for most default scenarios\n  pathToLogFile is the full path and filename of the log file\n  application is the path to the application you want to collect the log from\n\n\nAs soon as you exit your application, the logging completes and you’ll have your log file.\n\nHappy logging!\n",
      "categories": [],
      "tags": ["development"],
      
      "collection": "posts",
      "url": "/intellitrace-tips-and-tricks-intellitrace-everywhere/"
    },{
      
      "title": "BITE–Branch Info Team Explorer Extension",
      "date": "2013-07-02 15:38:00 +0000",
      
      "content": "Update 2013-08-30: The extension is now available for VS 2013.\n\nBranching is something that you should definitely be doing if you’re a modern developer. It doesn’t matter if you have branch-per-release or dev-main-live kind of branching – you need to be able to separate development streams.\n\nSo let’s pick on the classic dev-main-live scenario. You create a solution in Visual Studio (in a MAIN folder), check it into Source Control in TFS and then create branches to DEV and LIVE. Now you have three versions of the same solution – one on each branch. I always recommend opening from Source Control so that you know which branch you’re on. However, if you’ve opened the solution and then been working for a while, you may want to double-check which branch you’re working from. Hmmm, you’re stuck – there’s no way to do this other than checking the folder path for the solution.\n\nEnter BITE\n\nWouldn’t it be cool if you could instantly see which branch your solution is on? And how about selecting one of the other branches and clicking a “Switch” button to switch to the same solution on another branch? Now you can – using BITE – the Branch Info Team Explorer Extension! (Yes, I know it’s cheesy, but I couldn’t help it once I’d seen the acronym).\n\nOnce you’ve installed the extension from the VS Gallery, you’ll see a new Link under the Pending Changes section:\n\n\n\n\nClick on “Branch Info” to see the extension in action.\n\n\n\n\nHere I’ve opened a solution on the MAIN branch (see the Current Branch label). I can see both the local and server paths for the solution. Also, there’s a drop-down labelled “Other Branches”. If I select one of the other branches, I can click the “Switch” button and the corresponding solution opens.\n\nLet me know if you have any issues using this extension. (In case you missed the link, get the extension from the VS Gallery).\n\nHappy branching!\n",
      "categories": [],
      "tags": ["development","sourcecontrol"],
      
      "collection": "posts",
      "url": "/bitebranch-info-team-explorer-extension/"
    },{
      
      "title": "Adding Custom Team Field to MS Project Mappings",
      "date": "2013-07-11 16:26:00 +0000",
      
      "content": "Note: This post is NOT about TFS and Project Server integration – this is for adding, editing and deleting work items using MS Project.\n\nI was working with a customer who are using a custom field (called “Team”) instead of Area Path to denote “ownership” of work items to teams. You can find out how to do this on Martin Hinshelwood’s excellent post. So I did the customization (the team is working off MSF Agile, and not the Scrum template, but the steps are the same). Everything was looking great.\n\nThen their project managers (who work a lot in MS Project) started adding work using the MS Project. However, once they added in the User Stories, the items did not appear on the Backlogs. I realized that this is because the Backlogs are filtered to Team – any work item that does not have the Team field set (when using a custom team field) won’t show for the team.\n\nThe solution: customize the Team Project “TFSFieldMapping” – this is the file that controls how Work Item fields map to MS Project fields.\n\nCustomize TFSFieldMapping\n\nFirst, you have to download the mapping file. Open up a VS command prompt and navigate to this folder:\n\n%programfiles%\\Common Files\\microsoft shared\\Team Foundation Server\\11.0\n\n\n(This is a different folder than previous versions of TFS for some strange reason).\n\nRun the following command:\n\nTFSFieldMapping download /collection:CollectionURL /teamproject:ProjectName /mappingfile:MappingFile\n\n\nwhere collectionURL is the URL to your collection, ProjectName is the name of your Team Project and MappingFile is the file you want to create.\n\nNow edit the file in your favourite editor and add the following line:\n\n&lt;mapping workitemtrakingfieldreferencename=\"”Custom.Team”\" projectfield=\"”pjTaskText27”\" projectname=\"”Team”\"&gt;&lt;/mapping&gt;\n\n\nOf course the reference name is the reference name of your custom team field. The ProjectName attribute is a friendly name for the field – this will actually be the column name in Project. The project field is an unused field in the project plan (to see these, open Project and add a new column to the plan – the fields that display here are the fields that you need to use for the mapping – see the below picture).\n\n\n\n\nNow you can upload the mapping file again:\n\nTFSFieldMapping upload /collection:CollectionURL /teamproject:ProjectName /mappingfile:MappingFile\n\n\nNow when you open up a project plan, you’ll be able to add the “Team” field:\n\n\n\n\nThe column will even get its values from your global list:\n\n\n\n\nNow when you add work using MS Project, you can select the Team and the work will appear on the Backlogs accordingly. Of course this technique will work for any other fields too.\n\nHappy Mapping!\n",
      "categories": [],
      "tags": ["alm"],
      
      "collection": "posts",
      "url": "/adding-custom-team-field-to-ms-project-mappings/"
    },{
      
      "title": "Why You Absolutely Need to Unit Test",
      "date": "2013-07-18 20:56:00 +0000",
      
      "content": "I’ve written about why builds are absolutely essential in modern application development (Part 1) and what why Team Build is a great build engine (Part 2). However, if you don’t include unit tests in your builds, it’s like brushing your teeth without toothpaste – there’s a lot of movement, but it’s not the most effective way to do things. In this post, I want to put forward a few thoughts about why you absolutely need to be unit testing.\n\nHere’s the highlight list:\n\n\n  Coding with unit testing in mind forces you to think about the design and architecture of your code – making it better code\n  Unit testing provides immediate feedback – every change in code is tested (at least partly) as you’re coding\n  A small investment now leads to a massive saving later – bugs in production cost way more than bugs in development\n  Unit testing provides metrics for the quality of your code\n  Unit testing builds inherent quality into your releases\n\n\nLet’s consider each of these statements.\n\nThink-Driven Development\n\nYou’ve probably heard of terms like “test-driven development (TDD)” or “behaviour-driven development (BDD)”. I’d love to coin another term – think-driven development. I come from a development background – and since I love coding (as most developers do), I suffer from “never-mind-the-details-I-just-want-to-start-coding” syndrome. Most developers I meet do. Hopefully you never lose this passion – but there’s a lot to be said for taking a breath and thinking first.\n\nIf you just jump into the code, and you don’t have tests, you’ll probably hack something out that will work initially for a couple of scenarios (usually with the attendant caveat, “it works on my machine!?!”). However, this isn’t sustainable since it leads to what one of my university lecturer’s used to call “spaghetti code” (which was especially funny since he’s Italian). You write some code and get it deployed. Something breaks. You band-aid it and re-deploy. Now you’ve broken something else. So you add some sticky-tape (metaphorically, of course). Eventually you’re down to chewing gum and tin-foil, and things are going from bad to worse.\n\nIf you start out writing tests (or at least start writing your code with testing in mind), you’re more likely to come up with better code. If you’ve every tried to write unit tests for legacy code, you’ll understand what I mean. If you don’t code with tests in mind, not surprisingly, your code ends up untestable (or at least really hard to unit test). Writing testable code forces you to think of good decomposition, good interfaces, good separation of concerns, inversion of control and dependency injection and a whole slew of other principles we all learn about but somehow forget to use in our daily grind.\n\nSo start with your tests. This forces you to use all of the good stuff (they’re not called best practices for nothing). The little bit of thinking required up-front is going to save you a whole lot of pain down the line (not to mention decrease the amount of chewing gum you find in your source repository).\n\nImmediate Feedback\n\nLet me ask you a question – how long is the feedback loop between the time you write some code and when you get feedback about the validity of that code? Or phrased another way, think about the last bug you fixed. What’s the length of time between the writing of the code and the report of the bug? One day? One week? One year?\n\nThe fact is that the sooner after writing code that you find a bug, the cheaper it is to fix. Let’s consider two ends of the spectrum: coding-time and in production.\n\nWhen a bug is found in production, it can take a while to find it. Then it’s reported to a service desk. They investigate. They then escalate to 2nd line support. They investigate. They then escalate to the developers, who have to investigate and repro. In other words, a long time.\n\nWhat about when you’re coding? You write some code and run your unit tests. The test fails. You investigate and fix. In other words, a really short time.\n\nThe quicker you get feedback (and the more often you get feedback) the more efficient and effective you’re going to be. On a large scale, this is broadly the reason for Agile methodologies – the rapid iteration increases the frequency of the feedback loop. It works at a project management level, and it works at a coding level too.\n\nHere’s how that looks on a graph:\n\n\n\n\nThe earlier you find your bugs, the less time you’ll spend finding them and the cheaper it is to fix them. If you don’t have unit tests, you’re already moving your first feedback opportunity to the 3rd zone (manual testing). If you don’t do manual testing, you’re pushing it further out again into UAT. If you don’t do that – then the 1st feedback you’re going to get is from production – which is the most time-intensive and the most costly.\n\nGetting immediate feedback while you’re coding is great when you’re doing “greenfield” projects (projects that have never been deployed before. It really starts to shine when you come back after 6 (or 12 or 18) months to add features – you still have your suite of tests to ensure that you’re not breaking anything with your new code.\n\nSpend a little now – save a lot later on\n\nAlmost without fail the teams that don’t unit test claim that “they don’t have time to unit test”. I argue that in the vast majority of such cases, it’s exactly because you don’t invest in unit tests that you don’t have time to code properly. Every time you deploy untested code, you increase your technical debt. The sad fact is that technical debt tends to grow exponentially.\n\nLook again at the graph above. Where do you think you’ll spend more time finding and fixing bugs? Obviously the further “to the right” you are, the more time you need to fix a bug. So invest a little “now” while you’re coding, so that you don’t have to spend a lot of time later on dealing with production issues!\n\nQuality Metrics\n\nHow do you measure the quality of your code? Bugs in Production per time-period? Mean Time to Resolve (MTTR)? Most of these measurements are helpful to some degree, but because they’re “production time” statistics, the feedback is too late in the cycle.\n\nThe teams I work with that do have metrics invariably have unit test metrics – pass / fail rates and code coverage statistics. The teams that don’t have metrics almost always don’t have unit tests. Unit tests provide you with an objective measure of the quality of your code. How can you improve if you don’t know where you currently are?\n\nWhat coverage should you be aiming for? This is debatable, but I always recommend that you rather concentrate on your trends – make sure each deployment is better than the last. That way the absolute number doesn’t really matter. But the only way you can do any sort of trend analysis is if you’re actually collecting the metrics. That’s why I love unit tests (with code coverage) in TFS Team Builds, since the metrics get collated into the warehouse and it’s really easy to do trend reports. Showing business a steadily improving graph of quality is one of the best things you’ll ever do as a developer!\n\nInherent Quality\n\nAll of these practices lead to inherent quality – a quality that’s built into what you’re doing day-to-day, rather than something you tack on at the end of an iteration (or quickly before release). If you build inherent quality into your processes and practices, you can sleep soundly the night of your big deployment. If you don’t, you’ll have to keep the phone close-by for all those inevitable calls of, “Help, the system is broken…”\n\nLegacy Code\n\nIn my previous company we had large amounts of “legacy” code – code that was in production, that we constantly supported, that had no tests. When I started pushing for unit tests and coverage, we inevitably got to the awkward moment where someone blurted, “There’s just too much code to start testing now!”. I argued that 1 test was better than 0 tests. And 2 tests are better than 1 test, and so on.\n\nWe then made it a policy that whatever code you worked on (be that new features or bug-fixes) needed to be unit tested. This started us on the path to building up our comprehensive test suite. We also put a policy in place that your code coverage had to be higher for this deployment than the previous deployment in order for your deployment to be approved. At first our coverage was 1.5 or 2%. After only 6 months, we were somewhere in the 40%’s for coverage. And the number of production issues we were working on was decreased dramatically.\n\nConclusion\n\nJust like you can’t afford not to do automated builds, you really can’t afford not to do unit testing. The investment now will provide massive benefits all along your application lifecycle – not only for you and your team, but for your business and stakeholders too.\n\nFor more reading, Martin Hinshelwood wrote a great post about test-first that I highly recommend.\n\nHappy testing!\n",
      "categories": [],
      "tags": ["build","testing","development"],
      
      "collection": "posts",
      "url": "/why-you-absolutely-need-to-unit-test/"
    },{
      
      "title": "Frequent Status Updates–What They Really Mean",
      "date": "2013-08-20 21:39:00 +0000",
      
      "content": "Are you (as a developer) inundated with frequent status updates? Requests like: “How far are you?” “What did you do today?” “Where are we?” Or are you a project manager that requests frequent status updates? Then this post is for you.\n\nLet’s start by defining frequent – I think this is going to be different for different teams, and will vary with the Application Lifecycle Management (ALM) maturity within the team. I would go so far as to say that one status update request a week is too frequent. Certainly one a day is far too frequent.\n\nFrequent Status Updates reveal some symptoms of poor process:\n\n\n  Measuring the Wrong Things\n  Large Batch Size\n  Lack of Transparency\n  Lack of Trust\n  Unpredictable Meeting Times\n\n\nMeasuring the Wrong Things\n\nIn his book The Principles of Product Development Flow, Donald Reinersten frequently discusses the impact of measuring “proxy variables”. Proxy variables are quantified measures that are substitutes for real economic measures. For example, most developers measure and try really hard to reduce cycle time, but if you ask them how much it would cost to delay a release by a week, they would have no idea. Instead of measuring the economic impact of decisions (the real variable), they’re measuring cycle time (a proxy variable).\n\nSimilarly, frequent status updates are attempts to measure productivity or value-added. However, “being busy” is simply a proxy variable for delivering value. The Observer Effect shows us that the very act of measuring a system alters the system. Status updates usually require a fair amount of effort (especially if you don’t have a proper work management tool) so every time you ask for status updates, you slow the team.\n\nThis gets even worse when the time to gather the status report is long – this can mean that by the time you get the report, the report is out of date. That’s not helpful or valuable to anyone.\n\nAnother problem that the frequent updates could indicate is measuring success by conformance to the plan. Again this is just a proxy to did the team deliver value. Product Development is volatile and unpredictable at times – trying to manage that by eliminating change is usually a bad idea. Responding rapidly to and managing change is far more valuable. If a project manager is always checking on the team to make sure they’re conforming to a plan, then perhaps that indicates that the plan is incorrect?\n\nLarge Batch Size\n\nIf you get “value” at the end of a batch, then it stands to reason that the larger your batch is, the longer you’ll wait to get value. This can cause a knee-jerk reaction: treatment of a symptom (long spaces between feedback), rather than treatment of a cause (too large batch sizes). In other words, if you have to request feedback to see where you are, then perhaps you should shrink your batch size so that you get “feedback” (delivery of value) quicker. Shorter batches negate the need to status updates since they end quickly.\n\nThere are a myriad of other benefits to smaller batch sizes (including improving cycle times) that I won’t go into here. However, smaller batch sizes allow the end of the batch to provide the feedback, rather than having to ask for status mid-batch. This removes the “cost” of gathering the status report during the execution of a batch.\n\nLack of Transparency\n\nFrequent status updates can highlight the fact that there’s very little transparency into what work is in the pipeline. If you don’t have an effective tool to manage work that anyone can query or report from, then you’ll see a lot of “how far are you” requests. I’d even go so far as to say if you don’t have a tool to visualize your pipeline, you’re still going to get a lot of status update requests.\n\nLack of Trust\n\nOften this is an extension of Lack of Transparency. If there is a lack of trust between the project manager and the team, then often a symptom of this lack of trust is frequent status updates. The project manager doesn’t trust the team to deliver value, so they continually “police” the team to make sure that the team is working according to the plan. In high-trust environments, there is a lack of these frequent “how far are you” requests.\n\nUnpredictable Meeting Times\n\nThis may seem basic, but it’s surprisingly common. If you have a status update meeting “when some milestone is reached”, and there is some slippage, then you need to re-organise the meeting again. If, however, you have a fixed status meeting date and time (like a daily stand-up, perhaps?) then you never have to re-organise meetings. You know exactly when the status is going to be communicated.\n\nHow to Counter Frequent Status Updates\n\nMeasure the Right Things\n\nA busy developer is not necessarily a productive developer. Conversely, having spare capacity does not mean that work won’t be done on time. We need to move beyond the archaic and out-dated models of Product Development Management and embrace new paradigms. This means that we need to start measuring the right things – such as cycle times, batch sizes and value delivered. In The Principles of Product Development Flow, Reinersten starts off by bringing us back to basics: economics. If you can’t express your measurement in financial terms, then it’s probably not worth measuring. To refer back to the cycle time argument, you can only focus on reducing cycle time if you actually know how much the cost of delay is. Start measuring the right things, and you’ll see your “how far are you?” requests dry up (much to everyone’s relief).\n\nReduce Batch Size\n\nWorking smaller queues has many benefits – one of these is that if the queue is short enough, it finishes before you need an update. This eliminates the need for a status update altogether.\n\nSurface (and Visualize) Work In Progress\n\nHaving a way to visualize your work in progress is critical – whether you use a whiteboard with stickys or have some sort of electronic system, make sure you can visualize your work in progress, especially your queues. This will not only allow anyone to see what’s going on, but to start making decisions without having to call everyone together.\n\nImprove Trust\n\nTrust is earned, and that’s certainly the case in Product Development. But remember, it’s a two-way street – business needs to earn ITs trust, and the other way around too. This is a “soft-skill”, and while it’s the hardest to get right, in my opinion it’s the one with the highest reward level. High trust environments foster creativity, productivity and sustainability, so work on making your environment one of high trust.\n\nSchedule Predictable Meetings\n\nJust do it – make a regular, time-boxed meeting (like a daily stand-up). You’ll never have to guess if the meeting is on or not, you always know it’s on. Time-box it to keep it snappy and meaningful so that people value them. You’ll start finding that this meeting gives you pretty much everything you need to know, so you won’t need to ask people where they are.\n\nConclusion\n\nIf you’re a victim (or perpetrator!) of frequent status updates, you may want to assess your ALM process to do a health check. Frequent status updates indicate bad health and you should change your processes accordingly.\n",
      "categories": [],
      "tags": ["alm"],
      
      "collection": "posts",
      "url": "/frequent-status-updateswhat-they-really-mean/"
    },{
      
      "title": "Fix: InRelease Demo “Hangs” on Keller’s 2013 Preview VM",
      "date": "2013-08-26 19:02:00 +0000",
      
      "content": "Update 2012-09-04: Brian Keller posted a fix that seems to work for this problem (so you can run the InRelease build without connecting to a physical external network).\n\nI love Brian Keller’s VMs. I use them as the starting point for my TFS demos (which I’ve probably done hundreds of now). Brian’s latest VM with TFS 2013 is not quite what the 2012 one was (no reporting services or cube, no MS Project and a host of other things missing) but it’s enough to get started with.\n\nOne of the big new features of TFS 2013 is Release Management (with the acquisition of InRelease). An Brian provides a hands-on-lab that lets you kick the tires a bit. The lab starts with you making a small modification to the FabrikamFiber website, and then using a TFS build to trigger a deployment in InRelease. I dutifully made my change and queued the build – but the build hangs, and never completes.\n\n\n\n\nI can run other builds successfully, and if I turn off “Trigger Deployment” in the build, the build completes. That led me to conclude that InRelease was hanging for some reason. Neither the event logs, nor the InRelease logs had any useful info.\n\nThe Solution: Connect the VM to an External Network\n\nI mailed Brian and he gave me a few things to try – the one that ended up working was connecting the VM to an external network. From that point on, the demo works as scripted. (Read this post to see how to connect your HyperV VM to your WiFi, and this one about connecting your VM to a 3G dongle).\n\nI’m not sure why you need an external network for this – there seems to be some challenges in the networking logic of InRelease. This is just a preview release, so hopefully this gets fixed before release.\n\nAnyway, if you’ve been having this issue while working the lab, then hopefully you can now continue to check it out.\n\nHappy releasing!\n",
      "categories": [],
      "tags": ["alm"],
      
      "collection": "posts",
      "url": "/fix-inrelease-demo-hangs-on-kellers-2013-preview-vm/"
    },{
      
      "title": "Getting Results from Backlog Overview Report in TFS 2013 Preview",
      "date": "2013-08-28 17:19:00 +0000",
      
      "content": "One of my favourite reports in TFS is the Backlog Overview (Scrum) or User Story Overview (Agile). So after installing and playing with TFS 2013 Preview, I went to see what the report looks like.\n\nWhat I found wasn’t pretty: though I could verify that there was data in the warehouse, the report stubbornly refused to show any data.\n\n\n\n\nI thought that something was broken with my warehouse, so I dug into the rdl file and into the warehouse database. I could see data, but one of the queries wasn’t returning any data. It turns out the query is looking for the “root” level of “deliverables”. This defaults to the set (PBI, User Story, Requirement) which would work perfectly in the out-of-the-box 2012 templates. But the 2013 templates now root “higher up” in Features. So you have to add Feature to the list. Here’s how to do it:\n\n\n  Browse to the reports folder root (usually this is http://server/reports) where server is the name of your TFS box. Now navigate through TfsReports to the collection and team project folder where your “Backlog Overview” report is:\n\n\n\n\n\n\n  Hover your mouse over the “Backlog Overview” report and click the arrow to expand the menu. Select “Manage”.\n\n\n\n\n\n\n  Click on the Parameters tab on the left and find the parameter called “DeliverableCategory”. Add “Feature” to the list. Don’t forget to scroll down and press the “Apply” button!\n\n\n\n\n\nVoila! You now have data when you browse to the report. The PBIs are grouped under their respective Features, which is a nice bonus.\n\n\n\n\nHappy reporting!\n",
      "categories": [],
      "tags": ["alm"],
      
      "collection": "posts",
      "url": "/getting-results-from-backlog-overview-report-in-tfs-2013-preview/"
    },{
      
      "title": "Branch Info Team Explorer Extension (BITE) Now Available for VS 2013",
      "date": "2013-08-31 03:24:00 +0000",
      
      "content": "Update 2013-09-12: I’ve updated the extension to work with VS 2013 RC (since there were some breaking changes from Preview).\n\nI wrote a Team Explorer Extension (BITE) a few months ago to show you which branch your solution is on and how to easily change to the same solution on another branch.\n\nToday I opened up and converted the extension for VS 2013.\n\nSince the architecture of the Home page in Team Explorer has changed a little, it wasn’t simply open and recompile for VS 2013. The BITE page actually stayed the same – but the classes that allow me to hook into the Team Explorer had to change quite a lot. And of course there’s scant documentation – even for extending 2012, never mind the dearth of information about extending Team Explorer 2013. Anyway, nothing that Reflector couldn’t help me with…\n\nHere’s what the extension looks like in the VS 2013 Team Explorer Home page:\n\n\n\n\nWhile I was at it, I cleaned up the UI a little especially when you don’t have a solution open or the solution you have open is not branched. Other than that, it works exactly like before – only for TFVC, of course!\n\nHappy branching!\n",
      "categories": [],
      "tags": ["tfsapi","alm"],
      
      "collection": "posts",
      "url": "/branch-info-team-explorer-extension-bite-now-available-for-vs-2013/"
    },{
      
      "title": "New Feature: Lightweight Query Charts in Web Access",
      "date": "2013-09-09 22:40:00 +0000",
      
      "content": "I have installed TFS 2013 RC. I upgraded my TFS Express (that I use for mucking around with code) from TFS 2012.3 and everything went smoothly. I then opened up Web Access and was pleased to see one of the best features yet for TFS work items: lightweight charts.\n\nThese charts allow you to quickly and easily create visualizations against your work item queries. Here is a “dashboard” against a query that lists “Tasks” in my Team Project:\n\n\n\nCreating Charts\n\nCreating charts is really simple. Navigate to Web Access and click “WORK” and then “Queries” to go to the query hub. Select or create create a query. Make sure you add any columns that you want to group by onto the query – for example, State, Assigned To or Iteration Path. Once you’ve saved your query, click on the “Charts” tab:\n\n\n\n\nNow enter a name, select the chart type and the field to group by. For example, here I am doing a pie graph grouped by “State”:\n\n\n\n\nYou can also make a Pivot table – this allows you to select rows and columns. For example, here’s one showing Assigned To vs State:\n\n\n\nLimitations\n\nUnfortunately, you can only use “count of work items” as the value setting. I tried to see if I could add “Remaining Work” and show remaining work per user, but no dice at the moment. We’ll have to wait for a future update to get this sort of functionality.\n\nWish List\n\nI’d love to see the ability to “Favourite” one of the charts so that it appears on the landing page. It would also be nice if you could edit the colours, add filters and email the charts (or at least a chart link). Also, you can’t drag-and-drop to re-order the charts. We’ll have to see what the product team gets time to actually squeeze into this feature.\n\nIn the meantime, happy charting!\n",
      "categories": [],
      "tags": ["alm"],
      
      "collection": "posts",
      "url": "/new-feature-lightweight-query-charts-in-web-access/"
    },{
      
      "title": "Contributions to TFS Build Extensions",
      "date": "2013-09-12 00:23:00 +0000",
      
      "content": "Some of the code I’ve written before has made it into the TFS Build Extensions latest release. They are my “Include Merges (and associated work items) in a build” and “Fail code based on Code Coverage” activities.\n\nIf you use the activities, make sure to leave comments or suggestions on the codeplex site.\n\nHappy build customizing!\n",
      "categories": [],
      "tags": ["news"],
      
      "collection": "posts",
      "url": "/contributions-to-tfs-build-extensions/"
    },{
      
      "title": "Fix: You Open TfcvTemplate.12.xaml and Don’t See Any Parameters",
      "date": "2013-09-14 00:23:00 +0000",
      
      "content": "I upgraded my demo environment from 2013 Preview to 2013 RC. Everything looked good until I got to the builds. I had configured a couple of default builds – the 2013 default template is actually stored in the TFS database (not in source control like the old Default xaml files) unless you actually download it for customizing.\n\nHowever, when I opened the build definition, the parameters section of the Process tab was empty:\n\n\n\n\nAll the other tabs worked just fine, and all the other (XAML from source control) templates worked just fine too.\n\nI quickly mailed the Champs List, and got some great assistance from Jason Prickett of the product team. I attached a debugger to VS and opened the template, and I got some “could not load assembly” errors for Newtonsoft.Json.dll.\n\nJason then told me the solution was simple: copy the dll. So I copied\n\n\nC:\\Program Files\\Microsoft Team Foundation Server 12.0\\Tools\\Newtonsoft.Json.dll\n\n\nto\n\n\nC:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\Common7\\IDE\\PrivateAssemblies\\\n\n\nand that resolved the problem.\n\nNow I can create, edit and run builds again. And I’m loving the new RC features.\n\nHappy building!\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/fix-you-open-tfcvtemplate12xaml-and-dont-see-any-parameters/"
    },{
      
      "title": "Monitoring Web Applications – Continuous IntelliTrace",
      "date": "2013-09-27 20:01:00 +0000",
      
      "content": "If you have Visual Studio Ultimate and are not using IntelliTrace in production, you should be drawn and quartered. This is arguably the best feature of Visual Studio Ultimate, and in my opinion this feature alone justifies the pricing (never mind Web Performance and Load testing, Code Maps, Code Lens, UML diagrams and Layer diagrams).\n\nThe standalone IntelliTrace collector is amazing, and will run anywhere. It’s especially useful for diagnosing problems in Web Applications running in IIS. (For a recap on how to use this tool, see my series starting here).\n\nOften when I talk about collecting IntelliTrace logs, people invariably ask, “Why can’t I leave the collector running all the time?”. This depends on how resilient you are to the performance impact of the collector, as well as what “mode” you’re using. If you using the verbose “Call and Events” mode, you’re going to see a performance knock. If you use the “Events Only” mode, you may see less impact (of course the logs won’t be as rich) but even this can be too much degradation.\n\nAbout a week ago, Microsoft released the Microsoft Monitoring Agent. This agent can be run “standalone” or be connected to System Center Operations Manager (SCOM) 2012 R2. I’m going to show you how you can run this agent in standalone mode in this post.\n\nWhy use the Monitoring Agent?\n\nAh, we get to the crux of the matter – why use this agent instead of the IntelliTrace standalone collector? The answer is two-fold:\n\n\n  You get performance monitoring “for free” when you use the Monitoring Agent\n  You can leave the agent on – permanently\n  You can target a specific web application (instead of a whole application pool)\n\n\nUnfortunately (for some) the agent is something you install – not like the IntelliTrace standalone collector that is just xcopy-able. If installing agents in your production environments is not a challenge, then you should be switching to the Monitoring Agent.\n\nRunning Continuously\n\nThere’s a caveat to running the monitor continuously (isn’t there always?). You really only want to do this in one of the three monitoring modes available – “monitor” mode. (The other two are “trace” and “custom”).\n\nThe IntelliTrace standalone collector comes with two xml configuration files out-the-box: events only (default) and events and call information (trace). “Trace” mode will give you the same as the “events and call information” mode of IntelliTrace standalone collector – it’s verbose, but it’ll knock your performance. You’ll have to be selective about when you run this mode. The “custom” mode let’s you run the collector using a custom IntelliTrace xml file, so you can tailor the logging just so.\n\nThe Monitoring Agent’s “monitor” mode is like the events only (default) setting of the IntelliTrace collector, but even more lightweight. Instead of collecting all events, it only collects exceptions that bubble up to the global exception handler and “slow” events (events that take the server longer than 5 seconds to respond). Also, it’ll only record 60 triggers of each event daily. And since you can specify which web site to monitor, you don’t have to impact your whole application pool.\n\nThe upshot of this is that you can turn monitoring on – and leave it on. Then you can “checkpoint” the log at any time – either when there’s a problem or proactively whenever you want to.\n\nRunning the Monitoring Agent\n\nI downloaded and installed the Monitoring Agent from the download site. I ran the installer, and configured it to run standalone (i.e. skipped hooking it up to SCOM). Thereafter I opened a PowerShell command prompt, but the module isn’t imported for some reason. No worries – just run this command:\n\nImport-Module installPath\\Agent\\Microsoft.EnterpriseManagement.Modules.PowerShell.dll\n\nFrom there, you can follow Larry Guger’s post to see how to run the agent, checkpoint the log and review the results (especially performance events).\n\nHappy monitoring!\n",
      "categories": [],
      "tags": ["alm"],
      
      "collection": "posts",
      "url": "/monitoring-web-applications-continuous-intellitrace/"
    },{
      
      "title": "TFS 2013 Default Build – The GetEnvironmentVariable<T> Activity",
      "date": "2013-10-30 20:34:00 +0000",
      
      "content": "If you’ve upgraded to TFS 2013, then you’ll notice that there’s a new Default Build template. In fact, to support Git repositories, the product team moved the default template into a super-secret-database-backed-folder-you-can’t-get-hold-of-place in TFS. This means that you won’t see it in the BuildProcessTemplates folder.\n\nBut the product team did make the default template quite flexible by building in pre- and post-build and pre- and post-test script arguments. To see how to use a pre-build script, refer to my post about versioning assemblies to match build number.\n\nSo if you’re going to customize the default template, you’ll have to download it first. Once you download it, you’ll see that it’s quite a bit “smaller” than the old default template. The team has “bundled” a bunch of very fine-grained activities into higher level activities. However, this means that some of the items that existed in the 2012 default template no longer exist. For example, the AssociateChangeSetsAndWorkItems activity in 2012 returns the associated work items – but the 2013 AssociateChanges activity has no return parameter. So how do you get the associated changesets? Another example is the SourcesDirectory – this used to be available from an assign activity (which created the variable and put it into a variable) in 2012 – but there’s no variable for this value in the 2013 workflow.\n\nHow then do you get these values? I’ll show you how you can get access to them via a new Activity called “GetEnvironmentVariable”. We’ll do this for SourcesDirectory and associated changes.\n\nDownloading the TfvcTemplate.12.xaml Template\n\nIf you’re going to customize the workflow for the default activity, you’ll need to download it. In VS 2013, Go to the Team Explorer and click on Builds. Click “New Build Definition”. Click on the Process tab. Now expand the “Show Details” button on the right to show the template details. You’ll see a “Download” link. Click it and save the template somewhere (possibly to the BuildProcessTemplates folder?)\n\n\n\n\nNow you can open the template to edit it (don’t forget to add it into Source Control!).\n\nOnce it’s open, add in your variables – I’m just scoping mine to the whole workflow, so I’ll add them with the root “Overall build process” activity. Click on “Variables” and add a string variable called “sourcesDir” and an IList variable called “associatedChangesets”.\n\n\n\n\nNow find the place where you want to get the variables – in my case, I’ll go right down to the bottom of the Try-Catch in the RunOnAgent activity just after the “Publish Symbols…” activity.\n\nBEWARE : The finally of this Try-Catch invokes a “ResetEnvironment” activity which will clear all the environment variables. If you need variables after this point of the workflow, be sure to remove this activity.\n\nFrom the toolbox, under the “Team Foundation Build Activities” section, drag on a GetEnvironmentVariable activity. I made the type “String” for the 1st activity, and renamed it to “Get Sources Dir”. Then press F4 to get the properties of the activity – set the result to “sourcesDir”.\n\nThe name parameter you can get from an enumeration - Microsoft.TeamFoundation.Build.Activities.Extensions.WellKnownEnvironmentVariables. This enum has a list of all the variables you can query using the activity.\n\nI set the value to “Microsoft.TeamFoundation.Build.Activities.Extensions.WellKnownEnvironmentVariables.SourcesDirectory”\n\n\n\n\nNow drag on another GetEnvironmentVariable activity and set the type to IList (optionally change the name). Set the result to “associatedChangesets” and the name to “Microsoft.TeamFoundation.Build.Activities.Extensions.WellKnownEnvironmentVariables.AssociatedChangesets”. (You’ll see AssociatedCommits too if you’re doing a Git build customization).\n\nThat’s all there is to it – you can now use the variables however you need to.\n\nHappy building!\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/tfs-2013-default-build-the-getenvironmentvariablet-activity/"
    },{
      
      "title": "WebDeploy and Release Management – The Proper Way",
      "date": "2013-11-29 23:50:00 +0000",
      
      "content": "I’ve just completed recording a Release Management webcast (more on Imaginet’s Visual Studio webcasts here). While doing the webcast, I wanted to show how you can use tokens which Release Management can substitute during the Release Workflow. Brian Keller suggests a .token file (basically an exact copy of your web.config file except that you use tokens instead of values) in his Release Management hands on lab, but I hate having to keep 2 copies of the same file around.\n\nOf course, being a huge fan of Web Deploy, I could use config transforms. The problem with mixing config transforms and Release Management is that you’d have to have a configuration per environment in your solution, and you’d end up having to create n number of web deploy packages where n is the number of Release Management stages you have. So if you had Dev, QA and Prod stages, you’d have to add at least one configuration to your solution so that you’d have Debug (for Dev), QA (for the QA environment) and Release (for Prod). Technically you wouldn’t be deploying the same package to each environment, even though they could be built at the same time from the same source files.\n\nI bingled a bit and found two posts that looked useful. The first was about tokenization, and had the downside is that you’re still doing an xcopy deployment rather than a web deploy package deployment, and you have to do some rather nasty gymnastics with the build parameters in order get it to work. The second was a lot cleaner, except for the fact that you have to know the folder on the server where the website ends up after the web deploy command, since the token replacement is done after the invocation of the web deploy Tool.\n\nI was convinced there was a cleaner solution, and I managed to come up with one. Basically, we use Web Deploy Parameters to tokenize the web.config file, and then do the token replacement before invoking Web Deploy.\n\nParameters.xml\n\nWeb Deploy lets you define parameters in a file called Parameters.xml. If there isn’t one in your project (alongside the web.config) then it creates a default one during publishing, so normally you don’t see it at all.\n\nLet’s imagine that you have the following web.config snippet:\n\n&lt;connectionstrings&gt;\n  &lt;add name=\"FabrikamFiber-Express\" connectionstring=\"somestring\" providername=\"System.Data.SqlClient\"&gt;\n&lt;/add&gt;&lt;/connectionstrings&gt;\n&lt;appsettings&gt;\n  &lt;add key=\"webpages:Version\" value=\"2.0.0.0\"&gt;\n  &lt;add key=\"PreserveLoginUrl\" value=\"true\"&gt;\n  &lt;add key=\"ClientValidationEnabled\" value=\"true\"&gt;\n  &lt;add key=\"UnobtrusiveJavaScriptEnabled\" value=\"true\"&gt;\n  &lt;add key=\"DemoEnv\" value=\"Development\"&gt;\n&lt;/add&gt;&lt;/add&gt;&lt;/add&gt;&lt;/add&gt;&lt;/add&gt;&lt;/appsettings&gt;\n\n\nThere’s a connection string and an appSetting key that we want to tokenize. Right click the project and add a new xml file called “Parameters.xml”. Right click the file, select Properties and set the “Build Action” to None to make sure this file doesn’t end up deployed to your website. Now we add the following xml:\n\n&lt;!--?xml version=\"1.0\" encoding=\"utf-8\" ?--&gt;\n&lt;parameters&gt;\n  &lt;parameter name=\"DemoEnv\" description=\"Please enter the name of the Environment\" defaultvalue=\" __EnvironmentName__\" tags=\"\"&gt;\n    &lt;parameterentry kind=\"XmlFile\" scope=\"\\\\web.config$\" match=\"/configuration/appSettings/add[@key='DemoEnv']/@value\"&gt;\n  &lt;/parameterentry&gt;&lt;/parameter&gt;\n&lt;/parameters&gt;\n\n\nWe create a parameter with a name, description and default value. The format of the default value is important – it needs to be pre- and post-fixed with double underscore “__” since this is the token format for Release Management. We then specify a “kind” of XmlFile, set web.config as the scope and specify an XPath to find the parameter in the web.config file.\n\n(You’ll notice that we don’t need to specify any parameter for the connection string, since that will be tokenized in the publish profile)\n\nPublish Profile\n\nRight-click your web project and select “Publish” to create (or edit) a publish profile. Expand the dropdown and select  to create a new profile. I named mine “Release”. Click Next.\n\n\n\n\nOn the Connection page, select “Web Deploy Package” as the publish method and enter a name for the package location. Typically this is (name of your project).zip. For Site name, enter “__SiteName__” to create a Release Management token for your site name. Click Next.\n\n\n\n\nOn the Settings page, select your configuration (I selected Release, which applies and config transforms in the Web.Release.config file in my solution, such as removing the Debug attribute from the  element). For each connection string you have, instead of entering in a real connection, again enter a Release Management token – I entered “__FabrikamFiber-Express-Connection__”.\n\n\n\n\nClick Close and save the profile. The profile appears under the Properties\\PublishProfiles folder of your web project.\n\n\n\n\nNow check your solution into source control.\n\nIf you do actually publish, you’ll see a SetParameters.xml file alongside the web deploy zip file. The contents of the file should be something like this:\n\n&lt;!--?xml version=\"1.0\" encoding=\"utf-8\"?--&gt;\n&lt;parameters&gt;\n  &lt;setparameter name=\"IIS Web Application Name\" value=\" __SiteName__\"&gt;\n  &lt;setparameter name=\"DemoEnv\" value=\" __EnvironmentName__\"&gt;\n  &lt;setparameter name=\"FabrikamFiber-Express-Web.config Connection String\" value=\" __FabrikamFiber-Express-Connection__\"&gt;\n  &lt;setparameter name=\"FabrikamFiber.DAL.Data.FabrikamFiberWebContext-Web.config Connection String\" value=\"FabrikamFiber.DAL.Data.FabrikamFiberWebContext_ConnectionString\"&gt;\n&lt;/setparameter&gt;&lt;/setparameter&gt;&lt;/setparameter&gt;&lt;/setparameter&gt;&lt;/parameters&gt;\n\n\nYou can see that there are 3 Release Management tokens – SiteName, EnvironmentName and FabrikamFiber-Express-Connection. These are the tokens we’ll replace when creating a release component for this website.\n\nBuilding the Package\n\nYou can now create a build – make sure you use the ReleaseDefaultTemplate11.1.xaml (from the Release Management bin folder). Specify any arguments you want to as usual, but make sure you have this in the MSBuild arguments:\n\n\n/p:DeployOnBuild=true;PublishProfile=Release\n\n\nThat will instruct web deploy to create the package when we build using the settings of the Release profile. I recommend that you set “Release to Build” to false just to make sure your build is producing the correct artifacts.\n\n\n\n\nAfter running the build, you should see the following in the _PublishedWebSites folder of your drop:\n\n\n\n\nIf you open the SetParameters.xml file (the one highlighted above) then you should see your Release Management tokens.\n\nCreate a Web Deploy Tool in Release Management\n\nDownload the InRelease MS Deploy wrapper from here. This is a simple exe that wraps the call to Web deploy so that any errors are reported in a way that Release Management understands. Let’s then go to Release Management-&gt;Inventory-&gt;Tools and click on New to create a new Tool:\n\nEnter the following parameters:\n\n\n\n\nYou can see that I made a new parameter called “WebAppName” to make this a generic tool.\n\nAt the bottom under Resources, click the “Add” button and import the irmsdeploy.exe that you downloaded.\n\nUpdate: 2013-12-02 When running through this “demo” again on my VM checkpoint, the WebDeploy custom tool kept failing with an “Unable to find file” exception. After trying this several times and tearing my hair out in chunks, I thought I would make sure this wasn’t a security issue – turns out, it was exactly that. Once you’ve downloaded the irmsdeploy.exe file, make sure you right-click it, select properties and “Unblock” it (see the below screenshot).\n\n\n\n\nCreate a Component\n\nOnce you’ve set up your Release Path, you’ll be able to define the release workflow for each stage. You’re going to need to create a component for your website in order to deploy it. Navigate to “Configure Apps-&gt;Components” and click New. Enter a name (and optional description) and then enter the following on the Source tab:\n\n\n\n\nThe “Path to Package” is the path that contains the web deploy package.\n\nNow click on the “Deployment” tab:.\n\n\n\n\nSelect WebDeploy from the Tool dropdown – that will automatically create the WebAppName parameter for this component.\n\nFinally, move to the “Configuration Variables” tab:\n\n\n\n\nChange the “Variable Replacement Mode” to “Before Installation” and set the “File Extension Filter” to “*.SetParameters.xml”. Now add all of your parameters (these are the tokens that are in your SetParameters.xml file after the build). You can type descriptions too.\n\nSpecifying Values in a Release Template\n\nWe’re finally ready to use the component inside a Release Template. Create a new template for a release path, and inside a Server activity drop your component. When you expand it, you’ll see that you can specify values for the parameters. Here are screenshots of my Dev and QA components:\n\n\n\n\nThe best part is, not only can you use these parameters in Release Management, but if you import this web deploy package directly in IIS, you get prompted to supply values for the parameters too:\n\n\n\n\nHappy Releasing!\n",
      "categories": [],
      "tags": ["releasemanagement"],
      
      "collection": "posts",
      "url": "/webdeploy-and-release-management-the-proper-way/"
    },{
      
      "title": "Build Script Hooks for TFS 2012 Builds",
      "date": "2014-01-22 21:51:00 +0000",
      
      "content": "EDIT: My colleague Tyler Doerksen pointed out in his comments that my solution doesn’t do any error checking of the scripts. If your script fails, the build happily continues. I’ve added another post to show how to add error handling.\n\nOne of my favorite features about the TFS 2013 Builds is the script hooks – there are pre- and post-build as well as pre- and post-test hooks. These make customizing build a whole lot easier. For example, customizing the build so that your assembly versions match your build number is a snap.\n\nI set out to implement the same logic in a 2012 build this morning – unfortunately, the RunScript build activity from the 2013 template is only in the 2013 TFS Build assembly.\n\nSo I came up with a “poor-man’s” run-script equivalent for 2012 builds (with the best part being you don’t need any custom assemblies, so the edit can be applied directly to your build template without having to pull it into a solution). I’ll walk you through the steps of customizing the default build – in this example I’m only doing pre- and post-build scripts, but the principles would be the same for pre- and post-test scripts.\n\nChallenge 1 – Invoking PowerShell\n\nThe first challenge is how do you invoke a PowerShell script from within the build? It’s fairly easy: use the InvokeProcess activity.\n\nFirst you’ll need to add workflow arguments for the pre- and post-build script paths as well as their corresponding args. Open your build workflow and click on “Arguments”. Enter 4 “In String” arguments as follows:\n\n\n\n\nYou can add defaults if you like.\n\nI always like to put custom arguments in a separate section of the build parameters, so that anyone creating a build from the template can see them and read their descriptions. You do this by clicking the “…” button in the Default Value column of the “Metadata” argument and filling in some metadata for your arguments:\n\n\n\n\nNow go to the workflow and find the MSBuild Activity in the heart of the workflow that does the compilation (be careful – there’s one that does a clean of the workspace too – you don’t want that one). Just above the ForEach (For Each Project in BuildSettings.ProjectsToBuild) activity, add an If activity (this will automatically add in a sequence to wrap the activities we’re adding) and set its condition to\n\n\nNot String.IsNullOrEmpty(PreBuildScriptPath)\n\n\nand in the Then of the If activity add a “ConvertWorkspaceItem” activity and an InvokeProcess activity. In the InvokeProcess activity, drag a “WriteBuildMessage” and “WriteBuildError” activity onto the area below stdOutput and errOutput respectively. Set the “Message” property of each Write activity to stdOutput and errOutput respectively.\n\n\n\n\nClick on the sequence activity that your activities are in. We’ll need two local variables: preBuildScriptLocalPath and postBuildScriptLocalPath (both strings).\n\nNow in the ConvertWorkspaceItem activity, set the following properties:\n\n\n  DisplayName: “Get pre-build script local path”\n  Input: PreBuildScriptPath\n  Output: preBuildScriptLocalPath\n  Workspace: Workspace\n\n\nSet the following properties in the InvokeProcess activity:\n\n\n  Arguments: String.Format(“ ““&amp; ‘{0}’ {1}”” “, prebuildScriptLocalPath, PreBuildScriptArgs)\n  DisplayName: “Run pre-build script”\n  FileName: “PowerShell”\n\n\nNow you can copy this whole “If” activity and paste it below the “ForEach” (the one that does contains the MSBuild activity) and rename pre to post – this implements your post-build hook.\n\nDon’t forget that you’ll need to change PowerShell’s execution policy on your build server. Log in to your build server and run PowerShell as an administrator. Run the following:\n\n\nSet-ExecutionPolicy RemoteSigned\n\n\nNow you’re almost set…\n\nChallenge 2 – Environment Variables\n\nWhen I created a script for a 2013 build to version the assemblies, I relied on the fact that the 2013 build sets some environment variables that you can use in your scripts. Here’s a snippet showing 2 environment variables I used in my versioning script:\n\nParam(\n  [string]$pathToSearch = $env:TF_BUILD_SOURCESDIRECTORY,\n  [string]$buildNumber = $env:TF_BUILD_BUILDNUMBER,\n\n\nYou can see I’m getting $env:TF_BUILD_BUILDNUMBER. Well, in the 2012 workflow, these variables aren’t set, so you have to add an activity to do it.\n\nJust above your “If” activity for the pre-build script invocation, add an InvokeMethod activity (this is in the “Primitives” section of the workflow designer toolbox).\n\n\n\n\nSet the following properties:\n\n\n  MethodName: SetEnvironmentVariable\n  TargetType: System.Environment\n\n\nThen you need to set different parameters for each environment variable you want to set. Click on the “…” next to the value of the Parameters property:\n\n\n\n\nIn this one I set 2 parameters: “In String TF_BUILD_SOURCESDIRECTORY” and “In String SourcesDirectory” to set the sources directory environment variable. I did the same for binaries directory and build number, each time using the name from this list (see the TF_BUILD environment variables section) and the value from the corresponding workflow argument or variable. Then I could use the same PowerShell script that I used in my 2013 builds without having to modify it.\n\nCreating a Build Definition\n\nNow when you create a build definition, make sure that you include the folder that contains your scripts into the build workspace. Then set your script paths (using the source control paths) and arguments appropriately, for example:\n\n\n\n\nHappy customizing!\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/build-script-hooks-for-tfs-2012-builds/"
    },{
      
      "title": "Error Handling Poor Man’s RunScript in 2012 Builds",
      "date": "2014-01-23 16:45:00 +0000",
      
      "content": "Yesterday I posted about how to create script hooks in a 2012 build template. My colleague Tyler Doerksen commented and pointed out that there was no error handling in my solution.\n\nReturning Error Codes from PowerShell\n\nI knew that if I could get the script to return an error code, it would be simple to add an If activity to check it. The trick is to use “exit 0” for success and “exit 1” for failures. I also changed any error messages from Write-Host to Write-Error so that they go to errOutput and not stdOutput. Here’s the updated “UpdateVersion” script:\n\nParam(\n  [string]$pathToSearch = $env:TF_BUILD_SOURCESDIRECTORY,\n  [string]$buildNumber = $env:TF_BUILD_BUILDNUMBER,\n  [string]$searchFilter = \"AssemblyInfo.*\",\n  [regex]$pattern = \"\\d+\\.\\d+\\.\\d+\\.\\d+\"\n)\n\nif ($buildNumber -match $pattern -ne $true) {\n    Write-Error \"Could not extract a version from [$buildNumber] using pattern [$pattern]\"\n    exit 1\n} else {\n    try {\n        $extractedBuildNumber = $Matches[0]\n        Write-Host \"Using version $extractedBuildNumber\"\n\n        gci -Path $pathToSearch -Filter $searchFilter -Recurse | %{\n            Write-Host \" -&amp;gt; Changing $($_.FullName)\" \n        \n            # remove the read-only bit on the file\n            sp $_.FullName IsReadOnly $false\n\n            # run the regex replace\n            (gc $_.FullName) | % { $_ -replace $pattern, $extractedBuildNumber } | sc $_.FullName\n        }\n\n        Write-Host \"Done!\"\n        exit 0\n    } catch {\n        Write-Error $_\n        exit 1\n    }\n}\n\n\nError Handling in the Build\n\nGo back to the InvokeProcess activity that calls your script. Go to its parent activity (usually a sequence) and add a variable Int32 called “scriptResult”. On the InvokeProcess, set the result property to “scriptResult”.\n\n\n\n\nNow you just need to add an If activity below the InvokeProcess that has condition “scriptResult &lt;&gt; 0” and add a Throw in the “Then”. I’m just throwing an Exception with an error message.\n\n\n\n\nHere’s the output if the script fails:\n\n\n\n\nHappy building!\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/error-handling-poor-mans-runscript-in-2012-builds/"
    },{
      
      "title": "CrossBrowser Testing: ChromeDriver Window Hangs after Test",
      "date": "2014-01-31 22:07:00 +0000",
      
      "content": "I have been doing some coded UI testing and running tests using Chrome (via the Selenium components). However, I noticed that when my test completed successfully, the Selenium (ChromeDriver) window stayed open and never terminated. Here’s a code snippet of my original code:\n\n[TestMethod]\npublic void TestTimesheetIsDeployedChrome()\n{\n    BrowserWindow.CurrentBrowser = \"chrome\";\n\n    var testUrl = ConfigurationManager.AppSettings[\"TimesheetUrl\"];\n    Assert.IsFalse(string.IsNullOrEmpty(testUrl), \"Could not find testUrl in App Settings\");\n    var window = BrowserWindow.Launch(testUrl);\n\n    UIMap.Login();\n    UIMap.ValidateLogonSuccess();\n    // ...\n    UIMap.LogOff();\n    UIMap.ValidateLogoffSucceeded();\n\n    window.Close();\n}\n\n\nPretty straight forward. Except that the call to “window.Close();” closed the browser, but not the ChromeDriver command window – so the test never terminated.\n\n\n\n\nSo even though the test passes, I have to manually close the command window before the test run itself terminates.\n\nAfter playing around a bit, I came up with this code to kill the command window (this works for Chrome – haven’t tested it for Firefox). Just replace “window.Close()” with this code:\n\n// kill the process so we don't lock up - for some reason window.Close() locks\nwindow.Process.Kill();\n\nvar procs = Process.GetProcesses().Where(p =&amp;gt; p.ProcessName.ToLower().Contains(\"chromedriver\"));\nforeach (var p in procs)\n{\n    p.Kill();\n}\n\n\nThat did it nicely for me (and made my Build-Deploy-Test workflows in my lab terminate correctly).\n\nHappy (cross-browser) testing!\n",
      "categories": [],
      "tags": ["testing"],
      
      "collection": "posts",
      "url": "/crossbrowser-testing-chromedriver-window-hangs-after-test/"
    },{
      
      "title": "Fix: Release Management “Service Unavailable 503”",
      "date": "2014-02-07 17:01:00 +0000",
      
      "content": "At a customer we installed Release Management for their TFS 2013 TFS Server. The server component installation went really smoothly – however, it was only when we installed the Client that we realized that the Release Management service was not right – we kept getting a 503 Service Unavailable error. I opened IIS and could see that the Release Management application pool was stopped. I started the app pool, but it immediately shut down. We checked the event log and saw a few obscure error messages about NullReferenceExceptions – nothing particularly helpful.\n\nThe Problem: SharePoint\n\nIt turns out that the problem was (surprise, surprise) SharePoint. The server we were using (for a small team) was a single server TFS – data and application tier on the same machine, as well as SSRS, SSAS and SharePoint. Unfortunately, SharePoint doesn’t play well with others – specifically with other 32-bit web applications. SharePoint’s install adds a global ISAPI module, but for some reason doesn’t add a bitness filter – so if you have a 32-bit web application (like Release Management) the 64-bit SharePoint module gets loaded anyway, and this causes the 32-bit application pool to crash.\n\nThe Fix\n\n\n  Log on to your TFS Server and open a command prompt as administrator.\n  cd to \\windows\\system32\\inetsrv\n  Now enter:\nappcmd.exe set config -section:system.webServer/globalModules /[name='SPNativeRequestModule'].preCondition:integratedMode,bitness64\n\n\nNow restart your Release Management application pool, and you’ll be good to go.\n\nHappy releasing!\n",
      "categories": [],
      "tags": ["releasemanagement"],
      
      "collection": "posts",
      "url": "/fix-release-management-service-unavailable-503/"
    },{
      
      "title": "Fix: Release Management WebDeploy Deployment Fails: Access Denied",
      "date": "2014-02-07 18:17:00 +0000",
      
      "content": "If you’re using WebDeploy and Release Management (as you should to release Web Applications) you may hit the following error:\n\n\nInfo: Adding sitemanifest (sitemanifest).Info: Creating application (Default Web Site/MyWebsite)Error: An error occurred when reading the IIS Configuration File 'MACHINE/REDIRECTION'. The identity performing the operation was 'DOMAIN\\tfsservice'.Error: Filename: \\\\?\\C:\\Windows\\system32\\inetsrv\\config\\redirection.configError: Cannot read configuration file due to insufficient permissions\n\n\nSeems that the WebDeploy command can’t access some files in c:\\Windows\\system32\\inetsrv. It may be the irmsdeploy.exe MSDeploy wrapper that I’m using for doing WebDeploy in Release Management (see my post about how to do this), since logging into the machine and running the webdeploy.cmd file manually works just fine.\n\nThe Resolution\n\nYou have to add permissions for the release management agent identity to the folder, but this is a folder who’s owner identity is TrustedInstaller – meaning you have to change the owner to yourself first.\n\n\n  Right click the insetsrv folder in c:\\windows\\system32 and select Properties.\n  Click on the Security tab and click the “Advanced” button:\n\n  Click on the owner tab and then on the Edit button:\n\n  Select yourself (I logged in as TfsSetup which is in the local admin group on this machine), check “Replace owner on subcontainers and objects” checkbox and click “OK”:\n\n  Close all the dialogs and then right-click the inetsrv folder again and click Properties. Now you can allow read access to the Release Management agent identity to this folder.\n\n\nOnce you’ve changed the permissions, you will need to reboot the machine. After the reboot, the WebDeploy through Release Management should work without a hitch.\n\nHappy releasing!\n",
      "categories": [],
      "tags": ["releasemanagement"],
      
      "collection": "posts",
      "url": "/fix-release-management-webdeploy-deployment-fails-access-denied/"
    },{
      
      "title": "Integrating TFS and Project Server – Two Way Manual Sync",
      "date": "2014-04-10 18:55:00 +0000",
      
      "content": "I often do road-shows showing off TFS and VS to customers around South Africa. Usually I’m doing this with Ahmed Salijee, the Developer Platform Specialist (DPS) for Developer Tools in Microsoft South Africa. Ahmed is an amazing speaker (we’ve co-presented regularly) and is great at helping customers at a strategic level – and, as he likes to say, for his sins, he gets to help customers with their licensing queries!\n\nAhmed and I agree on most aspects of Application Lifecycle Management (ALM) using TFS – but one of the places we disagree on is the integration of TFS and Project Server.\n\nPhilosophy: Why You Shouldn’t Be Using Project Server in the First Place\n\nIf you don’t care to wax philosophical about Project Server, then skip this section. However, I think it’s important to step back and think about what you’re getting into if you’re planning on using Project Server for tracking software development projects.\n\nProject Plans are made for environments where “change is bad”. Think about what Project Managers do – they create a plan, perhaps entering in requirements, breaking those down into tasks with estimates that they then farm out to team members. Then they do some curious things: they set milestones and they baseline the project. Let’s examine what milestones and baselining mean.\n\nMilestones are points along the way that Project Managers use to answer a simple question: are we conforming to the plan? Baselines communicate an idea: “What we’ve planned is what we value – we should not deviate from this plan. If we do, then it’ll Be Bad”. Project Plans work well when tasks are exactly predictable and repeatable. Unfortunately, that’s not the case in software development. There are many reasons why this is the case – requirements are language-based, and as such are subject to misinterpretations. Coding is an abstract art – and as such it’s extremely difficult to estimate how long a task will take with any accuracy. Even if you get the code bits right, there are always unforeseen issues in integration of components, deployments, testing and so on. This is why people lie about progress – who wants to deviate from the baseline when the baseline is the Ultimate Good? We’d rather bend the truth about how far we are so that, on paper at least, we look good.\n\nThat’s why Agile has come to the form. The basic philosophy of any Agile technique is embrace the change. If we know things are going to change, then why not embrace the change? Let’s shorten cycle times so that we can get more rapid feedback – that way we minimize the risk of doing the wrong thing for too long. Let’s focus on measuring what value we deliver to business, rather than tracking conformance to a plan. Let’s make it concrete: imagine two teams. Team A sticks to the Project Plan closely, and after months of work deliver a system that is average (statistics show that most waterfall-based projects don’t even reach delivery, so we’re being optimistic about Team A’s delivery). Team B deviates widely from their Project Plan, deliver small components frequently and at the end of the project have exceeded expectations. Which team is “better”? The one that stuck to the Plan or the one that Delivered Value? And yet, if we really care about delivering value, why do we beat on teams to Stick To The Plan?\n\nTFS and Project Server Integration: Good and Bad\n\nSo you’re insisting on integrating with Project Server anyway. Fair enough. Let’s examine the TFS to Project Server integration.\n\nThe good: avoid double entry. That’s about it. The integration allows Work Items added in TFS to be synced over to Project Server (or vice-versa). The integration simply means you don’t have to add (or update progress) in two places.\n\nPerhaps I could add that once you have tasks in a project plan you can do resource leveling and all the other “voodoo” that Project is capable of, but frankly I think that that’s a waste of effort. Let’s imagine you’ve spent a couple of days doing all the leveling. After 3 days, Bob gets sick and is off for 2 days. So you re-level everything, carefully watching as your project drifts from the baseline. Next thing Joe comes and tells you he forgot that one of the changes he’s making will require an extra 2 days of refactoring. You adjust again, sweating a little as your plan deviates further from the baseline. You call up Frank and tell him he’ll have to work the weekend so that you can catch up. Every time something happens, you’re frantically re-adjusting your project plan. At the end, the actual is so far off the baseline, you wonder why you bothered in the first place.\n\nIf you train your teams to self-organize, you can track progress of delivered value (which is a much better thing to measure than “conformance to a plan”), rather than dictate who should be doing what when. As changes come in, you embrace the change and adjust course, smiling because change is now a positive thing – not a shame.\n\nIf you’re still insisting on integrating TFS and Project Server, there are some caveats that you’ll need to know about before embarking on the integration:\n\n\n  Installing the Integration will modify your process templates. The integration adds a bunch of fields and a Project Server form to your work items.\n  You need to configure which PS Projects can be linked to which TFS Team Projects. A PS Project can only be linked to 1 TFS Team Project. If you have a lot of PS Projects (or a lot of TFS Team Projects, or lots of both) you’re going to end up with a lot of integration admin.\n  You need to configure which work items are synced across – at TFS Team Project level. The integration requires you to tell the connector which Work Items it needs to sync. Again, if you have large amounts of PS or TFS Team Projects, you’re looking at a lot of admin.\n  Updating tasks in TFS does not fill in the Timesheet in Project Server. TFS has no knowledge of when work is done – only that work has been done. That means that if you’re going to want to do billing from Project Server, your team members are going to end up filling in Timesheets in Project Server. Updating Timesheets in Project Server does sync actuals and remaining work for work items though.\n  The connector is notoriously hard to debug. If the connector has errors, it can be really hard to track them down.\n  If you have change approvals enabled on Project Server, a project manager can reject changes made to a plan. Imagine a team member updates a work item, which causes the connector to send the change to Project Server. The project manager then rejects the changes. At this point, the sync engine turns off sync for this work item, and the only way to know is to open the work item and take a look. The history has an entry stating the reject reason, and in order to re-sync this work item going forward, you have to re-enable the sync for this one work item.\n  You cannot assign multiple resources to a Task in the Project Server Project Plan. TFS only allows one resource to be Assigned To a Work Item at any one time – which means that if you’re used to multiple resources on the same Task in Project Server, you’re going to have to split the tasks.\n\n\nTwo Way Manual Sync\n\nSo since the integration is so hard (and fragile), perhaps you can consider this alternative: two way manual sync. This page explains in detail the differences between syncing to MS Project versus syncing to Project Server – what I propose here is a “middle ground” that give you best of both worlds without all the pesky configuration required for the integration extension.\n\nHere are the steps to get going:\n\n\n  On Project Web Access (PWA), create a new enterprise project.\n  Open the Project and press “Build Team” and add the resources that will form part of this project.\n  Open MS Project and connect to the new Enterprise Project. Check out the plan for editing.\n  On the Team Tab, click “Get Work Items” and select a query for the work items you want to bring into the plan.\n\n  (Tip: I normally work in the Iteration Backlog and then hit the “Create Query” button to create the iteration backlog query)\n\n  Select the work items from the query and click Add.\n  Now you can work with the Tasks in Project – leveling resources etc. etc. You can also set a baseline if you want to.\n  (Tip: Establish predecessor relationships. Then select all the rows by clicking the row ID – the leftmost column –  of the 1st task and then shift-clicking the ID of the last task. Then right click and select “Auto Schedule”. This creates the initial Gantt for you).\n\n  Once you’re done, hit Publish in the Team tab to save your changes back to TFS. In this screenshot, I added a new Task at the bottom of the Project Plan and hit Publish. This then brought back the TFS Work Item id (48).\n\n  Now you need to publish the changes to Project Server. Hit File and then click the Publish button.\n\n  This can take a few seconds, so make sure you watch the status bar to see that the publish succeeded.\n\n  When you close the project plan, make sure you check it in.\n\n\nNow imagine that the TFS team is updating their tasks. To pull those updates in, let’s open the Project Plan again:\n\n\n  Go to the Team Tab and press “Refresh”. (Notice in this screenshot that the task actuals / remaining have been updated).\n\n  One gotcha: If new tasks were added in TFS, you’ll have to schedule them to see the time estimates (see Task 49 in the above screenshot). It’s a good idea to always hit “auto-schedule” on new (or all) tasks to get new tasks into the Gantt correctly.\n  Now you need to publish to Project Server. Again, go to File and click the Publish button.\n\n\nFinally, consider when new Tasks are added to the Plan in Project Server by another user.\n\n\n  Open the Project Plan from Project Server\n  You’ll immediately see the new tasks\n  Add the Work Item Type column and map the new Tasks to work item types. You can also bring in the Area Path and Iteration Path columns.\n  Now go to the Team Tab and hit Publish.\n\n\nOther Useful Stuff\n\nI wrote a series of posts about integrating TFS and Project server – you can find the fist post here. Also, you can customize the field mappings between TFS and MS Project (not Server) using the guide in this post. Also, if you’re looking for a Timesheet solution and don’t want to use Project Server, then look out for Imaginet’s soon-to-be-released new Timesheet product.\n\nI also can’t recommend highly enough Donald G. Reinertsen’s The Principles of Product Development Flow – it’ll revolutionize the way you think about delivering value to business.\n\nConclusion\n\nPhilosophically I think software development teams should stay away from Project Server (or Project Plans) entirely – focus more effort on measuring delivered value to business than conformance to a plan. However, this change is cultural (and needs to be pervasive) so I know that there are teams that are still going to have to integrate to Project Server. Before you embark on the long and painful process of using the server integration, consider using my two way manual sync to see how it works.\n\nHappy Project Tracking!\n",
      "categories": [],
      "tags": ["alm"],
      
      "collection": "posts",
      "url": "/integrating-tfs-and-project-server-two-way-manual-sync/"
    },{
      
      "title": "Project Plans, Agile, Pizza and Startups",
      "date": "2014-04-14 17:49:00 +0000",
      
      "content": "Last week I posted about how to integrate TFS and Project Server “manually”. In the post I did put in a bit of philosophy about why I think project plans can be a Bad Thing. Prasanna Adavi posted a thoughtful comment on my philosophy, and I wanted to reply just as thoughtfully, so I decided to add the reply as a new post rather than just reply to the comments inline.\n\nHere is Prasanna’s comment:\n\n“I disagree with you on your notion of how “project plans” work. It is wrong to assume that just because a project manager plans and baselines a project, they will never veer from it. To the contrary, It is an attempt to make the customer think before hand as to what they perceive as the value, and then think twice before they “change” the requirements. A good project manager is never afraid to reset the baseline if it is necessary to do so.\n\nMoreover, who said that a “waterfall” has to be one huge waterfall? Depending upon the nature of the project (especially Software Projects), they can be scheduled as multiple mini-waterfalls, yielding quick, unique features. Actually, the way I see Agile is multiple mini-waterfalls. But within each waterfall, there needs to be a commitment. Leaving it open ended and reworking things again and again, because customer does not know what he wants, is not really ‘embracing change’.\n\nThink about this: When you order a pizza, do you ‘commit’ to certain toppings, or do you say lets start making the thing, and we will add toppings as we see the value?\n\nAnd finally, If your entire team or company is working on one software project/feature set, then it is great to think in terms of “Agile”. But when you are on a ‘agile’ project that never ends because a customer constantly keeps seeing ‘value’ in the latest change, that is not a project anymore, and it becomes quite difficult to commit to anything else.”\n\nResponse\n\nOf course when I speak about Project Managers and Project Plans, I generalize. Some Project Managers are obviously better than others. In my experience, however, Project Managers that manage software projects still use the wrong paradigm when they produce a plan – and the fundamental paradigm of a Project Plan is that we know what we’re building and we know how long each task is going to take. If either of these suppositions is incorrect, then the Project Plan’s use diminishes dramatically. Of course in Software Development, it’s nearly always the case that at the beginning of a project (when we’re supposed to be creating the plan) that requirements cannot be known fully – and we can guess how long each task will take, but it’s just that – a guess.\n\nI like to show a diagram called the “Cone of Uncertainty” when I talk about Project Planning:\n\n\n\n\nThe Cone shows how the larger the “thing” we’re estimating is (and the further in the future the end date will be), the more uncertainty there is in that estimation. So planning something small right now has little variance – we might say that changing a table in a database will take 2 hours (with about half an hour variance). But if we plan something big (like a Project) and estimate 3 months, then the variance is likely to be +- 1.5 months. We just don’t know enough now to be more accurate.\n\nThis is why Agile is about working in small sprints (or iterations) between 1 and 4 weeks in length. That time period is a good fit for the kind of estimations that generally apply to detailed tasks on a software project. Planning further ahead (in detail) is simply wasted effort. Of course, if you don’t have any plan further out than your current sprint, that’s a bad thing too – you need to be working towards some larger goal or initiative. So Prasanna is theoretically correct that Agile is a series of mini-waterfalls – in the sense that the larger Project is divided up into smaller iterations. However, to put a lot of effort into Planning all the iterations in detail is simply a waste of time – things change so rapidly that by the time you’re half-way into iteration 1, the plan for iteration 3 is obsolete.\n\nSo being Agile doesn’t imply “not planning”. It just says, to use the 80/20 principle, put 80% of your planning effort into the 20% that matters most – in this case, that’s the current sprint. So plan in detail your next sprint, but wait till you’re nearing the end of sprint 1 before you plan sprint 2 and so on.\n\nLet’s imagine that you’re on a team working in 2 week sprints. Now if you’re only planning in detail 2 weeks at a time, do you really need a Project Plan? Isn’t a list of requirements and their tasks (a backlog) enough?\n\nThe point I was making on my previous post was that putting a lot of effort into a large and detailed project plan, and then artificially sticking to that plan, is a waste of effort. And if you’re going to change your baseline often, then why baseline in the first place? Rather plan in detail in the short term (the next sprint) and let changes that come from that spring influence the detailed plan for the next.\n\nAnother point is that when you adjust your sprint based on feedback, then you’re “responding to change”. Normally a baseline changes because things are going to take longer than you expected – so the change isn’t based on change (or value), it’s based on poor estimates.\n\nCommitment\n\nOf course just because you’re an Agile team that can respond to change rapidly, that doesn’t mean that you’re at the mercy of the customer’s whims. One of the principles of Agile is to lock down the sprint. So if the customer does decide to change things, they’ll have to wait till at least the next sprint. And of course, since the Product Owner (who represents the customer) is the one that needs to prioritize work, they’ll have to bump something off the backlog if a new change or requirement is added in. In fact, I’d go so far as to say that if your team is doing Agile correctly, you’ll get fewer change requests from your customers and they’ll soon learn to be a lot clearer about what they want and be more sure that they actually want it. Having a detailed Project Plan isn’t necessarily going to make your customers think more about what they want – even if baselines move. But giving them responsibility, having them see that every change shifts the goal-posts, will mean they will feel the change much more “personally”. Also if you deliver something (even if it’s small) at the enf of every sprint, customers get a feeling of momentum. You may even find that the seemingly small piece of functionality that you deliver is enough, and the customer doesn’t really need the rest of The Thing that you would have been working on for the next 2 months. Rapid feedback on value delivered is far better than explaining why a baseline has to move.\n\nPizza or Degree?\n\nFinally, the Pizza analogy. Again, if you go back to the Cone of Uncertainty, there is very little “risk” in getting toppings wrong on a pizza – so of course you commit to the entire pizza when you order it. I think the analogy doesn’t really work. I think a better analogy is a startup company.\n\nLet’s imagine 2 startups – one called ProjectX and one called AgileX. ProjectX spends 4 weeks getting a detailed plan in place for their product, which is going to take 3 months to build. AgileX in the meantime spends 4 days on a general direction, and a day in detailed planning for their project. They release their product’s V1 after another 2 weeks. The product isn’t fully featured, but what there is of the product works. They get some feedback which changes some of what they know about the product, and adjust accordingly. If they continue to add small features and get feedback in 2 week cycles, they would have released features 15 times before ProjectX’s product comes out the door. They would have had 14 opportunities for feedback from their Customers before ProjectX even had 1 round of feedback on working software. Even if ProjectX followed Prasanna’s “mini-waterfalls”, there’d be less movement. A mini-waterfall would require too much planning for 2 weeks, so ProjectX decides on 3 1-month mini-waterfalls. That still means that they only release 3 times in the same period, and only get feedback on 2 occasions. Still way less than AgileX. Is the more detailed longer-termed planning helping or hindering?\n\nWhat do you, dear reader, think of Project Planning?\n\nHappy planning!\n",
      "categories": [],
      "tags": ["alm"],
      
      "collection": "posts",
      "url": "/project-plans-agile-pizza-and-startups/"
    },{
      
      "title": "Application Insights Telemetry for WAWS or Customer-Hosted Sites Without MMA",
      "date": "2014-04-18 02:25:00 +0000",
      
      "content": "There are a couple of tools that change the development landscape significantly for .Net developers. One such tool is IntelliTrace – this revolutionizes how you debug production issues. Another game-changing tool is Application Insights.\n\nThere are a few broad categories for Application Insights telemetry – APM (Application Performance Monitoring), Availability, Usage and Diagnostics. In order to leverage APM and Diagnostics, you need to install Microsoft Monitoring Agent (MMA) on your IIS server. This isn’t a problem if you’re using an on-premises IIS or using your own VM in the cloud. However, if you’ve got a website that’s running on Windows Azure Websites (WAWS) or your site is going to be hosted on someone else’s tin or cloud, then you can’t install MMA. However, not all is lost – you can still get Availability and Usage telemetry for these scenarios.\n\nAdd Application Insights to Your Web App\n\nNormally this is as easy as installing the Application Insights VS Extensions, right-clicking your Web Application and selecting “Add Application Insights Telemetry”. This adds an ApplicationInsights.config file to your project that allows you to configure Application Insights. This works great in conjunction with MMA. Of course, the scenario we’re talking about is one where there is no MMA.\n\nThis scenario is supported, but there’s not a lot of guidance on how to do it. In cases where I’m running my site on WAWS or I’ve created a website that my customer is going to be hosting on their infrastructure (be that on their own premises, their own public cloud or even on Azure) and there’s no MMA, you can still get some really good telemetry. I also had the goal of minimizing admin and configuration footprint as much as possible, since I don’t want to have to maintain lots of config files on other people’s infrastructure. Happily, you can get a lot of good telemetry with a single application setting. Let’s see how to do it.\n\nCreate an Application in Application Insights\n\nFor the WAWS (or customer-hosted website) scenarios, you need to create an application in Application Insights. This will generate a unique (guid) key for the application. This is the only piece of config you really need.\n\nThe first thing you need to do is “Choose Your Adventure” in Application Insights. There isn’t an adventure path for the scenario we’re trying, so we’ll hijack the Web Service adventure. Log into your VSO account and go to Application Insights. Click on “Overview” and then “Add Application”. Follow the prompts along the following path:\n\n\n\n\nAppInsights will give us a list of instructions at this point. Since we’re going off-track on this adventure, we’ll ignore most of the instructions – we really just need the Application ID.\n\nType a name for your application into “Step 2” and click “Create”. Then copy the guid from Step 4 (this is the application ID we need to tie AppInsights events to this application in AppInsights):\n\n\n\n\nNote: You can get the application ID at any time by clicking on the gear icon in the top right of AppInsights. Then click the “Keys &amp; Downloads” tab, select your application in the drop-down on the top left and scroll down to the “Web Services SDK” section. The guid there is the one you need for your application:\n\n\n\n\nFollow the instructions in Step 3 – you can follow them here if you like. Select your web application in Solution Explorer, right-click and select “Manage NuGet packages”. Then type in “application insights” (including the quotes) and install the Application Insights SDK package.\n\n\n\n\nOnce that’s done, you can add the following code to your Global.asax.cs file:\n\nvar appInsightsKey = ConfigurationManager.AppSettings[\"AppInsightsID\"];\nif (string.IsNullOrEmpty(appInsightsKey))\n{\n    ServerAnalytics.Enabled = false;\n}\nelse\n{\n    ServerAnalytics.Start(appInsightsKey);\n}\n\n\nNow add a key to your application settings in your Web.config:\n\n&lt;add key=\"AppInsightsID\" value=\"guid\"&gt;\n&lt;/add&gt;\n\n\nThe value for the key is the guid you copied earlier when you created the Application in AppInsights. If you deploy this to another customer, create a new Application in AppInsights and update the key in the config file. That’s it.\n\nSetup Usage Telemetry from the Browser\n\nTo get usage telemetry from the browser (like Language, OS, Browser Type, Geographic Region etc.) you need to add some javascript to the pages you want to track. Back in Application Insights, you can click on the “Usage” tab. Make sure your new application is selected on the top left. Then click “Web site” for the type of application you want usage stats from. Click on “Click here to show instructions” at the bottom.\n\n\n\n\nIf your site is on WAWS, you’ll want to create a ping test when you’ve published your application – if it’s already in Azure, then go ahead and enter the site url for the ping-test. This is a synthetic monitor that can give you a bit of basic availability information about your site. Of course when it’s on your customer’s infrastructure there may not even be a publically accessible url.\n\nWhat’s important for now though is that you’ll see the usage insights snippet in step 3. You can copy that code if you like – though I made a slight modification to this code to make the application ID a parameter, rather than a hard-coded value.\n\n\n\n\nI’ve got an MVC application, so I’ve added the usage snippet javascript (mixed with a little Razor) into the _Layout.cshtml page:\n\n&lt;script type=\"text/javascript\"&gt;\n    var appInsightsKey = \"@System.Configuration.ConfigurationManager.AppSettings[\"AppInsightsID\"]\";\n    if (appInsightsKey !== null || appInsightsKey.length &gt; 0) {\n        window.appInsights = { queue: [], applicationInsightsId: null, accountId: null, appUserId: null, configUrl: null, start: function (n) { function u(n, t) { n[t] = function () { var i = arguments; n.queue.push(function () { n[t].apply(n, i) }) } } function f(n) { var t = document.createElement(\"script\"); return t.type = \"text/javascript\", t.src = n, t.async = !0, t } function r() { i.appendChild(f(\"//az416426.vo.msecnd.net/scripts/ai.0.js\")) } var i, t; this.applicationInsightsId = n; u(this, \"logEvent\"); u(this, \"logPageView\"); i = document.getElementsByTagName(\"script\")[0].parentNode; this.configUrl === null ? r() : (t = f(this.configUrl), t.onload = r, t.onerror = r, i.appendChild(t)); this.start = function () { } } };\n        appInsights.start(appInsightsKey);\n        appInsights.logPageView();\n    }\n&lt;/script&gt;\n\n\nYou can see how I added a C# call (using Razor) to read the applicationID from the web.config – it’s the same call that the code in Global.asax.cs makes.\n\nCustom Events\n\nSo that will “buy” me some usage telemetry as well as page hits – all for free (almost). However, I wanted to add some specific events and even do some timing on events so that I can monitor performance. To do that I had to add some code into my methods.\n\nYou can log an event (just give it a name – the name allows a hierarchy separated by “/”). You can log a timed event – call StartTimedEvent() with the same naming convention. Later, you call event.End() or event.Cancel() to stop timing. Furthermore, you can log these events with metrics (and/or properties) using a Dictionary. Here are some snippets of stuff you can do:\n\nCreate a Timed Event for a Controller method:\n\nvar aiEvent = ServerAnalytics.CurrentRequest.StartTimedEvent(\"DummySite/Index\");\n\ntry\n{\n    // do stuff\n    aiEvent.End();\n}\ncatch(Exception ex)\n{\n    // log stuff\n    aiEvent.Cancel();\n    throw;\n}\n\nreturn View();\n\n\nHere’s just logging an event:\n\npublic ActionResult Contact()\n{\n    ServerAnalytics.CurrentRequest.LogEvent(\"DummySite/Contact\");\n    ViewBag.Message = \"Your contact page.\";\n\n    return View();\n}\n\n\nHere’s an event with some properties:\n\npublic ActionResult About()\n{\n    var even = DateTime.Now.Second % 2 == 0 ? \"even\" : \"odd\";\n    var props = new Dictionary&lt;string, object=\"\"&gt;() { { \"mod\", even } };\n\n    ServerAnalytics.CurrentRequest.LogEvent(\"DummySite/About\", props);\n\n    ViewBag.Message = \"Your application description page.\";\n\n    return View();\n}&lt;/string,&gt;\n\n\nOf course you can add whatever custom events you need – the beauty of the API is that once you’ve called the Start method (in the Global.asax.cs) you don’t need to worry about IDs or config at all.\n\nNow just publish and start monitoring! (Don’t forget to set your ping-test in the Availability tab in AppInsights if your site is publically available and you haven’t done it before).\n\nViewing the Data in Application Insights\n\nWhen you debug your application locally, you can check you events in the “Diagnostics-&gt;Streaming Data” page on AppInsights.\n\n\n\n\nOnce you’ve published your application, you’ll see events start flooding in.\n\nIn the Usage-&gt;Features-&gt;Events page, you’ll see your events, organized into the hierarchy you specified with the / notation:\n\n\n\n\nWhen I first saw this page, I couldn’t figure out where the timing for the timed events was. You can click on the little arrow (left of the pin icon for each event) or click the “DETAILS” link on the top left of the graph.\n\n\n\n\nClicking on the “dummysite/index” event arrow takes me to the details for that event, where I can analyze the timings:\n\n\n\n\nClicking on the “dummysite/about” event, I get to see the properties (in this case, “mod” is the property and the values are “even” or “odd” – you can filter the graph by a particular value):\n\n\n\n\nThe Usage-&gt;User page shows user/session specific telemetry – mine aren’t all that interesting since I only logged on with 1 user.\n\n\n\n\nClicking on Usage-&gt;Environment gives me some telemetry around browsers, OSs, locations and so on.\n\n\n\n\nAll in all, it’s a LOT of data for only a couple of minor edits to code. You can also add *some* of this data to Dashboards, so you can make custom Dashboards for monitoring your sites.\n\nBest of all, if I give this application to many different customers, all I need to do is supply them with a new AppInsightsID for their web.config file and I’ll instantly get telemetry. Very cool!\n\nHappy monitoring!\n",
      "categories": [],
      "tags": ["appinsights"],
      
      "collection": "posts",
      "url": "/application-insights-telemetry-for-waws-or-customer-hosted-sites-without-mma/"
    },{
      
      "title": "WebDeploy Gets Even More Awesome – Profile Specific Transforms",
      "date": "2014-04-23 16:37:00 +0000",
      
      "content": "I love WebDeploy – I have ever since I read Scott Hanselman’s post “Web Deploy Made Awesome: If You’re Using XCopy, You’re Doing It Wrong”. Whenever I’m helping teams that build web applications improve their ALM processes, invariable I end up moving them onto Web Deploy. Not only is it an easier and cleaner way to deploy, but you get the bonus of being able to manage configuration files (Web.config) in your project.\n\nConfig as Code\n\nMaturing in ALM means that you need to head towards continuous deployment. You need to be able to deploy your application with a single click of a button. However, one challenge with this is managing configurations. How do you manage Web.config files in dev, UAT and Production environments? Do you find yourself copying config files out the way before you deploy? That’s BAD. You should be managing your configs as if they were code. They need to be source controlled.\n\nFair enough, I hear you say. So you’ll just copy all your config files from all your servers into a folder and manage them from there. Better – but still lots of pain. A far better approach is to use config transforms.\n\nConfig transforms came into web applications in VS 2010. With the latest release of VS 2013, not only are they also available for web sites (NOTE: please don’t use websites – always use web applications!) but you can new preview your transforms from VS and there’s now support for profile-specific transforms.\n\nLet’s have a look at a config transform in a newly created MVC web application. You can see that the Web.config file expands to show 2 other files – one per project configuration:\n\n\n\n\nIf you add project configurations, then you can right-click the Web.config and select “Add config Transforms” and VS will create a new transform file for you.\n\nLet’s have a look at the Web.Release.config (I’ve removed the comments that are auto-generated):\n\n&lt;configuration xmlns:xdt=\"http://schemas.microsoft.com/XML-Document-Transform\"&gt;\n  &lt;system.web&gt;\n    &lt;compilation xdt:transform=\"RemoveAttributes(debug)\"&gt;\n  &lt;/compilation&gt;&lt;/system.web&gt;\n&lt;/configuration&gt;\n\n\nYou can see that in the  tag there’s an xdt:Transform to remove the debug attribute. When applied, the transform uses the structure of the transform file to find the correct element and perform the necessary transforms – whether it’s an insert, a remove or some other transform. You can read about the syntax for transforms here. Note that when you debug out of VS, your application will use the Web.config file irrespective of if you’re running in Debug or Release – the transforms only happen when you publish or package your web app.\n\nThis is all existing “2012” stuff. What’s new in the latest release?\n\nPreview and Profile Specific Transforms\n\nYou can now preview your transform by right-clicking on the transform file (Web.Release.config, for example) and selecting “Preview Transform”. This immediately shows a diff – the original Web.config on the left and the transformed config on the right:\n\n\n\n\nNow you can debug your transforms!\n\nSo let’s imagine you have a UAT environment. You want to publish your site, so you create a new project configuration called UAT. You then copy the release transforms and put in your UAT connection strings and so on. After doing some testing in UAT, you realize that you actually need to debug, so you’ll have to update your Web.UAT.config file again, or create a UAT-DEBUG project configuration. Soon you’ll get lots of project configurations, and that’s just a mess.\n\nHowever, you can now create a profile-specific transform on top of the project configuration transforms! Let’s create a publish profile. I’ll right-click the web project and select “Publish”. I’ll create a new profile called “UAT-Debug” and give it whatever settings I need to. On the “Configuration” page, I’ll change the configuration from Release to Debug.\n\n\n\n\nThen I’ll click on “Close” to save the profile without publishing. That will create a pubxml file for me. I’ll repeat the process and create a publish profile called “UAT-Release”. Looking under the Properties of my project I’ll see the pubxml files:\n\n\n\n\nNow I can right-click a pubxml and select “Add Config Transform” – which creates a new transform for me that’s tied to this publish profile.\n\n\n\n\nI’ll create one for each profile. You’ll see that I now have 4 transforms (but still only 2 project configurations – hooray!)\n\n\n\n\nLet’s insert an attribute in both of the new config files:\n\n&lt;appsettings&gt;\n  &lt;add key=\"Profile\" value=\"UAT-debug\" xdt:transform=\"Insert\"&gt;\n&lt;/add&gt;&lt;/appsettings&gt;\n\n\nI’ve also removed the debug attribute from the compilation tag on the UAT-Release config and removed that transform from the debug one:\n\n\n\n\nNow when I right-click Web.UAT-Release.config and select “Preview Transform” I can see the diff. Note that the right-hand file tells me that it has actually applied 2 transforms: first the Release transform (since the profile specifies Release for the project configuration) and secondly the Web.UAT-Release.config transforms themselves.\n\n\n\n\nVoila! I can now maintain 2 project configurations (Debug and Release) and have a publish profile and transform config per project config and environment. Awesome!\n\nIf you’re using Release Management, don’t forget to check out my post about how to use config transforms for parameterizing releases!\n\nHappy transforming!\n",
      "categories": [],
      "tags": ["releasemanagement"],
      
      "collection": "posts",
      "url": "/webdeploy-gets-even-more-awesome-profile-specific-transforms/"
    },{
      
      "title": "Colin’s ALM Corner – Updated Blog Engine",
      "date": "2014-04-25 18:26:08 +0000",
      
      "content": "I have been using Blogger ever since I started my blog back in 2010. Once you get the template right (and set up a domain) it’s not a bad hosting platform. It works nicely with Windows Live Writer (as every self-respecting blog engine should). However, I felt it was time for a change – I wanted to take charge of my own blogging platform.\n\nA couple of week’s ago I read a post by Scott Hanselman about Mad’s Krisensen’s MiniBlog engine. I had a look and liked it instantly – but there was no way to port from Blogger to MiniBlog. So I left it to stew in the back of my mind (*ominous chuckle - BWAHAHAHA*).\n\nPorting to MiniBlog from Blogger\n\nI finally had another look a few days ago to see if I could port my existing blog posts over. While there was no native way to do this, I found a util (BloggerBackup) that let me export my blog posts (in ATOM format). I promptly exported all my posts.\n\nThe next trick was to import them into MiniBlog format. Fortunately there’s a little util that converts from BlogEngine.NET (or WordPress) to MiniBlog called MiniBlogFormatter. I cloned the repo and wrote my own formatter. This wasn’t too hard – using some Linq-to-XML I had something going pretty quickly. Here’s the code:\n\npublic class BloggerATOMFormatter\n{\n    public void Format(string originalFolderPath, string targetFolderPath)\n    {\n        FormatPosts(originalFolderPath, targetFolderPath);\n    }\n\n    private void FormatPosts(string originalFolderPath, string targetFolderPath)\n    {\n        var oldPostList = new Dictionary&amp;lt;string, string&amp;gt;();\n        foreach (string file in Directory.GetFiles(originalFolderPath, \"*.xml\").Where(s =&amp;gt; !s.EndsWith(\"comments.xml\")))\n        {\n            var originalDoc = LoadDocument(file);\n            XNamespace atomNS = @\"http://www.w3.org/2005/Atom\";\n\n            var entry = originalDoc.Element(atomNS + \"entry\");\n\n            var title = entry.Element(atomNS + \"title\").Value;\n            var oldUrl = (from link in entry.Elements(atomNS + \"link\")\n                          where link.Attributes().ToList().Any(a =&amp;gt; a.Name == \"rel\" &amp;amp;&amp;amp; a.Value == \"alternate\")\n                          select link).First().Attribute(\"href\").Value.Replace(\"http://www.colinsalmcorner.com\", \"\");\n            \n            var content = FixContent(entry.Element(atomNS + \"content\").Value);\n            var publishDate = DateTime.Parse(entry.Element(atomNS + \"published\").Value);\n            var lastModDate = DateTime.Parse(entry.Element(atomNS + \"updated\").Value);\n            var slug = FormatterHelpers.FormatSlug(title);\n            var categories = from cat in entry.Elements(atomNS + \"category\")\n                             select cat.Attribute(\"term\").Value;\n\n            var post = new Post();\n            post.Author = \"Colin Dembovsky\";\n            post.Categories = categories.ToArray();\n            post.Content = content;\n            post.IsPublished = true;\n            post.PubDate = publishDate;\n            post.Title = title;\n            post.Slug = slug;\n            post.LastModified = lastModDate;\n            post.Comments = GetCommentsForPost(file);\n\n            var newId = Guid.NewGuid().ToString();\n            Storage.Save(post, Path.Combine(targetFolderPath, newId + \".xml\"));\n            oldPostList[oldUrl] = newId;\n        }\n        SaveOldPostMap(targetFolderPath, oldPostList);\n    }\n\n    private void SaveOldPostMap(string targetFolderPath, Dictionary&amp;lt;string, string&amp;gt; oldPostList)\n    {\n        var mapElement = new XElement(\"OldPostMap\");\n        foreach(var key in oldPostList.Keys)\n        {\n            mapElement.Add(\n                new XElement(\"OldPost\",\n                    new XAttribute(\"oldUrl\", key),\n                    new XAttribute(\"postId\", oldPostList[key])\n                )\n            );\n        }\n        var doc = new XDocument(mapElement);\n        doc.Save(Path.Combine(targetFolderPath, \"oldPosts.map\"));\n    }\n\n    private List&amp;lt;Comment&amp;gt; GetCommentsForPost(string file)\n    {\n        var commentsFile = file.Replace(\".xml\", \".comments.xml\");\n        if (!File.Exists(commentsFile))\n        {\n            return new List&amp;lt;Comment&amp;gt;();  \n        }\n\n        var commentsDoc = LoadDocument(commentsFile);\n        XNamespace atomNS = @\"http://www.w3.org/2005/Atom\";\n\n        var list = new List&amp;lt;Comment&amp;gt;();\n        foreach (var originalComment in commentsDoc.Descendants(atomNS + \"entry\"))\n        {\n            var authorElement = originalComment.Element(atomNS + \"author\");\n            var name = authorElement.Element(atomNS + \"name\").Value;\n            var email = authorElement.Element(atomNS + \"email\").Value;\n            var uriElement = authorElement.Element(atomNS + \"uri\");\n            string website = null;\n            if (uriElement != null)\n            {\n                website = uriElement.Value;\n            }\n\n            var content = originalComment.Element(atomNS + \"content\").Value;\n            var publishDate = DateTime.Parse(originalComment.Element(atomNS + \"published\").Value);\n\n            var comment = new Comment();\n            comment.Author = name;\n            comment.Email = email;\n            comment.PubDate = publishDate;\n            comment.Content = content;\n            comment.IsAdmin = false;\n            comment.Website = website;\n            list.Add(comment);\n        }\n\n        return list.OrderBy(c =&amp;gt; c.PubDate).ToList();\n    }\n\n    private string FixContent(string originalContent)\n    {\n        var regex = new Regex(\"&amp;lt;pre class=\\\"brush: \\\\w*;\\\"&amp;gt;(.*?)&amp;lt;/pre&amp;gt;\", RegexOptions.IgnoreCase);\n        foreach(Match match in regex.Matches(originalContent))\n        {\n            var formatted = match.Groups[1].Value.Replace(\"&amp;lt;br /&amp;gt;\", Environment.NewLine);\n            originalContent = originalContent.Replace(match.Groups[1].Value, formatted);\n        }\n        return originalContent.Replace(\"&amp;lt;p&amp;gt;&amp;lt;/p&amp;gt;&amp;lt;br /&amp;gt;\", \"\").Replace(\"&amp;lt;p&amp;gt;&amp;lt;/p&amp;gt;\", \"\").Replace(\"&amp;lt;h3&amp;gt;\", \"&amp;lt;h2&amp;gt;\").Replace(\"&amp;lt;/h3&amp;gt;\", \"&amp;lt;/h2&amp;gt;\");\n    }\n\n    private XDocument LoadDocument(string file)\n    {\n        return XDocument.Parse(File.ReadAllText(file));\n    }\n}\n\n\nThere is a bit of “colinsALMcorner” specific code here, but if you’re looking to move from Blogger to MiniBlog you should be able to use most of this code. I had some issues with the formatting of the &lt;pre&gt; sections for Syntax Highlighter – once I had that sorted, the formatter worked flawlessly.\n\nRedirecting Existing Posts\n\nOne of the challenges I had was what about search engines that already reference existing posts? Since I wanted to host MiniBlog on Azure and point my domain to the new site, I wanted to preserve any existing reference. However, the naming scheme for posts in Blogger is different from that in MiniBlog.\n\nWhat I ended up doing was creating a map file as part of my convert-from-blogger-file-to-MiniBlog-file in the MiniBlogFormatter. I then created a simple HttpHandler that can server a “301 Moved Permanently” redirect when you hit an old post. Here’s the code:\n\npublic class OldPostHandler : IHttpHandler\n{\n    public bool IsReusable\n    {\n        get { return false; }\n    }\n\n    public void ProcessRequest(HttpContext context)\n    {\n        var oldUrl = context.Request.RawUrl;\n        var oldPost = Storage.GetOldPost(oldUrl);\n\n        if (oldPost == null)\n        {\n            throw new HttpException(404, \"The post does not exist\");\n        }\n\n        var newUrl = \"/post/\" + oldPost.Slug;\n        context.Response.Status = \"301 Moved Permanently\";\n        context.Response.AddHeader(\"Location\", newUrl);\n    }\n}\n\n\nIt’s small, neat and quick – keeping in line with the MiniBlog philosophy. Here’s the Storage.GetOldPost() method:\n\npublic static Post GetOldPost(string url)\n{\n    var map = GetOldPostMap();\n    if (map.ContainsKey(url))\n    {\n        return GetAllPosts().SingleOrDefault(p =&amp;gt; p.ID == map[url]);\n    }\n    return null;\n}\n\npublic static Dictionary&amp;lt;string, string&amp;gt; GetOldPostMap()\n{\n    GetAllPosts();\n\n    if (HttpRuntime.Cache[\"oldPostMap\"] != null)\n    {\n        return (Dictionary&amp;lt;string, string&amp;gt;)HttpRuntime.Cache[\"oldPostMap\"];\n    }\n    return new Dictionary&amp;lt;string, string&amp;gt;();\n}\n\nprivate static void LoadOldPostMap()\n{\n    var map = new Dictionary&amp;lt;string, string&amp;gt;();\n    var mapFile = Path.Combine(_folder, \"oldPosts.map\");\n    if (File.Exists(mapFile))\n    {\n        var doc = XDocument.Load(mapFile);\n        foreach (var mapping in doc.Descendants(\"OldPost\"))\n        {\n            var oldUrl = mapping.Attribute(\"oldUrl\").Value;\n            var newId = mapping.Attribute(\"postId\").Value;\n            map[oldUrl] = newId;\n        }\n    }\n    HttpRuntime.Cache.Insert(\"oldPostMap\", map);\n}\n\n\nGetAllPosts() add’s a call to LoadOldPostMap() which finds the map file and reads it into memory. I only have 87 posts, so it’s not too heavy.\n\nHere’s the code to invoke the handler in web.config:\n\n&amp;lt;handlers&amp;gt;\n  &amp;lt;remove name=\"CommentHandler\"/&amp;gt;\n  &amp;lt;add name=\"CommentHandler\" verb=\"*\" type=\"CommentHandler\" path=\"/comment.ashx\"/&amp;gt;\n  &amp;lt;remove name=\"PostHandler\"/&amp;gt;\n  &amp;lt;add name=\"PostHandler\" verb=\"POST\" type=\"PostHandler\" path=\"/post.ashx\"/&amp;gt;\n  &amp;lt;remove name=\"MetaWebLogHandler\"/&amp;gt;\n  &amp;lt;add name=\"MetaWebLogHandler\" verb=\"POST,GET\" type=\"MetaWeblogHandler\" path=\"/metaweblog\"/&amp;gt;\n  &amp;lt;remove name=\"FeedHandler\"/&amp;gt;\n  &amp;lt;add name=\"FeedHandler\" verb=\"GET\" type=\"FeedHandler\" path=\"/feed/*\"/&amp;gt;\n  &amp;lt;remove name=\"FeedsHandler\"/&amp;gt;\n  &amp;lt;add name=\"FeedsHandler\" verb=\"GET\" type=\"FeedHandler\" path=\"/feeds/*\"/&amp;gt;\n  &amp;lt;remove name=\"CssHandler\"/&amp;gt;\n  &amp;lt;add name=\"CssHandler\" verb=\"GET\" type=\"MinifyHandler\" path=\"*.css\"/&amp;gt;\n  &amp;lt;remove name=\"JsHandler\"/&amp;gt;\n  &amp;lt;add name=\"JsHandler\" verb=\"GET\" type=\"MinifyHandler\" path=\"*.js\"/&amp;gt;\n  &amp;lt;remove name=\"OldPostHandler\"/&amp;gt;\n  &amp;lt;add name=\"OldPostHandler\" verb=\"GET\" type=\"OldPostHandler\" path=\"*.html\"/&amp;gt;\n&amp;lt;/handlers&amp;gt;\n\n\nYou’ll see that I also added a “FeedsHandler” as well to work with the blogger feeds format, so that existing subscribers wouldn’t be affected by the switch (hopefully).\n\nI then styled the site (since it’s based on bootstrap that wasn’t a problem). I also added a tag-cloud function and a search function. Both turned out to be really simple.\n\nTag Cloud\n\nI needed a method that would return all the categories and their frequency for the tag cloud. Here’s the code in the backend:\n\npublic static Dictionary&amp;lt;string, int&amp;gt; GetTags()\n{\n    var categories = Storage.GetAllPosts().SelectMany(p =&amp;gt; p.Categories).Distinct();\n    var tags = new Dictionary&amp;lt;string, int&amp;gt;();\n    foreach(var cat in categories)\n    {\n        var count = Storage.GetAllPosts().Where(p =&amp;gt; p.Categories.Any(c =&amp;gt; c.Equals(cat, StringComparison.OrdinalIgnoreCase))).Count();\n        tags[cat] = count;\n    }\n    return tags;\n}\n\n\nNext I had to find a way to present a tag cloud on the page using javascript. There are lots of ways of doing this – I ended up using this jQuery tagcloud script. Here’s the html for my tag cloud:\n\n&amp;lt;div id=\"tagcloud\"&amp;gt;\n    @{\n        var tags = Blog.GetTags();\n        foreach (var tag in tags.Keys)\n        {\n            &amp;lt;a href=\"/category/@tag\" rel=\"@tags[tag]\"&amp;gt;@tag&amp;lt;/a&amp;gt;\n        }\n    }\n&amp;lt;/div&amp;gt;\n\n&amp;lt;script type=\"text/javascript\"&amp;gt;\n    // tag cloud script\n    $(\"#tagcloud a\").tagcloud({\n        size: {\n            start: 0.8,\n            end: 1.75,\n            unit: 'em'\n        },\n        color: {\n            start: \"#7cc0f4\",\n            end: \"#266ca2\"\n        }\n    });\n&amp;lt;/script&amp;gt;\n\n\nSearch\n\nI regularly search my own blog – it’s a “working journal” of sorts. Having a search function was pretty important to me. Again the solution was really simple. Here’s the search code:\n\npublic static List&amp;lt;Post&amp;gt; Search(string term)\n{\n    term = term.ToLower();\n    return (from p in Storage.GetAllPosts()\n            where p.Title.ToLower().Contains(term) || p.Content.ToLower().Contains(term) || p.Comments.Any(c =&amp;gt; c.Content.ToLower().Contains(term))\n            select p).ToList();\n}\n\n\nOnce I had the results, I created a new search.cshtml page that shows just the first few lines of the blog post:\n\n@{\n    var term = Request.QueryString[\"term\"];\n\n    Page.Title = Blog.Title;\n    Layout = \"~/themes/\" + Blog.Theme + \"/_Layout.cshtml\";\n    \n    if (string.IsNullOrEmpty(term))\n    {\n        &amp;lt;h1&amp;gt;Oops!&amp;lt;/h1&amp;gt;\n        &amp;lt;p&amp;gt;Something went wrong with your search. Try again...&amp;lt;/p&amp;gt;\n    }\n    else\n    {\n        &amp;lt;h1&amp;gt;Results for search: '@term'&amp;lt;/h1&amp;gt;\n        \n        var list = Blog.Search(term);\n        if (list.Count == 0)\n        {\n            &amp;lt;p&amp;gt;No matches...&amp;lt;/p&amp;gt;\n        }\n        else\n        {\n            foreach(var p in list)\n            {\n                @RenderPage(\"~/themes/\" + Blog.Theme + \"/PostSummary.cshtml\", p);\n            }\n        }\n    }\n}\n\n\nThe final bit was to get a search control. I ended up doing one entirely in css:\n\ninput {\n    outline: none;\n}\ninput[type=search] {\n    -webkit-appearance: textfield;\n    -webkit-box-sizing: content-box;\n    font-family: inherit;\n    font-size: 80% !important;\n}\ninput::-webkit-search-decoration,\ninput::-webkit-search-cancel-button {\n    display: none; /* remove the search and cancel icon */\n}\n\n/* search input field */\ninput[type=search] {\n    background: #ededed url(images/search-icon.png) no-repeat 9px center;\n    border: solid 1px #ccc;\n    padding: 5px 5px 5px 10px;\n    width: 130px;\n    \n    -webkit-border-radius: 10em;\n    -moz-border-radius: 10em;\n    border-radius: 10em;\n    \n    -webkit-transition: all .5s;\n    -moz-transition: all .5s;\n    transition: all .5s;\n}\ninput[type=search]:focus {\n    width: 100%;\n    background-color: #fff;\n    border-color: #6dcff6;\n    \n    -webkit-box-shadow: 0 0 5px rgba(109,207,246,.5);\n    -moz-box-shadow: 0 0 5px rgba(109,207,246,.5);\n    box-shadow: 0 0 5px rgba(109,207,246,.5);\n}\n\n/* placeholder */\ninput:-moz-placeholder {\n    color: #999;\n}\ninput::-webkit-input-placeholder {\n    color: #999;\n}\n\n\nAnd here’s the search control in my side-bar:\n\n&amp;lt;section&amp;gt;\n    &amp;lt;br /&amp;gt;\n    &amp;lt;form action=\"/search\" method=\"get\" role=\"form\" id=\"searchForm\"&amp;gt;\n        &amp;lt;fieldset&amp;gt;\n            &amp;lt;input type=\"search\" placeholder=\"Search this blog\" name=\"term\"&amp;gt;\n        &amp;lt;/fieldset&amp;gt;\n    &amp;lt;/form&amp;gt;\n    &amp;lt;hr /&amp;gt;\n&amp;lt;/section&amp;gt;\n\n\nApprove or Delete Comments from the Alert Mail\n\nWhen someone writes a comment on a post, MiniBlog sends you an email. I like to moderate comments, so that’s how I’ve configured MiniBlog. In the mail there are 2 links – one to approve and one to delete the comment. However, I kept getting 403 “unauthorized” then clicking the links if I wasn’t logged in on the site. I made a small tweak to the CommentHandler Accept and Delete methods to redirect me to the login page instead of throwing a 403:\n\nif (!context.User.Identity.IsAuthenticated)\n{\n    // was throwing 403 here\n    FormsAuthentication.RedirectToLoginPage();\n    return;\n}\n\n\nNow when I hit the link from my mail, I get redirected to the login screen. Once logged in, the comment is approved/deleted and all’s well.\n\nPublishing to Azure\n\nAfter testing posting from Windows Live Writer (no issues there) I then published the site to Azure. I changed my DNS records from Blogger to Azure and hey presto – new site is up!\n\nConclusion\n\nI’m really happy with the new look &amp; feel and with the other modern web benefits (like SEO optimization and of course, speed) that MiniBlog brings. Thanks Mads!\n\nI expect there may be a glitch or two for the switch over, but hopefully everything works well. Let me know in the comments if you experience any issues.\n\nHappy reading!\n",
      "categories": [],
      "tags": ["news"],
      
      "collection": "posts",
      "url": "/colins-alm-corner-updated-blog-engine/"
    },{
      
      "title": "Colin’s ALM Corner – New Theme and Live Tiles",
      "date": "2014-04-30 22:29:34 +0000",
      
      "content": "Last week I updated my blog engine from Blogger to MiniBlog. The last couple of days I’ve been updating the theme and style. Every good blog needs some good bling!\n\nApart from the new look and feel, you can now also pin my site to Windows 8 or Windows Phone 8.1 Preview. When you pin the tile, you get an update of the latest posts on the Live Tile.\n\nLet me know what you think of the new design!\n\nHappy reading!\n",
      "categories": [],
      "tags": ["news"],
      
      "collection": "posts",
      "url": "/colins-alm-corner-new-theme-and-live-tiles/"
    },{
      
      "title": "Enabling JavaScript Code Coverage Link in Builds",
      "date": "2014-05-14 20:43:03 +0000",
      
      "content": "In a previous post I wrote about how to do JavaScript unit testing in VS 2012. The same procedure applies to VS 2013 – but the Chutzpah test adapter now allows you to run code coverage too. At the end of my post I link to another blog entry about how to enable the tests to run during team builds.\n\nI recently added some tests to a VS 2013 solution I was working on and was pleased to see that when you “Analyze Code Coverage for all Tests” in the Test Explorer, VS pops open a nicely formatted html page that shows you your JavaScript coverage. I wanted to have that file available in my build results too. Looking at the test results folder of the local VS test run, I saw that Chutzpah created an html file called “_Chutzpah.coverage.html”. I wanted that script to be copied to the drop folder of the build and create a link in the build summary that you could click to open it.\n\nPost-Test Script\n\nFortunately you can do this without even having to customize the build template – as long as you’re using the TfvcTemplate.12.xaml template – the default build template that ships with TFS 2013. This build has some really useful script hooks – and there’s one for running a post-test script. I knew I could easily copy the Chutpah result file to the drop folder – no problem. But how do you add to the build summary report from a script that’s running “outside” the workflow? If you customize the workflow you can use the WriteCustomSummaryInformation activity, but I wanted to do this without modifying the template.\n\nAfter mailing the ChampsList, Jakob Ehn pointed me in the right direction – I needed to use the InformationNodeCoverters.AddCustomSummaryInformation method. Once I had that, the rest of the PowerShell script was almost trivial. I did hit one snag – I need the Team Project Collection URI for the script to work, but for some reason the value of the TF_BUILD_COLLECTIONURI build environment variable was empty. Updating my build agent to VS 2013 Update 2 resolved this issue. Here’s the script:\n\nParam(\n  [string]$testResultsDir = $env:TF_BUILD_TESTRESULTSDIRECTORY,\n  [string]$dropLocation = $env:TF_BUILD_DROPLOCATION,\n  [string]$tpcUri = $env:TF_BUILD_COLLECTIONURI,\n  [string]$buildUri = $env:TF_BUILD_BUILDURI\n)\n\n$coverageFileName = \"\\_Chutzpah.coverage.html\"\n$jsScriptResultsFile = $testResultsDir + $coverageFileName\nif (Test-Path($jsScriptResultsFile)) {\n    try {\n        Write-Host \"Copying Chutzpah coverage files\"\n        copy $jsScriptResultsFile $dropLocation\n\n        # add the link into the build summary\n        Write-Host \"Loading TFS assemblies\"\n        [Reflection.Assembly]::LoadWithPartialName('Microsoft.TeamFoundation.Client')\n        [Reflection.Assembly]::LoadWithPartialName('Microsoft.TeamFoundation.Build.Client')\n    \n        Write-Host \"Getting build object\"\n        $tpc = [Microsoft.TeamFoundation.Client.TfsTeamProjectCollectionFactory]::GetTeamProjectCollection($tpcUri)\n        $buildService = $tpc.GetService([Microsoft.TeamFoundation.Build.Client.IBuildServer])\n        $build = $buildService.GetBuild($buildUri)\n\n        Write-Host \"Writing Chutzpah coverage link to build summary\"\n        $message = \"Javascript testing was detected. Open [coverage results]($dropLocation\\$coverageFileName)\"\n        [Microsoft.TeamFoundation.Build.Client.InformationNodeConverters]::AddCustomSummaryInformation($build.Information, $message, \"ConfigurationSummary\", \"Javascript Coverage\", 200)\n        $build.Information.Save();\n\n        # all is well with the world\n        Write-Host \"Success!\"\n        exit 0\n    }\n    catch {\n        Write-Error $_\n        exit 1\n    }\n} else {\n    # let the build know there were no coverage files\n    Write-Warning \"No Chutzpah coverage file detected\"\n    exit 0\n}\n\n\nI saved this to my build scripts folder under source control and checked it in.\n\nOpening up the build definition, I had to create a second build run to run the JavaScript tests – here’s the settings I used:\n\n\n\n\nNote how I’ve enabled Code Coverage in the options dropdown.\n\nI added this folder to the source mappings for my build and then called the script in the post-test settings of the build:\n\n\n\n\nNow when I run my build, I get a link to the JavaScript coverage file:\n\n\n\n\nClicking on the “coverage results” link opens the results page:\n\n\n\n\nAs a next project, I want to see if I can incorporate the coverage results into the build warehouse so that there’s metrics not only on .NET coverage over time, but also for JavaScript tests.\n\nHappy testing!\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/enabling-javascript-code-coverage-link-in-builds/"
    },{
      
      "title": "Imaginet Timesheet: Time Tracking for TFS and Visual Studio Online",
      "date": "2014-05-30 02:33:06 +0000",
      
      "content": "We’ve been working on a rewrite of our Timetracking tool (formerly Notion Timesheet) and it’s going live today – Imaginet Timesheet! Timesheet lets you log time against TFS work items using a web interface. The web site can be installed on any IIS server (if you want to host it on-premises) or even onto Windows Azure Web Sites (WAWS) if you have a public-facing TFS or are using Visual Studio Online. Once you’ve installed it, just log in, select a date-range (week) and a query and start logging time.\n\nIt’s free for up to 5 users so you can try it out to see if it works for you and your organization. There are some report samples out-the-box and you can also easily create your own reports using PowerPivot.\n\nWe used Entity Framework, MVC, Bootstrap and Knockout to make the site. Most of our JavaScript is typed using TypeScript. Of course we have unit tests (.NET and JavaScript) and a build that builds the installer (Wix) package. It was a fun project to work on and I think we’ve turned out a great product. Download your copy today!\n\nHere’s the overview video:\n\nHappy timetracking!\n",
      "categories": [],
      "tags": ["news"],
      
      "collection": "posts",
      "url": "/imaginet-timesheet-time-tracking-for-tfs-and-visual-studio-online/"
    },{
      
      "title": "PowerShell DSC: Remotely Configuring a Node to “RebootNodeIfNeeded”",
      "date": "2014-06-26 16:35:19 +0000",
      
      "content": "I’ve started to experiment a bit with some PowerShell DSC – mostly because it’s now supported in Release Management (in Update 3 CTP at least).\n\nSometimes when you apply a configuration to a node (machine), the node requires a reboot (for example adding .NET4.5 requires the node to reboot). You can configure the node to reboot immediately (instead of just telling you “a reboot is required”) by changing a setting in the node’s LocalConfigurationManager. Of course, since this is configuration, it’s tempting to try to do this in a DSC script – for example:\n\nConfiguration SomeConfig\n{\n   Node someMachine\n   {\n      LocalConfigurationManager\n      {\n         RebootNodeIfNeeded = $true\n      }\n   }\n}\n\n\nThis configuration “compiles” to a mof file and you can apply it successfully. However, it doesn’t actually do anything.\n\nSet-DscLocalConfigurationManager on a Remote Node\n\nFortunately, there is a way to change the settings on the LocalConfigurationManager remotely – you use the cmdlet Set-DscLocalConfigurationManager with a CimSession object (i.e. you invoke it remotely). I stumbled across this when looking at the documentation for DSC Local Configuration Manager where the very last sentence says “To see the current Local Configuration Manager settings, you can use the Get-DscLocalConfigurationManager cmdlet. If you invoke this cmdlet with no parameters, by default it will get the Local Configuration Manager settings for the node on which you run it. To specify another node, use the CimSession parameter with this cmdlet.”\n\nHere’s a script that you can modify to set “RebootNodeIfNeeded” on any node:\n\nConfiguration ConfigureRebootOnNode\n{\n    param (\n        [Parameter(Mandatory=$true)]\n        [ValidateNotNullOrEmpty()]\n        [String]\n        $NodeName\n    )\n\n    Node $NodeName\n    {\n        LocalConfigurationManager\n        {\n            RebootNodeIfNeeded = $true\n        }\n    }\n}\n\nWrite-Host \"Creating mofs\"\nConfigureRebootOnNode -NodeName fabfiberserver -OutputPath .\\rebootMofs\n\nWrite-Host \"Starting CimSession\"\n$pass = ConvertTo-SecureString \"P2ssw0rd\" -AsPlainText -Force\n$cred = New-Object System.Management.Automation.PSCredential (\"administrator\", $pass)\n$cim = New-CimSession -ComputerName fabfiberserver -Credential $cred\n\nWrite-Host \"Writing config\"\nSet-DscLocalConfigurationManager -CimSession $cim -Path .\\rebootMofs -Verbose\n\n# read the config settings back to confirm\nGet-DscLocalConfigurationManager -CimSession $cim\n\n\nJust replace “fabfiberserver” with your node name and .\\ the script. The last line of the script reads back the LocalConfigurationManager settings on the remote node, so you should see the RebootNodeIfNeeded setting is true.\n\n\n\n\nHappy configuring!\n",
      "categories": [],
      "tags": ["development"],
      
      "collection": "posts",
      "url": "/powershell-dsc-remotely-configuring-a-node-to-rebootnodeifneeded/"
    },{
      
      "title": "Using PowerShell DSC in Release Management: The Hidden Manual",
      "date": "2014-07-04 21:05:08 +0000",
      
      "content": "Just in case you missed it, Release Management Update 3 CTP now supports deploying using PowerShell DSC. I think this is a great feature and adds to the impressive toolset that Microsoft is putting out into the DevOps area. So I decided to take this feature for a spin!\n\nBleeding Edge\n\n&lt;rant&gt;I had a boss once who hated being on the bleeding edge – he preferred being at “latest version – 1” of any OS, SQL Server or VS version (with a few notable exceptions). Being bleeding edge can mean risk and churn, but I prefer being there all the same. Anyway, in the case of the Release Management (RM) CTP, it was a little painful – mostly because the documentation is poor. Hopefully this is something the Release Management team will improve on. I know the release is only CTP, but how can the community provide feedback if they can’t even figure out how to use the tool?&lt;/rant&gt;\n\nOn top of the Release Management struggles, PowerShell DSC itself isn’t very well documented (yet) since it itself is pretty new technology. This is bleeding BLEEDING edge stuff.\n\nAnyway, after struggling on my own for a few days I mailed the product group and got “the hidden manual” as a reply (more on this later). At least the team responds fairly quickly when MVPs contact them!\n\nIssues\n\nSo here’s a summary of the issues I faced:\n\n\n  The DSC feature only works on domain joined machines. I normally don’t use domains on my experimental VMs, so I had to make one, but most organizations nowadays use domains anyway, so this isn’t such a big issue.\n  Following the RM DSC manual, I wanted to enable CredSSP. I ran the Enable-WSManCredSSP command from the manual, but got some credential issues later on.\n  The current public documentation on DSC in RM is poor – in fact, without mailing the product group I would never have gotten my Proof-of-Concept release to work at all (fortunately you now have this post to help you!)\n  You have to change your DSC scripts to use in Release Management (you can’t have the exact same script run in RM and in a console – the mof compilation is invoked differently, especially with config data)\n\n\nProof of Concept – A “Start of Release” Walkthrough\n\nI want to eventually build up to a set of scripts that will allow me to deploy a complete application (SQL database and ASP.NET website) onto a set of “fresh” servers using only DSC. This will enable me to create some new and unconfigured servers and target them in the Release – the DSC will ensure that SQL gets installed and configured correctly, that IIS, ASP.NET, MVC and any other prerequisites get set up correctly on the IIS server and finally that the database and website are deployed correctly. All without having to install or configure anything manually. That’s the dream. The first step was to create a few DSC scripts and then to get Release Management to execute them as part of the deployment workflow.\n\nI had to create a custom DSC resource (I may change this later) – but that’s a post for another day. Assume that I have the resource files ready for deployment to a node (a machine). Here’s the script to copy an arbitrary resource to the modules folder of a target node so that subsequent DSC scripts can utilize the custom resource:\n\nConfiguration CopyDSCResource {\n    param (\n        [Parameter(Mandatory=$false)]\n        [ValidateNotNullOrEmpty()]\n        [String]\n        $ModulePath = \"$env:ProgramFiles\\WindowsPowershell\\Modules\"\n    )\n\n    Node $AllNodes.NodeName\n    {\n        #\n        # Copy the custom DSC Resource to the target server\n        #\n        File DeployWebDeployResource\n        {\n            Ensure = \"Present\"\n            SourcePath = \"$($Node.SourcePath)\\$($Node.ModuleName)\"\n            DestinationPath = \"$ModulePath\\$($Node.ModuleName)\"\n            Recurse = $true\n            Force = $true\n            Type = \"Directory\"\n        }\n    }\n}\n\nCopyDSCResource -ConfigurationData $configData -Verbose\n\n\nThe last  of the script “compiles” the DSC script into a mof file that is then used to push this configuration to the target node. I wanted to parameterize the script, so I tried to introduce the RM parameter notation, which is __ pre- and post-fix (such as __ModuleName__). No such luck. I have to hardcode configuration data in the configuration data file.\n\nTo accomplish that I’m using configuration data for executing this script. This is standard DSC practice – however, there’s one trick. For RM, you need to put the configuration data into a variable. Here’s what an “ordinary” config data script looks like:\n\n@{\n    AllNodes = @(\n        @{\n            NodeName = \"*\"\n            SourcePath = \"\\\\rmserver\\Assets\\Resources\"\n            ModuleName = \"DSC_ColinsALMCorner.com\"\n         },\n\n        @{\n            NodeName = \"fabfiberserver\"\n            Role = \"WebServer\"\n         }\n    );\n}\n\n\nTo get this to work with RM, you need to change the 1st line to this:\n\n\n$configData = @{\n\n\nThis puts the configuration hashtable into a variable called “$configData”. This is the variable that I’m using in the CopyDSCResource DSC script to specify configuration data (see the last line of the previous script).\n\nMeanwhile, in RM, I’ve set up an environment (using “New Standard Environment”) and added my target server (defaulting to port 5985 for PSRemoting). I’ve configured a Release Path and now I want to configure the Component that is going to execute the script for me.\n\nI click on “Configure Apps” –&gt; Components and add a new component. I give it a name and specify the package path:\n\n\n\n\nYou can access the package path in your scripts using “$applicationPath”.\n\nNow I click on the “Deployment” tab and configure the tool – I select the “Run PowerShell on Standard Environment” tool (which introduces some parameters) and leave everything as default.\n\n\n\n\nNow let’s configure the Release Template. Click on “Configure Apps” –&gt; “Release Templates” and add a new Template. Give it a name and select a Release Path. In the toolbox, right-click on the Components node and add in the DSC script component we just created. Now drag into the designer the server and into the server activity drag the DSC component. We’ll then enter the credentials and the paths to the scripts:\n\n\n\n\nSince I’m accessing a network share, I specify “UseCredSSP” to true. Both ScriptPath and ConfigurationFilePath are relative to the package path (configured in the Source tab of the component). I specify the DSC script for the ScriptPath and the config data file for the ConfigurationFilePath. Finally, I supply a username and password for executing the command. We can now run the deployment!\n\nCreate a new Release and select the newly created template. Specify the build number (either a TFS build or external folder, depending on how you configured your components) and Start it.\n\n\n\n\nHopefully you get a successful deployment!\n\n\n\nIssues You May Face\n\nOf course, not everything will run smoothly. Here are some errors I faced and what I did to rectify them.\n\nCredential Delegation Failure\n\nSymptom: You get the following error message in the deployment log:\n\n\nSystem.Management.Automation.Remoting.PSRemotingTransportException: Connecting to remote server fabfiberserver.fab.com failed with the following error message : The WinRM client cannot process the request. A computer policy…\n\n\nFix: In the RM DSC manual, they tell you to run an Enable-WSManCredSSP command to allow credential delegation. I have VMs that have checkpoints, so I’ve run this PoC several times, and each time I get stuck I just start again at the “clean” checkpoint. Even though this command always works in PowerShell, I found that sometimes I would get this error. The fix is to edit a group policy on the RM server machine. Type gpedit.msc to open up the console and browse to “Computer Configuration\\Administrative Templates\\System\\Credentials Delegation”. Then click on the “Allow delegating fresh credentials with NTLM-only server authentication”. Enable this rule and then add in your target servers (click the “Show…” button). You can use wildcards if you want to delegate any machine on a domain. Interestingly, the Enable-WSManCredSSP command seems to “edit” the “Allow delegating fresh credentials” setting, not the NTLM-only one. Perhaps there’s a PowerShell command or extra argument that will edit the NTLM-only setting?\n\n\n\nConfiguration Data Errors\n\nSymptom: You get the following error message in the deployment log:\n\n\nSystem.Management.Automation.RemoteException: Errors occurred while processing configuration 'SomeConfig'.\n\n\nFix: I found that this message occurs for 2 main reasons: first, you forget to put your config data hashtable into a variable (make sure your line 1 is $configData = @{) or you have an error in your hashtable (like a forgotten comma or extra curly brace). If you get this error, then check your configuration data file.\n\nCannot Find Mof File\n\nSymptom: You get the following error message in the deployment log:\n\n\nSystem.Management.Automation.RemoteException: Unable to find the mof file.\n\n\nFix: This could mean that you’ve got an “-OutputPath” specified when you invoke your config (the last line of the config script) so that the mof file ends up in some other directory. Or you have the name of your node incorrect. I found that specifying “fabfiberserver.fab.com” caused this error in my scenario – but when I changed the name to “fabfiber” I didn’t get this error. You’ll have to try the machine name or the FQDN to see which one RM is happy with.\n\nChallenges\n\nThe ability to run DSC during Releases is a promising tool – but there are some challenges. Here is my list of pros and cons with this feature:\n\nPros of DSC in Release Management\n\n\n  You don’t have to install a deployer agent on the target nodes\n  You can use existing DSC PowerShell scripts (with some small RM specific tweaks) in your deployment workflows\n\n\nCons of DSC in Release Management\n\n\n  Only works on domain machines at present\n  Poor documentation makes figuring out how to structure scripts and assets to RM’s liking a challenge\n  You have to change your “normal” DSC script structure to fit the way RM likes to invoke DSC\n  You can’t parameterize the scripts (so that you can reuse scripts in different workflows)\n\n\nConclusion\n\nThe ability to run DSC in Release Management workflows is great – not having to install and configure the deployer agent is a bonus and being able to treat “config as code” in a declarative manner is a fantastic feature. However, since DSC is so new (and poorly documented) there’s a steep learning curve. The good news is that if you’ve already invested in DSC, the latest Release Management allows you to leverage that investment during deployments. This is overall a very exciting feature and I look forward to seeing it grow and mature.\n\nI’ll be posting more in this series as I get further along with my experimentation!\n\nHappy releasing!\n",
      "categories": [],
      "tags": ["devops"],
      
      "collection": "posts",
      "url": "/using-powershell-dsc-in-release-management-the-hidden-manual/"
    },{
      
      "title": "More DSC Release Management Goodness: Readying a Webserver for Deployment",
      "date": "2014-07-10 15:22:08 +0000",
      
      "content": "In my previous couple of posts (PowerShell DSC: Configuring a Remote Node to “RebootIfNeeded” and Using PowerShell DSC in Release Management: The Hidden Manual) I started to experiment with Release Management’s new PowerShell DSC capabilities. I’ve been getting some great help from Bishal Prasad, one of the developers on Release Management – without his help I’d never have gotten this far!\n\nMeta Mofs\n\nTo configure a node (the DSC parlance for a machine) you need to create a DSC script that configures the LocalConfigurationManager. When I first saw this, I thought this was a great feature – unfortunately, when you invoke the config script, it doesn’t produce a mof file (like “normal” DSC scripts that use resources like File and WindowsFeature) so you can’t use Start-DscConfiguration to push it to remote servers. You need to invoke Set-DscLocalConfigurationManager. The reason is that a config that targets LocalConfigurationManager produces a meta.mof instead of a mof file.\n\nIf you try to run a PowerShell script in Release Management that produces a meta.mof, you’ll see a failure like this:\n\n\nUnable to find the mof file. Make sure you are generating the MOF in the DSC script in the current directory.\n\n\nOf course this is because Release Management expects a mof file, and if you’re just producing a meta.mof file, the invocation will fail.\n\nWe may see support for meta.mofs in future versions of Release Management (hopefully sooner than later) but until then the workaround is to make sure that you include the LocalConfigurationManager settings inside a config that produces a mof file. Then you include two commands at the bottom of the script: first the command to “compile” the configuration – this produces a mof file as well as a meta.mof file. Then you call Set-DscLocalConfigurationManager explicitly to push the meta.mof and let Release Management handle the mof file. Here’s an example that configures a node to reboot if needed and ensures that the Webserver role is present:\n\nConfiguration WebServerPreReqs\n{\n    Node $AllNodes.where{ $_.Role -eq \"WebServer\" }.NodeName\n    {\n        # tell the node to reboot if necessary\n        LocalConfigurationManager\n        {\n            RebootNodeIfNeeded = $true\n        }\n\n        WindowsFeature WebServerRole\n        {\n            Name = \"Web-Server\"\n            Ensure = \"Present\"\n        }\n    }\n}\n\nWebServerPreReqs -ConfigurationData $configData\n\n# invoke Set-DscLocalConfigurationManager directly since RM doesn't yet support this\nSet-DscLocalConfigurationManager -Path .\\WebServerPreReqs -Verbose\n\n\nYou can see that there is a LocalConfigurationManager setting (line 6). Line 19 “compiles” the config – given a list of nodes in $configData that includes just a single node (say fabfiberserver) you’ll see fabfiberserver.mof and fabfiberserver.meta.mof files in the current directory after calling the script. Since RM itself takes care of pushing the mof file, we need to explicitly call Set-DscLocalConfigurationManager in order to push the meta.mof file (line 22).\n\nNow you can use this script just like you would any other DSC script in RM.\n\nSetting up the Release\n\nUtilizing this script in a Release is easy – create a component that has “Source” set to your build output folder (or a network share for deploying bins that are not built in TFS build) and set the deployment tool to “Run PowerShell on Standard Environment”. I’ve called my component “Run DSC Script”.\n\n\nNow on the Release Template, right-click the Components node in the toolbox and add in the script component, then drag it onto the designer inside your server block (which you’ll also need to drag on from your list of servers). Then just set the paths and username and password correctly and you’re good to go.\n\nI’ve saved this script as “WebServerPreReqs.ps1” in the Scripts folder of my build output folder – you can see the path there in the ScriptPath parameter. My configData script is also in the scripts folder (remember the ScriptPath and ConfigurationFilePath are relative to the source path that you configure in the component). Now you can start a release!\n\nInspecting the Logs\n\nOnce the release has completed, you can open the tool logs for the “Run DSC Script” component and you’ll see two “sets” of entrties. Both sets are prefixed with [SERVERNAME], indicating which node the logs pertain to. Here we can see a snippet of the Set-DscLocalConfigurationManager invocation logs (explicitly deploying the meta.mof):\n\n\n[FABFIBERSERVER]: LCM:  [Start  Set     ][FABFIBERSERVER]: LCM:  [Start  Resource]  [MSFT_DSCMetaConfiguration][FABFIBERSERVER]: LCM:  [Start  Set     ]  [MSFT_DSCMetaConfiguration][FABFIBERSERVER]: LCM:  [End    Set     ]  [MSFT_DSCMetaConfiguration]  in 0.0620 seconds.[FABFIBERSERVER]: LCM:  [End    Resource]  [MSFT_DSCMetaConfiguration][FABFIBERSERVER]: LCM:  [End    Set     ]Operation 'Invoke CimMethod' complete.Set-DscLocalConfigurationManager finished in 0.207 seconds.\n\n\nJust after these entries, you’ll see a second set of entries – this time for the remainder of the DSC invocation that RM initiates (which deploys the mof):\n\n\nAn LCM method call arrived from computer FABFIBERSERVER with user sid S-1-5-21-3349151495-1443539541-1735948571-1106.[FABFIBERSERVER]: LCM:  [Start  Resource]  [[WindowsFeature]WebServerRole][FABFIBERSERVER]: LCM:  [Start  Test    ]  [[WindowsFeature]WebServerRole][FABFIBERSERVER]:                            [[WindowsFeature]WebServerRole] The operation 'Get-WindowsFeature' started: Web-Server[FABFIBERSERVER]:                            [[WindowsFeature]WebServerRole] The operation 'Get-WindowsFeature' succeeded: Web-Server[FABFIBERSERVER]: LCM:  [End    Test    ]  [[WindowsFeature]WebServerRole]  in 0.8910 seconds.\n\n\nIn the next post I’ll look at using DSC to configure the rest of my webserver features as well as create a script for installing and configuring SQL Server. Then we’ll be in a good position to configure deployment of a web application (and its database) onto an environment that we know has everything it needs to run the application.\n\nIn the meantime – happy releasing!\n",
      "categories": [],
      "tags": ["devops"],
      
      "collection": "posts",
      "url": "/more-dsc-release-management-goodness-readying-a-webserver-for-deployment/"
    },{
      
      "title": "Install and Configure SQL Server using PowerShell DSC",
      "date": "2014-07-15 21:35:13 +0000",
      
      "content": "I’m well into my journey of discovering the capabilities of PowerShell DSC and Release Management’s DSC feature (See my previous posts: PowerShell DSC: Configuring a Remote Node to “Reboot If Needed”, Using PowerShell DSC in Release Management: The Hidden Manual and More DSC Release Management Goodness: Readying a Webserver for Deployment). I’ve managed to work out how to use Release Management to run DSC scripts on nodes. Now I am trying to construct a couple of scripts that I can use to deploy applications to servers – including, of course, configuring the servers – using DSC. (All scripts for this post are available for download here).\n\nSQL Server Installation\n\nTo install SQL Server via a script, there are two prerequisites: the SQL install sources and a silent (or unattended) installation command.\n\nFortunately the SQL server installer takes care of the install command – you run the install wizard manually, specifying your installation options as you go. On the last page, just before clicking “Install”, you’ll see a path to the ini conifguration file. I saved the configuration file and cancelled the install. Then I opened the config file and tweaked it slightly (see this post and this post on some tweaking ideas)– till I could run the installer from the command line (using the /configurationFile switch). That takes care of the install command itself.\n\n\n\n\nThere are many ways to make the SQL installation sources available to the target node. I chose to copy the ISO to the node (using the File DSC resource) from a network share, and then use a Script resource to mount the iso. Once it’s mounted, I can run the setup command using the ini file.\n\nSQL Server requires .NET 3.5 to be installed on the target node, so I’ve added that into the script using the WindowsFeature resource. Here’s the final script:\n\nConfiguration SQLInstall\n{\n    param (\n        [Parameter(Mandatory=$true)]\n        [ValidateNotNullOrEmpty()]\n        [String]\n        $PackagePath,\n\n        [Parameter(Mandatory=$true)]\n        [ValidateNotNullOrEmpty()]\n        [String]\n        $WinSources\n    )\n\n    Node $AllNodes.where{ $_.Role.Contains(\"SqlServer\") }.NodeName\n    {\n        Log ParamLog\n        {\n            Message = \"Running SQLInstall. PackagePath = $PackagePath\"\n        }\n\n        WindowsFeature NetFramework35Core\n        {\n            Name = \"NET-Framework-Core\"\n            Ensure = \"Present\"\n            Source = $WinSources\n        }\n\n        WindowsFeature NetFramework45Core\n        {\n            Name = \"NET-Framework-45-Core\"\n            Ensure = \"Present\"\n            Source = $WinSources\n        }\n\n        # copy the sqlserver iso\n        File SQLServerIso\n        {\n            SourcePath = \"$PackagePath\\en_sql_server_2012_developer_edition_x86_x64_dvd_813280.iso\"\n            DestinationPath = \"c:\\temp\\SQLServer.iso\"\n            Type = \"File\"\n            Ensure = \"Present\"\n        }\n\n        # copy the ini file to the temp folder\n        File SQLServerIniFile\n        {\n            SourcePath = \"$PackagePath\\ConfigurationFile.ini\"\n            DestinationPath = \"c:\\temp\"\n            Type = \"File\"\n            Ensure = \"Present\"\n            DependsOn = \"[File]SQLServerIso\"\n        }\n\n        #\n        # Install SqlServer using ini file\n        #\n        Script InstallSQLServer\n        {\n            GetScript = \n            {\n                $sqlInstances = gwmi win32_service -computerName localhost | ? { $_.Name -match \"mssql*\" -and $_.PathName -match \"sqlservr.exe\" } | % { $_.Caption }\n                $res = $sqlInstances -ne $null -and $sqlInstances -gt 0\n                $vals = @{ \n                    Installed = $res; \n                    InstanceCount = $sqlInstances.count \n                }\n                $vals\n            }\n            SetScript = \n            {\n                # mount the iso\n                $setupDriveLetter = (Mount-DiskImage -ImagePath c:\\temp\\SQLServer.iso -PassThru | Get-Volume).DriveLetter + \":\"\n                if ($setupDriveLetter -eq $null) {\n                    throw \"Could not mount SQL install iso\"\n                }\n                Write-Verbose \"Drive letter for iso is: $setupDriveLetter\"\n                \n                # run the installer using the ini file\n                $cmd = \"$setupDriveLetter\\Setup.exe /ConfigurationFile=c:\\temp\\ConfigurationFile.ini /SQLSVCPASSWORD=P2ssw0rd /AGTSVCPASSWORD=P2ssw0rd /SAPWD=P2ssw0rd\"\n                Write-Verbose \"Running SQL Install - check %programfiles%\\Microsoft SQL Server\\120\\Setup Bootstrap\\Log\\ for logs...\"\n                Invoke-Expression $cmd | Write-Verbose\n            }\n            TestScript =\n            {\n                $sqlInstances = gwmi win32_service -computerName localhost | ? { $_.Name -match \"mssql*\" -and $_.PathName -match \"sqlservr.exe\" } | % { $_.Caption }\n                $res = $sqlInstances -ne $null -and $sqlInstances -gt 0\n                if ($res) {\n                    Write-Verbose \"SQL Server is already installed\"\n                } else {\n                    Write-Verbose \"SQL Server is not installed\"\n                }\n                $res\n            }\n        }\n    }\n}\n\n# command for RM\n#SQLInstall -ConfigurationData $configData -PackagePath \"\\\\rmserver\\Assets\" -WinSources \"d:\\sources\\sxs\"\n\n# test from command line\nSQLInstall -ConfigurationData configData.psd1 -PackagePath \"\\\\rmserver\\Assets\" -WinSources \"d:\\sources\\sxs\"\nStart-DscConfiguration -Path .\\SQLInstall -Verbose -Wait -Force\n\n\nHere’s some analysis:\n\n\n  (Line 7 / 12) The config takes in 2 parameters: $PackagePath (location of SQL ISO and config ini file) and $WinSources (Path to windows sources).\n  (Line 15) I changed my config data so that I can specify a comma-separated list of roles (since a node might be a SQLServer and a WebServer) so I’ve made the comparer a “contains” rather than an equals (as I’ve had in my previous scripts) – see the config script below.\n  (Line 22 / 29) Configure .NET 3.5 and .NET 4.5 Windows features, using the $WinSources path if the sources are required\n  (Line 37) Copy the SQL iso to the target node from the $PackagePath folder\n  (Line 46) Copy the ini file to the target node from the $PackagePath folder\n  (Line 58) Begins the Script to install SQL server\n  The Get-Script does a check to see if there is a SQL server service running. If there is, it returns the SQL instance count for the machine.\n  The Set-Script mounts the iso, saving the drive letter to a variable. Then I invoke the setup script (passing in the config file and required passwords) writing the output to Write-Verbose, which will appear on the DSC invoking machine as the script executes.\n  The Test-Script does the same basic “is there a SQL server service running” check. If there is, skip the install – else run the install. Of course this could be refined to ensure each and every component is installed, but I didn’t want to get that granular.\n  The last couple of lines of the script show the command for Release Management (commented out) as well as the command to run the script manually from a PowerShell prompt.\n\n\nHere’s my DSC config script:\n\n#$configData = @{\n@{\n    AllNodes = @(\n        @{\n            NodeName = \"*\"\n            PSDscAllowPlainTextPassword = $true\n         },\n\n        @{\n            NodeName = \"fabfiberserver\"\n            Role = \"WebServer,SqlServer\"\n         }\n    );\n}\n\n# Note: different 1st line for RM or command line invocation\n# use $configData = @{ for RM\n# use @{ for running from command line\n\n\nYou can download the above scripts (and my SQL configuration ini file for reference) here.\n\nWhat’s Next\n\nAfter running this script, I have a server with SQL Server installed and configured according to my preferences (which are contained in the ini file). From here, I can run restores or dacpac deployments and so on. Of course this is going to be executed from within Release Management as part of the release pipeline.\n\nNext up will be the full WebServer DSC script – and then we’ll be ready to tackle the actual application deployment, since we’ll have servers ready to host our applications.\n\nUntil then, happy releasing!\n",
      "categories": [],
      "tags": ["devops"],
      
      "collection": "posts",
      "url": "/install-and-configure-sql-server-using-powershell-dsc/"
    },{
      
      "title": "Bulk Migrate Work Item Comments, Links and Attachments",
      "date": "2014-08-26 02:50:57 +0000",
      
      "content": "I was working at a customer that had set up a test TFS environment. When we set up their “real” TFS, they did a get-latest of their code and imported their code – easy enough. They did have about 100 active work items that they also wanted to migrate. Not being a big fan of TFS Integration Platform, I usually recommend using Excel to port work items en masse.\n\nThere are a couple of “problems” with the Excel approach:\n\n\n  When you create work items in the new Team Project, they have to go into the “New” state (or the first state for the work item)\n  You can’t migrate test cases (since the steps don’t play nicely in Excel) – and you can’t migrate test results either.\n  You can’t migrate comments, hyperlinks or attachments in Excel (other than opening each work item one by one)\n\n\nYou can mitigate the “new state” limitation by creating several sheets – one for “New” items, one for “Active” items, one for “Resolved” items and so on. The “New” items are easy – just import “as-is”. For the other states, import them into the “New” state and then bulk update the state to the “target” state. Keeping the sheets separated by state makes this easier to manage. Another tip I advise is to add a custom field to the new Team Project (you don’t have to expose it on the forms if you don’t want to) called “OldID” that you set to the id of the old work item – that way you’ve always got a link back to the original work item if you need it.\n\nFor test case, you have to go to the API to migrate them over to the new team project – I won’t cover that topic in this post.\n\nFor comments, hyperlinks and attachments I quickly wrote a PowerShell script that does exactly that! I’ve uploaded it to OneDrive so you can download it here.\n\nHere’s the script itself:\n\n$oldTpcUrl = \"http://localhost:8080/tfs/oldCollection\"\n$newTpcUrl = \"http://localhost:8080/tfs/newCollection\"\n\n$csvFile = \".\\map.csv\" #format: oldId, newId\n$user = \"domain\\user\"\n$pass = \"password\"\n\n[Reflection.Assembly]::LoadWithPartialName('Microsoft.TeamFoundation.Common')\n[Reflection.Assembly]::LoadWithPartialName('Microsoft.TeamFoundation.Client')\n[Reflection.Assembly]::LoadWithPartialName('Microsoft.TeamFoundation.WorkItemTracking.Client')\n\n$oldTpc = [Microsoft.TeamFoundation.Client.TfsTeamProjectCollectionFactory]::GetTeamProjectCollection($oldTpcUrl)\n$newTpc = [Microsoft.TeamFoundation.Client.TfsTeamProjectCollectionFactory]::GetTeamProjectCollection($newTpcUrl)\n\n$oldWorkItemStore = $oldTpc.GetService([Microsoft.TeamFoundation.WorkItemTracking.Client.WorkItemStore])\n$newWorkItemStore = $newTpc.GetService([Microsoft.TeamFoundation.WorkItemTracking.Client.WorkItemStore])\n\n$list = Import-Csv $csvFile\n$cred = new-object System.Net.NetworkCredential($user, $pass)\n\nforeach($map in $list) {\n    $oldItem = $oldWorkItemStore.GetWorkItem($map.oldId)\n    $newItem = $newWorkItemStore.GetWorkItem($map.newId)\n\n    Write-Host \"Processing $($map.oldId) -&amp;gt; $($map.newId)\" -ForegroundColor Cyan\n    \n    foreach($oldLink in $oldItem.Links | ? { $_.BaseType -eq \"HyperLink\" }) {\n        Write-Host \" processing link $($oldLink.Location)\" -ForegroundColor Yellow\n\n        if (($newItem.Links | ? { $_.Location -eq $oldLink.Location }).count -gt 0) {\n            Write-Host \" ...link already exists on new work item\"\n        } else {\n            $newLink = New-Object Microsoft.TeamFoundation.WorkItemTracking.Client.Hyperlink -ArgumentList $oldLink.Location\n            $newLink.Comment = $oldLink.Comment\n            $newItem.Links.Add($newLink)\n        }\n    }\n\n    if ($oldItem.Attachments.Count -gt 0) {\n        foreach($oldAttachment in $oldItem.Attachments) {\n            mkdir $oldItem.Id | Out-Null\n            Write-Host \" processing attachment $($oldAttachment.Name)\" -ForegroundColor Magenta\n\n            if (($newItem.Attachments | ? { $_.Name.Contains($oldAttachment.Name) }).count -gt 0) {\n                Write-Host \" ...attachment already exists on new work item\"\n            } else {\n                $wc = New-Object System.Net.WebClient\n                $file = \"$pwd\\$($oldItem.Id)\\$($oldAttachment.Name)\"\n\n                $wc.Credentials = $cred\n                $wc.DownloadFile($oldAttachment.Uri, $file)\n\n                $newAttachment = New-Object Microsoft.TeamFoundation.WorkItemTracking.Client.Attachment -ArgumentList $file, $oldAttachment.Comment\n                $newItem.Attachments.Add($newAttachment)\n            }\n        }\n    \n        try {\n            $newItem.Save();\n            Write-Host \" Attachments and links saved\" -ForegroundColor DarkGreen\n        }\n        catch {\n            Write-Error \"Could not save work item $newId\"\n            Write-Error $_\n        }\n    }\n\n    $comments = $oldItem.GetActionsHistory() | ? { $_.Description.length -gt 0 } | % { $_.Description }\n    if ($comments.Count -gt 0){\n        Write-Host \" Porting $($comments.Count) comments...\" -ForegroundColor Yellow\n        foreach($comment in $comments) {\n            Write-Host \" ...adding comment [$comment]\"\n            $newItem.History = $comment\n            $newItem.Save()\n        }\n    }\n    \n    Write-Host \"Done!\" -ForegroundColor Green\n}\n\nWrite-Host\nWrite-Host \"Migration complete\"\n\n\nWhen you run this, open the script and fix the top 5 lines (the variables for this script). Enter in the Team Project Collection URL’s (these can be the same if you’re migrating links from one Team Project to another in the same Collection). The person running the script needs to have read permissions on the old server and contributor permission on the new server. You then need to make a cvs file with 2 columns: oldId and newId. Populate this with the mapping from the old work item Id to the new work item Id. Finally, enter a username and password (this is simply for fetching the attachments) and you can run the script.\n\nHappy migrating!\n",
      "categories": [],
      "tags": ["tfsapi"],
      
      "collection": "posts",
      "url": "/bulk-migrate-work-item-comments-links-and-attachments/"
    },{
      
      "title": "Branch Is Not Equal to Environment: CODE-PROD Branching Strategy",
      "date": "2014-08-27 23:34:05 +0000",
      
      "content": "Over the last couple of months I’ve done several implementations and upgrades of TFS 2013. Most organizations I work with are not developing boxed software – they’re developing websites or apps for business. The major difference is that boxed software often has more than one version of a product “in production” – some customers will be on version 1.0 while others will be on version 2.0 and so on. In this model, branches for each major version, with hot-fix branches where necessary – are a good way to keep these code bases separate while still being able to merge bug fixes across versions. However, I generally find that this is overkill for a “product” that only ever has one version in production at any one time – like internal applications or websites.\n\nIn this case, a well-established branching model is Dev-Main-Live.\n\nDev-Main-Live\n\n\n\nDev-Main-Live (or sometimes Dev-Integration-Prod or other variants) is a fairly common branching model – new development is performed on the Dev branch (with multiple developers coding simultaneously). When changes are to be tested, they are merged to Main. There code is tested in a test or UAT environment, and when testing is complete the changes are merged to Live before being deployed to production. This means that if there are production issues (what? we have bugs?!?) those can be fixed on the Live branch – thus they can be tested and deployed independently from the Dev code which may not be production ready.\n\nThere are some issues with this approach:\n\n\n  You shouldn’t be taking so long to test that you need a separate Main branch. I only advise this for extensive test cycles – but you should be aiming to shorten your test cycles anyway. This makes the Main branch fairly obsolete – I’ve seen teams who always “merge through” Main to get changes from Dev to Live – so I’ve started advising getting rid of the Main branch altogether.\n  If you build code from Main, deploy it to Test and sign-off, you have to merge to Live before doing a build from the Live branch. This means that what you’re deploying isn’t what you tested (since you tested pre-merge). I’ve seen some teams deploy from the Main branch build, wait for several days, and then merge to the Live branch. Also a big no-no!\n  Usually bug fixes that are checked in on the Live branch don’t make it back to the Dev branch since you have to merge through Main – so the merge of new dev and bug fixes on the Live branch get done when Dev gets merged onto Live (through Main). This is too late in the cycle and can introduce merge bugs or rework.\n\n\nThis model seems to work nicely since the branches “represent” the environments – what I have in Dev is in my dev environment, what’s on Main is in my Test environment and what’s in Live is in production, right? This “branch equals environment” mindset is actually hard to manage, so I’m starting to recommend a new approach.\n\nThe Solution: Code-Prod with Builds\n\n\n\nSo how should you manage code separation as well as know what code is in which environment at any time? The answer is to simplify the branching model and make use of builds.\n\nIn this scenario new development is done on the CODE branch (the name is to consciously separate the idea of the branch from the environment). When you’re ready to go to production, merge into PROD and do a build. The TFS build will (by default) label the code that is used to build the binaries. You’ll be able to tie the binary version to the build label if you use my versioning script you can always match binaries to builds. So you’ll be able to recreate a build, even if you lose the binaries somehow.\n\nSo now you have built “the bits” – notice how there is no mention of environment yet. You should be thinking of build and deploy as separate activities. Why? Because then you’ll be able to build a single package that can be deployed (and tested) in a number of environments. Of course you’re going to have to somehow manage configuration files for your different environments – for web projects you can refer to my post about how to parameterize the web.config so that you can deploy to any environment (the post is specific to Release Management, but the principles are the same for other deployment mechanisms and for any type of application that needs different configurations for different environments).\n\nDeployment – To Lab or To Release?\n\nLet’s start off considering the “happy path” – you’ve done some coding in CODE, merged to PROD and produced a “production build”. It needs to be tested (of course you’ve already unit tested as part of your build). Now you have two choices – Lab Management or Release Management. I like using a combination of Lab and Release, since each has a some good benefits. You can release to test using Lab Management (including automated deploy and test) so that your testers have an environment to test against – Lab Management allows rich data diagnostic collection during both automated and manual testing. You then use Release Management to get the bits into the release pipeline for deployment to UAT and Production environments, including automated deployment workflows and sign-offs. This way you only get builds into the release pipeline that have passed several quality gates (unit testing, automated UI testing and even manual testing) before getting into UAT. Irrespective of what approach you take, make sure you can take one build output and deploy it to multiple environments.\n\nBut What About Bugs in Production?\n\nIf you get bugs in production before you do the merge, the solution is simple – fix the bug on the PROD branch, then build, test and release back to production. No messy untested dev CODE anywhere.\n\nBut what do you do if you have bugs after your merge, but before you’ve actually deployed to production? Hopefully you’re moving towards shorter release / test cycles, so this window should be short (and rare). But even if you do hit this scenario, there is a way to do the bug fix and keep untested code out. It’s a bit complicated (so you should be trying to avoid this scenario), but let me walk you through the scenario.\n\nLet’s say we have a file in a web project called “Forecast.cs” that looks like this:\n\npublic class Forecast\n{\n    public int ID { get; set; }\n\n    public DateTime Date { get; set; }\n    public DayOfWeek Day \n    {\n        get { return Date.DayOfWeek; } \n    }\n\n    public int Min { get; set; }\n\n    public int Max { get; set; }\n}\n\n\nWe’ve got a PROD build (1.0.0.4) and the label for 1.0.0.4 shows this file to be on version 51.\n\n\n\n\nWe now make a change and add a property called “CODEProperty” (line 15) on the CODE branch:\n\npublic class Forecast\n{\n    public int ID { get; set; }\n\n    public DateTime Date { get; set; }\n    public DayOfWeek Day \n    {\n        get { return Date.DayOfWeek; } \n    }\n\n    public int Min { get; set; }\n\n    public int Max { get; set; }\n\n    public int CODEProperty { get; set; }\n}\n\n\nWe then check-in, merge to PROD and do another build (1.0.0.5). This version is then deployed out for testing in our UAT environment. Forecast.cs is now on version 53 in the 1.0.0.5 label, while all other files are on 51.\n\n\n\n\nSuddenly, the proverbial paw-paw hits the fan and there’s an urgent business-stopping bug in our currently deployed production version (1.0.0.4). So we go to source control, search for the 1.0.0.4 label in the PROD branch that the build created and select “Get This Version” to get the 1.0.0.4 version locally.\n\n\n\n\nWe fix the bug (by adding a property called “HotfixProperty” – line 15 below). Note how there is no “CODEProperty” since this version of Forecast is before the CODEProperty checkin.\n\npublic class Forecast\n{\n    public int ID { get; set; }\n\n    public DateTime Date { get; set; }\n    public DayOfWeek Day \n    {\n        get { return Date.DayOfWeek; } \n    }\n\n    public int Min { get; set; }\n\n    public int Max { get; set; }\n\n    public int HotfixProperty { get; set; }\n}\n\n\nSince we’re not on the latest version (we did a “Get-label”) we won’t be able to check in. So we shelve the change (calling the shelveset “1.0.0.4 Hotfix”). We then open the build template and edit the Get Version property and tell the build to get 1.0.0.4 too by specifying L followed by the label name – so the full “Get version” value is LPROD_1.0.0.4:\n\n\n\n\nNext we queue the build, telling the build to apply the Shelveset too:\n\n\n\n\nWe won’t be able to “Check in changes after successful build” since the build won’t be building with the Latest version. We’ll have to do that ourselves later. The build completes – we now have build 1.0.0.6 which can be deployed straight to production to “handle” the business-stopping bug.\n\nFinally we do a Get Latest of the solution in PROD, unshelve the changeset to merge the Hotfix with the development code, clear the Get version property on the build and queue the next build that includes both the changes from CODE as well as the hotfix from PROD. This build is now 1.0.0.7. Meanwhile, testing is completed on 1.0.0.5, and so we can then fast-track the testing for 1.0.0.7 to release the new CODEProperty feature, including the hotfix from build 1.0.0.6.\n\nHere’s a summary of what code is in what build:\n\n\n  1.0.0.4 – baseline PROD code\n  1.0.0.5 – CODEProperty change coming from a merge from CODE branch into PROD branch\n  1.0.0.6 – baseline PROD plus the hotfix shelveset (no CodeProperty at all) which includes the HotfixProperty\n  1.0.0.7 – CODEProperty merged with HotfixProperty\n\n\nHere’s the 1.0.0.7 version of Forecast.cs (see lines 15 and 17):\n\npublic class Forecast\n{\n    public int ID { get; set; }\n\n    public DateTime Date { get; set; }\n    public DayOfWeek Day \n    {\n        get { return Date.DayOfWeek; } \n    }\n\n    public int Min { get; set; }\n\n    public int Max { get; set; }\n\n    public int CODEProperty { get; set; }\n\n    public int HotfixProperty { get; set; }\n}\n\n\nIf we turn on Annotation, you’ll see that CODEProperty is changeset 52 (in green below), and HotfixProperty is changeset 54 (in red below):\n\n\n\n\nYes, it’s a little convoluted, but it’ll work – the point is that this is possible without a 3rd branch in Source Control. Also, you should be aiming to shorten your test / release cycles so that this situation is very rare. If you hit this scenario often, you could introduce the 3rd branch (call it INTEGRATION or MAIN or something) that can be used to isolate bug-fixes in PROD from new development in CODE that isn’t ready to go out to production.\n\nHere’s a summary of the steps if there is a bug in current production when you haven’t deployed the PROD code (after a merge from CODE) to production yet:\n\n\n  PROD code is built (1.0.0.4) and released to production.\n  CODE is merged to PROD and build 1.0.0.5 is created, but not deployed to production yet\n  Get by Label – the current PROD label (1.0.0.4)\n  Fix the bug and shelve your changes\n  Edit the build to change the Get version to the current PROD label (1.0.0.4)\n  Queue the build with your hotfix shelveset (this will be build 1.0.0.6)\n  Test and deploy the hotfix version (1.0.0.6) to production\n  Get Latest and unshelve to merge the CODE code and the hotfix\n  Clear the Get version field of the build and queue the new build (1.0.0.7)\n  Test and deploy to production\n\n\nConclusion\n\nThe key to good separation of work streams is to not mistake the branch for the environment, nor confuse build with deploy. Using the CODE-PROD branching scenario, builds with versioning and labels, parameterized configs and Lab/Release management you can:\n\n\n  Isolate development code from production code, so that you can do new features while still fixing bugs in production and not have untested development pollute the hotfixes\n  Track which code is deployed where (using binary versions and labels)\n  Recreate builds from labels\n  Deploy a single build to multiple environments, so that what you test in UAT is what you deploy to production\n\n\nHappy building and deploying!\n",
      "categories": [],
      "tags": ["devops","development","releasemanagement","build"],
      
      "collection": "posts",
      "url": "/branch-is-not-equal-to-environment/"
    },{
      
      "title": "A Day of DevOps, Release Management, Software Quality and Agile Project Requirements Management",
      "date": "2014-09-03 14:43:22 +0000",
      
      "content": "Unfortunately there is no TechEd Africa this year – Microsoft have opted to go for smaller, more focused sessions in multiple cities (or at least that’s what I gather). I think it’s a shame, since the TechEd Africa event was always fantastic – and who doesn’t like getting out the office for a couple of days?\n\nAnyway, the good news is that Microsoft are hosting “A Day of DevOps, Release Management, Software Quality and Agile Project Requirements Management” in Cape Town (Wed 10th at Crystal Towers Hotel) and in Johannesburg (on Mon 15th at the Microsoft offices on William Nicol). I’ll be presenting at both events, so make sure you get along.\n\nThe event has 2 themes – DevOps in the morning and Agile Project Management in the afternoon. You can attend one or the other or both events.\n\nI’m particularly excited about the DevOps session – here’s some of the content that I’ll be covering:\n\n\n  Release management and automation, release pipelines and approvals to accelerate deployment to operations, including using DSC, Chef and Puppet\n  Treating Configuration as Code\n  Application Insights\n  Cloud Based Load Testing\n  Production Debugging and Monitoring\n  Leveraging Azure for DevOps and Dev/Test Environments\n  System Centre and TFS Integration\n\n\nFor more details, go to the Microsoft SA Developer blog post here.\n\nHope to see you there!\n",
      "categories": [],
      "tags": ["news"],
      
      "collection": "posts",
      "url": "/a-day-of-devops-release-management-software-quality-and-agile-project-requirements-management/"
    },{
      
      "title": "Test Result Traceability Matrix Tool",
      "date": "2014-09-19 19:28:22 +0000",
      
      "content": "I am often asked if there is a way to see a “traceability matrix” in TFS. Different people define a “traceability matrix” in different ways. If you want to see how many tests there are for a set of requirements, then you can use SmartExcel4TFS. However, this doesn’t tell you what state the current tests are in – so you can’t see how many tests are passing / failing etc.\n\nTest Points\n\nOf course this is because there is a difference between a test case and a test point in TFS. A test point is the combination of Test Case, Test Suite and Test Configuration. So let’s say you have Test ABC in Suite 1 and Suite 2 and have it for 2 configurations (Win7 and Win8, for example). Then you’ll really have 1 test case and 4 test points (2 suites x 2 configurations). So if you want to know “is this test case passing?” you really have to ask, “Is this test case passing in this suite and for this configuration?”.\n\nHowever, you can do a bit of a “cheat” by making an assumption: if the most recent result is Pass/Fail/Not Run/Blocked, then assume the “result of the test” is Pass/Fail/Not Run/Blocked. Of course if the “last result” is failed, you’d have to find exactly which suite/configuration the failure relates to in order to get any detail. Anyway, for most situations this assumption isn’t too bad.\n\nTest Result Traceability Matrix Tool\n\nGiven the assumption that the most recent test point result is the “result” of the Test Case, it’s possible to create a “test result traceability matrix”. If you plot Requirement vs Test Case in a grid, and then color the intersecting cells with the appropriate “result”, you can get a good idea of what state tests are in in relation to your requirements. So I’ve written a utility that will generate this matrix for you (see the bottom of this post for the link).\n\nHere’s the output of a run:\n\n\n\n\nThe first 3 columns are:\n\n\n  Requirement ID\n  Requirement Title\n  Requirement State\n\n\nThen I sum the total of the test case results per category for that requirement – you can see that Requirement 30 has 2 Passed Tests and 1 Failed test (also 0 blocked and 0 not run). If you move along the same row, you’ll see the green and red blocks where the tests cases intersect with their requirements. The colors are as follows:\n\n\n  Green = Passed\n  Red = Failed\n  Orange = Blocked\n  Blue = Not Run\n\n\nYou can see I’ve turned on conditional formatting for the 4 totals columns. I’ve also added filtering to the header, so you can sort / filter the requirements on id, title or state.\n\nSome Notes\n\nThis tool requires the following arguments:\n\n\n  TpcUrl – the URL to the team project collection\n  ProjectName – the name of the Team Project you’re creating the matrix for\n  (Optional) RequirementQueryName – if you don’t specify this, you’ll get the matrix for all requirements in the team project. Alternatively, you can create a flat-list query to return only requirements you want to see (for example all Stories in a particular area path) and the matrix will only show those requirements.\n\n\nI speak of “requirements” – the tool essentially gets all the work items in the “requirements category” as a top-level query and then fetches all work items in the “test case category” that are linked to the top-level items. So this will work as long as your process template has a Requirements / Test Case category.\n\nThe tool isn’t particularly efficient – so if you have large numbers of requirements, test cases and test plans the tool could take a while to run. Also, the tool selects the first “requirementsQuery” that matches the name you pass in – so make sure the name of your requirements query is unique. The tool doesn’t support one-hop or tree queries for this query either.\n\nLet me know what you think!\n\nDownload\n\nHere’s a link to the executable: you’ll need Team Explorer 2013 and Excel to be installed on the machine you run this tool from. To run it, download and extract the zip. The open up a console and run TestResultMatrix.exe.\n\nHappy matrix generating!\n",
      "categories": [],
      "tags": ["testing"],
      
      "collection": "posts",
      "url": "/test-result-traceability-matrix-tool/"
    },{
      
      "title": "Source Control Operations During Deployments in Release Management",
      "date": "2014-10-16 16:57:03 +0000",
      
      "content": "Before we start: Don’t ever do this.\n\nBut if you really have to, then it can be done. There are actually legitimate cases for doing source control operations during a deployment. For example, you don’t have source control and you get “files” from a vendor that need to be deployed to servers. Or you have a vendor application that has “extensions” that are just some sort of script file that is deployed onto the server – so you don’t compile anything for customizations. Typically these sorts of applications are legacy applications.\n\nSimple Solution: Install Team Explorer on the Target Servers\n\nThe simplest way to do source control operations is just to install Team Explorer on your target server. Then you can use the “Run Command” tool from Release Management and invoke tf.exe directly, or create a script that does a number of tf operations.\n\nHowever, I was working at a customer where they have hundreds of servers, so they don’t want to have to manually maintain Team Explorer on all their servers.\n\nCreating a TF.exe Tool\n\nPlaying around a bit, I realized that you can actually invoke tf.exe on a machine that doesn’t have Team Explorer. You copy tf.exe to the target machine – as well as all its dependencies – and you’re good to go. Fortunately it’s not a huge list of files – around 20 altogether.\n\nThat covers the exe itself – however, a lot of TF commands are “location dependent” – they use the directory you’re in to give context to the command. For example, running “tf get” will get files for the current directory (assuming there is a mapping in the workspace). When RM deploys a tool to the target server, it copies the tool files to a temporary directory and executes them from there. This means that we need a script that can “remember” the path where the tool (tf.exe) is but execute from a target folder on the target server.\n\nPowerShell is my scripting language of choice – so here’s the PowerShell script to wrap the tf.exe call:\n\n&lt;p&gt;param(\n    [string]$targetPath,\n    [string]$tfArgs\n)\n\ntry {\n    $tf = \"$pwd\\tf.exe\"\n    Push-Location\n\n    if (-not(Test-Path $targetPath)) {\n        mkdir $targetPath\n    }\n\n    cd $targetPath\n    &amp;amp;$tf $tfArgs.Split(\" \")\n    \n    if (-not($?)) {\n        throw \"TF.exe failed\"\n    }\n}\nfinally {\n    Pop-Location\n}\n&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;\n\n\nNotes:\n\n\n  Line 2: We pass in the $targetPath – this is the path on the target server we want to perform tf commands from\n  Line 3: We in $tfArgs – these are the arguments to pass to tf.exe\n  Line 7-8: get the path to tf.exe and store it\n  Line 10-12: if the $targetPath does not exist, create it\n  Line 14: change directory to the $targetPath\n  Line 15: Invoke tf.exe passing the $tfArgs we passed in as parameters\n  Line 17-19: Since this script invokes tf.exe, you could get a failure from the invocation, but have the script still “succeed”. In order to make sure the deployment fails if tf.exe fails, we need to check if the tf.exe invocation succeeded or not – that’s what these lines are doing\n  Line 22: Change directory back to the original directory we were in – not strictly necessary, but “clean”\n\n\nHere’s the list of dependencies for tf.exe:\n\n\n  Microsoft.TeamFoundation.Build.Client.dll\n  Microsoft.TeamFoundation.Build.Common.dll\n  Microsoft.TeamFoundation.Client.dll\n  Microsoft.TeamFoundation.Common.dll\n  Microsoft.TeamFoundation.TestManagement.Client.dll\n  Microsoft.TeamFoundation.VersionControl.Client.dll\n  Microsoft.TeamFoundation.VersionControl.Common.dll\n  Microsoft.TeamFoundation.VersionControl.Common.Integration.dll\n  Microsoft.TeamFoundation.VersionControl.Common.xml\n  Microsoft.TeamFoundation.VersionControl.Controls.dll\n  Microsoft.TeamFoundation.WorkItemTracking.Client.DataStoreLoader.dll\n  Microsoft.TeamFoundation.WorkItemTracking.Client.dll\n  Microsoft.TeamFoundation.WorkItemTracking.Client.QueryLanguage.dll\n  Microsoft.TeamFoundation.WorkItemTracking.Common.dll\n  Microsoft.TeamFoundation.WorkItemTracking.Proxy.dll\n  Microsoft.VisualStudio.Services.Client.dll\n  Microsoft.VisualStudio.Services.Common.dll\n  TF.exe\n  TF.exe.config\n\n\nOpen up the Release Management client and navigate to Inventory-&gt;Tools. Click New to create a new tool, and specify a good name and description. For the command, specify “powershell” and for arguments type the following:\n\n\n-command ./tf.ps1 –targetPath ‘ __TargetPath__ ’ –tfArgs ‘ __TFArgs__ ’\n\n\nNote that the quotes around the parameters __TargetPath__ and __TFArgs__ should be single-quotes.\n\nFinally, click “Add” on the Resources section and add all the tf files – don’t forget the tf.ps1 file!\n\n\n\nCreating TF Actions\n\nOnce you have the tf.exe tool, you can then create TF.exe actions – like “Create Workspace” and “Get Files”. Let’s do “Create Workspace”:\n\nNavigate to Inventory-&gt;Actions and click “New”. Enter an appropriate name and description. I created a new Category called “TFS Source Control” for these actions, but this is up to you. For “Tool used” specify the TF.exe tool you just created. When you select this tool, it will bring in the arguments for the tool – we’re going to edit those to be more specific for this particular Action. I set my arguments to:\n\n\n-command ./tf.ps1 -targetPath ' __TargetPath__' -tfArgs 'workspace /new /noprompt /collection:\n\n\nhttp://rmserver:8080/tfs/ __TPC__ \n\n\n \" __WorkspaceName__\"'\n\n\n(Note where the single and double quotes are).\n\nThe parameters are as follows:\n\n\n  __TargetPath__: the path we want to create the workspace in\n  __TPC__: the name of the Team Project Collection in the rmserver TFS – this can be totally hardcoded (if you only have one TFS server) or totally dynamic (if you have multiple TFS servers). In this case, we have a single server but can run deployments for several collections, so that’s why this parameter is “partly hardcoded” and “partly dynamic”\n  __WorkspaceName__: the name we want to give to the workspace\n\n\nUsing Create Workspace Action in a Release Template\n\nNow that you have the action, you can use it in a release template:\n\n\n\n\nHere you can see that I’ve create some other actions (Delete Workspace and TF Get) to perform other TF.exe commands. This workflow deletes the workspace called “Test”, then creates a new Workspace in the “c:\\files” folder, and then gets a folder from source control. From there, I can copy or run or do whatever I need to with the files I got from TFS.\n\nHappy releasing from Source Control (though you can’t really be happy about this – it’s definitely a last-resort).\n",
      "categories": [],
      "tags": ["releasemanagement","sourcecontrol"],
      
      "collection": "posts",
      "url": "/source-control-operations-during-deployments-in-release-management/"
    },{
      
      "title": "New vNext Config Variable Options in RM Update 4 RC",
      "date": "2014-10-21 22:59:47 +0000",
      
      "content": "Update 4 RC for Release Management was released a few days ago. There are some good improvements – some are minor, like the introduction of “Agent-based” labels improves readability for viewing agent-based vs non-agent based templates and components. Others are quite significant – like being able to use the Manual Intervention activity and tags in vNext templates, being able to use server-drops as release source and others. By far my favorite new feature of the update is the new variable capabilities.\n\nVariables: System, Global, Server, Component and Action\n\nBe aware that, unfortunately, these capabilities are only for vNext components (so they won’t work with regular agent-based components or workflows). It’s also unlikely that agent-based components will ever get these capabilities. I’ve mentioned before that I think PowerShell DSC is the deployment mechanism of the future, so you should be investing in it now already. If you’re currently using agent-based components, they do have variables that can be specified at design-time (in the deployment workflow surface) – just as they’ve always had.\n\nThe new vNext variable capabilities allow you to use variables inside your PowerShell scripts without having to pass them or hard-code them. For example, if you define a global variable called “MyGlobalVar” you can just use it by accessing $MyGlobalVar in your PowerShell script.\n\nGlobal Variables\n\nGlobal variables are defined under “Administration-&gt;Settings-&gt;Configuration Variables”. Here you can defined variables, giving them a name, type, default value and description.\n\n\n\n\nServer Variables\n\nServer variables can be defined on vNext servers under “Configure Paths-&gt;Servers”. Same format as System variables.\n\n\n\n\nComponent Variables\n\nvNext components can now have configuration variables defined on them “at design time”.\n\n\n\n\nYou can also override values and event specify additional configuration variables when you add the “DSC” component onto the design surface:\n\n\n\n\nAnother cool new feature is the fact that ComponentName and ServerName are now dropdown lists on the “Deploy using DSC/PS” and “Deploy using Chef” activities, so you don’t have to type them manually:\n\n\n\n\nAll these variables are available inside the script by simple using $variableName. You may event get to the point where you no longer need a PSConfiguration file at all!\n\nYou can also see all your variables by opening the “Resource Variables” tab:\n\n\n\nSystem Variables\n\nRM now exposes a number of system variables for your scripts. These are as follows:\n\n\n  Build directory\n  Build number (for component in the release)\n  Build definition (for component)\n  TFS URL (for component)\n  Team project (for component)\n  Tag (for server which is running the action)\n  Application path (destination path where component is copied)\n  Environment (for stage)\n  Stage\n\n\nYou can access these variable easily by simply using $name (for example: $BuildDirectory or $Stage). If you mouse over the “?” icon on right of the Component or Server screens, the tooltip will tell you what variables you have access to.\n\n\n\nRelease Candidate\n\nFinally, remember that this Release Candidate (as opposed to CTPs) is “go-live” so you can install it on your production TFS servers and updating to the RTM is supported. There may be minor glitches with the RC, but you’ll get full support from MS if you encounter any.\n\nHappy releasing!\n",
      "categories": [],
      "tags": ["releasemanagement"],
      
      "collection": "posts",
      "url": "/new-vnext-config-variable-options-in-rm-update-4-rc/"
    },{
      
      "title": "Matching Binary Version to Build Number Version in TFS 2013 Builds",
      "date": "2014-10-22 08:40:02 +0000",
      
      "content": "Jim Lamb wrote a post about how to use a custom activity to match the compiled versions of your assemblies to the TFS build number. This was not a trivial exercise (since you have to edit the workflow itself) but is the best solution for this sort of operation. Interestingly the post was written in November 2009 and updated for TFS 2010 RTM in February 2010.\n\nI finally got a chance to play with a VM that’s got TFS 2013 Preview installed. I was looking at the changes to the build engine. The Product Team have simplified the default template (they’ve collapsed a lot of granular activities into 5 or 6 larger activities). In fact, if you use the default build template, you won’t even see it (it’s not in the BuildProcessTemplates folder – you have to download it if you want to customize it).\n\nThe good news is that the team have added pre- and post-build and pre- and post-test script hooks into the default workflow. I instantly realised this could be used to solve the assembly-version-matches-build-number problem in a much easier manner.\n\nUsing the Pre-Build Script\n\nThe solution is to use a PowerShell script that can replace the version in the AssemblyInfo files before compiling with the version number in the build. Here’s the procedure:\n\n\n  Import the UpdateVersion.ps1 script into source control (the script is below)\n  Change the build number format of your builds to produce something that contains a version number\n  Point the pre-build script argument to the source control path of the script in step 1\n\n\nThe script itself is pretty simple – find all the matching files (AssemblyInfo.* by default) in a target folder (the source folder by default). Then extract the version number from the build number using a regex pattern, and do a regex replace on all the matching files.\n\nIf you’re using TFVC, the files are marked read-only when the build agent does a Get Latest, so I had to remove the read-only bit as well. The other trick was getting the source path and the build number – but you can use environment variables when executing any of the pre- or post- scripts (as detailed here).\n\nParam(\n  [string]$pathToSearch = $env:TF_BUILD_SOURCESDIRECTORY,\n  [string]$buildNumber = $env:TF_BUILD_BUILDNUMBER,\n  [string]$searchFilter = \"AssemblyInfo.*\",\n  [regex]$pattern = \"\\d+\\.\\d+\\.\\d+\\.\\d+\"\n)\n\ntry\n{\n    if ($buildNumber -match $pattern -ne $true) {\n        Write-Host \"Could not extract a version from [$buildNumber] using pattern [$pattern]\"\n        exit 1\n    } else {\n        $extractedBuildNumber = $Matches[0]\n        Write-Host \"Using version $extractedBuildNumber\"\n\n        gci -Path $pathToSearch -Filter $searchFilter -Recurse | %{\n            Write-Host \" -&amp;gt; Changing $($_.FullName)\" \n        \n            # remove the read-only bit on the file\n            sp $_.FullName IsReadOnly $false\n\n            # run the regex replace\n            (gc $_.FullName) | % { $_ -replace $pattern, $extractedBuildNumber } | sc $_.FullName\n        }\n\n        Write-Host \"Done!\"\n    }\n}\ncatch {\n    Write-Host $_\n    exit 1\n}\n\n\nSave this script as “UpdateVersion.ps1” and put it into Source Control (I use a folder called $/Project/BuildProcessTemplates/CommonScripts to house all the scripts like this one for my Team Project).\n\n\n\n\nThe open your build and specify the source control path to the pre-build script (leave the arguments empty, since they’re all defaulted) and add a version number to your build number format. Don’t forget to add the script’s containing folder as a folder mapping in the Source Settings tab of your build.\n\n\n\n\n\n\n\nNow you can run your build, and your assembly (and exe) versions will match the build number:\n\n\n\n\nI’ve tested this script using TFVC as well as a TF Git repository, and both work perfectly.\n\nHappy versioning!\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/matching-binary-version-to-build-number-version-in-tfs-2013-builds/"
    },{
      
      "title": "Moving to Northwest Cadence",
      "date": "2014-10-22 11:31:08 +0000",
      
      "content": "\n\nThat’s right – I’m moving from Imaginet to Northwest Cadence (NWC) at the end of this month. That’s the high-level version – read no further if you don’t need back-story! A huge thanks to all at Imaginet for an awesome 4 years. It was a pleasure working with all of you.\n\nGetting into ALM\n\nI’m often asked how I got into ALM. Well, I studied Computer Science up to Masters at Rhodes University (which was an awesome 6 years!). When I left in 2002 I knew I could program – and one of my favorite courses of all time by Prof. Terry was building our own parser and compiler (using Pascal) so I knew I had enough meta-knowledge to learn any language. My first job was for a small company (Systems Fusion) that made ISP software using C++ and CORBA on Linux. Those were the bad old days – CVS for source control and builds that took about 3 hours using make files. Blegh!\n\nDuring that time I got married, and my wife and I didn’t enjoy the Johannesburg lifestyle. So I called up an old study mate who happened to be working for a financial services company in East London. In Oct 2004, we moved to East London and I joined Real People (a financial services company). We had around 20 developers in the MS stack – SQL server, webservices and ASP.NET websites. The team were very cowboy, using zip files for source control and deploying to Production to test. Even though I didn’t know what ALM was at that time, I knew this wasn’t a sustainable way to do development.\n\nAfter 2 or 3 months, I got my grubby paws on Team Foundation Server 2005 beta 2. I (eventually) got it installed and configured and we adopted TFS as our primary ALM tool. Our processes were still chaotic, but at least we were starting to utilize good source control, branching and even automated builds. Over the next 5 years, I became the architect and ALM-guy for the team – when I left in Oct 2010 we had around 60 developers, 15 team projects, 250 build definitions and I’d done countless customizations to work items, templates, reports and builds. I’d also learned (intuitively) a lot about process – the good, the bad and the ugly! We did a lot of things right in those days – but we also had lots of room for improvement.\n\nDuring my time as the “accidental admin” for TFS, I spent a lot of time on forums and blogs as I tried to figure out how to do stuff in TFS. Sadly, it appeared that there were very few people in South Africa that were doing any ALM using TFS. Or if they were, they certainly weren’t publishing any content!\n\nNotion Solutions\n\nIn 2009 I attended my first TechEd Africa. There I listened to an ALM talk by legend Chris Menegay, who explained that he ran a company called Notion Solutions that did ALM consulting in the US. I loved the idea and thought that there was probably some scope for ALM consulting in SA. I saw Chris again at TechEd Africa in 2010 and this time asked if he did any work in South Africa. We hit if off and Chris offered to hire me or help me start ALM Consulting in SA. Eventually we started Notion South Africa in Oct of 2010, and I officially became an ALM consultant. At around that time, Notion Solutions became part of Imaginet.\n\nSeattle\n\nWhen I was studying my Masters, I was sponsored by a company in Seattle – a startup that was focusing on FireWire technologies. During June of 2000, the company flew me to Seattle for a 3 week working holiday. It was my fist time flying (I was 22 at the time) and my first time to the US. I instantly fell in love with Seattle – I got to see some of the city, meet a few people and even go camping while I was there. I didn’t even mind the rain! In fact, I loved it so much I decided to move there.\n\nUnfortunately, the startup company was unable to offer me a position at the end of 2001 when I graduated – they’d gone under in the dot bomb. And so my dream of moving to Seattle went cold.\n\nMVP Award\n\nZoom forward to 2011 – I was awarded my MVP award in ALM. In Feb 2012, I got to attend the MVP Summit in Bellevue, Seattle. It was great being back in one of my favorite cities of all time! I attended the summit in Feb 2013, and then the summit was changed to Nov, so there was another summit in Nov 2013. Each time I visited Seattle, I was more and more keen to live and work there. Also, one of my best mates from Rhodes moved there in 2006 or so to work for Adobe – even more incentive to move there!\n\nAt MVP Summit in Feb, 2012, I shared a room with Chris Menegay – an experience in itself! When we were chatting, I mentioned that I’d love to live in Seattle. Chris said I should chat to Steven Borg, cofounder and strategist of Northwest Cadence – an ALM company based in Seattle. I didn’t approach Steve then – I wasn’t ready for a move yet. However, at Summit in Feb 2013, I made a tongue-in-cheek comment to Martin Hinshelwood: “If I moved to Seattle, would NWC hire me?”. He immediately suggested that I chat to Steve (I didn’t know that Martin was in the process of moving back to Scotland), which I did. Steve and I hit it off from the first conversation we had, and over the next couple of months we got to know each other and we decided I’d be a fit for NWC (and NWC would be a fit for me!). We’re now processing legal paperwork to get me over to start working – and while we wait, I’m going to be working remotely for NWC from Cape Town (I moved from East London to Cape Town earlier this month).\n\nConclusion\n\nThis is going to be an exciting transition for me and my family – but I must mention that I learned a lot during my time with Imaginet. My colleagues were an amazing bunch to work with – a lot of the “old guard” (the original Notion Solutions crowd that I met when I joined) have moved on – most of them to Microsoft – but I still have frequent contact with most of them. Big ups to Steve St. Jean, Ed Blankenship, Donovan Brown, Abel Wang, Dave McKinstry and others! I’ll never forget how confident I was on my first ALM gig – because I knew that if I got stuck I could always reach out to some of the most knowledgeable ALM Consultants on the planet simply by mailing the internal distribution list we had called “NotionTech”. Later it changed to ALMTech, but it was still the same level of awesomeness!\n\nI’ll still be involved in ALM (though I won’t be working much in SA anymore) and I’ll continue to blog here, so you’ll still see plenty of content from me.\n\nBlessings!\n",
      "categories": [],
      "tags": ["news"],
      
      "collection": "posts",
      "url": "/moving-to-northwest-cadence/"
    },{
      
      "title": "Using WebDeploy in vNext Releases",
      "date": "2014-10-27 15:43:21 +0000",
      
      "content": "A few months ago Release Management (RM) Update 3 preview was released. One of the big features in that release was the ability to deploy without agents using PowerShell DSC. Once I saw this feature, I started a journey to see how far I could take deployments using this amazing technology. I had to learn how DSC worked, and from there I had to figure out how to use DSC with RM! The ride was a bit rocky at first, but I feel comfortable with what I am able to do using RM with PowerShell DSC.\n\nReadying Environments for Deployment\n\nIn my mind there were two distinct steps that I wanted to be able to manage using RM/DSC:\n\n\n  Configure an environment (set of machines) to make them ready to run my application\n  Deploy my application to these servers\n\n\nThe RM/DSC posts I’ve blogged so far deal with readying the environment:\n\n\n  Using PowerShell DSC in Release Management: The Hidden Manual\n  More DSC Release Management Goodness: Readying a Webserver for Deployment\n  Install and Configure SQL Server using PowerShell DSC\n  New vNext Config Variable Options in RM Update 4 RC\n\n\nSo we’re now at a point where we can ensure that the machines that we want to deploy our application to are ready for our application – in the case of a SQL server SQL is installed and configured correctly. In the case of a webserver, IIS is installed and configured, additional runtimes are present (like MVC) and Webdeploy is installed and ports opened so that I can deploy using Webdeploy. So how then do I deploy my application?\n\nGood Packages\n\nGood deployment always beings with good packages. To get a good package, you’ll need an automated build that ties into source control (and hopefully work items) and performs automated unit testing with coverage. This gives you some metrics as to the quality of your builds. The next critical piece that you’ll need is to make sure that you can manage multiple configurations – after all, you’ll be wanting to deploy the same package to Production that you deployed and testing in UAT, so the package shouldn’t have configuration hard-coded in. In my agent-based Webdeploy/RM post, I show how you can create a team build that puts placeholders into the SetParameters.xml file, so that you can put in environment-specific values when you deploy. The package I created for that deployment process can be used for deployment via DSC as well – just showing that if you create a good package during build, you have more release options available to you.\n\nBesides the package, you’ll want to source control your DSC scripts. This way you can track changes that you make to your scripts over time. Also, having the scripts “travel” with your binaries means you only have to look in one location to find both deployment packages (or binaries) and the scripts you need to deploy them. Here’s how I organized my website and scripts in TF Version Control:\n\n\n\n\nThe actual solution (with my websites, libraries and database schema project) is in the FabrikamFiber.CallCenter folder. I have some 3rd party libraries that are checked into the lib folder. The build folder has some utilities for running the build (like the xunit test adapter). And you can also see the DscScripts folder where I keep the scripts for deploying this application.\n\nBy default on a team build, only compiled output is placed into the drop folder – you don’t typically get any source code. I haven’t included the scripts in my solution or projects, so I used a post-build script to copy the scripts from the source folder to the bin folder during the build – the build then copies everything in the bin folder to the drop folder. You could use this technique if you wanted to share scripts with multiple solutions – in that case you’d have the scripts in a higher level folder in SC. Here’s the script:\n\nParam(\n  [string]$srcPath = $env:TF_BUILD_SOURCESDIRECTORY,\n  [string]$binPath = $env:TF_BUILD_BINARIESDIRECTORY,\n  [string]$pathToCopy\n)\n\ntry\n{\n    $sourcePath = \"$srcPath\\$pathToCopy\"\n    $targetPath = \"$binPath\\$pathToCopy\"\n\n    if (-not(Test-Path($targetPath))) {\n        mkdir $targetPath\n    }\n\n    xcopy /y /e $sourcePath $targetPath\n\n    Write-Host \"Done!\"\n}\ncatch {\n    Write-Host $_\n    exit 1\n}\n\n\n\n  Lines 2-3: you can use the $env parameters that get set when team build executes a custom script. Here I am using the sources and binaries directory settings.\n  Line 4: the subfolder to copy from the $srcPath to the $binPath.\n  Line 12-14: ensure that the target path exists.\n  Line 16: xcopy the files to the target folder.\n\n\nCalling the script with $pathToCopy set to DscScripts will result in my DSC scripts being copied to the drop folder along with my build binaries. Using the TFVC 2013 default template, here’s what my advanced build parameters look like:\n\n\n\n\n  The MSBuild arguments build a Webdeploy package for me. The profile (specified when you right-click the project and select “Publish”) also inserts RM placeholders into environment specific settings (like connection strings, for example). I don’t hard-code the values since this same package can be deployed to multiple environments. Later we’ll see how the actual values replace the tokens at deploy time.\n  The post-build script is the script above, and I pass “-pathToCopy DscScripts” to the script in order to copy the scripts to the bin (and ultimately the drop) folder.\n  I also use a pre-build script to version my assemblies so that I can match the binary file versions with the build.\n\n\nHere’s what my build output folders look like:\n\n\n\n\nThere are 3 “bits” that I really care about here:\n\n\n  The DscScripts folder has all the scripts I need to deploy this application.\n  The FabrikamFiber.Schema.dacpac is the binary of my database schema project.\n  The _PublishedWebsites folder contains 2 folders: the “xcopyable” site (which I ignore) and the FabrikamFiber.Web_package folder which is shown on the right in the figure above, containing the cmd file to execute WebDeploy, the SetParameters.xml file for configuration and the zip file containing the compiled site.\n\n\nHere’s what my SetParameters file looks like:\n\n&amp;lt;?xml version=\"1.0\" encoding=\"utf-8\"?&amp;gt;\n&amp;lt;parameters&amp;gt;\n  &amp;lt;setParameter name=\"IIS Web Application Name\" value=\" __SiteName__\" /&amp;gt;\n  &amp;lt;setParameter name=\"FabrikamFiber-Express-Web.config Connection String\" value=\" __FabFiberExpressConStr__\" /&amp;gt;\n&amp;lt;/parameters&amp;gt;\n\n\nNote the “__” (double underscore) pre- and post-fix, making SiteName and FabFiberExpressConStr parameters that I can use in both agent-based and agent-less deployments.\n\nNow that all the binaries and scripts are together, we can look at how to do the deployment.\n\nDeploying a DacPac\n\nTo deploy the database component of my application, I want to use the DacPac (the compiled output of my SSDT project). The DacPac is a “compiled model” of how I want the database to look. To deploy a DacPac, you invoke sqlpackage.exe (installed with SQL Server Tools when you install and configure SQL Server). SqlPackage then reverse engineers the target database (the database you’re deploying the model to) into another model, does a compare and produces a diff script. You can also make SqlPackage run the script (which will make the target database look exactly like the DacPac model you compiled your project into).\n\nTo do this inside a DSC script, I implement a “Script” resource. The Script resource has 3 parts: a Get-Script, a Set-Script and a Test-Script. The Get-Script is executed when you run DSC in interrogative mode – it won’t change the state of the target node at all. The Test-Script is used to determine if any action must be taken – if it return $true, then no action is taken (the target is already in the desired state). If the Test-Script returns $false, then the target node is not in the desired state and the Set-Script is invoked. The Set-Script is executed in order to bring the target node into the desired state.\n\nA Note on Script Resource Parameters\n\nA caveat here though: the Script resource can be a bit confusing in terms of parameters. The DSC script actually has 2 “phases” – first, the PowerShell script is “compiled” into a mof file. This file is then pushed to the target server and executed during the “deploy” phase. The parameters that you use in the configuration script are available on the RM server at “compile” time, while parameters in the Script resources are only available on the target node during “deploy” time. That means that you can’t pass a parameter from the config file “into” the Script resource – all parameters in the Script resource need to be hard-coded or calculated on the target node at execution time.\n\nFor example, let’s look at this example script:\n\nConfiguration Test\n{\n    params (\n        [string]$logLocation\n    )\n\n    Node myNode\n    {\n        Log LogLocation\n        {\n            Message = \"The log location is [$logLocation]\"\n        }\n\n        Script DoSomething\n        {\n            Get-Script { @{ \"DoSomething\" = \"Yes\" } }\n            Test-Script { $false }\n            Set-Script\n            {\n                Write-Host \"Log location is [$logLocation]\"\n                $localParam = \"Hello there\"\n                Write-Host \"LocalParam is [$localParam]\"\n            }\n        }\n    }\n}\n\n\nHere the intent is to have a parameter called $logLocation that we pass into the config script. When you see this script, it seems to make perfect sense – however, while the log will show the message “The log location is [c:\\temp]”, for example (line 11), when the Set-Script of the Script resource runs on the target node, you’ll see the message “Log location is []” (Line 20). Why? Because the $logLocation parameter does not exist when this script is run at deploy time on the target node. The parameter is available to the Log resource (or other resources like File) but won’t be to the Script resource. You will be able to create other parameters “at deploy time” (like $localParam on Line 21). This is frustrating, but kind of understandable. The Script resource script blocks are not evaluated for parameters. I found a string manipulation hack that allows you to fudge config parameters into the script blocks, but decided against using it.\n\nConfigData\n\nBefore we look at the DSC script used to deploy the database, I need to show you my configData script:\n\n#@{\n$configData = @{\n    AllNodes = @(\n        @{\n            NodeName = \"*\"\n            PSDscAllowPlainTextPassword = $true\n         },\n\n        @{\n            NodeName = \"fabfiberserver\"\n            Role = \"WebServer\"\n         },\n\n        @{\n            NodeName = \"fabfiberdb\"\n            Role = \"SqlServer\"\n         }\n    );\n}\n\n# Note: different 1st line for RM or command line\n# use $configData = @{ for RM\n# use @{ for running from command line\n\n\n\n  Line 1: When running from the command line, you just specify a hash-table. DSC requires this hash-table to be put into a variable. I have both in the script (though I default to the format RM requires) just so that I can test the script outside of RM.\n  Line 3: AllNodes is a hash-table of all the nodes I want to affect with my configuration scripts.\n  Lines 5/6 – common properties for all nodes (the name is “*” so DSC applies these properties to all nodes).\n  Line 10/11 and 15/16: I specify the nodes I have as well as a Role property. This is so that I can deploy the same configuration to multiple servers that have the same role (like a web farm for example).\n  You can specify other parameters, each with another value for each server.\n\n\nHere’s the DSC script I use to deploy a DacPac to a target server:\n\nConfiguration FabFibWeb_Db\n{\n    param (\n        [Parameter(Mandatory=$true)]\n        [ValidateNotNullOrEmpty()]\n        [String]\n        $PackagePath\n    )\n\n    Node fabfiberdb #$AllNodes.where{ $_.NodeName -ne \"*\" -and $_.Role.Contains(\"SqlServer\") }.NodeName\n    {\n        Log DeployAppLog\n        {\n            Message = \"Starting SqlServer node configuration. PackagePath = $PackagePath\"\n        }\n\n        #\n        # Update the application database\n        #\n        File CopyDBSchema\n        {\n            Ensure = \"Present\"\n            SourcePath = \"$PackagePath\\FabrikamFiber.Schema.dacpac\"\n            DestinationPath = \"c:\\temp\\dbFiles\\FabrikamFiber.Schema.dacpac\"\n            Type = \"File\"\n        }\n\n        Script DeployDacPac\n        {\n            GetScript = { @{ Name = \"DeployDacPac\" } }\n            TestScript = { $false }\n            SetScript =\n            {\n                $cmd = \"&amp;amp; 'C:\\Program Files (x86)\\Microsoft SQL Server\\110\\DAC\\bin\\sqlpackage.exe' /a:Publish /sf:c:\\temp\\dbFiles\\FabrikamFiber.Schema.dacpac /tcs:'server=localhost; initial catalog=FabrikamFiber-Express'\"\n                Invoke-Expression $cmd | Write-Verbose\n            }\n            DependsOn = \"[File]CopyDBSchema\"\n        }\n\n        Script CreateLabUser\n        {\n            GetScript = { @{ Name = \"CreateLabUser\" } }\n            TestScript = { $false }\n            SetScript = \n            {\n                $sql = @\"\n                    USE [master]\n                    GO\n\n                    IF (NOT EXISTS(SELECT name from master..syslogins WHERE name = 'Lab'))\n                    BEGIN\n                        CREATE LOGIN [lab] WITH PASSWORD=N'P2ssw0rd', DEFAULT_DATABASE=[master], CHECK_EXPIRATION=OFF, CHECK_POLICY=OFF\n    \n                        BEGIN\n                            USE [FabrikamFiberExpress]\n                        END\n\n                        CREATE USER [lab] FOR LOGIN [lab]\n                        ALTER ROLE [db_owner] ADD MEMBER [lab]\n                    END\n\"@\n                \n                $cmdPath = \"c:\\temp\\dbFiles\\createLogin.sql\"\n                sc -Path $cmdPath -Value ($sql -replace '\\n', \"`r`n\")\n                \n                &amp;amp; \"C:\\Program Files\\Microsoft SQL Server\\110\\Tools\\Binn\\sqlcmd.exe\" -S localhost -U sa -P P2ssw0rd -i $cmdPath\n            }\n        }\n    }\n}\n\n# command for RM\nFabFibWeb_Db -ConfigurationData $configData -PackagePath $applicationPath\n\n# test from command line\n#FabFibWeb -ConfigurationData configData.psd1 -PackagePath \"\\\\rmserver\\builddrops\\ __ReleaseSite\\__ ReleaseSite_1.0.0.3\"\n#Start-DscConfiguration -Path .\\FabFibWeb -Verbose -Wait\n\n\nLet’s take a look at what is going on:\n\n\n  Line 7: I need a parameter to tell me where the DacPac is – this will be my build drops folder.\n  Line 10: I specify the node I want to bring into the desired state. I wanted to apply this config to all nodes that have the role “SqlServer” and this worked from the command line – for some reason I couldn’t get it to work with RM, so I hardcode the node-name here. I think this is particular to my environment, since this should work.\n  Lines 12-15: Log a message.\n  Lines 20-26: Use the File resource to copy the DacPac from a subfolder in the $PackagePath to a known folder on the local machine. I did this because I couldn’t pass the drop-folder path in to the Script resource – so I copied using the File Resource to a known location and can just “hard code” that location in my Script resources.\n  Line 28: This is the start of the script Resource for invoking sqlpackage.exe.\n  Line 30: Just return the name of the resource.\n  Line 31: Always return false – meaning that the Set-Script will always be run. You could have some check here if you didn’t want the script to execute for some specific condition.\n  Lines 32-36: This is the script that actually does the work – I create the command and then Invoke it, piping output to the verbose log for logging. I use “/a:Publish” to tell SqlPackage to execute the incremental changes on the database, using the DacPac as the source file (/sf) and targeting the database specified in the target connection string (/tcs).\n  Line 37: Invoking the DacPac is dependent on the DacPac being present, so I express the dependency.\n  The final resource in this script is also a Script resource – the Get- and Test-Scripts are self-explanatory. The Set-Script takes the SQL string I have in the script, writes it to a file (using sc – Set-Content) and then executes the file using sqlcmd.exe. This is specific to my environment, but shows that you can execute arbitrary SQL against a server fairly easily using the Script resource.\n  Line 73: When using DSC with RM, you need to compile the configuration (do this by invoking the Configuration) into mof files. Don’t call Start-DscConfiguration (which pushes the mof files to the target nodes for running the configuration) since RM will do this step. You can see how I use $applicationPath – this is the path that you specify when you create the vNext component (relative to a drop folder) – we’ll see later how to set this up. RM sets this parameter when before it calls the script. Also, you need to specify the parameter that contains the configuration hash-table. In my case this is $configData, which you’ll see at the top of the configData script above. RM “executes” this script so the parameter is in memory by the time the DSC script is executed.\n\n\nWhen working with DSC, you have to think about idempotency. In other words, the script must produce the same result every time you run it – no matter what the starting state is. Since deploying a DacPac to a database is already idempotent, I don’t have too much to worry about in this case, so that’s why the Test-Script for the DeployDacPac Script resource always returns false.\n\nDeploying a Website using WebDeploy\n\nYou could be publishing your website out of Visual Studio. But don’t – seriously, don’t EVER do this. So you’re smart: you’ve got an automated build to compile your website. Well done! Now you could be deploying this site using xcopy. Don’t – primarily because managing configuration is hard to do using this method, and you usually end up deploying all sorts of files that you don’t actually require (like web.debug.config etc.). You should be using WebDeploy!\n\nI’ve got a post about how to use WebDeploy with agent-based templates. What follows is how to deploy sites using WebDeploy in vNext templates (using PowerShell DSC). In a previous post I show how you can use DSC to ready a webserver for your application. Now we can look at what we need to do to actually deploy a site using WebDeploy. Here’s the script I use:\n\nConfiguration FabFibWeb_Site\n{\n    param (\n        [Parameter(Mandatory=$true)]\n        [ValidateNotNullOrEmpty()]\n        [String]\n        $PackagePath\n    )\n\n    Node fabfiberserver #$AllNodes.where{ $_.NodeName -ne \"*\" -and $_.Role.Contains(\"WebServer\") }.NodeName\n    {\n        Log WebServerLog\n        {\n            Message = \"Starting WebServer node configuration. PackagePath = $PackagePath\"\n        }\n\n        #\n        # Deploy a website using WebDeploy\n        #\n        File CopyWebDeployFiles\n        {\n            Ensure = \"Present\"         \n            SourcePath = \"$PackagePath\\_PublishedWebsites\\FabrikamFiber.Web_Package\"\n            DestinationPath = \"c:\\temp\\Site\"\n            Recurse = $true\n            Force = $true\n            Type = \"Directory\"\n        }\n\n        Script SetConStringDeployParam\n        {\n            GetScript = { @{ Name = \"SetDeployParams\" } }\n            TestScript = { $false }\n            SetScript = {\n                $paramFilePath = \"c:\\temp\\Site\\FabrikamFiber.Web.SetParameters.xml\"\n\n                $paramsToReplace = @{\n                    \" __FabFiberExpressConStr__\" = \"data source=fabfiberdb;database=FabrikamFiber-Express;User Id=lab;Password=P2ssw0rd\"\n                    \" __SiteName__\" = \"Default Web Site\\FabrikamFiber\"\n                }\n\n                $content = gc $paramFilePath\n                $paramsToReplace.GetEnumerator() | % {\n                    $content = $content.Replace($_.Key, $_.Value)\n                }\n                sc -Path $paramFilePath -Value $content\n            }\n            DependsOn = \"[File]CopyWebDeployFiles\"\n        }\n        \n        Script DeploySite\n        {\n            GetScript = { @{ Name = \"DeploySite\" } }\n            TestScript = { $false }\n            SetScript = {\n                &amp;amp; \"c:\\temp\\Site\\FabrikamFiber.Web.deploy.cmd\" /Y\n            }\n            DependsOn = \"[Script]SetConStringDeployParam\"\n        }\n\n        #\n        # Ensure App Insights cloud monitoring for the site is enabled\n        #\n        Script AppInsightsCloudMonitoring\n        {\n            DependsOn = \"[Script]DeploySite\"\n            GetScript = \n            {\n                @{\n                    WebApplication = 'Default Web Site/FabrikamFiber';\n                }\n            }\n            TestScript =\n            {\n                $false\n            }\n            SetScript =\n            {\n                # import module - requires change to PSModulePath for this session\n                $mod = Get-Module -Name Microsoft.MonitoringAgent.PowerShell\n                if ($mod -eq $null)\n                {\n                    $env:PSModulePath = $env:PSModulePath + \";C:\\Program Files\\Microsoft Monitoring Agent\\Agent\\PowerShell\\\"\n                    Import-Module Microsoft.MonitoringAgent.PowerShell -DisableNameChecking\n                }\n        \n                Write-Verbose \"Starting cloud monitoring on FabFiber site\"\n                Start-WebApplicationMonitoring -Cloud -Name 'Default Web Site/FabrikamFiber'\n            }\n        }\n    }\n}\n\n# command for RM\nFabFibWeb_Site -ConfigurationData $configData -PackagePath $applicationPath\n\n# test from command line\n#FabFibWeb -ConfigurationData configData.psd1 -PackagePath \"\\\\rmserver\\builddrops\\ __ReleaseSite\\__ ReleaseSite_1.0.0.3\"\n#Start-DscConfiguration -Path .\\FabFibWeb -Verbose -Wait\n\n\nYou’ll see some similarities to the database DSC script – getting nodes by role (“WebServer” this time instead of “SqlServer”), Log resources to log messages and the “compilation” command which passes in the $configData and $applicationPath.\n\n\n  Lines 20-28: I copy the entire FabrikamFiber.Web_package folder (containing the cmd, SetParameters and zip file) to a temp folder on the node.\n  Line 30: I use a Script Resource to do config replacement.\n  Lines 32-33: Always execute the Set-Script, and return the name of the resource when interrogating the target system.\n  Lines 34-47: The “guts” of this script – replacing the tokens in the SetParameters file with real values and then invoking WebDeploy.\n  Line 35: Set a parameter to the known local location of the SetParameters file.\n  Lines 37-40: Create a hash-table of key/value pairs that will be replaced in the SetParameters file. I have 2: the site name and the database connection string. You can see the familiar __ pre- and post-fix for the placeholders names – I can use this same package in agent-based deployments if I want to.\n  Line 42: read in the contents of the SetParameters file.\n  Lines 43-45: Replace the token placeholders with the actual values from the hash-table.\n  Line 46: overwrite the SetParameters file – it now has actual values instead of just placeholder values.\n  Lines 51-59: I use another Script resource to execute the cmd file (invoking WebDeploy).\n  Lines 64-90: This is optional – I include it here as a reference of how to ensure that the site is being monitored using Application Insights once it’s deployed.\n\n\nThe Release\n\nIn order to run vNext (a.k.a. agent-less a.k.a DSC) deployments, you need to import your target nodes. Since vNext servers are agent-less, you don’t need to install anything on the target node. You just need to make sure you can run remote PowerShell commands against the node and have the username/password for doing so. When adding a new server, just type in the name of the machine and specify the remote port, which is 5985 by default. This adds the server into RM as a “Standard” server. These servers always show their status as “Ready”, but this can be misleading since there is no agent. You can then compose your servers into “Standard Environments”. Next you’ll want to create a vNext Release Path (which specifies the environments you’re deploying to as well as who is responsible for approvals).\n\n\n\n\nYou can specify other configuration variables and defaults in RM Update 4 RC.\n\nvNext Components\n\nIn order to use the binaries and scripts we’ve created, we need to specify a vNext component in RM. Here’s how I specify the component:\n\n\n\n\nAll this is really doing is setting the value of the $packagePath (which I set to the root of the drop folder here). Also note how I only need a single component even though I have several scripts to invoke (as we’ll see next).\n\nThe vNext Template\n\nI create a new vNext template. I select a vNext release path. I right-click the “Components” node in the toolbox and add in the vNext component I just created. Since I am deploying to (at least) 2 machines, I drag a “Parallel” activity onto the design surface. On the left of the parallel, I want scripts for my SQL servers. On the right, I want scripts for my webservers. Since I’ve already installed SQL on my SQL server, I am not going to use that script – I’ll just deploy my database model. On the webserver, I want to run the prerequisites script (to make sure IIS, Webdeploy, MVC runtime and the MMA agent are all installed and correctly configured. Then I want to deploy my website using Webdeploy. So I drag on 3 “Deploy using PS/DSC” activities. I select the appropriate server and component from the “Server” and “Component” drop-downs respectively. I set the username/password for the identity that RM will use to remote onto the target nodes. Then I set the path to the scripts (relative to the root of the drop folder, which is the “Path to Package” in the component I sepcified (which becomes $applicationPath inside the DSC script). I also set the path to the PsConifgurationPath to my configData.psd1 script. Finally I set UseCredSSP and UseHTTPS both to false and SkipCaCheck to true (you can vary these according to your environment).\n\n\n\n\nNow I can trigger the release (either through the build or manually). Here’s what a successful run looks like and a snippet of one of the logs:\n\n\n\nTo Agent or Not To Agent?\n\nLooking at the features and improvements to Release Management Update 3 and Update 4, it seems that the TFS product team are not really investing in agent-based deployments and templates any more. If you’re using agent-based deployments, it’s a good idea to start investing in DSC (or at the very least just plain ol’ PowerShell) so that you can use agent-less (vNext) deployments. As soon as I saw DSC capabilities in Update 3, I guessed this was the direction the product team would pursue, and Update 4 seems to confirm that guess. While there is a bit of a learning curve, this technology is very powerful and will ultimately lead to better deployments – which means better quality for your business and customers.\n\nHappy deploying!\n",
      "categories": [],
      "tags": ["releasemanagement"],
      
      "collection": "posts",
      "url": "/using-webdeploy-in-vnext-releases/"
    },{
      
      "title": "Real Config Handling for DSC in RM",
      "date": "2014-10-30 17:35:49 +0000",
      
      "content": "In my previous post I showed you how to use PowerShell DSC and Release Management to configure machines and deploy an application. There was one part of the solution that I wasn’t satisfied with, and in the comments section you’ll see that @BigFan picks it up: the configuration is hard-coded.\n\ncScriptWithParams Resource\n\nThe primary reason I’ve had to hard-code the configuration is that I use the Script resource heavily. Unfortunately the Script resource cannot utilize configuration (or parameters)! I do explain this in my previous post (see the section headed “A Note on Script Resource Parameters”). For a while I tried to write my own custom resource, but eventually abandoned that project. However, after completing my previous post, I decided to have another stab at the problem. And voila! I created a custom Script resource that (elegantly, I think) can be parameterized. You can get it from GitHub.\n\nLet’s first look at how to utilize the new resource – I’ll discuss how I created the resource after that.\n\nParameterized Scripts\n\nIn my previous solution, the Script resource that executed the Webdeploy command (which deploys my web application) looks like this:\n\nScript SetConStringDeployParam\n{\n    GetScript = { @{ Name = \"SetDeployParams\" } }\n    TestScript = { $false }\n    SetScript = {\n        $paramFilePath = \"c:\\temp\\Site\\FabrikamFiber.Web.SetParameters.xml\"\n\n        $paramsToReplace = @{\n            \" __FabFiberExpressConStr__\" = \"data source=fabfiberdb;database=FabrikamFiber-Express;User Id=lab;Password=P2ssw0rd\"\n            \" __SiteName__\" = \"Default Web Site\\FabrikamFiber\"\n        }\n\n        $content = gc $paramFilePath\n        $paramsToReplace.GetEnumerator() | % {\n            $content = $content.Replace($_.Key, $_.Value)\n        }\n        sc -Path $paramFilePath -Value $content\n    }\n    DependsOn = \"[File]CopyWebDeployFiles\"\n}\n\n\nYou can see how lines 9 and 10 are hard-coded. Ideally these values should be read from a configuration somewhere.\n\nHere’s what the script looks like when you use the cScriptWithParams resource:\n\ncScriptWithParams SetConStringDeployParam\n{\n    GetScript = { @{ Name = \"SetDeployParams\" } }\n    TestScript = { $false }\n    SetScript = {\n        $paramFilePath = \"c:\\temp\\Site\\FabrikamFiber.Web.SetParameters.xml\"\n\n        $paramsToReplace = @{\n            \" __FabFiberExpressConStr__\" = $conStr\n            \" __SiteName__\" = $siteName\n        }\n\n        $content = gc $paramFilePath\n        $paramsToReplace.GetEnumerator() | % {\n            $content = $content.Replace($_.Key, $_.Value)\n        }\n        sc -Path $paramFilePath -Value $content\n    }\n    cParams =\n    @{\n        conStr = $conStr;\n        siteName = $siteName;\n    }\n    DependsOn = \"[File]CopyWebDeployFiles\"\n}\n\n\nSome notes:\n\n\n  Line 1: The name of the resource is “cScriptWithParams” – the custom Script resource I created. In order to use this custom resource, you need the line “Import-DscResource -Name cScriptWithParams” at the top of your Configuration script (above the first Node element).\n  Lines 9/10: The values for the connection string and site name are now variables instead of hard-coded\n  Lines 19-23: This is the property that allows you to “pass in” values for the variables. It’s a hash-table of string key-value pairs, where the key is the name of the variable used in any of the Get, Set or Test scripts and the value is the value you want to set the variable to. We could get the values from anywhere – a DSC config file (where we would have $Node.ConStr for example) – in this case it’s from 2 global variables called $conStr and $siteName (we’ll see later where these get specified).\n\n\nRemoving Config Files Altogether\n\nNow that we can (neatly) parameterize the custom scripts we want to run, we can use the new config variable options in RM to completely remove the need for a config file. Of course you could still use the config file if you wanted to. Here’s the final script for deploying my web application:\n\nConfiguration FabFibWeb_Site\n{\n    Import-DscResource -Name cScriptWithParams\n\n    Node $ServerName\n    {\n        Log WebServerLog\n        {\n            Message = \"Starting Site Deployment. AppPath = $applicationPath.\"\n        }\n\n        #\n        # Deploy a website using WebDeploy\n        #\n        File CopyWebDeployFiles\n        {\n            Ensure = \"Present\"         \n            SourcePath = \"$applicationPath\\_PublishedWebsites\\FabrikamFiber.Web_Package\"\n            DestinationPath = \"c:\\temp\\Site\"\n            Recurse = $true\n            Force = $true\n            Type = \"Directory\"\n        }\n\n        cScriptWithParams SetConStringDeployParam\n        {\n            GetScript = { @{ Name = \"SetDeployParams\" } }\n            TestScript = { $false }\n            SetScript = {\n                $paramFilePath = \"c:\\temp\\Site\\FabrikamFiber.Web.SetParameters.xml\"\n\n                $paramsToReplace = @{\n                    \" __FabFiberExpressConStr__\" = $conStr\n                    \" __SiteName__\" = $siteName\n                }\n\n                $content = gc $paramFilePath\n                $paramsToReplace.GetEnumerator() | % {\n                    $content = $content.Replace($_.Key, $_.Value)\n                }\n                sc -Path $paramFilePath -Value $content\n            }\n            cParams =\n            @{\n                conStr = $ConStr;\n                siteName = $SiteName;\n            }\n            DependsOn = \"[File]CopyWebDeployFiles\"\n        }\n        \n        Script DeploySite\n        {\n            GetScript = { @{ Name = \"DeploySite\" } }\n            TestScript = { $false }\n            SetScript = {\n                &amp;amp; \"c:\\temp\\Site\\FabrikamFiber.Web.deploy.cmd\" /Y\n            }\n            DependsOn = \"[cScriptWithParams]SetConStringDeployParam\"\n        }\n    }\n}\n\n# command for RM\nFabFibWeb_Site\n\n&amp;lt;# \n#test from command line\n$ServerName = \"fabfiberserver\"\n$applicationPath = \"\\\\rmserver\\builddrops\\ __ReleaseSite\\__ ReleaseSite_1.0.0.3\"\n$conStr = \"testing\"\n$siteName = \"site Test\"\nFabFibWeb\nStart-DscConfiguration -Path .\\FabFibWeb -Verbose -Wait\n#&amp;gt;\n\n\nNotes:\n\n\n  Line 3: Importing the custom resource (presumes the custom resource is “installed” locally – see next section for how to do this)\n  Line 5: I leverage the $ServerName variable that RM sets – I don’t have to hard-code the node name\n  Lines 15-23: Copy the Webdeploy files from the build drop location to a local folder (again I use an RM parameter, $applicationPath, which is the drop folder)\n  Lines 25-49: Almost the same Script resource we had before, but subtly changed to handle variables by changing it to a cScriptWithParams resource.\n  Lines 33/34: The hard-coded values have been replaced with variables.\n  Lines 43-47: We need to supply a hash-table of key/value pairs for our parameterized scripts. In this case, we need to supply conStr and siteName. For the values, we pass in $conStr and $siteName, which RM will feed in for us (we’ll specify these on the Release Template itself)\n  Line 64: “Compile” the configuration (into a .mof file) for RM to push to the target server\n  Lines 66-74: If you test this script from the command line, you just create the variables required and execute it. This is exactly what RM does under the hood when executing this script.\n\n\nUsing the Script in a Release Template\n\nNow that we have the script, let’s see how we consume it. (Of course it’s checked into source control, along with the Custom Resource, and part of a build so that it ends up in the build drop folder with our application. Of course – goes without saying!)\n\nWe define the vNext Component the same way we did last time:\n\n\n\n\nNothing magical here – this really just defines the root folder of the build drop for use in the deployment.\n\nNext we create the vNext template using our desired vNext release path. On the designer, you’ll see the major difference: we’re defining the variables on the surface itself:\n\n\n\n\nOur script uses $ServerName (which you can see is set to fabfiberserver). It also uses ConStr and SiteName (these are the parameter values we specified in lines 44/45 of the above script – $ConStr  and $SiteName). Of course if we deploy to another server (say in our production environment) we would simply specify other values for that server.\n\nDeploying a Custom Resource\n\nThe final trick is how you deploy the custom resource. To import it using Import-DSCResource, you need to have it in ProgramFiles\\WindowsPowerShell\\Modules. If you’re testing the script from you workstation, you’ll need to copy it to this path on your workstation. You’ll also need to copy it to that folder on the target server. Sounds like a job for a DSC script with a File resource! Unfortunately it can’t be part of the web application script we created above since it needs to be on the server before you run the Import-DscResource command. No problem – we’ll run 2 scripts on the template. Here’s the script to deploy the custom resource:\n\nConfiguration CopyCustomResource\n{\n    Node $ServerName\n    {\n        File CopyCustomResource\n        {\n            Ensure = \"Present\"\n            SourcePath = \"$applicationPath\\$modSubFolder\\$modName\"\n            DestinationPath = \"$env:ProgramFiles\\WindowsPowershell\\Modules\\$modName\"\n            Recurse = $true\n            Force = $true\n            Type = \"Directory\"\n        }\n    }\n}\n\n&amp;lt;#\n# test from command line\n$ServerName = \"fabfiberserver\"\n$applicationPath = \"\\\\rmserver\\builddrops\\ __ReleaseSite\\__ ReleaseSite_1.0.0.3\"\n$modSubfolder = \"CustomResources\"\n$modName = \"DSC_ColinsALMCorner.com\"\n#&amp;gt;\n\n# copy the resource locally\n#cp \"$applicationPath\\$modSubFolder\\$modName\" $env:ProgramFiles\\WindowsPowerShell\\Modules -Force -Recurse\n\n# command for RM\nCopyCustomResource\n\n&amp;lt;#\n# test from command line\nCopyCustomResource\nStart-DscConfiguration -Path .\\CopyCustomResource -Verbose -Wait\n#&amp;gt;\n\n\nThis is very straight-forward:\n\n\n  Line 3: Again we’re using RM’s variable so that don’t have to hard-code the node name\n  Lines 5-13: Copy the resource files to the PowerShell modules folder\n  Line 26: Use this to copy the resource locally to your workstation for testing it\n  Line 29: This “compiles” this config file before RM deploys it (and executes it) on the target server\n  Lines 17-23 and 31-35: Uncomment these to run this from the command line for testing\n\n\nHere’s how to use the script in a release template:\n\n\n\n\nBy now you should be able to see how this designer is feeding values to the script!\n\ncScriptWithParams: A Look Inside\n\nTo make the cScriptWithParams custom resource, I copied the out-of-the-box script and added the cParams hash-table parameter to the Get/Set/Test TargetResource functions. I had some issues with type conversions, so I eventually changed the HashTable to an array of Microsoft.Management.Infrastructure.CimInstance. I then make sure this gets passed to the common function that actually invokes the script (ScriptExecutionHelper). Here’s a snippet from the Get-TargetResource function:\n\nfunction Get-TargetResource \n{\n    [CmdletBinding()]\n     param \n     (         \n       [parameter(Mandatory = $true)]\n       [ValidateNotNullOrEmpty()]\n       [string]\n       $GetScript,\n  \n       [parameter(Mandatory = $true)]\n       [ValidateNotNullOrEmpty()]\n       [string]$SetScript,\n\n       [parameter(Mandatory = $true)]\n       [ValidateNotNullOrEmpty()]\n       [string]\n       $TestScript,\n\n       [Parameter(Mandatory=$false)]\n       [System.Management.Automation.PSCredential] \n       $Credential,\n\n       [Parameter(Mandatory=$false)]\n       [Microsoft.Management.Infrastructure.CimInstance[]]\n       $cParams\n     )\n\n    $getTargetResourceResult = $null;\n\n    Write-Debug -Message \"Begin executing Get Script.\"\n \n    $script = [ScriptBlock]::Create($GetScript);\n    $parameters = $psboundparameters.Remove(\"GetScript\");\n    $psboundparameters.Add(\"ScriptBlock\", $script);\n    $psboundparameters.Add(\"customParams\", $cParams);\n\n    $parameters = $psboundparameters.Remove(\"SetScript\");\n    $parameters = $psboundparameters.Remove(\"TestScript\");\n\n    $scriptResult = ScriptExecutionHelper @psboundparameters;\n\n\nNotes:\n\n\n  Lines 24-26: the extra parameter I added\n  Line 36: I add the $cParams to the $psboundparameters that will be passed to the ScriptExecutionHelper function\n  Line 41: this is the original call to the ScriptExecutionHelper function\n\n\nFinally, I customized the ScriptExecutionHelper function to utilize the parameters:\n\nfunction ScriptExecutionHelper \n{\n    param \n    (\n        [ScriptBlock] \n        $ScriptBlock,\n    \n        [System.Management.Automation.PSCredential] \n        $Credential,\n\n        [Microsoft.Management.Infrastructure.CimInstance[]]\n        $customParams\n    )\n\n    $scriptExecutionResult = $null;\n\n    try\n    {\n        $executingScriptMessage = \"Executing script: {0}\" -f ${ScriptBlock} ;\n        Write-Debug -Message $executingScriptMessage;\n\n        $executingScriptArgsMessage = \"Script params: {0}\" -f $customParams ;\n        Write-Debug -Message $executingScriptArgsMessage;\n\n        # bring the cParams into memory\n        foreach($cVar in $customParams.GetEnumerator())\n        {\n            Write-Debug -Message \"Creating value $($cVar.Key) with value $($cVar.Value)\"\n            New-Variable -Name $cVar.Key -Value $cVar.Value\n        }\n\n        if($null -ne $Credential)\n        {\n           $scriptExecutionResult = Invoke-Command -ScriptBlock $ScriptBlock -ComputerName . -Credential $Credential\n        }\n        else\n        {\n           $scriptExecutionResult = &amp;amp;$ScriptBlock;\n        }\n        Write-Debug -Message \"Completed script execution\"\n        $scriptExecutionResult;\n    }\n    catch\n    {\n        # Surfacing the error thrown by the execution of Get/Set/Test script.\n        $_;\n    }\n}\n\n\nNotes:\n\n\n  Lines 11/12: The new “hashtable” of variables\n  Lines 25-30: I use New-Variable to create global variables for each key/value pair in $customParams\n  The remainder of the script is unmodified\n\n\nThe only limitation I hit was the the values must be strings – I am sure this has to do with the way the values are serialized when a DSC configuration script is “compiled” into a .mof file.\n\nAs usual, happy deploying!\n",
      "categories": [],
      "tags": ["releasemanagement"],
      
      "collection": "posts",
      "url": "/real-config-handling-for-dsc-in-rm/"
    },{
      
      "title": "Azure Outage – I was a victim too, dear Reader",
      "date": "2014-11-21 22:35:02 +0000",
      
      "content": "This morning I went to check on my blog – the very blog you’re busy reading – and I was greeted with a dreaded YSOD (Yellow Screen of Death). What? That can’t be! I haven’t deployed anything since about 10 days ago, so I know it wasn’t my code! What gives?\n\nIt turns out that I had some garbled post files. My blog is built on MiniBlog which stores all posts as xml files. One of the changes I made to my engine is to increment a view counter on each post so that I can track which posts are being hit. I suppose there is some risk in doing this, since there is a lot of writing to the files. Turns out about 6 files in my posts directory were either empty or partially empty – I suspect that IIS was writing the files when the outage happened a couple of days ago. Anyway, turns out MiniBlog isn’t that resilient when the xml files it’s reading are not well-formed xml! That just goes to show you that even though your code has been stable for months, stuff can still go wrong!\n\nSo I applied a fix (which moves the broken file out the way and sends me an email with the exception information) and it looks like the site is up again. Although at time of writing, the site is dog slow and I can’t seem to WebDeploy – I’ve had to use FTP to fix the posts and update my code, and I can’t see the files using the Azure SDK File Explorer (though I am able to see the logs, fortunately). I suspect there are still Azure infrastructure problems.\n\nSo other than having my counts go wonky, I may have lost a couple of comments. If one of your comments disappeared, Dear Reader, I humbly apologize.\n\nAlso, this is my first post from my Surface 3 Pro! Woot!\n\nHappy reading-as-long-as-Azure-stays-up!\n",
      "categories": [],
      "tags": ["news"],
      
      "collection": "posts",
      "url": "/azure-outage-i-was-a-victim-too-dear-reader/"
    },{
      
      "title": "Don’t Just Fix It, Red-Green Refactor It!",
      "date": "2014-12-11 21:04:30 +0000",
      
      "content": "I’m back to doing some dev again – for a real-life, going-to-charge-for application! It’s great to be based from home again and to be on some very cutting edge dev.\n\nI’m very comfortable with ASP.NET MVC, but this project is the first Nancy project I’ve worked on. We’re also using Git on VSO for source control and backlogs, MyGet to host internal NuGet packages, Octopus deploy for deployment, Python (with various libs, of course!) for number crunching and Azure to host VMs and websites (which are monitored with AppInsights). All in all it’s starting to shape up to a very cool application – details to follow as we approach go-live (play mysterious music here)…\n\nHo Hum Dev\n\nEver get into a groove that’s almost too automatic? Ever been driving home and you arrive and think, “Wait a minute – how did I get here?”. You were thinking so intently on something else that you just drove “on automatic” without really paying attention to what you were doing.\n\nDev can sometimes get into this kind of groove. I was doing some coding a few days ago and almost missed a good quality improvement opportunity – fortunately, I was able to look up long enough to see a better way to do things, and hopefully save myself some pain down the line.\n\nI was debugging some code, and something wasn’t working the way I expected. Here’s a code snippet showing two properties I was working with:\n\nprotected string _name;\npublic string Name\n{\n    get \n    {\n        if (string.IsNullOrEmpty(_name))\n        {\n            SplitKey();\n        }\n        return _name;\n    }\n    set \n    {\n        _name = value;\n        CombineKey();\n    }\n}\n\nprotected ComponentType _type;\npublic string Type\n{\n    get\n    {\n        return _type.ToString();\n    }\n    set\n    {\n        _type = ParseTypeEnum(value);\n        CombineKey();\n    }\n}\n\n\nSee how the getter for the Type property doesn’t match the code for the getter for Name? Even though I have unit tests for this getter, the tests are all passing!\n\nNow the simple thing to do would have been to simply add the missing call to SplitKey() and carry on – but I wanted to know why the tests weren’t failing. I knew there were issues with the code (I had hit them while debugging) so I decided to take a step back and try some good practices: namely red-green refactor.\n\nWorking with Quality in Mind\n\nWhen you’re coding you should be working with quality in mind – that’s why I love unit testing so much. If you’re doing dev without unit testing, you’re only setting yourself up for long hours of painful in-production debugging. Not fun. Build with quality up front – while it may feel like it’s taking longer to deliver, you’ll save time in the long run since you’ll be adding new features instead of debugging poor quality code.\n\nHere’s what you *should* be doing when you come across “hanky” code:\n\n\n  Do some coding\n  While running / debugging, find some bug\n  BEFORE FIXING THE BUG, write a FAILING unit test that exposes the bug\n  Refactor/fix till the test passes\n\n\nSo I opened up the tests for this entity and found the issue: I was only testing one scenario. This highlights that while code coverage is important, it can give you a false sense of security!\n\nHere’s the original test:\n\n[TestMethod]\npublic void SetsPropertiesCorrectlyFromKeys()\n{\n    var component = new Component()\n    {\n        Key = \"Logger_Log1\"\n    };\n\n    Assert.AreEqual(\"Logger\", component.Type);\n    Assert.AreEqual(\"Log1\", component.Name);\n}\n\n\nComponentType comes from an enumeration – and since Logger is the 1st value in the enum, it defaults to Logger if you don’t explicitly set the value. So while I had a test that was covering the entire method, it wasn’t testing all the combinations!\n\nSo I added a new test:\n\n[TestMethod]\npublic void SetsPropertiesCorrectlyFromKeys2()\n{\n    var component = new Component()\n    {\n        Key = \"Service_S0\"\n    };\n\n    Assert.AreEqual(\"Service\", component.Type);\n    Assert.AreEqual(\"S0\", component.Name);\n}\n\n\nNow when I ran the tests, the 2nd test failed. Excellent! Now I’ve got a further test that will check for a bad piece of code.\n\nTo fix the bug, I had to add another enum value and of course, add in the missing SplitKey() call in the Type property getter:\n\npublic enum ComponentType\n{\n    Unknown,\n    Logger,\n    Service\n}\n\n...\n\nprotected ComponentType _type;\npublic string Type\n{\n    get\n    {\n        if (_type == ComponentType.Unknown)\n        {\n            SplitKey();\n        }\n        return _type.ToString();\n    }\n    set\n    {\n        _type = ParseTypeEnum(value);\n        CombineKey();\n    }\n}\n\n\nNow both tests are passing. Hooray!\n\nConclusion\n\nI realize the red-green refactoring isn’t a new concept – but I wanted to show a real-life example of how you should be thinking about your dev and debugging. Even though the code itself had 100% code coverage, there were still bugs. But debugging with quality in mind means you can add tests that cover specific scenarios – and which will reduce the amount of buggy code going into production.\n\nHappy dev’ing!\n",
      "categories": [],
      "tags": ["development"],
      
      "collection": "posts",
      "url": "/dont-just-fix-it-red-green-refactor-it/"
    },{
      
      "title": "Gulp – Workaround for Handling VS Solution Configuration",
      "date": "2015-01-16 19:30:54 +0000",
      
      "content": "We’ve got some TypeScript models for our web frontend. If you’re doing any enterprise JavaScript development, then TypeScript is a must. It’s much more maintainable and even gives you some compile-time checking.\n\nEven though TypeScript is a subset of JavaScript, you still need to “compile” the TypeScript into regular JavaScript for your files to be used in a web application. Visual Studio 2013 does this out of the box (you may need to install TypeScript if you’ve never done so). However, what if you want to concat the compiled scripts to reduce requests from the site? Or minify them?\n\nWebEssentials\n\nWebEssentials provides functionality like bundling and minification within Visual Studio. The MVC bundling features let you bundle your client side scripts “server side” – so the server takes a bundle file containing a list of files to bundle as well as some settings (like minification) and produces the script file “on-the-fly”. However, if you’re using some over framework, such as Nancy – you’re out of luck. Well, not entirely.\n\nGulp\n\nWhat if you could have a pipeline (think: build engine) that could do some tasks such as concatenation and minification (and other tasks) on client-side scripts? Turns out there is a tool for that – it’s called Gulp (there are several other tools in this space too). I’m not going to cover Gulp in much detail in this post – you can look to Scott Hanselman’s excellent intro post. There are also some excellent tools for Visual Studio that support Gulp – notably Task Runner Explorer Extension (soon to be baked into VS 2015).\n\nConfigurations for Gulp\n\nAfter a bit of learning curve, we finally got our Gulp file into a good place – we were able to compile TypeScript to JavaScript, concat the files preserving ordering for dependencies, minify, include source maps and output to the correct directories. We even got this process kicked off as part of our TFS Team build for our web application.\n\nHowever, I did run into a hitch – configurations. It’s easy enough to specify a configuration (or environment) for Gulp using the NODE_ENV setting from the command line. Just set the value in the CLI you’re using (so “set NODE_ENV Release” for DOS prompt, and “$env:NODE_ENV = ‘Release’” for PowerShell) and invoke gulp. However, it seems that configurations are not yet supported within Visual Studio. I wanted to minify only for Release configurations – and I found there was no obvious way to do this.\n\nI even managed to find a reply to a question on the Task Runner Explorer on VS Gallery where Mads Krisensen states there is no configuration support for the extension yet – he says it’s coming though (see here – look for the question titled “Passing build configuration into Gulp”).\n\nThe good news is I managed to find a passable workaround.\n\nThe Workaround\n\nIn my gulp file I have the following lines:\n\n// set a variable telling us if we're building in release\nvar isRelease = true;\nif (process.env.NODE_ENV &amp;amp;&amp;amp; process.env.NODE_ENV !== 'Release') {\n    isRelease = false;\n}\n\n\nThis is supposed to grab the value of the NODE_ENV from the environment for me. However, running within the Task Runner Explorer quickly showed me that it was not able to read this value from anywhere.\n\nAt the top of the Gulp file, there is a /// comment that allows you to bind VS events to the Gulp file – so if you want a task executed before a build, you can set BeforeBuild=’default’ inside the ///. At first I tried to set the environment using “set ENV_NODE $(Configuration)” in the pre-build event for the project, but no dice.\n\nHere’s the workaround:\n\n\n  Remove the BeforeBuild binding from the Gulp file (i.e. when you build your solution, the Gulp is not triggered). You can see your bindings in the Task Runner Explorer – you want to make sure “Before Build” and “After Build” are both showing 0:\n\n  Add the following into the Pre-Build event on your Web project Build Events tab:\n  set NODE_ENV=$(ConfigurationName)\n  gulp\n\n\n\nThat’s it! Now you can just change your configuration from “Debug” to “Release” in the configuration dropdown in VS and when you build, Gulp will find the correct environment setting. Here you can see I set the config to Debug, the build executes in Debug and Gulp is correctly reading the configuration setting:\n\n\n\nCaveats\n\nThere are always some – in this case only two that I can think of:\n\n\n  You won’t see the Gulp output in the Task Runner Explorer Window on builds – however, if you use the Runner to invoke tasks yourself, you’ll still see the output. The Gulp output will now appear in the Build output console when you build.\n  If you’ve set a Watch task (to trigger Gulp when you change a TypeScript file, for example) it won’t read the environment setting. For me it’s not a big deal since build is invoked prior to debugging from VS anyway. Also, for our build process I default the value to “release” just in case.\n\n\nHappy gulping!\n",
      "categories": [],
      "tags": ["development"],
      
      "collection": "posts",
      "url": "/gulp-workaround-for-handling-vs-solution-configuration/"
    },{
      
      "title": "Reflections on DSC for Release Management",
      "date": "2015-01-19 19:05:59 +0000",
      
      "content": "A couple of months ago I did a series of posts (this one has the summary of all my RM/DSC posts) about using PowerShell DSC in Release Management. I set out to see if I could create a DSC script that RM could invoke that would prep the environment and install the application. I managed to get it going, but never felt particularly good about the final solution – it always felt a little bit hacky. Not the entire solution per se – really just the application bit.\n\nThe main reason for this was the fact that I need to hack the Script Resource in order to let me run commands on the target node with parameters. Initially I thought that the inability to do this natively in DSC was short-sighted from the architecture of DSC – but the more I thought about it, the more I realized that I was trying to shoehorn application installation into DSC.\n\nDSC scripts should be declarative – my scripts were mostly declarative, but the application-specific parts of the script were very much imperative – and that started to smell.\n\nIdempotency\n\nI wrote about what I consider to be the most important mental shift when working with PowerShell DSC – idempotency. The scripts you create need to be idempotent – that is they need to end up in the same end state no matter what the starting state is. This works really well for the environment that an application needs to run in – but it doesn’t really work so well for the application itself.\n\nMy conclusion is simple: use DSC to specify the environment, and use plain ol’ PowerShell to install your application.\n\nPowerShell DSC resources are split into 3 actions – Get, Test and Set. The Get method gets the state of the resource on the node. The Set method “makes it so” – it enforces the state the script specifies. The Test method checks to see if the target node’s state matches the state the script specifies. Let’s consider an example: the WindowsFeature resource. Consider the following excerpt:\n\nWindowsFeature WebServerRole\n{\n    Name = \"Web-Server\"\n    Ensure = \"Present\"\n}\n\n\nWhen executing, this resource will check the corresponding WindowsFeature (IIS) on the target node using the Test method. If IIS is present, no action is taken (the node state matches the desired state specified in the script). If it’s not installed, the Set method is invoked to install/enable the IIS. Of course if we simply wanted to query the state of the WindowsFeature, the Get method would tell us the state (installed or not) of IIS.\n\nThis Get-Test-Set paradigm works well for environments – however, it starts to break down when you try to apply it to an application. Consider a Web Application with a SQL Database backend. How to you test if the application is in a particular state? You could check the schema of the database as an indication of the state; you could check if the site exists as an indication of the web site state. Of course this may not be sufficient for checking the state of your application.\n\n(On a side note, if you’re using WebDeploy to deploy your website and you’re using Database Projects, you don’t need to worry, since these mechanisms are idempotent).\n\nThe point is, you may be deploying an application that doesn’t use an idempotent mechanism. In either case, you’re better off not trying to shoehorn application installation into DSC. Also, Release Management lets you execute both DSC and “plain” PowerShell against target nodes – so use them both.\n\nWebServerPreReqs Script\n\nI also realized that I never published my “WebServerPreReqs” script. I use this script to prep a Web Server for my web application. There are four major sections to the script: Windows Features, runtimes, WebDeploy and MMC.\n\nFirst, I ensure that Windows is in the state I need it to be – particularly IIS. I ensure that IIS is installed, as well as some other options like Windows authentication. Also, I ensure that the firewall allows WMI.\n\nScript AllowWMI \n{\n    GetScript = { @{ Name = \"AllowWMI\" } }\n    TestScript = { $false }\n    SetScript = \n    {\n        Set-NetFirewallRule -DisplayGroup \"Windows Management Instrumentation (WMI)\" -Enabled True\n    }\n}\n\nWindowsFeature WebServerRole\n{\n    Name = \"Web-Server\"\n    Ensure = \"Present\"\n}\n\nWindowsFeature WebMgmtConsole\n{\n    Name = \"Web-Mgmt-Console\"\n    Ensure = \"Present\"\n    DependsOn = \"[WindowsFeature]WebServerRole\"\n}\n\nWindowsFeature WebAspNet\n{\n    Name = \"Web-Asp-Net\"\n    Ensure = \"Present\"\n    DependsOn = \"[WindowsFeature]WebServerRole\"\n}\n\nWindowsFeature WebNetExt\n{\n    Name = \"Web-Net-Ext\"\n    Ensure = \"Present\"\n    DependsOn = \"[WindowsFeature]WebServerRole\"\n}\n\nWindowsFeature WebAspNet45\n{\n    Name = \"Web-Asp-Net45\"\n    Ensure = \"Present\"\n    DependsOn = \"[WindowsFeature]WebServerRole\"\n}\n\nWindowsFeature WebNetExt45\n{\n    Name = \"Web-Net-Ext45\"\n    Ensure = \"Present\"\n    DependsOn = \"[WindowsFeature]WebServerRole\"\n}\n\nWindowsFeature WebHttpRedirect\n{\n    Name = \"Web-Http-Redirect\"\n    Ensure = \"Present\"\n    DependsOn = \"[WindowsFeature]WebServerRole\"\n}\n\nWindowsFeature WebWinAuth\n{\n    Name = \"Web-Windows-Auth\"\n    Ensure = \"Present\"\n    DependsOn = \"[WindowsFeature]WebServerRole\"\n}\n\nWindowsFeature WebScriptingTools\n{\n    Name = \"Web-Scripting-Tools\"\n    Ensure = \"Present\"\n    DependsOn = \"[WindowsFeature]WebServerRole\"\n}\n\n\nNext I install any runtimes my website requires – in this case, the MVC framework. You need to supply a network share somewhere for the installer – of course you could use a File resource as well, but you’d still need to have a source somewhere.\n\n#\n# Install MVC4\n#\nPackage MVC4\n{\n    Name = \"Microsoft ASP.NET MVC 4 Runtime\"\n    Path = \"$AssetPath\\AspNetMVC4Setup.exe\"\n    Arguments = \"/q\"\n    ProductId = \"\"\n    Ensure = \"Present\"\n    DependsOn = \"[WindowsFeature]WebServerRole\"\n}\n\n\nI can’t advocate WebDeploy as a web deployment mechanism enough – if you’re not using it, you should be! However, in order to deploy an application remotely using WebDeploy, the WebDeploy agent needs to be running on the target node and the firewall port needs to be opened. No problem – easy to specify declaratively using DSC. I add the required arguments to get the installer to deploy and start the WebDeploy agent (see the Arguments setting in the Package WebDeploy resource). I also use a Script resource to Get-Test-Set the firewall rule for WebDeploy:\n\n#\n# Install webdeploy\n#\nPackage WebDeploy\n{\n    Name = \"Microsoft Web Deploy 3.5\"\n    Path = \"$AssetPath\\WebDeploy_amd64_en-US.msi\"\n    Arguments = \"ADDLOCAL=MSDeployFeature,MSDeployAgentFeature\"\n    ProductId = \"\"\n    Ensure = \"Present\"\n    Credential = $Credential\n    DependsOn = \"[WindowsFeature]WebServerRole\"\n}\n\n#\n# Enable webdeploy in the firewall\n#\nScript WebDeployFwRule\n{\n    GetScript = \n    {\n        write-verbose \"Checking WebDeploy Firewall exception status\"\n        $Rule = Get-NetFirewallRule -DisplayName \"WebDeploy_TCP_8172\"\n        Return @{\n            Result = \"DisplayName = $($Rule.DisplayName); Enabled = $($Rule.Enabled)\"\n        }\n    }\n    SetScript =\n    {\n        write-verbose \"Creating Firewall exception for WebDeploy\"\n        New-NetFirewallRule -DisplayName \"WebDeploy_TCP_8172\" -Direction Inbound -Action Allow -Protocol TCP -LocalPort 8172\n    }\n    TestScript =\n    {\n        if (Get-NetFirewallRule -DisplayName \"WebDeploy_TCP_8172\" -ErrorAction SilentlyContinue) \n        {\n            write-verbose \"WebDeploy Firewall exception already exists\"\n            $true\n        } \n        else \n        {\n            write-verbose \"WebDeploy Firewall exception does not exist\"\n            $false\n        }\n    }\n    DependsOn = \"[Package]WebDeploy\"\n}\n\n\nFinally, I wanted to make sure that MMC is installed so that I can monitor my application using Application Insights. This one was a little tricky since there isn’t an easy way to install the agent quietly – I have to unzip the installer and then invoke the MSI within. However, it’s still not that hard.\n\n#\n# MMA\n# Since this comes in an exe that can't be run silently, first copy the exe to the node,\n# then unpack it. Then use the Package Resource with custom args to install it from the\n# unpacked msi.\n#\nFile CopyMMAExe\n{\n    SourcePath = \"$AssetPath\\MMASetup-AMD64.exe\"\n    DestinationPath = \"c:\\temp\\MMASetup-AMD64.exe\"\n    Force = $true\n    Type = \"File\"\n    Ensure = \"Present\"\n}\n\nScript UnpackMMAExe\n{\n    DependsOn =\"[File]CopyMMAExe\"\n    TestScript = { $false }\n    GetScript = {\n        @{\n            Result = \"UnpackMMAExe\"\n        }\n    }\n    SetScript = {\n        Write-Verbose \"Unpacking MMA.exe\"\n        $job = Start-Job { &amp;amp; \"c:\\temp\\MMASetup-AMD64.exe\" /t:c:\\temp\\MMA /c }\n        Wait-Job $job\n        Receive-Job $job\n    }\n}\n\nPackage MMA\n{\n    Name = \"Microsoft Monitoring Agent\"\n    Path = \"c:\\temp\\MMA\\MOMAgent.msi\"\n    Arguments = \"ACTION=INSTALL ADDLOCAL=MOMAgent,ACSAgent,APMAgent,AdvisorAgent AcceptEndUserLicenseAgreement=1 /qn /l*v c:\\temp\\MMA\\mmaInstall.log\"\n    ProductId = \"\"\n    Ensure = \"Present\"\n    Dependson = \"[Script]UnpackMMAExe\"\n}\n\n\nAfter running this script against a Windows Server in any state, I can be sure that the server will run my application – no need to guess or hope.\n\nYou can download the entire script from here.\n\nRelease Management\n\nNow releasing my application is fairly easy in Release Management – execute two vNext script tasks: the first runs WebServerPreReqs DSC against the target node; the second runs a plain PowerShell script that invokes WebDeploy for my application using the drop folder of my build as the source.\n\nConclusion\n\nPowerShell DSC is meant to be declarative – any time you’re doing any imperative scripting, rip it out and put it into plain PowerShell. Typically this split is going to be along the line of environment vs application. Use DSC for environment and plain PowerShell scripts for application deployment.\n\nHappy releasing!\n",
      "categories": [],
      "tags": ["releasemanagement"],
      
      "collection": "posts",
      "url": "/reflections-on-dsc-for-release-management/"
    },{
      
      "title": "JSPM, NPM, Gulp and WebDeploy in a TeamBuild",
      "date": "2015-03-17 17:00:27 +0000",
      
      "content": "I’ve been coding a web project using Aurelia for the last couple of weeks (more posts about what I’m actually doing to follow soon!). Aurelia is an amazing SPA framework invented by Rob Eisenberg (@EisenbergEffect).\n\nJSPM\n\nAurelia utilizes npm (Node Package Manager) as well as the relatively new jspm – which is like npm for “browser package management”. In fact Rob and his Aurelia team are working very closely with the jspm team in order to add in functionality that will improve how Aurelia is bundled and packaged – but I digress.\n\nTo utilize npm and jspm, you need to specify the dependencies that you have on any npm/jspm packages in a packages.json file. Then you can run “npm install” and “jspm install” and the package managers spring into action pulling down all your dependencies. This works great while you’re developing – but can be a bit strange when you’re deploying with WebDeploy (and you should be!)\n\nWebDeploy (out of the box) only packages files that are included in your project. This is what you want for any of your source (or content) files. But you really don’t want to include dependencies in your project (or in source control for that matter) since the package managers are going to refresh the dependencies during the build anyway. That’s the whole point of using Package Managers in the first place! The problem is that when you package your website, none of the dependencies will be included in the package (since they’re not included in the VS project).\n\nThere are a couple solutions to this problem:\n\n\n  You could execute the package manager install commands after you’ve deployed your site via WebDeploy. However, if you’re deploying to WAWS (or don’t have access to running scripts on the server where your site is hosted) you won’t be able to – and you are going to end up with missing dependencies.\n  You could include the packages folder in your project. The problem with this is that if you upgrade a package, you’ll end up having to exclude the old package (and its dependencies) and include the new package (and any of its dependencies). You lose the value of using the Package Manager in the first place.\n  Customize WebDeploy to include the packages folder when creating the deployment package. Now we’re talking!\n\n\nIncluding Package Folders in WebDeploy\n\nOf course as I considered this problem I was not happy with either running the Package Manager commands on my hosting servers (in the case of WAWS this isn’t even possible) or including the package files in my project. I then searched out Sayed Ibrahim Hashimi’s site to see what guidance he could offer (he’s a build guru!). I found an old post that explained how to include “extra folders” in web deployment – however, that didn’t quite work for me. I had to apply the slightly more up-to-date property group specified in this post. Sayed had a property group for &lt;CopyAllFilesToSingleFolderForPackageDependsOn&gt; but you need the same property group for &lt;CopyAllFilesToSingleFolderForMsdeployDependsOn&gt;.\n\nMy final customized target to include the jspm package folder in WebDeploy actions is as follows (you can add this to the very bottom of your web project file, just before the closing &lt;/Project&gt; tag):\n\n&amp;lt;!-- Include the jspm_packages folder when packaging in webdeploy since they are not included in the project --&amp;gt;\n&amp;lt;PropertyGroup&amp;gt;\n  &amp;lt;CopyAllFilesToSingleFolderForPackageDependsOn&amp;gt;\n    CustomCollectFiles;\n    $(CopyAllFilesToSingleFolderForPackageDependsOn);\n  &amp;lt;/CopyAllFilesToSingleFolderForPackageDependsOn&amp;gt;\n\n  &amp;lt;CopyAllFilesToSingleFolderForMsdeployDependsOn&amp;gt;\n    CustomCollectFiles;\n    $(CopyAllFilesToSingleFolderForPackageDependsOn);\n  &amp;lt;/CopyAllFilesToSingleFolderForMsdeployDependsOn&amp;gt;\n&amp;lt;/PropertyGroup&amp;gt;\n\n&amp;lt;Target Name=\"CustomCollectFiles\"&amp;gt;\n  &amp;lt;ItemGroup&amp;gt;\n    &amp;lt;_CustomFiles Include=\".\\jspm_packages\\**\\*\"&amp;gt;\n      &amp;lt;DestinationRelativePath&amp;gt;%(RecursiveDir)%(Filename)%(Extension)&amp;lt;/DestinationRelativePath&amp;gt;\n    &amp;lt;/_CustomFiles&amp;gt;\n    &amp;lt;FilesForPackagingFromProject Include=\"%(_CustomFiles.Identity)\"&amp;gt;\n      &amp;lt;DestinationRelativePath&amp;gt;jspm_packages\\%(RecursiveDir)%(Filename)%(Extension)&amp;lt;/DestinationRelativePath&amp;gt;\n    &amp;lt;/FilesForPackagingFromProject&amp;gt;\n  &amp;lt;/ItemGroup&amp;gt;\n&amp;lt;/Target&amp;gt;\n\n\nNow when I package my site, I get all the jspm packages included.\n\nTeamBuild with Gulp, NPM, JSPM and WebDeploy\n\nThe next challenge is getting this all to work on a TeamBuild. Let’s quickly look at what you need to do manually to get a project like this to compile:\n\n\n  Pull the sources from source control\n  Run “npm install” to install the node pacakges\n  Run “jspm install –y” to install the jspm packages\n  (Optionally) Run gulp – in our case this is required since we’re using TypeScript. We’ve got gulp set up to transpile our TypeScript source into js, do minification etc.\n  Build in VS – for our WebAPI backend\n  Publish using WebDeploy (could just be targeting a deployment package rather than pushing to a server)\n\n\nFortunately, once you’ve installed npm and jspm and gulp globally (using –g) you can create a simple PowerShell script to do steps 2 – 4. The out of the box build template does the rest for you. Here’s my Gulp.ps1 script, which I specify in the “Pre-build script path” property of my TeamBuild Process:\n\nparam(\n    [string]$sourcesDirectory = $env:TF_BUILD_SOURCESDIRECTORY\n)\n\n$webDirectory = $sourcesDirectory + \"\\src\\MyWebProject\"\nPush-Location\n\n# Set location to MyWebProject folder\nSet-Location $webDirectory\n\n# refresh the packages required by gulp (listed in the package.json file)\n$res = npm install 2&amp;gt;&amp;amp;1\n$errs = ($res | ? { $_.gettype().Name -eq \"ErrorRecord\" -and $_.Exception.Message.ToLower().Contains(\"err\") })\nif ($errs.Count -gt 0) {\n    $errs | % { Write-Error $_ }\n    exit 1\n} else {\n    Write-Host \"Successfully ran 'npm install'\"\n}\n\n# refresh the packages required by jspm (listed in the jspm section of package.json file)\n$res = jspm install -y 2&amp;gt;&amp;amp;1\n$errs = ($res | ? { $_.gettype().Name -eq \"ErrorRecord\" -and $_.Exception.Message.ToLower().Contains(\"err\") })\nif ($errs.Count -gt 0) {\n    $errs | % { Write-Error $_ }\n    exit 1\n} else {\n    Write-Host \"Successfully ran 'jspm install -y'\"\n}\n\n# explicitly set the configuration and invoke gulp\n$env:NODE_ENV = 'Release'\nnode_modules\\.bin\\gulp.cmd build\n\nPop-Location\n\n\nOne last challenge – one of the folders (a lodash folder) ends up having a path &gt; 260 characters. TeamBuild can’t remove this folder before doing a pull of the sources, so I had to modify the build template in order to execute a “CleanNodeDirs” command (I implemented this as an optional “pre-pull” script). However, this is a chicken-and-egg problem – if the pull fails because of old folders, then you can’t get the script to execute to clean the folders before the pull… So the logic I wrap the “pre-pull” invocation in an If activity that first checks if the “pre-pull” script exists. If it does, execute it, otherwise carry on.\n\nThe logic for this is as follows:\n\n\n  On a clean build (say a first build) the pre-pull script does not exist\n  When the build checks for the pre-pull script, it’s not there – the build continues\n  The build executes jspm, and the offending lodash folder is created\n  The next build initializes, and detects that the pre-pull script exists\n  The pre-pull script removes the offending folders\n  The pull and the remainder of the build can now continue\n\n\nUnfortunately straight PowerShell couldn’t delete the folder (since the path is &gt; 260 chars). I resorted to invoking cmd. I repeat it twice since the first time it complains that the folder isn’t empty – running the 2nd time completes the delete. Here’s the script:\n\nParam(\n  [string]$srcDir = $env:TF_BUILD_SOURCESDIRECTORY\n)\n\n# forcefully remove left over node module folders\n# necessary because the folder depth means paths end up being &amp;gt; 260 chars\n# run it twice since it sometimes complains about the dir not being empty\n# supress errors\n$x = cmd /c \"rd $srcDir\\src\\MyWebProject\\node_modules /s /q\" 2&amp;gt;&amp;amp;1\n$x = cmd /c \"rd $srcDir\\src\\MyWebProject\\node_modules /s /q\" 2&amp;gt;&amp;amp;1\n\n\nConclusion\n\nGetting NPM, JSPM, Gulp, WebDeploy and TeamBuild to play nicely is not a trivial exercise. Perhaps vNext builds will make this all easier – I’ve yet to play with it. For now, we’re happy with our current process.\n\nAny build/deploy automation can be tricky to set up initially – especially if you’ve got as many moving parts as we have in our solution. However, the effort pays off, since you’ll be executing the build/deploy cycle many hundreds of times over the lifetime of an agile project – each time you can deploy from a single button-press is a win!\n\nHappy packaging!\n",
      "categories": [],
      "tags": ["development"],
      
      "collection": "posts",
      "url": "/jspm-npm-gulp-and-webdeploy-in-a-teambuild/"
    },{
      
      "title": "Aurelia: Object Binding Without Dirty Checking",
      "date": "2015-03-23 21:31:53 +0000",
      
      "content": "Over the past few weeks I have been developing a Web UI using Aurelia by Rob Eisenberg. It’s really well thought out – though it’s got a steep learning curve at the moment since the documentation is still very sparse. Of course it hasn’t officially released yet, so that’s understandable!\n\nTypeScript\n\nI love TypeScript – if it wasn’t for TypeScript, I would really hate Javascript development! Aurelia is written in ES6 and ES7 which is transpiled to ES5. You can easily write Aurelia apps in TypeScript – you can transpile in Gulp if you want to – otherwise Visual Studio will transpile to Javascript for you anyway. Since I use TypeScript, I also use Mike Graham’s TypeScript Aurelia sample repos. He has some great samples there if you’re just getting started with Aurelia/TypeScript. Code for this post comes from the “aurelia-vs-ts” solution in that repo.\n\nBinding in Aurelia\n\nAurelia has many powerful features out the box – and most of its components are pluggable too – so you can switch out components as and when you need to. Aurelia allows you to separate the view (html) from the view-model (a Javascript class). When you load a view, Aurelia binds the properties of the view-model with the components in the view. This works beautifully for primitives – Aurelia knows how to create a binding between an HTML element (or property) and the object property. Let’s look at home.html and home.ts to see how this works:\n\n&amp;lt;template&amp;gt;\n  &amp;lt;section&amp;gt;\n    &amp;lt;h2&amp;gt;${heading}&amp;lt;/h2&amp;gt;\n\n    &amp;lt;form role=\"form\" submit.delegate=\"welcome()\"&amp;gt;\n      &amp;lt;div class=\"form-group\"&amp;gt;\n        &amp;lt;label for=\"fn\"&amp;gt;First Name&amp;lt;/label&amp;gt;\n        &amp;lt;input type=\"text\" value.bind=\"firstName\" class=\"form-control\" id=\"fn\" placeholder=\"first name\"&amp;gt;\n      &amp;lt;/div&amp;gt;\n      &amp;lt;div class=\"form-group\"&amp;gt;\n        &amp;lt;label for=\"ln\"&amp;gt;Password&amp;lt;/label&amp;gt;\n        &amp;lt;input type=\"text\" value.bind=\"lastName\" class=\"form-control\" id=\"ln\" placeholder=\"last name\"&amp;gt;\n      &amp;lt;/div&amp;gt;\n      &amp;lt;div class=\"form-group\"&amp;gt;\n        &amp;lt;label&amp;gt;Full Name&amp;lt;/label&amp;gt;\n        &amp;lt;p class=\"help-block\"&amp;gt;${fullName | upper}&amp;lt;/p&amp;gt;\n      &amp;lt;/div&amp;gt;\n      &amp;lt;button type=\"submit\" class=\"btn btn-default\"&amp;gt;Submit&amp;lt;/button&amp;gt;\n    &amp;lt;/form&amp;gt;\n  &amp;lt;/section&amp;gt;\n&amp;lt;/template&amp;gt;\n\n\nThis is the view (html) for the home page (views\\home.html). You bind to variables in the view-model using the ${var} syntax (lines 3 and 16). You can also bind attributes directly – like value.bind=”firstName” in line 8 binds the value of the input box to the “firstName” property. Line 16 uses a value converter to convert the value of the bound parameter to uppercase. Line 5 binds a function to the submit action. I don’t want to get into all the Aurelia binding capabilities here – that’s for another discussion.\n\nHere’s the view-model (views\\home.ts):\n\nexport class Home {\n    public heading: string;\n    public firstName: string;\n    public lastName: string;\n\n    constructor() {\n        this.heading = \"Welcome to Aurelia!\";\n        this.firstName = \"John\";\n        this.lastName = \"Doe\";\n    }\n\n    get fullName() {\n        return this.firstName + \" \" + this.lastName;\n    }\n\n    welcome() {\n        alert(\"Welcome, \" + this.fullName + \"!\");\n    }\n}\n\nexport class UpperValueConverter {\n    toView(value) {\n        return value &amp;amp;&amp;amp; value.toUpperCase();\n    }\n}\n\n\nThe code is very succinct – and easy to test. Notice the absence of any “binding plumbing”. So how does the html know to update when values in the view-model change? (If you’ve ever used Knockout you’ll be wondering where the observables are!)\n\nDirty Binding\n\nThe bindings for heading, firstName and lastName are primitive bindings – in other words, when Aurelia binds the html to the property, it creates an observer on the property so that when the property is changed, a notification of the change is triggered. It’s all done under the covers for you so you can just assume that any primitive on any model will trigger change notifications to anything bound to them.\n\nHowever, if you’re not using a primitive, then Aurelia has to fall-back on “dirty binding”. Essentially it sets up a polling on the object (every 120ms). You’ll see this if you put a console.debug into the getter method:\n\nget fullName() {\n    console.debug(\"Getting fullName\");\n    return this.firstName + \" \" + this.lastName;\n}\n\n\nHere’s what the console looks like when you browse (the console just keeps logging forever and ever):\n\n\n\n\nUnfortunately there simply isn’t an easy way around this problem.\n\nDeclaring Dependencies\n\nJeremy Danyow did however leverage the pluggability of Aurelia and wrote a plugin for observing computed properties without dirty checking called aurelia-computed. This is now incorporated  into Aurelia and is plugged in by default.\n\nThis plugin allows you to specify dependencies explicitly – thereby circumventing the need to dirty check. Here are the changes we need to make:\n\n\n  Add a definition for the declarePropertyDependencies() method in Aurelia.d.ts (only necessary for TypeScript)\n  Add an import to get the aurelia-binding libs\n  Register the dependency\n\n\nAdd these lines to the bottom of the aurelia.d.ts file (in the typings\\aurelia folder):\n\ndeclare module \"aurelia-binding\" {\n    function declarePropertyDependencies(moduleType: any, propName: string, deps: any[]): void;\n}\n\n\nThis just lets Visual Studio know about the function for compilation purposes.\n\nNow change home.ts to look as follows:\n\nimport aub = require(\"aurelia-binding\");\n\nexport class Home {\n    public heading: string;\n    public firstName: string;\n    public lastName: string;\n\n    constructor() {\n        this.heading = \"Welcome to Aurelia!\";\n        this.firstName = \"John\";\n        this.lastName = \"Doe\";\n    }\n\n    get fullName() {\n        console.debug(\"Getting fullName\");\n        return this.firstName + \" \" + this.lastName;\n    }\n\n    welcome() {\n        alert(\"Welcome, \" + this.fullName + \"!\");\n    }\n}\n\naub.declarePropertyDependencies(Home, \"fullName\", [\"firstName\", \"lastName\"]);\n\nexport class UpperValueConverter {\n    toView(value) {\n        return value &amp;amp;&amp;amp; value.toUpperCase();\n    }\n}\n\n\nThe highlighted lines are the lines I added in. Line 24 is the important line – this explicitly registers a dependency on the “fullName” property of the Home class – on “firstName” and “lastName”. Now any time either firstName or lastName changes, the value of “fullName” is recalculated. Bye-bye polling!\n\nHere’s the console output now:\n\n\n\n\nWe can see that the fullName getter is called 4 times. This is a lot better than polling the value every 120ms. (I’m not sure why it’s called 4 times – probably to do with how the binding is initially set up. Both firstName and lastName change when the page loads and they are instantiated to “John” and “Doe” so I would expect to see a couple firings of the getter function at least).\n\nBinding to an Object\n\nSo we’re ok to bind to primitives – but we get stuck again when we want to bind to objects. Let’s take a look at app-state.ts (in the scripts folder):\n\nimport aur = require(\"aurelia-router\");\n\nexport class Redirect implements aur.INavigationCommand {\n    public url: string;\n    public shouldContinueProcessing: boolean;\n\n    /**\n      * Application redirect (works with approuter instead of current child router)\n      *\n      * @url the url to navigate to (ex: \"#/home\")\n      */\n    constructor(url) {\n        this.url = url;\n        this.shouldContinueProcessing = false;\n    }\n\n    navigate(appRouter) {\n        appRouter.navigate(this.url, { trigger: true, replace: true });\n    }\n}\n\nclass AppState {\n    public isAuthenticated: boolean;\n    public userName: string;\n\n    /**\n      * Simple application state\n      *\n      */\n    constructor() {\n        this.isAuthenticated = false;\n    }\n\n    login(username: string, password: string): boolean {\n        if (username == \"Admin\" &amp;amp;&amp;amp; password == \"xxx\") {\n            this.isAuthenticated = true;\n            this.userName = \"Admin\";\n            return true;\n        }\n        this.logout();\n        return false;\n    }\n\n    logout() {\n        this.isAuthenticated = false;\n        this.userName = \"\";\n    }\n}\n\nvar appState = new AppState();\nexport var state = appState;\n\n\nThe AppState is a static global object that tracks the state of the application. This is a good place to track logged in user, for example. I’ve added in the highlighted lines so that we can expose AppState.userName. Let’s open nav-bar.ts (in views\\controls) and add a getter so that the nav-bar can display the logged in user’s name:\n\nimport auf = require(\"aurelia-framework\");\nimport aps = require(\"scripts/app-state\");\n\nexport class NavBar {\n    static metadata = auf.Behavior.withProperty(\"router\");\n\n    get userName() {\n        console.debug(\"Getting userName\");\n        return aps.state.userName;\n    }\n}\n\n\nWe can now bind to userName in the nav-bar.html view:\n\n&amp;lt;template&amp;gt;\n  &amp;lt;nav class=\"navbar navbar-default navbar-fixed-top\" role=\"navigation\"&amp;gt;\n    &amp;lt;div class=\"navbar-header\"&amp;gt;\n      &amp;lt;button type=\"button\" class=\"navbar-toggle\" data-toggle=\"collapse\" data-target=\"#bs-example-navbar-collapse-1\"&amp;gt;\n        &amp;lt;span class=\"sr-only\"&amp;gt;Toggle Navigation&amp;lt;/span&amp;gt;\n        &amp;lt;span class=\"icon-bar\"&amp;gt;&amp;lt;/span&amp;gt;\n        &amp;lt;span class=\"icon-bar\"&amp;gt;&amp;lt;/span&amp;gt;\n        &amp;lt;span class=\"icon-bar\"&amp;gt;&amp;lt;/span&amp;gt;\n      &amp;lt;/button&amp;gt;\n      &amp;lt;a class=\"navbar-brand\" href=\"#\"&amp;gt;\n        &amp;lt;i class=\"fa fa-home\"&amp;gt;&amp;lt;/i&amp;gt;\n        &amp;lt;span&amp;gt;${router.title}&amp;lt;/span&amp;gt;\n      &amp;lt;/a&amp;gt;\n    &amp;lt;/div&amp;gt;\n\n    &amp;lt;div class=\"collapse navbar-collapse\" id=\"bs-example-navbar-collapse-1\"&amp;gt;\n      &amp;lt;ul class=\"nav navbar-nav\"&amp;gt;\n        &amp;lt;li repeat.for=\"row of router.navigation\" class=\"${row.isActive ? 'active' : ''}\"&amp;gt;\n          &amp;lt;a href.bind=\"row.href\"&amp;gt;${row.title}&amp;lt;/a&amp;gt;\n        &amp;lt;/li&amp;gt;\n      &amp;lt;/ul&amp;gt;\n\n      &amp;lt;ul class=\"nav navbar-nav navbar-right\"&amp;gt;\n        &amp;lt;li&amp;gt;&amp;lt;a href=\"#\"&amp;gt;${userName}&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;\n        &amp;lt;li class=\"loader\" if.bind=\"router.isNavigating\"&amp;gt;\n          &amp;lt;i class=\"fa fa-spinner fa-spin fa-2x\"&amp;gt;&amp;lt;/i&amp;gt;\n        &amp;lt;/li&amp;gt;\n      &amp;lt;/ul&amp;gt;\n    &amp;lt;/div&amp;gt;\n  &amp;lt;/nav&amp;gt;\n&amp;lt;/template&amp;gt;\n\n\nI’ve added line 24. Of course we’ll see polling if we run the solution as is. So we can just declare the dependency, right? Let’s try it:\n\nimport auf = require(\"aurelia-framework\");\nimport aub = require(\"aurelia-binding\");\nimport aps = require(\"scripts/app-state\");\n\nexport class NavBar {\n    static metadata = auf.Behavior.withProperty(\"router\");\n\n    get userName() {\n        return aps.state.userName;\n    }\n}\n\naub.declarePropertyDependencies(NavBar, \"userName\", [aps.state.userName]);\n\n\nSeems to compile and run – but the value of userName is never updated!\n\nIt turns out that we can only declare dependencies to the same object (and only to primitives) using declarePropertyDependencies. Seems like we’re stuck.\n\nThe Multi-Observer\n\nI posed this question on the gitter discussion page for Aurelia. The guys working on Aurelia (and the community) are very active there – I’ve been able to ask Rob Eisenberg himself questions! Jeremy Danyow is also active on there (as is Mike Graham) so getting help is usually quick. Jeremy quickly verified that declarePropertyDependencies cannot register dependencies on other objects. However, he promptly whacked out the “Multi-Observer”. Here’s the TypeScript for the class:\n\nimport auf = require(\"aurelia-framework\");\n\nexport class MultiObserver {\n    static inject = [auf.ObserverLocator];\n\n    constructor(private observerLocator: auf.ObserverLocator) {\n    }\n\n    /**\n     * Set up dependencies on an arbitrary object.\n     * \n     * @param properties the properties to observe\n     * @param callback the callback to fire when one of the properties changes\n     * \n     * Example:\n     * export class App {\n     * static inject() { return [MultiObserver]; }\n     * constructor(multiObserver) {\n     * var session = {\n     * fullName: 'John Doe',\n     * User: {\n     * firstName: 'John',\n     * lastName: 'Doe'\n     * }\n     * };\n     * this.session = session;\n     *\n     * var disposeFunction = multiObserver.observe(\n     * [[session.User, 'firstName'], [session.User, 'lastName']],\n     * () =&amp;gt; session.fullName = session.User.firstName + ' ' + session.User.lastName);\n     * }\n     * }\n     */\n    observe(properties, callback) {\n        var subscriptions = [], i = properties.length, object, propertyName;\n        while (i--) {\n            object = properties[i][0];\n            propertyName = properties[i][1];\n            subscriptions.push(this.observerLocator.getObserver(object, propertyName).subscribe(callback));\n        }\n\n        // return dispose function\n        return () =&amp;gt; {\n            while (subscriptions.length) {\n                subscriptions.pop()();\n            }\n        }\n    }\n}\n\n\nAdd this file to a new folder called “utils” under “views”. To get this to compile, you have to add this definition to the aurelia.d.ts file (inside the aurelia-framework module declaration):\n\ninterface IObserver {\n    subscribe(callback: Function): void;\n}\n\nclass ObserverLocator {\n    getObserver(object: any, propertyName: string): IObserver;\n}\n\n\nNow we can use the multi-observer to register a callback when any property on any object changes. Let’s do this in the nav-bar.ts file:\n\nimport auf = require(\"aurelia-framework\");\nimport aub = require(\"aurelia-binding\");\nimport aps = require(\"scripts/app-state\");\nimport muo = require(\"views/utils/multi-observer\");\n\nexport class NavBar {\n    static metadata = auf.Behavior.withProperty(\"router\");\n    static inject = [muo.MultiObserver];\n\n    dispose: () =&amp;gt; void;\n    userName: string;\n\n    constructor(multiObserver: muo.MultiObserver) {\n        // set up a dependency on the session router object\n        this.dispose = multiObserver.observe([[aps.state, \"userName\"]],() =&amp;gt; {\n            console.debug(\"Setting new value for userName\");\n            this.userName = aps.state.userName;\n        });\n    }\n\n    deactivate() {\n        this.dispose();\n    }\n}\n\n\nWe register the function to execute when the value of the property on the object changes – we can execute whatever code we want in this callback.\n\nHere’s the console after logging in:\n\n\n\n\nThere’s no polling – the view-model is bound to the userName primitive on the view-model. But whenever the value of userName on the global state object changes, we get to update the value. We’ve successfully avoided the dirty checking!\n\nOne last note: we register the dependency callback into a function object called “dispose”. We can then simply call this function when we want to unregister the callback (to free up resources). I’ve put the call in the deactivate() method, which is the method Aurelia calls on the view-model when navigating away from it. In this case it’s not really necessary, since the nav-bar is “global” and we won’t navigate away from it. But if you use the multi-observer in a view-model that is going to be unloaded (or navigated away from), be sure to put the dispose function somewhere sensible.\n\nA big thank you to Jeremy Danyow for his help!\n\nHappy binding!\n",
      "categories": [],
      "tags": ["development"],
      
      "collection": "posts",
      "url": "/aurelia-object-binding-without-dirty-checking/"
    },{
      
      "title": "Aurelia – Debugging from within Visual Studio",
      "date": "2015-03-24 19:18:06 +0000",
      
      "content": "In my last couple of posts I’ve spoken about the amazing Javascript framework, Aurelia, that I’ve been coding in. Visual Studio is my IDE of choice – not only because I’m used to it but because it’s just a brilliant editor – even for Javascript, Html and other web technologies. If you’re using VS for web development, make sure that you install Web Essentials – as the name implies, it’s essential!\n\nDebugging\n\nOne of the best things about doing web development in VS – especially if you have a lot of Javascript – is the ability to debug from within VS. You set breakpoints in your script, run your site in IE, and presto! you’re debugging. You can see call-stack, autos, set watches – it’s really great. Unfortunately, until recently I haven’t been able to debug Aurelia projects in VS. We’ll get to why that is shortly – but I want to take a small tangent to talk about console logging in Aurelia. It’s been the lifesaver I’ve needed while I work out why debugging Aurelia wasn’t working.\n\nConsole\n\nFew developers actually make use of the browser console while developing – which is a shame, since the console is really powerful. The easiest way to see it in action is to open an Aurelia project, locate app.ts (yes, I’m using TypeScript for my Aurelia development) and add a “console.debug(“hi there!”) to the code:\n\nimport auf = require(\"aurelia-framework\");\nimport aur = require(\"aurelia-router\");\n\nexport class App {\n    static inject = [aur.Router];\n\n    constructor(private router: aur.Router) {\n        console.log(\"in constructor\");\n        this.router.configure((config: aur.IRouterConfig) =&amp;gt; {\n            config.title = \"Aurelia VS/TS\";\n            config.map([\n                { route: [\"\", \"welcome\"], moduleId: \"./views/welcome\", nav: true, title: \"Welcome to VS/TS\" },\n                { route: \"flickr\", moduleId: \"./views/flickr\", nav: true },\n                { route: \"child-router\", moduleId: \"./views/child-router\", nav: true, title: \"Child Router\" }\n            ]);\n        });\n    }\n}\n\n\nLine 8 is where I add the call to console.log. Here it is in IE’s console when I run the solution:\n\n\n\n\n(To access the console in Chrome or in IE, press F12 to bring up “developer tools” – then just open the console tab). Here’s the same view in Chrome:\n\n\n\n\nThere are a couple of logging methods: log(), info(), warn(), error() and debug(). You can also group entries together and do host of other useful debugging tricks, like timing or logging stack traces.\n\nLogging an Object\n\nBeside simply logging a string message you can also log an object. I found this really useful to inspect objects I was working with – usually VS lets you inspect objects, but since I couldn’t access the object in VS, I did it in the console. Let’s change the “console.log” line to “console.log(“In constructor: %O”, this);” The “%O” argument tells the console to log a hyperlink to the object that you can then use to inspect it. Here is the same console output, this time with “%O” (Note: you have to have the console open for this link to actually expand – otherwise you’ll just see a log entry, but won’t be able to inspect the object properties):\n\n\n\n\nYou can now expand the nodes in the object tree to see the properties and methods of the logged object.\n\nAurelia Log Appenders\n\nIf you’re doing a lot of debugging, then you may end up with dozens of view-models. Aurelia provides a LogManager class – and you can add any LogAppender implementation you want to create custom log collectors. (I do this for Application Insights so that you can have Aurelia traces sent up to App Insights). Aurelia also provides an out-of-the-box ConsoleLogAppender. Here’s how you can add it (and set the logging level) – I do this in main.ts just before I bootstrap Aurelia:\n\nauf.LogManager.addAppender(new aul.ConsoleAppender());\nauf.LogManager.setLevel(auf.LogManager.levels.debug);\n\n\nNow we can change the app.ts file to create a logger specifically for the class – anything logged to this will be prepended by the class name:\n\nimport auf = require(\"aurelia-framework\");\nimport aur = require(\"aurelia-router\");\n\nexport class App {\n    private logger: auf.Logger = auf.LogManager.getLogger(\"App\");\n\n    static inject = [aur.Router];\n\n    constructor(private router: aur.Router) {\n        this.logger.info(\"Constructing app\");\n\n        this.router.configure((config: aur.IRouterConfig) =&amp;gt; {\n            this.logger.debug(\"Configuring router\");\n            config.title = \"Aurelia VS/TS\";\n            config.map([\n                { route: [\"\", \"welcome\"], moduleId: \"./views/welcome\", nav: true, title: \"Welcome to VS/TS\" },\n                { route: \"flickr\", moduleId: \"./views/flickr\", nav: true },\n                { route: \"child-router\", moduleId: \"./views/child-router\", nav: true, title: \"Child Router\" }\n            ]);\n        });\n    }\n}\n\n\nOn line 5 I set up a logger for the class – which I then use in lines 10 and 13. Here’s the console output:\n\n\n\n\nYou can see how the “info” and the “debug” are colored differently (and info has a little info icon in the left gutter) and both entries are prepended with “[App]” – this makes wading through the logs a little bit easier. Also, when I want to switch the log level, I just set it down to LogManager.levels.error and no more info or debug messages will appear in the console – no need to remove them from the code.\n\nWhy Can’t VS Debug Aurelia?\n\nBack to our original problem: debugging Aurelia in Visual Studio. Here’s what happens when you set a breakpoint using the skeleton app:\n\n\n\n\nVisual Studio says that “No symbols have been loaded for this document”. What gives?\n\nThe reason is that Visual Studio cannot debug modules loaded using system.js. Let’s look at how Aurelia is bootstrapped in index.html:\n\n&amp;lt;body aurelia-app&amp;gt;\n    &amp;lt;div class=\"splash\"&amp;gt;\n        &amp;lt;div class=\"message\"&amp;gt;Welcome to Aurelia&amp;lt;/div&amp;gt;\n        &amp;lt;i class=\"fa fa-spinner fa-spin\"&amp;gt;&amp;lt;/i&amp;gt;\n    &amp;lt;/div&amp;gt;\n    &amp;lt;script src=\"jspm_packages/system.js\"&amp;gt;&amp;lt;/script&amp;gt;\n    &amp;lt;script src=\"config.js\"&amp;gt;&amp;lt;/script&amp;gt;\n\n    &amp;lt;!-- jquery layout scripts --&amp;gt;\n    &amp;lt;script src=\"Content/scripts/jquery-1.8.0.min.js\"&amp;gt;&amp;lt;/script&amp;gt;\n    &amp;lt;script src=\"Content/scripts/jquery-ui-1.8.23.min.js\"&amp;gt;&amp;lt;/script&amp;gt;\n    &amp;lt;script src=\"Content/scripts/jquery.layout.min.js\"&amp;gt;&amp;lt;/script&amp;gt;\n\n    &amp;lt;script&amp;gt;\n    //System.baseUrl = 'dist';\n    System.import('aurelia-bootstrapper');\n    &amp;lt;/script&amp;gt;\n&amp;lt;/body&amp;gt;\n\n\nYou can see that system.js is being used to load Aurelia and all its modules – it will also be the loader for your view-models. I’ve pinged the VS team about this – but haven’t been able to get an answer from anyone as to why this is the case.\n\nSwitching the Loader to RequireJS\n\nAurelia (out of the box) uses jspm to load its packages – and it’s a great tool. Unfortunately, for anyone who wants to debug with VS you’ll have to find another module loader. Fortunately Aurelia allows you to swap out your loader! I got in touch with Mike Graham via the Aurelia gitter discussion page – and he was kind enough to point me in the right direction – thanks Mike!\n\nFollowing some examples by Mike Graham, I was able to switch from system.js to requirejs. The switch is fairly straight-forward – here they are:\n\n\n  Create a bundled require-compatible version of aurelia using Mike’s script and add it to the solution as a static script file. Updating the file means re-running the script and replacing the aurelia-bundle. Unfortunately this is not as clean an upgrade path as jspm, where you’d just run “jspm update” to update the jspm packages automatically.\n  Change the index.html page to load require.js and then configure it.\n  Make a call to load the Aurelia run-time using requirejs.\n  Fix relative paths to views in router configurations – though this may not be required for everyone, depending on how you’re referencing your modules when you set up your routes.\n\n\nHere’s an update index page that uses requirejs:\n\n&amp;lt;body aurelia-main&amp;gt;\n    &amp;lt;div class=\"splash\"&amp;gt;\n        &amp;lt;div class=\"message\"&amp;gt;Welcome to Aurelia AppInsights Demo&amp;lt;/div&amp;gt;\n        &amp;lt;i class=\"fa fa-spinner fa-spin\"&amp;gt;&amp;lt;/i&amp;gt;\n    &amp;lt;/div&amp;gt;\n\n    &amp;lt;script src=\"Content/scripts/core-js/client/core.js\"&amp;gt;&amp;lt;/script&amp;gt;\n    &amp;lt;script src=\"Content/scripts/requirejs/require.js\"&amp;gt;&amp;lt;/script&amp;gt;\n    &amp;lt;script&amp;gt;\n        var baseUrl = window.location.origin\n        console.debug(\"baseUrl: \" + baseUrl);\n        require.config({\n            baseUrl: baseUrl + \"/dist\",\n            paths: {\n                aurelia: baseUrl + \"/Content/scripts/aurelia\",\n                webcomponentsjs: baseUrl + \"/Content/scripts/webcomponentsjs\",\n                dist: baseUrl + \"/dist\",\n                views: baseUrl + \"/dist/views\",\n                resources: baseUrl + \"/dist/resources\",\n            }\n        });\n\n        require(['aurelia/aurelia-bundle-latest']);\n    &amp;lt;/script&amp;gt;\n&amp;lt;/body&amp;gt;\n\n\nNow instead of loading system.js, you need to load core.js and require.js. Then I have a script (this could be placed into its own file) which configures requirejs (lines 9-24). I set the baseUrl for requirejs as well as some paths. You’ll have to play with these until requirejs can successfully locate all or your dependencies and view-models. Line 23 then loads the Aurelia runtime bundle via requirejs – this then calls your main or app class, depending on how you configure the &lt;body&gt; tag (either as aurelia-main or aurelia-app).\n\nNow that you’re loading Aurelia using requirejs, you can set breakpoints in your ts file (assuming that you’re generating symbols through VS or through Gulp/Grunt):\n\n\n\n\nVoila – you can now debug Aurelia using VS!\n\nConclusion\n\nWhen you’re doing Aurelia development using Visual Studio, you’re going to have to decide between the ease of package update (using jspm) or debugging ability (using requirejs). Using requirejs requires (ahem) a bit more effort since you need to bundle Aurelia manually, and I found getting the requirejs paths correct proved a fiddly challenge too. However, the ability to set breakpoints in your code in VS and debug is, in my opinion, worth the effort. I figure you’re probably not going to be updating the Aurelia framework that often (once it stabilizes after release) but you’ll be debugging plenty. Also, don’t forget to use the console and log appenders! Every tool in your arsenal makes you a better developer.\n\nHappy debugging!\n\nP.S. If you know how to debug modules that are loaded using system.js from VS, please let the rest of us know!\n",
      "categories": [],
      "tags": ["development"],
      
      "collection": "posts",
      "url": "/aurelia-debugging-from-within-visual-studio/"
    },{
      
      "title": "Aurelia, Karma and More VS Debugging Goodness",
      "date": "2015-04-06 23:56:47 +0000",
      
      "content": "In my previous post I walked through how to change Aurelia to load modules via Require.js so that you can set breakpoints and debug from VS when you run your Aurelia project. In this post I want to share some tips about unit testing your Aurelia view-models.\n\nUnit Testing Javascript\n\nIf you aren’t yet convinced of the value of unit testing, please read my post about why you absolutely should be. Unfortunately, unit testing Javascript in Visual Studio (and during automated builds) is a little harder to do than running unit tests on managed code. This post will show you some of the techniques I use to unit test Javascript in my Aurelia project – though of course you don’t need to be using Aurelia to make use of these techniques. If you want to see the code I’m using for this post, check out this repo.\n\nBut I’ve already got tests!\n\nThis post isn’t going to go too much into how to unit test – there are hundreds of posts about how to test. I’m going to assume that you already have some unit tests. I’ll discuss the following topics in this post:\n\n\n  Basic Karma/Jasmine overview\n  Configuring Karma and RequireJS\n  Running Karma from Gulp\n  Using a SpecRunner.html page to enable debugging unit tests\n  Fudges to enable PhantomJS\n  Code Coverage\n  Running tests in your builds (using TeamBuild)\n  Karma VS Test adapter\n\n\nKarma and Jasmine\n\nThere are many JavaScript testing frameworks out there. I like Jasmine as a (BDD) testing framework, and I like Karma (which used to be called Testacular) as a test runner. One of the things I like about Karma is that you can run your tests in several browsers – it also has numerous “reporters” that let you track the tests, and even add code coverage. Aurelia itself uses Karma for its testing.\n\nConfiguring Karma and RequireJS\n\nTo configure karma, you have to set up a karma config file – by convention it’s usually called karma.conf.js. If you use karma-cli, you can run “karma init” to get karma to lead you through a series of questions to help you set up a karma config file for the first time. I wanted to use requirejs, mostly because using requirejs means I can set breakpoints in Visual Studio and debug. So I made sure to answer “yes” for that question. Unfortunately, that opens a can of worms!\n\nThe reason for the “can of worms” is that karma tries to serve all the files necessary for the test run – but if they are AMD modules, then you can’t “serve” them – they need to be loaded by requirejs. In order to do that, we have to fudge the karma startup a little. We specify the files that should be served in the karma.conf.js file, being careful to “exclude” the files. This flag tells karma to serve the file when it is requested, but not to execute it (think of it as treating the file as static text rather than a JavaScript file to execute). Then, we create a “test-main.js” file to configure requirejs, load the modules and then launch karma.\n\nHere’s the karma.conf.js file:\n\n// Karma configuration\nmodule.exports = function (config) {\n    config.set({\n        basePath: \"\",\n\n        frameworks: [\"jasmine\", \"requirejs\", \"sinon\"],\n\n        // list of files / patterns to load in the browser\n        files: [\n            // test specific files\n            \"test-main.js\",\n            \"node_modules/jasmine-sinon/lib/jasmine-sinon.js\",\n\n            // source files\n            { pattern: \"dist/**/*.js\", included: false },\n\n            // test files\n            { pattern: 'test/unit/**/*.js', included: false },\n\n            // framework and lib files\n            { pattern: \"Content/scripts/**/*.js\", included: false },\n        ],\n\n        // list of files to exclude\n        exclude: [\n        ],\n\n        // available reporters: https://npmjs.org/browse/keyword/karma-reporter\n        reporters: [\"progress\"],\n\n        // web server port\n        port: 9876,\n\n        // enable / disable colors in the output (reporters and logs)\n        colors: true,\n\n        // possible values: config.LOG_DISABLE || config.LOG_ERROR || config.LOG_WARN || config.LOG_INFO || config.LOG_DEBUG\n        logLevel: config.LOG_DEBUG,\n\n        // enable / disable watching file and executing tests whenever any file changes\n        autoWatch: true,\n\n        // available browser launchers: https://npmjs.org/browse/keyword/karma-launcher\n        browsers: [\"Chrome\"],\n\n        // Continuous Integration mode\n        // if true, Karma captures browsers, runs the tests and exits\n        singleRun: true\n    });\n};\n\n\nNotes:\n\n\n  \n    \n      \n        \n          Line 6: We tell karma what frameworks to use when running the tests – jasmine (the test framework), requirejs (for loading modules) and sinon (for mocking). These are installed using “npm install [karma-jasmine\n          karma-requirejs\n          karma-sinon] respectively\n        \n      \n    \n  \n  Lines 11/12: We load files that the tests will need – test-main to configure the modules for the test and the sinon file to load the sinon libs. Since these files are not “excluded”, karma executes them on load.\n  Line 15: We serve all the source files we are testing, using the “exclude” to tell karma to serve them but not execute them (so it only serves them when requested – requirejs will load them)\n  Line 18: We serve all the test (spec) files to run (again, not executing them)\n  Line 21: We serve libraries (including the Aurelia framework)\n\n\nHere’s the test-main.js file:\n\nvar allTestFiles = [];\nvar allSourceFiles = [];\n\nvar TEST_REGEXP = /(spec|test)\\.js$/i;\nvar SRC_REGEXP = /dist\\/[a-zA-Z]+\\/[a-zA-Z]+.js$/im;\n\nvar normalizePathToSpecFiles = function (path) {\n    return path.replace(/^\\/base\\//, '').replace(/\\.js$/, '');\n};\n\nvar normalizePathToSourceFiles = function (path) {\n    return path.replace(/^\\/base\\/dist\\//, '').replace(/\\.js$/, '');\n};\n\nvar loadSourceModulesAndStartTest = function () {\n    require([\"aurelia/aurelia-bundle\"], function () {\n        require(allSourceFiles, function () {\n            require(allTestFiles, function () {\n                window. __karma__.start();\n            });\n        });\n    });\n};\n\nObject.keys(window. __karma__.files).forEach(function (file) {\n    if (TEST_REGEXP.test(file)) {\n        allTestFiles.push(normalizePathToSpecFiles(file));\n    } else if (SRC_REGEXP.test(file)) {\n        allSourceFiles.push(normalizePathToSourceFiles(file));\n    }\n});\n\nrequire.config({\n    // Karma serves files under /base, which is the basePath from your config file\n    baseUrl: \"/\",\n\n    paths: {\n        test: \"/base/test\",\n        dist: \"/base/dist\",\n        views: \"/base/dist/views\",\n        resources: \"/base/dist/resources\",\n        aurelia: \"/base/Content/scripts/aurelia\",\n    },\n\n    // dynamically load all test files\n    deps: [\"aurelia/aurelia-bundle\"],\n\n    // we have to kickoff jasmine, as it is asynchronous\n    callback: loadSourceModulesAndStartTest\n});\n\n\nNotes:\n\n\n  Line 4, 5: We set up regex patterns to match test (spec) files as well as source files\n  Line 8, 12: We normalize the path to test or source files. This is necessary since the paths that requirejs use are a different to the base path that karma sets up.\n  Lines 16-18: We load the modules we need in order of dependency – starting with Aurelia (frameworks), then the sources, and then the test files\n  Line 19: We need to start the karma engine ourselves, since we’re hijacking the default start to load everything via requirejs\n  Line 25: We hook into the karma function that loads files to normalize the file paths\n  Line 37: We set up paths for requirejs\n  Line 46: We tell requirejs that the most “basic” dependency is the Aurelia framework\n  Line 49: We tell karma to execute our custom launch function once the “base” dependency is loaded\n\n\nTo be honest, figuring out the final path and normalize settings was a lot of trial and error. I turned karma logging onto debug, and then just played around until karma was serving all the files and requirejs was happy with path resolution. You’ll have to play around with these paths yourself for your project structure.\n\nRunning Karma Test from the CLI\n\nNow we can run the karma tests: simply type “karma start” and karma will fire up and run the tests: you should see the Chrome window popping up (assuming you’re using the Chrome karma launcher) and a message telling you that the tests were run successfully.\n\nRunning Karma from Gulp\n\nNow that we have the tests running from the karma CLI, we can easily run them from within Gulp. We are using Gulp to transpile TypeScript to Javascript, compile LESS files to CSS and do minification and any other “production-izing” we need – so running tests in Gulp makes sense. Also, this way we make sure we’re using the latest sources we have instead of old stale code that’s been lying around (especially if you forget to run the gulp build tasks!). Here are the essential bits of the “unit-test” target in Gulp:\n\nvar gulp = require('gulp');\nvar karma = require(\"karma\").server;\n\ngulp.task(\"unit-test\", [\"build-system\"], function () {\n    return karma.start({\n        configFile: __dirname + \"/../../karma.conf.js\",\n        singleRun: true\n    });\n});\n\n\nNotes:\n\n\n  We import Gulp and Karma server. I didn’t install gulp-karma – rather, I just rely on “pure” karma.\n  We create a task called “unit-test” that fist calls “build-system” before invoking karma\n  The build-system task transpiles TypeScript to JavaScript – we make sure that we generate un-minified files and source maps in this task (so that later on we can set breakpoints and debug)\n  We tell karma where to find the karma config file (so you need to specify the path relative to __dirName, which is the current directory where the Gulp script is\n  We tell karma to perform a single run, rather then keeping the browsers open and running the tests every time we change a file\n\n\nWe can now run “gulp unit-test” from the command line, or we can execute the gulp “unit-test” task from the Visual Studio Task Runner Explorer (which is native to VS 2015 and can be installed into VS 2013 via an extension):\n\n\n\nDebugging Tests\n\nNow that we can run the tests from Gulp, we may want to debug while testing. In order to do that, we’ll need to make sure the tests can run in IE (since VS will break on code running in IE). The karma launcher creates its own dynamic page to launch the tests, so we’re going to need to code an html page ourselves if we want to be able to debug tests. I create a “SpecRunner.html” page in my unit-test folder:\n\n&amp;lt;!DOCTYPE html&amp;gt;\n&amp;lt;html&amp;gt;\n&amp;lt;head&amp;gt;\n    &amp;lt;meta charset=\"utf-8\"&amp;gt;\n    &amp;lt;title&amp;gt;Jasmine Spec Runner v2.2.0&amp;lt;/title&amp;gt;\n\n    &amp;lt;link rel=\"stylesheet\" href=\"../../node_modules/jasmine-core/lib/jasmine-core/jasmine.css\"&amp;gt;\n    &amp;lt;script src=\"/Content/scripts/jquery-2.1.3.js\"&amp;gt;&amp;lt;/script&amp;gt;\n    \n    &amp;lt;!-- source files here... --&amp;gt;\n    &amp;lt;script src=\"/Content/scripts/core-js/client/core.js\"&amp;gt;&amp;lt;/script&amp;gt;\n    &amp;lt;script src=\"/Content/scripts/requirejs/require.js\"&amp;gt;&amp;lt;/script&amp;gt;\n    \n    &amp;lt;script&amp;gt;\n        var baseUrl = window.location.origin;\n        require.config({\n            baseUrl: baseUrl + \"/src\",\n            paths: {\n                jasmine: baseUrl + \"/node_modules/jasmine-core/lib/jasmine-core/jasmine\",\n                \"jasmine-html\": baseUrl + \"/node_modules/jasmine-core/lib/jasmine-core/jasmine-html\",\n                \"jasmine-boot\": baseUrl + \"/node_modules/jasmine-core/lib/jasmine-core/boot\",\n                \"sinon\": baseUrl + \"/node_modules/sinon/pkg/sinon\",\n                \"jasmine-sinon\": baseUrl + \"/node_modules/jasmine-sinon/lib/jasmine-sinon\",\n                aurelia: baseUrl + \"/Content/scripts/aurelia\",\n                webcomponentsjs: baseUrl + \"/Content/scripts/webcomponentsjs\",\n                dist: baseUrl + \"/dist\",\n                views: baseUrl + \"/dist/views\",\n                resources: baseUrl + \"/dist/resources\",\n                test: \"/test\"\n            },\n            shim: {\n                \"jasmine-html\": {\n                    deps: [\"jasmine\"],\n                },\n                \"jasmine-boot\": {\n                    deps: [\"jasmine\", \"jasmine-html\"]\n                }\n            }\n        });\n\n        // load Aurelia and jasmine...\n        require([\"aurelia/aurelia-bundle\"], function() {\n            // ... then jasmine...\n            require([\"jasmine-boot\"], function () {\n                // .. then jasmine plugins...\n                require([\"sinon\", \"jasmine-sinon\"], function () {\n                    // build a list of specs\n                    var specs = [];\n                    specs.push(\"test/unit/aurelia-appInsights.spec\");\n\n                    // ... then load the specs\n                    require(specs, function () {\n                        // finally we can run jasmine\n                        window.onload();\n                    });\n                });\n            });\n        });\n    &amp;lt;/script&amp;gt;\n&amp;lt;/head&amp;gt;\n\n&amp;lt;body&amp;gt;\n&amp;lt;/body&amp;gt;\n&amp;lt;/html&amp;gt;\n\n\nNotes:\n\n\n  Lines 15-39: We configure requirejs for the tests\n  Lines 15: the base url is the window location – when debugging from VS this is usually http://localhost followed by some port\n  Lines 19-29: the paths requirejs needs to resolve all the modules we want to load, as well as some jasmine-specific libs\n  Lines 31-38: we need to shim a couple of jasmine libs to let requirejs know about their dependencies\n  Lines 42, 44, 46: We load the dependencies in order so that requirejs loads them in the correct order\n  Line 49: We create an array of all our test files\n  Lines 52, 54: After loading the test specs, we trigger the onload() method which start the karma tests\n\n\nAgain you see that we hijack the usual Jasmine startup so that we can get requirejs to load all the sources, libs and tests before launching the test runner. Now we set the SpecRunner.html page to be the startup page for the project, and hit F5:\n\n\n\n\nNow that we can finally run the tests from VS in IE, we can set a breakpoint, hit F5 and we can debug!\n\n\n\nPhantomJS – mostly harmless*, er, headless\n\nWhile debugging in IE or launching Chrome from karma is great, there are situations where we may want to run our tests without the need for an actual browser (like on the build server). Fortunately there is a tool that allows you to run “headless” tests – PhantomJS. And even better – there’s a PhantomJS launcher for karma! Let’s add it in:\n\nRun “npm install karma-phantomjs-launcher –save-dev” to install the PhantomJS launcher for karma. Then change the launcher config in the karma.conf.js file from [“Chrome”] to [“PhantomJS2”] and run karma. Unfortunately, this won’t work: you’ll likely see an error like this:\n\n\nTypeError: 'undefined' is not a function (evaluating 'Array.prototype.forEach.call.bind(Array.prototype.forEach)')\n\n\nThis sounds like a native JavaScript problem – perhaps since Aurelia uses ES6 (and even ES7) we need a more modern launcher. Let’s try install PhantomJS2 (the PhantomJS launcher that uses an experimental Phantom 2, a more modern version of PhantomJS). That seems to get us a little further:\n\n\nReferenceError: Can't find variable: Map\n\n\nHmm. Map is again, an ES6 structure. Fortunately there is a library with the ES5 polyfills for some newer ES6 structures like Map: harmony-collections. We run “npm install harmony-collections –save-dev” to install the harmony-collections package, and then reference it in the karma.conf.js file (on line 13):\n\n\n\"node_modules/harmony-collections/harmony-collections.min.js\",\n\n\nWe get a bit further, but there is still something missing:\n\n\nReferenceError: Can't find variable: Promise\n\n\nAgain a little bit of searching leads to another node package: so we run “npm install promise-polyfill –save-dev” and again reference the file (just after the harmony-collections reference):\n\n\n\"node_modules/promise-polyfill/Promise.min.js\",\n\n\nSuccess! We can now run our tests headless.\n\nIn another system I was coding, I ran into a further problem with the “find” method on arrays. Fortunately, we can polyfill the find method too! I didn’t find a package for that – I simply added the polyfill from here into one of my spec files.\n\nCode Coverage\n\nSo we can now run test from karma, from Gulp, from Task explorer, from VS using F5 and we can run them headless using PhantomJS2. If we add in a coverage reporter, we can even get some code coverage analysis: run “npm install karma-coverage –save-dev”. That will install a new reporter, so we need to add it in to the reporters section of karma.conf.js:\n\nreporters: [\"progress\", \"coverage\"],\n\ncoverageReporter: {\n    dir: \"test/coverage/\",\n    reporters: [\n        { type: 'lcov', subdir: 'report-lcov' },\n        { type: 'text-summary', subdir: '.', file: 'coverage-summary.txt' },\n        { type: 'text' },\n    ]\n},\n\n\nWe add the reporter in (just after “progress”). We also configure what sort of coverage information formats we want and which directory the output should go to. Since the coverage requires our code to be instrumented, we need to add in a preprocessor (just above reporters):\n\npreprocessors: {\n    \"dist/**/*.js\": [\"coverage\"]\n},\n\n\nThis tells the coverage engine to instrument all the js files in the dist folder. Any other files we want to calculate coverage from, we’ll need to add in the glob pattern.\n\n\n\n\nThe output in the image above is from the “text” output. For more detailed coverage reports, we browse to test/coverage/report-lcov/lcov-report/index. We can then click through the folders and then down to the files, where we’ll be able to see exactly which lines our test covered (or missed):\n\n\n\n\nThis will help us discover more test opportunities.\n\nRunning Test in TeamBuilds\n\nWith all the basics in place, we can easily include the unit tests into our builds. If you’re using TFS 2013 builds, you can just add a PowerShell script into your repo and then add that script as a pre- or post-test script. Inside the PowerShell you simply invoke “gulp unit-test” to run the unit tests via Gulp. I wanted to be a bit fancier, so I also added code to inspect the output from the test run and the coverage to add them into the build summary:\n\n\n\n\nThe full PowerShell script is here.\n\nSeeing Tests in Visual Studio\n\nFinally, just in case we don’t have enough ways of running the tests, we can install the Visual Studio Karma Test Adapter. This great adapter picks up the tests we’ve configured in karma and displays them in the test explorer window, where we can run them:\n\n\n\nConclusion\n\nUnit testing your front-end view-model logic is essential if you’re going to deploy quality code. Enabling a good experience for unit testing requires a little bit of thought and some work – but once you’ve got the basics in place, you’ll be good to go. Ensuring quality as you code means you’ll have better quality down the road – and that means more time for new features and less time fixing bugs. Using Gulp and Karma enables continuous testing, and augmenting these with the techniques I’ve outlines you can also debug tests, run tests several ways and even integrate the tests (and coverage) into your builds.\n\nHappy testing!\n\n* Mostly Harmless – from the Hitchhikers Guide to the Galaxy by Douglas Adams\n",
      "categories": [],
      "tags": ["development","testing"],
      
      "collection": "posts",
      "url": "/aurelia-karma-and-more-vs-debugging-goodness/"
    },{
      
      "title": "Why You Should Switch to Build VNext",
      "date": "2015-05-22 20:38:39 +0000",
      
      "content": "Now that VNext builds are in Preview, you should be moving your build definitions over from the “old” XAML definitions to the new VNext definitions. Besides the fact that I suspect at some point that XAML builds will be deprecated, the VNext builds are just much better, in almost every respect.\n\nWhy Switch?\n\nThere are several great reasons to switch to (or start using) the new VNext builds. Here’s a (non-exhaustive) list of some of my favorites:\n\n\n  Build is now an orchestrator, not another build engine. This is important – VNext build is significantly different in architecture from the old XAML engine. Build VNext is basically just an orchestrator. That means you can orchestrate whatever build engine (or mechanism) you already have – no need to lose current investments in engines like Ant, CMake, Gradle, Gulp, Grunt, Maven, MSBuild, Visual Studio, Xamarin, XCode or any other existing engine. “Extra” stuff – like integrating with work items, publishing drops and test results and other “plumbing” is handled by Build.VNext.\n  Edit build definitions in the Web. You no longer have to download, edit or – goodness – learn a new DSL. You can stitch together fairly complex builds right in Web Access.\n  Improved Build Reports. The build reports are much improved – especially Test Results, which are now visible on the Web (with nary a Visual Studio in sight).\n  Improved logging. Logging in VNext builds is significantly better – the logs are presented in a console window, and not hidden somewhere obscure.\n  Improved Triggers. The triggers have been improved – you can have multiple triggers for the same build, including CI triggers (where a checkin/commit triggers the build) and scheduled triggers.\n  Improved Retention Policies. Besides being able to specify multiple rules, you can now also use “days” to keep builds, rather than “number” of builds. This is great when a build is being troublesome and produces a number of builds – if you were using “number of builds” you’d start getting drop-offs that you don’t really want.\n  Composability. Composing builds from the Tasks is as easy as drag and drop. Setting properties is a snap, and properties such as “Always Run” make the workflow easy to master.\n  Simple Customization. Have scripts that you want to invoke? No problem – drag on a “PowerShell” or “Bat” Task. Got a one-liner that needs to execute? No problem – use the “Command Line” task and you’re done. No mess, no fuss.\n  Deep Customization. If the Tasks aren’t for you, or there isn’t a Task to do what you need, then you can easily create your own.\n  Open Source Toolbox. Don’t like the way an out-of-the-box Task works? Simply download its source code from the vso-agent-tasks Github repo, and fix it! Of course you can share your awesome Tasks once you’ve created them so that the community benefits from your genius (or madness, depending on who you ask!)\n  Cross Platform. The cross-platform agent will run on Mac or Linux. There’s obviously a windows agent too. That means you can build on whatever platform you need to.\n  Git Policies. Want to make sure that a build passes before accepting merges into a branch? No problem – set up a VNext build, and then add a Policy to your branch that forces the build to run (and pass) before merges are accepted into the branch (via Pull Requests). Very cool.\n  Auditing. Build definitions are now stored as JSON objects. Every change to the build (including changing properties) is kept in a history. Not only can you see who changed what when, but you can do side-by-side comparisons of the changes. You can even enter comments as you change the build to help browse history.\n  Templating. Have a brilliant build definition that you want to use as a template for other builds? No problem – just save your build definition as a template. When you create a new build next time, you can start from the template.\n  Deployment. You can now easily deploy your assets. This is fantastic for Continuous Delivery – not only can you launch a build when someone checks in (or commits) but you can now also include deployment (to your test rigs, obviously!). Most of the deployment love is for Azure – but since you can create your own Tasks, you can create any deployment-type Task you want.\n  Auto-updating agents. Agents will auto-update themselves – no need to update every agent in your infrastructure.\n  Build Pools and Queues. No more limitations on “1 TPC per Build Controller and 1 Controller per build machine”. Agents are xcopyable, and live in a folder. That means you can have as many agents (available to as many pools, queues and TPCs as you want) on any machine. The security and administration of the pools, queues and agents is also better in build vNext.\n  Capabilities and Demands. Agents will report their “capabilities” to TFS (or VSO). When you create builds, the sum of the capabilities required for each Task is the list of “demands” that the build requires. When a build is queued, TFS/VSO will find an agent that has capabilities that match the demands. A ton of capabilities are auto-discovered, but you can also add your own. For example, I added “gulp = 0.1.3” to my build agent so that any build with a “Gulp” task would know it could run on my agent. This is a far better mechanism of matching agents to builds than the old “Agent Tags”.\n\n\nHopefully you can see that there are several benefits to switching. Just do it! It’s worth noting that there are also Hosted VNext agents, so you can run your VNext builds on the “Hosted” queue too. Be aware though that the image for the agent is “stock”, so it may not work for every build. For example, we’re using TypeScript 1.5 beta, and the hosted agent build only has TypeScript extensions 1.4, so our builds don’t work on the Hosted agents.\n\nEnvironment Variables Name Change\n\nWhen you use a PowerShell script Task, the script is invoked in a context that includes a number of predefined environment variables. Need access to the build number? No problem – just look up $env.BUILD_BUILDNUMBER. It’s way easier to use the environment variables that to remember how to pass parameters to the scripts. Note – the prefix “TF_” has been dropped – so if you have PowerShell scripts that you were invoking as pre- or post-build or test scripts in older XAML builds, you’ll have to update the names.\n\nJust a quick tip: if you directly access $env.BUILD_BUILDNUMBER in your script, then you have to set the variable yourself before testing the script in a PowerShell console. I prefer to use the value as a default for a parameter – that way you can easily invoke the script outside of Team Build to test it. Here’s an example:\n\nParam(\n  [string]$pathToSearch = $env:BUILD_SOURCESDIRECTORY,\n  [string]$buildNumber = $env:BUILD_BUILDNUMBER,\n  [string]$searchFilter = \"VersionInfo.\",\n  [regex]$pattern = \"\\d+\\.\\d+\\.\\d+\\.\\d+\"\n)\n \nif ($buildNumber -match $pattern -ne $true) {\n    . . .\n}\n\n\nSee how I default the $pathToSearch and $buildNumber parameters using the $env variable? Invoking the script myself when testing is then easy – just supply values for the variables explicitly.\n\nNode Packages – Path too Long\n\nI have become a fan of node package manager – npm. Doing some web development recently, I have used it a log. The one thing I have against it (admittedly this is peculiar only to npm on Windows), is that the node_modules path can get very deep – way longer than good ol’ 260 character limit.\n\nThis means that any Task that does a wild-char search on folders is going to error when there’s a node_modules folder in the workspace. So you have to explicitly specify the paths to files like sln or test (for the VSBuild and VSTest Tasks) respectively – you can’t use the “**/*.sln” path wildcard (**) because it will try to search in the node_modules folder, and will error out when the path gets too long. No big deal – I just specify the path using the repo browser dialog. I was also forced to check “Continue on Error” on the VSBuild Task – the build actually succeeds (after writing a “Path too long” error in the log), but because the Task outputs the “Path too long” error to stderr, the Task fails.\n\n\n\n\nEDIT: If you are using npm and run into this problem, you can uncheck “Restore NuGet Packages” (the VSBuild Task internally does a wild-card search for package.config, and this is what is throwing the path too long error as it searches the node_modules folder). You’ll then need to add a “Nuget installer” Task before the VSBuild task and explicitly specify the path to your sln file.\n\n\n\nMigrating from XAML Builds\n\nMigrating may be too generous – you have to re-create your builds. Fortunately, trying to move our build from XAML to VNext didn’t take all that long, even with the relatively large customizations we had – but I was faced with Task failures due to the path limit, and so I had to change the defaults and explicitly specify paths wherever there was a “**/” folder. Also, the npm Task itself has a bug that will soon be fixed – for now I’m getting around that by invoking “npm install” as part of a “Command Line” Task (don’t forget to set the working directory):\n\n\n\nNo PreGet Tasks\n\nAt first I had npm install before my “UpdateVersion” script – however, the UpdateVersion script searches for files with a matching pattern using Get-ChildItem. Unfortunately, this errors out with “path too long” when it goes into the node_modules directory. No problem, I thought to myself – I’ll just run UpdateVersion before npm install. That worked – but the build still failed on the VSBuild Task. So I set “Continue on Error” on the VSBuild Task – and I got a passing build!\n\nI then queued a new build – and the build failed. The build agent couldn’t even get the sources because – well, “Path too long”. Our XAML build actually had a “pre-Pull” script hook so that we could delete the node_modules folder (using RoboCopy which can handle too long paths). However, VNext builds cannot execute Tasks before getting sources. Fortunately Chris Patterson, the build PM, suggested that I run the delete at the end of the build.\n\nInitially I thought this was a good idea – but then I thought, “What if the build genuinely fails – like failed tests? Then the ‘delete’ task won’t be run, and I won’t be able to build again until I manually delete the agent’s working folder”. However, when I looked at the Tasks, I saw that there is a “Run Always” checkbox on the Task! So I dropped a PowerShell Task at the end of my build that invokes the “CleanNodeDirs.ps1” script, and check “Always Run” so that even if something else in the build fails, the CleanNodeDirs script always runs. Sweet!\n\nCleanNodeDirs.ps1\n\nTo clean the node_modules directory, I initially tried “rm –rf node_modules”. But it fails – guess why? “Path too long”. After searching around a bit, I came across a way to use RoboCopy to delete folders. Here’s the script:\n\nParam(\n  [string]$srcDir = $env:BUILD_SOURCESDIRECTORY\n)\n\ntry {\n    if (Test-Path(\".\\empty\")) {\n        del .\\empty -Recurse -Force\n    }\n    mkdir empty\n\n    robocopy .\\empty \"$srcDir\\src\\Nwc.Web\\node_modules\" /MIR &amp;gt; robo.log\n    del .\\empty -Recurse -Force\n    del robo.log -Force\n\n    Write-Host \"Successfully deleted node_modules folder\"\n    exit 0\n} catch {\n    Write-Error $_\n    exit 1\n}\n\n\nBuild.VNext Missing Tasks\n\nThere are a couple of critical Tasks that are still missing:\n\n\n  No “Associate Work Items” Task\n  No “Create Work Item on Build Failure” Task\n  No “Label sources” Tasks\n\n\nThese will no doubt be coming soon. It’s worth working on converting your builds over anyway – when the Tasks ship, you can just drop them into your minty-fresh builds!\n\nConclusion\n\nYou really need to be switching over to BuildVNext – even though it’s still in preview, it’s still pretty powerful. The authoring experience is vastly improved, and the Task library is going to grow rapidly – especially since it’s open source. I’m looking forward to what the community is going to come up with.\n\nHappy building!\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/why-you-should-switch-to-build-vnext/"
    },{
      
      "title": "My First VSO Extension: Retry Build",
      "date": "2015-05-25 17:58:53 +0000",
      
      "content": "Visual Studio Online (VSO) and TFS 2015 keep getting better and better. One of the coolest features to surface recently is the ability to add (supported) extensions to VSO. My good friend Tiago Pascoal managed to hack VSO to add extensions a while ago, but it was achieved via browser extensions, not through a supported VSO extensibility framework. Now Tiago can add his extensions in an official manner!\n\nTL;DR – if you just want the code for the extension, then just go to this repo.\n\n\nRetry Build\n\nI was recently playing with Build VNext and got a little frustrated that there was no way to retry a build from the list of completed builds in Web Access. I had to click the build definition to queue it. I found this strange, since the build explorer in Visual Studio has an option to retry a build. I was half-way through writing a mail to the Visual Studio Product team suggesting that they add this option, when I had an epiphany: I can write that as an extension! So I did…\n\nI started by browsing to the Visual Studio Extensions sample repo on Github. I had to join the Visual Studio Partner program, which took a while since I signed up using my email address but adding my work Visual Studio account (instead of my personal account). Switching the account proved troublesome, but I was able to get it sorted with help from Will Smythe on the Product team. Make sure you’re the account owner and that you specify the correct VSO account when you sign up for the partner program!\n\nNext I cloned the repo and took a look at the code – it looked fairly straightforward, especially since all I wanted to do with this extension was add a menu command – no new UI at all.\n\nI followed the instructions for installing the “Contribution Point Guide” so that I could test that extensions worked on my account, as well as actually see the properties of the extension points. It’s a very useful extension to have when you’re writing extensions (does that sounds recursive?).\n\nTypeScript\n\nI’m a huge TypeScript fan, so I wanted to write my extension in TypeScript. There is a sample in the samples repo that has TypeScript, so I got some hints from that. There is a “Delete branch” sample that adds a menu command (really the only thing I wanted to do), so I started from that sample and wrote my extension.\n\nImmediately I was glad I had decided to use TypeScript – the d.ts (definition files) for the extension frameworks and services is very cool – getting IntelliSense and being able to type the objects that were passed around made discovery of the landscape a lot quicker than if I was just using plain JavaScript.\n\nThe code tuned out to be easy enough. However, when I ran the extension, I kept getting a\n\n\n’define’ is not defined\n\n\nerror. We’ll come back to that. Let’s first look at main.ts to see the extension:\n\nimport {BuildHttpClient} from \"TFS/Build/RestClient\";\nimport {getCollectionClient} from \"VSS/Service\";\nvar retryBuildMenu = (function () {\n    \"use strict\";\n\n    return &amp;lt;IContributedMenuSource&amp;gt; {\n        execute: (actionContext: any) =&amp;gt; {\n            var vsoContext = VSS.getWebContext();\n            var buildClient = getCollectionClient(BuildHttpClient);\n\n            VSS.ready(() =&amp;gt; {\n                // get the build\n                buildClient.getBuild(actionContext.id, vsoContext.project.name).then(build =&amp;gt; {\n                    // and queue it again\n                    buildClient.queueBuild(build, build.definition.project.id).then(newBuild =&amp;gt; {\n                        // and navigate to the build summary page\n                        // e.g. https://myproject.visualstudio.com/DefaultCollection/someproject/_BuildvNext#_a=summary&amp;amp;buildId=1347\n                        var buildPageUrl = `${vsoContext.host.uri}/${vsoContext.project.name}/_BuildvNext#_a=summary&amp;amp;buildId=${newBuild.id}`;\n                        window.parent.location.href = buildPageUrl;\n                    });\n                });\n            });\n        }\n    };\n}());\n\nVSS.register(\"retryBuildMenu\", retryBuildMenu);\n\n\nNotes:\n\n\n  Lines 1/2: imports of framework objects – these 2 lines were causing an error for me initially\n  Line 3: the name of this function is only used on line 27 when we register it\n  Line 6: we’re returning an IContributedMenuSource struct\n  Line 7: the struct has an ‘execute’ method that is invoked when the user clicks on the menu item\n  Line 9: we get a reference to what is essentially the build service\n  Line 13: using the build it (a property I discovered on the actionContext object using the Contribution Point sample extension) we can get the completed build object\n  Line 15: I simple pop the build back onto the queue – all the other information is already in the build object (like branch, configuration and so on) from the previous queuing of the build\n  Line 18: I build an url that points to the summary page for the new build\n  Line 19: redirect the browser to the new build url\n  Line 13/15: note the use of the .then() syntax – these methods return promises (good asynch programming), so we use the .then() to execute once the async operation has completed\n  Line 27: registering the extension using the name (1st arg) which is the name we use in the extension.json file, and the function name we specified on line 3 (the 2nd arg)\n\n\nIt was in fact, simpler than I thought it would be. I was expecting to have to create a new build object from the old build object – turns out that wasn’t necessary at all. So I had my code and was ready to run – except that I ran into a snag. When I ran my code, I kept getting\n\n\n’define’ is not defined\n\n\n. To understand why, we need to quickly understand how the extensions are organized.\n\nAnatomy of an Extension\n\nA VSO extension consists of a couple of key files: the extension.json, the main.html and the main.ts or main.js file.\n\n\n  extension.json – the manifest file for the extension – used to register the extension\n  main.html – the main loading page for the extension – used to bootstrap the extension\n  main.js (or main.ts) – the main script entry point for the extension – used to provide the starting point for any extension logic\n\n\nThe “Build Inspector” sample has a main.ts, but this file doesn’t really do much – it only redirects to the main page of the extension custom UI. So there are no imports or requires. So I was at a bit of a loss as to why I was getting what looked like a require error when my extension was loaded. Here’s the html for the main.html page of the sample “Delete Branch” extension:\n\n&amp;lt;!DOCTYPE html&amp;gt;\n&amp;lt;html lang=\"en\"&amp;gt;\n&amp;lt;head&amp;gt;\n    &amp;lt;meta charset=\"UTF-8\"&amp;gt;\n    &amp;lt;title&amp;gt;Delete Branch&amp;lt;/title&amp;gt;\n&amp;lt;/head&amp;gt;\n&amp;lt;body&amp;gt;\n    &amp;lt;script src=\"sdk/scripts/VSS.SDK.js\"&amp;gt;&amp;lt;/script&amp;gt;\n    &amp;lt;script src=\"scripts/main.js\"&amp;gt;&amp;lt;/script&amp;gt;\n    &amp;lt;script&amp;gt;\n        VSS.init({ setupModuleLoader: true });\n    &amp;lt;/script&amp;gt;\n&amp;lt;/body&amp;gt;\n&amp;lt;/html&amp;gt;\n\n\nYou’ll see that the main.js file is imported in line 9, and then we’ve told VSO to use the module loader – necessary for any “require” work. So I was still baffled – here we’re telling the framework that we’re going to be using “require” and I’m getting a require error! (Remember, since the sample doesn’t use any requires in the main.js, it doesn’t error). My main.html page looked exactly the same – and then looked at the items.html page of the sample “Build Inspector” extension, and I got an idea – I need to require my main module, not just load it. Here’s what my main.html ended up looking like:\n\n&amp;lt;!DOCTYPE html&amp;gt;\n&amp;lt;html lang=\"en\"&amp;gt;\n&amp;lt;head&amp;gt;\n    &amp;lt;meta charset=\"UTF-8\"&amp;gt;\n    &amp;lt;title&amp;gt;Retry Build&amp;lt;/title&amp;gt;\n&amp;lt;/head&amp;gt;\n&amp;lt;body&amp;gt;\n    &amp;lt;script src=\"sdk/scripts/VSS.SDK.js\"&amp;gt;&amp;lt;/script&amp;gt;\n    &amp;lt;p&amp;gt;User will never see this&amp;lt;/p&amp;gt;\n    &amp;lt;script type=\"text/javascript\"&amp;gt;\n        // Initialize the VSS sdk\n        VSS.init({\n            setupModuleLoader: true,\n        });\n\n        // Wait for the SDK to be initialized, then require the script\n        VSS.ready(function () {\n            require([\"scripts/main\"], function (main) { });\n        });\n    &amp;lt;/script&amp;gt;\n&amp;lt;/body&amp;gt;\n&amp;lt;/html&amp;gt;\n\n\nYou can see how instead of just importing the main.js script (like the “Delete Branch” sample) I “require” the main script on line 18. Once I had that, no more errors and I was able to get the extension to work.\n\nOnce I had that worked out, I was able to quickly publish the extension to Azure, change the url in the extension.json file to point to my Azure site url, and I was done! The code is in this repo.\n\nConclusion\n\nWriting extensions for VSO is fun, and having a good sample library to start from is great. The “Contribution Points” sample is clever – letting you test the extension loading as well as giving very detailed information about the various hooks and properties available for extensions. Finally, the TypeScript definitions make navigating the APIs available a snap. While my first extension is rather basic, I am really pleased with the extensibility framework that the Product team have devised.\n\nHappy customizing!\n",
      "categories": [],
      "tags": ["tfsapi"],
      
      "collection": "posts",
      "url": "/my-first-vso-extension-retry-build/"
    },{
      
      "title": "PaaS and Time Compression in the Cloud",
      "date": "2015-06-22 16:31:08 +0000",
      
      "content": "Recently I got to write a couple of articles which were posted on the Northwest Cadence Blog. I am not going to reproduce them here, so please read them on the NWCadence blog from the links below.\n\nPaaS\n\nThe first, PaaS Architecture: Designing Apps for the Cloud covers some of the considerations you’ll need to make if you’re porting your applications to the Cloud. Moving your website to a VM with IIS is technically “moving to the cloud”, but this is IaaS (infrastructure as a service) rather than PaaS (platform as a service). If you’re going to unlock true scale from the cloud, you’re going to have to move towards PaaS.\n\nUnfortunately, you can’t simply push any site into PaaS, since you’ll need to consider where and how your data is stored, how your authentication is going to work, and most importantly, how your application will handle scale up – being spread over numerous servers simultaneously. The article deals with some these and other considerations you’ll need to make.\n\nTime Compression\n\nThe second article, Compressing Time: A Competitive Advantage is written off the back of Joe Weinman’s excellent white paper Time for the Cloud. Weinman asserts that “moving to the cloud” is not a guarantee of success in and of itself – companies must strategically utilize the advantages cloud computing offers. While there are many cloud computing advantages, Weinman focuses on what he calls time compression – the cloud’s ability to speed time to market as well as time to scale. Again, I consider some of the implications you’ll need to be aware of when you’re moving applications to the cloud.\n\nHappy cloud computing!\n",
      "categories": [],
      "tags": ["cloud"],
      
      "collection": "posts",
      "url": "/paas-and-time-compression-in-the-cloud/"
    },{
      
      "title": "Enable SAFe Features in Existing Team Projects After Upgrading to TFS 2015",
      "date": "2015-07-17 15:50:11 +0000",
      
      "content": "TFS 2015 has almost reached RTM! If you upgrade to CTP2, you’ll see a ton of new features, not least of which are significant backlog and board improvements, the identity control, Team Project rename, expanded features for Basic users, the new Build Engine, PRs and policies for Git repos and more. Because of the schema changes required for Team Project rename, this update can take a while. If you have large databases, you may want to run the “pre-upgrade” utility that will allow you to prep your server while it’s still online and decrease the time required to do the upgrade (which will need to be done offline).\n\nSAFe Support\n\nThe three out of the box templates have been renamed to simply Scrum, Agile and CMMI. Along with the name change, there is now “built in” support for SAFe. This means if you create a new TFS 2015 team project, you’ll have 3 backlogs – Epic, Feature and “Requirement” (where Requirement will be Requirement, User Story or PBI for CMMI, Agile and Scrum respectively). In Team Settings, team can opt into any of the 3 backlogs. Also, Epics, Features and “Requirements” now have an additional “Value Area” field which can be Business or Architectural, allowing you to track Business vs Architectural work.\n\nWhere are my Epics?\n\nAfter upgrading my TFS to 2015, I noticed that I didn’t have Epics. I remember when upgrading from 2012 to 2013, when you browsed to the Backlog a message popped up saying, “Some features are not available” and a wizard walked you through enabling the “backlog feature”, adding in missing work items and configuring the process template settings. I was expecting the same behavior when upgrading to TFS 2015 – but that didn’t happen. I pinged the TFS product team and they told me that, “Epics are not really a new ‘feature’ per se – just a new backlog level, so the ‘upgrade’ ability was not built in.” If you’re on VSO, your template did get upgraded, so you won’t have a problem – however, for on-premises Team Projects you have to apply the changes manually.\n\nDoing it Manually\n\nHere are the steps for enabling SAFe to your existing TFS 2013 Agile, Scrum or CMMI templates:\n\n\n  Add the Epic work item type\n  Add the “Value Area” field to Features and “Requirements”\n  Add the “Value Area” field to the Feature and “Requirement” form\n  Add the Epic category\n  Add the Epic Product Backlog\n  Set the Feature Product Backlog parent to Epic Backlog\n  Set the work item color for Epics\n\n\nIt’s a whole lot of “witadmin” and XML editing – never fun. Fortunately for you, I’ve created a script that will do it for you.\n\nIsn’t there a script for that?\n\nHere’s the script – but you can download it from here.\n\n&amp;lt;#\n.SYNOPSIS\n\nAuthor: Colin Dembovsky (http://colinsalmcorner.com)\nUpdates 2013 Templates to 2015 base templates, including addition of Epic Backlog and Area Value Field.\n\n\n.DESCRIPTION\n\nAdds SAFe support to the base templates. This involves adding the Epic work item (along with its backlog and color settings) as well as adding 'Value Area' field to Features and Requirements (or PBIs or User Stories).\n\nThis isn't fully tested, so there may be issues depending on what customizations of the base templates you have already made. The script attempts to add in values, so it should work with your existing customizations.\n\nTo execute this script, first download the Agile, Scrum or CMMI template from the Process Template Manager in Team Explorer. You need the Epic.xml file for this script.\n\n.PARAMETER tpcUri\n\nThe URI to the Team Project Collection - defaults to 'http://localhost:8080/tfs/defaultcollection'\n\n.PARAMETER project\n\nThe name of the Team Project to ugprade\n\n.PARAMETER baseTemplate\n\nThe name of the base template. Must be Agile, Scrum or CMMI\n\n.PARAMETER pathToEpic\n\nThe path to the WITD xml file for the Epic work item\n\n.PARAMETER layoutGroupToAddValueAreaControlTo\n\nThe name of the control group to add the Value Area field to in the FORM - defaults to 'Classification' (Agile), 'Details' (SCRUM) and '' (CMMI). Leave this as $null unless you've customized your form layout.\n\n.PARAMETER pathToWitAdmin\n\nThe path to witadmin.exe. Defaults to 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\witadmin.exe'\n\n.EXAMPLE\n\nUpgrade-Template -project FabrikamFiber -baseTemplate Agile -pathToEpic '.\\Agile\\WorkItem Tracking\\TypeDefinitions\\Epic.xml'\n\n#&amp;gt;\n\nparam(\n    [string]$tpcUri = \"http://localhost:8080/tfs/defaultcollection\",\n\n    [Parameter(Mandatory=$true)]\n    [string]$project,\n\n    [Parameter(Mandatory=$true)]\n    [ValidateSet(\"Agile\", \"Scrum\", \"CMMI\")]\n    [string]$baseTemplate,\n\n    [Parameter(Mandatory=$true)]\n    [string]$pathToEpic,\n\n    [string]$layoutGroupToAddValueAreaControlTo = $null,\n\n    [string]$pathToWitAdmin = 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\witadmin.exe'\n)\n\nif (-not (Test-Path $pathToEpic)) {\n    Write-Error \"Epic WITD not found at $pathToEpic\"\n    exit 1\n}\n\nif ((Get-Alias -Name witadmin -ErrorAction SilentlyContinue) -eq $null) {\n    New-Alias witadmin -Value $pathToWitAdmin\n}\n\n$valueAreadFieldXml = '\n&amp;lt;FIELD name=\"Value Area\" refname=\"Microsoft.VSTS.Common.ValueArea\" type=\"String\"&amp;gt;\n    &amp;lt;REQUIRED /&amp;gt;\n    &amp;lt;ALLOWEDVALUES&amp;gt;\n        &amp;lt;LISTITEM value=\"Architectural\" /&amp;gt;\n        &amp;lt;LISTITEM value=\"Business\" /&amp;gt;\n    &amp;lt;/ALLOWEDVALUES&amp;gt;\n    &amp;lt;DEFAULT from=\"value\" value=\"Business\" /&amp;gt;\n    &amp;lt;HELPTEXT&amp;gt;Business = delivers value to a user or another system; Architectural = work to support other stories or components&amp;lt;/HELPTEXT&amp;gt;\n&amp;lt;/FIELD&amp;gt;'\n$valueAreaFieldFormXml = '&amp;lt;Control FieldName=\"Microsoft.VSTS.Common.ValueArea\" Type=\"FieldControl\" Label=\"Value area\" LabelPosition=\"Left\" /&amp;gt;'\n\n$epicCategoryXml = '\n&amp;lt;CATEGORY name=\"Epic Category\" refname=\"Microsoft.EpicCategory\"&amp;gt;\n  &amp;lt;DEFAULTWORKITEMTYPE name=\"Epic\" /&amp;gt;\n&amp;lt;/CATEGORY&amp;gt;'\n\n$epicBacklogXml = '\n    &amp;lt;PortfolioBacklog category=\"Microsoft.EpicCategory\" pluralName=\"Epics\" singularName=\"Epic\" workItemCountLimit=\"1000\"&amp;gt;\n      &amp;lt;States&amp;gt;\n        &amp;lt;State value=\"New\" type=\"Proposed\" /&amp;gt;\n        &amp;lt;State value=\"Active\" type=\"InProgress\" /&amp;gt;\n        &amp;lt;State value=\"Resolved\" type=\"InProgress\" /&amp;gt;\n        &amp;lt;State value=\"Closed\" type=\"Complete\" /&amp;gt;\n      &amp;lt;/States&amp;gt;\n      &amp;lt;Columns&amp;gt;\n        &amp;lt;Column refname=\"System.WorkItemType\" width=\"100\" /&amp;gt;\n        &amp;lt;Column refname=\"System.Title\" width=\"400\" /&amp;gt;\n        &amp;lt;Column refname=\"System.State\" width=\"100\" /&amp;gt;\n        &amp;lt;Column refname=\"Microsoft.VSTS.Scheduling.Effort\" width=\"50\" /&amp;gt;\n        &amp;lt;Column refname=\"Microsoft.VSTS.Common.BusinessValue\" width=\"50\" /&amp;gt;\n        &amp;lt;Column refname=\"Microsoft.VSTS.Common.ValueArea\" width=\"100\" /&amp;gt;\n        &amp;lt;Column refname=\"System.Tags\" width=\"200\" /&amp;gt;\n      &amp;lt;/Columns&amp;gt;\n      &amp;lt;AddPanel&amp;gt;\n        &amp;lt;Fields&amp;gt;\n          &amp;lt;Field refname=\"System.Title\" /&amp;gt;\n        &amp;lt;/Fields&amp;gt;\n      &amp;lt;/AddPanel&amp;gt;\n    &amp;lt;/PortfolioBacklog&amp;gt;'\n$epicColorXml = '&amp;lt;WorkItemColor primary=\"FFFF7B00\" secondary=\"FFFFD7B5\" name=\"Epic\" /&amp;gt;'\n\n#####################################################################\nfunction Add-Fragment(\n    [System.Xml.XmlNode]$node,\n    [string]$xml\n) {\n    $newNode = $node.OwnerDocument.ImportNode(([xml]$xml).DocumentElement, $true)\n    [void]$node.AppendChild($newNode)\n}\n\nfunction Add-ValueAreaField(\n    [string]$filePath,\n    [string]$controlGroup\n) {\n    $xml = [xml](gc $filePath)\n    # check if the field already exists\n    if (($valueAreaField = $xml.WITD.WORKITEMTYPE.FIELDS.ChildNodes | ? { $_.refname -eq \"Microsoft.VSTS.Common.ValueArea\" }) -ne $null) {\n        Write-Host \"Work item already has Value Area field\" -ForegroundColor Yellow\n    } else {\n        # add field to FIELDS\n        Add-Fragment -node $xml.WITD.WORKITEMTYPE.FIELDS -xml $valueAreadFieldXml\n\n        # add field to FORM\n        # find the \"Classification\" Group\n        $classificationGroup = (Select-Xml -Xml $xml -XPath \"//Layout//Group[@Label='$layoutGroupToAddValueAreaControlTo']\").Node\n        Add-Fragment -node $classificationGroup.Column -xml $valueAreaFieldFormXml\n\n        # upload definition\n        $xml.Save((gi $filePath).FullName)\n        witadmin importwitd /collection:$tpcUri /p:$project /f:$filePath\n    }\n}\n#####################################################################\n\n$defaultControlGroup = \"Classification\"\nswitch ($baseTemplate) {\n    \"Agile\" { $wit = \"User Story\" }\n    \"Scrum\" { $wit = \"Product Backlog Item\"; $defaultControlGroup = \"Details\" }\n    \"CMMI\" { $wit = \"Requirement\" }\n}\nif ($layoutGroupToAddValueAreaControlTo -ne $null) {\n    $defaultControlGroup = $layoutGroupToAddValueAreaControlTo\n}\n\nWrite-Host \"Exporting requirement work item type $wit\" -ForegroundColor Cyan\nwitadmin exportwitd /collection:$tpcUri /p:$project /n:$wit /f:\"RequirementItem.xml\"\n\nWrite-Host \"Adding 'Value Area' field to $wit\" -ForegroundColor Cyan\nAdd-ValueAreaField -filePath \".\\RequirementItem.xml\" -controlGroup $defaultControlGroup\n\nWrite-Host \"Exporting work item type Feature\" -ForegroundColor Cyan\nwitadmin exportwitd /collection:$tpcUri /p:$project /n:Feature /f:\"Feature.xml\"\n\nWrite-Host \"Adding 'Value Area' field to Feature\" -ForegroundColor Cyan\nAdd-ValueAreaField -filePath \".\\Feature.xml\" -controlGroup $defaultControlGroup\n\nif (((witadmin listwitd /p:$project /collection:$tpcUri) | ? { $_ -eq \"Epic\" }).Count -eq 1) {\n    Write-Host \"Process Template already contains an Epic work item type\" -ForegroundColor Yellow\n} else {\n    Write-Host \"Adding Epic\" -ForegroundColor Cyan\n    witadmin importwitd /collection:$tpcUri /p:$project /f:$pathToEpic\n}\n\nwitadmin exportcategories /collection:$tpcUri /p:$project /f:\"categories.xml\"\n$catXml = [xml](gc \"categories.xml\")\nif (($catXml.CATEGORIES.ChildNodes | ? { $_.name -eq \"Epic Category\" }) -ne $null) {\n    Write-Host \"Epic category already exists\" -ForegroundColor Yellow\n} else {\n    Write-Host \"Updating categories\" -ForegroundColor Cyan\n    Add-Fragment -node $catXml.CATEGORIES -xml $epicCategoryXml\n    $catXml.Save((gi \".\\categories.xml\").FullName)\n    witadmin importcategories /collection:$tpcUri /p:$project /f:\"categories.xml\"\n\n    Write-Host \"Updating ProcessConfig\" -ForegroundColor Cyan\n    witadmin exportprocessconfig /collection:$tpcUri /p:$project /f:\"processConfig.xml\"\n    $procXml = [xml](gc \"processConfig.xml\")\n\n    Add-Fragment -node $procXml.ProjectProcessConfiguration.PortfolioBacklogs -xml $epicBacklogXml\n    Add-Fragment -node $procXml.ProjectProcessConfiguration.WorkItemColors -xml $epicColorXml\n\n    $featureCat = $procXml.ProjectProcessConfiguration.PortfolioBacklogs.PortfolioBacklog | ? { $_.category -eq \"Microsoft.FeatureCategory\" }\n    $parentAttrib = $featureCat.OwnerDocument.CreateAttribute(\"parent\")\n    $parentAttrib.Value = \"Microsoft.EpicCategory\"\n    $featureCat.Attributes.Append($parentAttrib)\n\n    $procXml.Save((gi \".\\processConfig.xml\").FullName)\n    witadmin importprocessconfig /collection:$tpcUri /p:$project /f:\"processConfig.xml\"\n}\n\nWrite-Host \"Done!\" -ForegroundColor Green\n\n\nRunning the Script\n\nTo run the script, just make sure you’re a Team Project administrator and log in to a machine that has witadmin.exe on it. Then open Team Explorer, connect to your server, and click Settings. Then click “Process Template Manager” and download the new template (Agile, Scrum or CMMI) to a folder somewhere. You really only need the Epic work item WITD. Make a note of where the Epic.xml file ends up.\n\nThen you’re ready to run the script. You’ll need to supply:\n\n\n  (Optional) The TPC Uri (defaults is http://localhost:8080/tfs/defaultcollection)\n  The Team Project name\n  The path to the Epic.xml file\n  The name of the base template – either Agile, Scrum or CMMI\n  (Optional) The path to witadmin.exe (defaults to C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\witadmin.exe)\n  (Optional) The name of the group you want to add the “Value Area” field to on the form – default is “Classification”\n\n\nYou can run\n\n\nGet-Help .\\Upgrade-TemplateTo2015.ps1\n\n\nto get help and examples.\n\nBear in mind that this script is a “best-effort” – make sure you test it in a test environment before going gung-ho on your production server!\n\nResults\n\nAfter running the script. you’ll be able to create Epic work items:\n\n\n\n\nYou’ll be able to opt in/out of the Epic backlog in the Team Settings page:\n\n\n\n\nYou’ll see “Value Area” on your Features and “Requirements”:\n\n\n\n\nHappy upgrading!\n",
      "categories": [],
      "tags": ["tfsconfig","alm"],
      
      "collection": "posts",
      "url": "/enable-safe-features-in-existing-team-projects-after-upgrading-to-tfs-2015/"
    },{
      
      "title": "Release Management 2015 with Build vNext: Component to Artifact Name Matching and Other Fun Gotchas",
      "date": "2015-07-29 15:55:14 +0000",
      
      "content": "I’ve been setting up a couple of VMs in Azure with a TFS demo. Part of the demo is release management, and I finally got to upgrade Release Management to the 2015 release. I wanted to test integrating with the new build vNext engine. I faced some “fun” gotchas along the way. Here are my findings.\n\nFail: 0 artifacts(s) found\n\nAfter upgrading Release Management server, I gleefully associated a component with my build vNext build. I was happy when the build vNext builds appeared in the drop-down. Since I was using the root of the output folder, I simply selected “\\” as the location of the component (since I have several folders that I want to use via scripts, so I usually just specify the root of the drop folder).\n\nI then queued the release – and the deployment failed almost instantly. “0 artifact(s) found corresponding to the name ‘FabFiber’ for BuildId: 91”. After a bit of head-scratching and head-to-table-banging, I wondered if the error was hinting at the fact that RM is actually looking for a published artifact named “FabFiber” in my build. Turns out that was correct.\n\nComponent Names and Artifact Names\n\nTo make a long story short: you have to match the component name in Release Management with the artifact name in your build vNext “Publish Artifact” task. This may seem like a good idea, but for me it’s a pain, since I usually split my artifacts into Scripts, Sites, DBs etc. and publish each as a separate artifact so that I get a neat folder layout for my artifacts. Since I use PowerShell scripts to deploy, I used to specify the root folder “\\” for the component location and then used Scripts\\someScript.ps1 as the path to the script. So I had to go back to my build and add a PowerShell script to first put all the folders into a “root” folder for me and then use a single “Publish Artifacts” task to publish the neatly laid out folder structure. I looked at this post from my friend Ricci Gian Maria to get some inspiration!\n\nHere’s the script that I created and checked into source control:\n\nparam(\n    [string]$srcDir,\n    [string]$targetDir,\n    [string]$fileFilter = \"*.*\"\n)\n\nif (-not (Test-Path $targetDir)) {\n    Write-Host \"Creating $targetDir\"\n    mkdir $targetDir\n}\n\nWrite-Host \"Executing xcopy /y '$srcDir\\$fileFilter' $targetDir\"\nxcopy /y \"$srcDir\\$fileFilter\" $targetDir\n\nWrite-Host \"Done!\"\n\n\nNow I have a couple of PowerShell tasks that copy the binaries (and other files) in the staging directory – which I am using as the root folder for my artifacts. I configure the msbuild arguments to publish the website webdeploy package to $(build.stagingDirectory)\\FabFiber, so I don’t need to copy it, since it’s already in the staging folder. For the DB components and scripts:\n\n\n  I configure the copy scripts to copy my DB components (dacpacs and publish.xmls) so I need 2 scripts which have the following args respectively:\n  -srcDir MAIN\\FabFiber\\FabrikamFiber.CallCenter\\FabrikamFiber.Schema\\bin$(BuildConfiguration) -targetDir $(build.stagingDirectory)\\db -fileFilter *.dacpac\n  -srcDir MAIN\\FabFiber\\FabrikamFiber.CallCenter\\FabrikamFiber.Schema\\bin$(BuildConfiguration) -targetDir $(build.stagingDirectory)\\DB -fileFilter *.publish.xml\n  I copy the scripts folder directly from the workspace into the staging folder using these arguments:\n  -srcDir MAIN\\FabFiber\\DscScripts -targetDir $(build.stagingDirectory)\\Scripts\n  Finally, I publish the artifacts like so:\n\n\n\nNow my build artifact (yes, a single artifact) looks as follows:\n\n\n\n\nBack in Release Management, I made sure I had a component named “FabFiber” (to match the name of the artifact from the Publish Artifact task). I then also supplied “\\FabFiber” as the root folder for my components:\n\n\n\n\nThat at least cleared up the “cannot find artifact” error.\n\nA bonus of this is that you can now use server drops for releases instead of having to use shared folder drops. Just remember that if you choose to do this, you have to set up a ReleaseManagementShare folder. See this post for more details (see point 7). I couldn’t get this to work for some reason so I reverted to a shared folder drop on the build.\n\nRenaming Components Gotcha\n\nDuring my experimentation I renamed the component in Release Management that I was using in the release. This caused some strange behavior when trying to create releases: the build version picker was missing:\n\n\n\n\nI had to open the release template and set the component from the drop-down everywhere that it was referenced!\n\n\n\n\nOnce that was done, I got the build version picker back:\n\n\n\n\nDeployments started working again – my name is Boris, and I am invincible!\n\n\n\nThe Parameter is Incorrect\n\nA further error I encountered had to do with the upgrade from RM 2013. At least, I think that was the cause. The deployment would copy the files to the target server, but when the PowerShell task was invoked, I got a failure stating (helpfully – not!), “The parameter is incorrect.”\n\n\n\n\nAt first I thought it was an error in my script – turns out that all you have to do to resolve this one is re-enter the password in the credentials for the PowerShell task in the release template. All of them. Again. Sigh… Hopefully this is just me and doesn’t happen to you when you upgrade your RM server.\n\nConclusion\n\nI have to admit that I have a love-hate relationship with Release Management. It’s fast becoming more of a HATE-love relationship though. The single feature I see that it brings to the table is the approval workflow – the client is slow, the workflows are clunky and debugging is a pain.\n\nI really can’t wait for the release of Web-based Release Management that will use the same engine as the build vNext engine, which should mean a vastly simpler authoring experience! Also the reporting and charting features we should see around releases are going to be great.\n\nFor now, the best advice I can give you regarding Release Management is to make sure you invest in agent-less deployments using PowerShell scripts. That way your upgrade path to the new Web-based Release Management will be much smoother and you’ll be able to reuse your investments (i.e. your scripts).\n\nPerhaps your upgrade experiences will be happier than mine – I can only hope, dear reader, I can only hope.\n\nHappy releasing!\n",
      "categories": [],
      "tags": ["releasemanagement","build"],
      
      "collection": "posts",
      "url": "/release-management-2015-with-build-vnext-component-to-artifact-name-matching-and-other-fun-gotchas/"
    },{
      
      "title": "Azure Web Apps, Kudu Console and TcpPing",
      "date": "2015-08-07 03:32:53 +0000",
      
      "content": "I was working with a customer recently that put a website into Azure Web Apps. This site needed to connect to their backend databases (which they couldn’t move to Azure because legacy systems still needed to connect to it). We created an Azure VNet and configured site-to-site connectivity that created a secure connection between the Azure VNet and their on-premises network.\n\nWe then had to configure point-to-site connections for the VNet so that we could put the Azure Web App onto the VNet. This would (in theory) allow the website to access their on-premises resources such as the database. We also had to upgrade the site to Standard pricing in order to do this.\n\nWe had to reconfigure the site-to-site gateway to allow dynamic routing in order to do this, which meant deleting and recreating the gateway. A bit of a pain, but not too bad. We then configured static routing from the on-premises network to the point-to-site addresses on the VNet.\n\nPing from Azure Web App?\n\nOnce we had that all configured, we wanted to test connectivity. If we had deployed a VM, it would have been simple – just open a cmd prompt and ping away. However, we didn’t have a server, since we were deploying an Azure Web App. So initially we deployed a dummy Azure Web App onto the VNet to test the connection. This became a little bit of a pain. However, I remembered reading about Kudu and decided to see if that would be easier.\n\nKudu to the Rescue\n\nIf you browse to http://&lt;yoursite&gt;.scm.azurewebsites.net (where &lt;yoursite&gt; is the name of your Azure Web App) then you’ll see the Kudu site.\n\n\n\n\nOnce you’ve opened the Kudu site, you can do all sorts of interesting things (see this blog post and this Scott Hanselman and David Ebbo video). If you open the Debug console (you can go CMD or PowerShell) then you get to play! I opened the CMD console and typed “help” – to my surprise I got a list of commands I could run:\n\n\n\n\nUnfortunately I didn’t see anything that would help me with testing connectivity. However, I remembered that I had read somewhere about the command “tpcping”. So I tried it:\n\n\ntcpping &lt;enter&gt;\n\n\n\n\nLooks promising! Even better than the “ping” command, you can also test for a specific port, not just the ip address. So I want to test if my site can reach my database server on port 1443, no problem:\n\n\ntcpping 192.168.0.1:1443 &lt;enter&gt;\n\n\n\n\nHmm, seems that address isn’t working.\n\nAfter troubleshooting for a while, we managed to sort the problem and tcpping gave us a nice “Success” message, so we knew we were good to go. Kudu saved us a lot of time!\n\nHappy troubleshooting!\n",
      "categories": [],
      "tags": ["cloud"],
      
      "collection": "posts",
      "url": "/azure-web-apps-kudu-console-and-tcpping/"
    },{
      
      "title": "Build vNext and SonarQube Runner: Dynamic Version Script",
      "date": "2015-08-11 19:34:58 +0000",
      
      "content": "SonarQube is a fantastic tool for tracking technical debt, and it’s starting to make some inroads into the .NET world as SonarSource collaborates with Microsoft. I’ve played around with it a little to start getting my hands dirty.\n\nInstall Guidance\n\nIf you’ve never installed SonarQube before, then I highly recommend this eGuide. Just one caveat that wasn’t too clear: you need to create the database manually before running SonarQube for the first time. Just create an empty database (with the required collation) and go from there.\n\nIntegrating into TeamBuild vNext – with Dynamic Versioning\n\nOnce you’ve got the server installed and configured, you’re ready to integrate with TeamBuild. It’s easy enough using build VNext Command Line task. However, one thing bugged me as I was setting this up – hard-coding the version number. I like to version my assemblies from the build number on the build using a PowerShell script. Here’s the 2015 version (since the environment variable names have changed):\n\nParam(\n  [string]$pathToSearch = $env:BUILD_SOURCESDIRECTORY,\n  [string]$buildNumber = $env:BUILD_BUILDNUMBER,\n  [string]$searchFilter = \"AssemblyInfo.*\",\n  [regex]$pattern = \"\\d+\\.\\d+\\.\\d+\\.\\d+\"\n)\n \nif ($buildNumber -match $pattern -ne $true) {\n    Write-Error \"Could not extract a version from [$buildNumber] using pattern [$pattern]\"\n    exit 1\n} else {\n    try {\n        $extractedBuildNumber = $Matches[0]\n        Write-Host \"Using version $extractedBuildNumber in folder $pathToSearch\"\n \n        $files = gci -Path $pathToSearch -Filter $searchFilter -Recurse\n\n        if ($files){\n            $files | % {\n                $fileToChange = $_.FullName  \n                Write-Host \" -&amp;gt; Changing $($fileToChange)\"\n                \n                # remove the read-only bit on the file\n                sp $fileToChange IsReadOnly $false\n \n                # run the regex replace\n                (gc $fileToChange) | % { $_ -replace $pattern, $extractedBuildNumber } | sc $fileToChange\n            }\n        } else {\n            Write-Warning \"No files found\"\n        }\n \n        Write-Host \"Done!\"\n        exit 0\n    } catch {\n        Write-Error $_\n        exit 1\n    }\n}\n\n\nSo now that I get dll’s versions matching my build number, why not SonarQube too? So I used the same idea to wrap the “begin” call into a PowerShell script which can get the build number too:\n\nParam(\n  [string]$buildNumber = $env:BUILD_BUILDNUMBER,\n  [regex]$pattern = \"\\d+\\.\\d+\\.\\d+\\.\\d+\",\n  [string]$key,\n  [string]$name\n)\n \n$version = \"1.0\"\nif ($buildNumber -match $pattern -ne $true) {\n    Write-Verbose \"Could not extract a version from [$buildNumber] using pattern [$pattern]\" -Verbose\n} else {\n    $version = $Matches[0]\n}\n\nWrite-Verbose \"Using args: begin /v:$version /k:$key /n:$name\" -Verbose\n$cmd = \"MSBuild.SonarQube.Runner.exe\"\n\n&amp;amp; $cmd begin /v:$version /k:$key /n:$name\n\n\nI drop this into the same folder as the MsBuild.SonarQube.Runner.exe so that I don’t have to fiddle with more paths. Here’s the task in my build:\n\n\n\n\nThe call to the SonarQube runner “end” doesn’t need any arguments, so I’ve left that as a plain command line call:\n\n\n\n\nNow when the build runs, the version number passed to SonarQube matches the version number of my assemblies which I can tie back to my builds. Sweet!\n\n\n\n\nOne more change you could make is to specify the key and name arguments as variables. That way you can manage them as build variables instead of managing them in the call to the script on the task.\n\nFinally, don’t forget to install the Roslyn SonarQube SonarLint extension. This will give you the same analysis that SonarQube uses inside VS.\n\nHappy SonarQubing!\n",
      "categories": [],
      "tags": ["build","development"],
      
      "collection": "posts",
      "url": "/build-vnext-and-sonarqube-runner-dynamic-version-script/"
    },{
      
      "title": "Developing a Custom Build vNext Task: Part 2",
      "date": "2015-08-20 22:32:51 +0000",
      
      "content": "In part 1 I showed you how to scaffold a task using tfx-cli, how to customize the manifest and how to implement the PowerShell script for my VersionAssemblies task. In this post I’ll show you how I went about developing the Node version of the task and how I uploaded the completed task to my TFS server.\n\nVS Code\n\nI chose to use VS Code as the editor for my tasks. Partly because I wanted to become more comfortable using VS Code, and partly because the task doesn’t have a project file – it’s just some files in a folder – perfect for VS Code. If you’re unfamiliar with VS Code, then I highly recommend this great intro video by Chris Dias.\n\nRestructure and Initialize a Git Repo\n\nIt was time to get serious. I wanted to publish the task to a Git repo, so I decided to reorganize a little. I wanted the root of my repo to have a README.md file and then have a folder per task. Each task should also have a markdown file. So I created a folder called cols-agent-tasks and moved the VersionAssemblies task into a subfolder called Tasks. Then I initialized the Git repo.\n\nNext I right clicked on my cols-agent-tasks folder and selected “Open with Code” to open the folder. Here’s how it looked:\n\n\n\n\nSee the Git icon on the left? Clicking on it allows you to enter a commit message and commit. You can also diff files and undo changes. Sweet.\n\nInstalling Node Packages\n\nI knew that the VSO agent has a client library (vso-task-lib) from looking at the out of the box tasks in the vso-agent-tasks repo. I wanted to utilize that in my node task. The task lib is a node package, and so I needed to pull the package down from npm. So I opened up a PowerShell prompt and did an “npm init” to initialize a package.json file (required for npm) and walked through the wizard:\n\n\n\n\nSince I chose MIT for the license, I added a license file too.\n\nNow that the package.json file was initialized, I could run “npm install vso-task-lib –-save-dev” to install the vso-task-lib and save it as a dev dependency (since it’s not meant to be bundled with the completed task):\n\n\n\n\nThe command also installed the q and shelljs libraries (which are dependencies for the vso-task-lib). I also noticed that a node_modules folder had popped up (where the packages are installed) and double checked that my .gitignore was in fact ignoring this folder (since I don’t want these packages committed into my repo).\n\nTypeScript Definitions Using tsd\n\nI was almost ready to start diving into the code – but I wanted to make sure that VS Code gave me intellisense for the task library (and other libs). So I decided to play with tsd, a node TypeScript definition manager utility. I installed it by running “npm install tsd –g” (the –g is for global, since its a system-wide util). Once its installed, you can run “tsd” to see what you can do with the tool.\n\nI first ran “tsd query vso-task-lib” to see if there is a TypeScript definition for vso-agent-lib. The font color was a bit hard to see, but I saw “zero results”. Bummer – the definition isn’t on DefinitelyTyped yet. So what about q and shelljs? Both of them returned results, so I installed them (using the –-save option to save the definitions I’m using to a json file):\n\n\n\n\nOn disk I could see a couple new files:\n\n\n  The “typings” folder with the definitions as well as a “global” definition file, tsd.d.ts, including q and shelljs (and node, which is installed when you install the type definition for shelljs)\n  A tsd.json file in the root (which aggregates all the definitions)\n\n\nOpening the versionAssemblies.js file, I was able to see intellisense for q, shelljs and node:\n\n\n\n\nSo what about the vso-task-lib? Since there was no definition on definitely typed, I had to import it manually. I copied the vso-task-lib.d.ts from the Microsoft repo into a folder called vso-task-lib in the typings folder and then updated the tsd.d.ts file adding another reference to this definition file. I also ended up declaring the vso-task-lib (since it doesn’t actaully declare an export) so that the imports worked correctly. Here’s the tsd.d.ts file:\n\n/// &amp;lt;reference path=\"q/Q.d.ts\" /&amp;gt;\n/// &amp;lt;reference path=\"node/node.d.ts\" /&amp;gt;\n/// &amp;lt;reference path=\"vso-task-lib/vso-task-lib.d.ts\" /&amp;gt;\n/// &amp;lt;reference path=\"shelljs/shelljs.d.ts\" /&amp;gt;\n\ndeclare module \"vso-task-lib\" {\n    export = VsoTaskLib;\n}\n\n\nCoverting versionAssemblies.js to TypeScript\n\nThings were looking good! Now I wanted to convert the versionAssemblies.js file to TypeScript. I simply changed the extension from js to ts, and I was ready to start coding in TypeScript. But of course TypeScript files need to be transpiled to Javascript, so I hit “Ctrl-Shift-B” (muscle memory). To my surprise, VS Code informed me that there was “No task runner configured”.\n\n\n\n\nSo I clicked on “Configure Task Runner” and VS Code created a .settings folder with a tasks.json file. There were some boilerplate examples of how to configure the task runner. After playing around a bit, I settled on the second example – which supposedly runs TypeScript using a tfconfig.json in the root folder:\n\n{\n    \"version\": \"0.1.0\",\n\n    // The command is tsc. Assumes that tsc has been installed using npm install -g typescript\n    \"command\": \"tsc\",\n\n    // The command is a shell script\n    \"isShellCommand\": true,\n\n    // Show the output window only if unrecognized errors occur.\n    \"showOutput\": \"silent\",\n\n    // Tell the tsc compiler to use the tsconfig.json from the open folder.\n    \"args\": [\"-p\", \".\"],\n\n    // use the standard tsc problem matcher to find compile problems\n    // in the output.\n    \"problemMatcher\": \"$tsc\"\n}\n\n\nNow I had to create a tsconfig.json file in the root, which I did. VS Code knows what to do with json files, so I just opened a { and was pleasantly surprised with schema intellisense for this file!\n\n\n\n\nI configured the “compilerOptions” and set the module to “commonjs”. Now pressing “Ctrl-Shift-B” invokes the task runner which transpiles my TypeScript file to Javascript – I saw the .js file appear on disk. Excellent! Setting sourceMaps to true will provide source mapping so that I can later debug.\n\nOne caveat – the build doesn’t automatically happen when you save a TypeScript file. You can configure gulp and then enable a watch so that when you change a TypeScript file the build kicks in – but I decided that was too complicated for this project. I just configured the keyboard shortcut “Ctrl-s” to invoke “workbench.action.tasks.build” in addition to saving the file (you can configure keyboard shortcuts by clicking File-&gt;Preferences-&gt;Keyboard Shortcuts. Surprise surprise it’s a json file…)\n\nImplementing the Build Task\n\nEverything I’d done so far was just setup stuff. Now I was ready to actually code the task!\n\nHere’s the complete script:\n\nimport * as tl from 'vso-task-lib';\nimport * as sh from 'shelljs';\n\ntl.debug(\"Starting Version Assemblies step\");\n\n// get the task vars\nvar sourcePath = tl.getPathInput(\"sourcePath\", true, true);\nvar filePattern = tl.getInput(\"filePattern\", true);\nvar buildRegex = tl.getInput(\"buildRegex\", true);\nvar replaceRegex = tl.getInput(\"replaceRegex\", false);\n\n// get the build number from the env vars\nvar buildNumber = tl.getVariable(\"Build.BuildNumber\");\n\ntl.debug(`sourcePath :${sourcePath}`);\ntl.debug(`filePattern : ${filePattern}`);\ntl.debug(`buildRegex : ${buildRegex}`);\ntl.debug(`replaceRegex : ${replaceRegex}`);\ntl.debug(`buildNumber : ${buildNumber}`);\n\nif (replaceRegex === undefined || replaceRegex.length === 0){\n    replaceRegex = buildRegex;\n}\ntl.debug(`Using ${replaceRegex} as the replacement regex`);\n\nvar buildRegexObj = new RegExp(buildRegex);\nif (buildRegexObj.test(buildNumber)) {\n    var versionNum = buildRegexObj.exec(buildNumber)[0];\n    console.info(`Using version ${versionNum} in folder ${sourcePath}`);\n    \n    // get a list of all files under this root\n    var allFiles = tl.find(sourcePath);\n\n    // Now matching the pattern against all files\n    var filesToReplace = tl.match(allFiles, filePattern, { matchBase: true });\n    \n    if (filesToReplace === undefined || filesToReplace.length === 0) {\n        tl.warning(\"No files found\");\n    } else {\n        for(var i = 0; i &amp;lt; filesToReplace.length; i++){\n            var file = filesToReplace[i];\n            console.info(` -&amp;gt; Changing version in ${file}`);\n            \n            // replace all occurrences by adding g to the pattern\n            sh.sed(\"-i\", new RegExp(replaceRegex, \"g\"), versionNum, file);\n        }\n        console.info(`Replaced version in ${filesToReplace.length} files`);\n    }\n} else {\n    tl.warning(`Could not extract a version from [${buildNumber}] using pattern [${buildRegex}]`);\n}\n\ntl.debug(\"Leaving Version Assemblies step\");\n\n\nNotes:\n\n\n  Lines 1-2: import the library references\n  Line 4: using the task library to log to the console\n  Lines 6-13: using the task library to get the inputs (matching names from the task.json file) as well as getting the build number from the environment\n  Lines 15-19: more debug logging\n  Lines 21-23: default the replace regex to the build regex if the value is empty\n  Line 26: compile the regex pattern into a regex object\n  Line 27: test the build number to see if we can extract a version number using the regex pattern\n  Lines 28-29: extract the version number from the build number and write the value to the console\n  Line 32: get a list of all files in the sourcePath (recursively) using the task library method\n  Line 35: filter the files to match the filePattern input, again using a task library method\n  Lines 37-38: check if there are files that match – warn if there aren’t any\n  Line 40: for each file that matches,\n  Lines 41-45: use shelljs’s sed() method to do the regex replacement inline\n  Line 45: I use the “g” option when compiling the regex to indicate that all matches should be replaced (as opposed to just the 1st match)\n  Line 47: log to the console how many files were updated\n  The remainder is just logging\n\n\nUsing the task library really made developing the task straightforward. The setup involved in getting intellisense to work was worth the effort!\n\nDebugging from VS Code\n\nNow that I had the code written, I wanted to test it. VS Code to the rescue again! Click on the “debug” icon on the left, and then click the gear icon at the top of the debug pane:\n\n\n\n\nThat creates a new file called launch.json in the .settings folder. What – a json file? Who would have guessed! Here’s my final file:\n\n{\n    \"version\": \"0.1.0\",\n    // List of configurations. Add new configurations or edit existing ones.\n    // ONLY \"node\" and \"mono\" are supported, change \"type\" to switch.\n    \"configurations\": [\n        {\n            // Name of configuration; appears in the launch configuration drop down menu.\n            \"name\": \"Launch versionAssemblies.js\",\n            // Type of configuration. Possible values: \"node\", \"mono\".\n            \"type\": \"node\",\n            // Workspace relative or absolute path to the program.\n            \"program\": \"Tasks/VersionAssemblies/versionAssemblies.ts\",\n            // Automatically stop program after launch.\n            \"stopOnEntry\": false,\n            // Command line arguments passed to the program.\n            \"args\": [],\n            // Workspace relative or absolute path to the working directory of the program being debugged. Default is the current workspace.\n            \"cwd\": \".\",\n            // Workspace relative or absolute path to the runtime executable to be used. Default is the runtime executable on the PATH.\n            \"runtimeExecutable\": null,\n            // Optional arguments passed to the runtime executable.\n            \"runtimeArgs\": [\"--nolazy\"],\n            // Environment variables passed to the program.\n            \"env\": { \n                \"BUILD_BUILDNUMBER\": \"1.0.0.5\",\n                \"INPUT_SOURCEPATH\": \"C:\\\\data\\\\ws\\\\col\\\\ColinsALMCornerCheckinPolicies\",\n                \"INPUT_FILEPATTERN\": \"AssemblyInfo.*\",\n                \"INPUT_BUILDREGEX\": \"\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+\",\n                \"INPUT_REPLACEREGEX\": \"\"\n            },\n            // Use JavaScript source maps (if they exist).\n            \"sourceMaps\": true,\n            // If JavaScript source maps are enabled, the generated code is expected in this directory.\n            \"outDir\": \".\"\n        },\n        {\n            \"name\": \"Attach\",\n            \"type\": \"node\",\n            // TCP/IP address. Default is \"localhost\".\n            \"address\": \"localhost\",\n            // Port to attach to.\n            \"port\": 5858,\n            \"sourceMaps\": false\n        }\n    ]\n}\n\n\nThe changes I made have been highlighted. I changed the name and program settings. I also added some environment variables to simulate the values that the build agent is going to pass into the task. Finally, I changed “sourceMaps” to true and the output dir to “.” so that I could debug my TypeScript files. Now I just press F5:\n\n\n\n\nThe debugger is working – but my code isn’t! Looks like I’m missing a node module – minimatch. No problem – just run “npm install minimatch -–save-dev” to add the module in and run again. Another module not found – this time shelljs. Run “npm install shelljs –-save-dev” and start again. Success! I can see watches in the left window, hover over variables to see their values, and start stepping through my code.\n\n\n\n\nMy code ended up being perfect. Just kidding – I had to sort out some errors, but at least debugging made it a snap.\n\nUploading the Task\n\nIn part 1 I introduced tfx-cli. I now returned to the command line in order to test uploading the task. I changed to the cols-agent-tasks\\Tasks directory and ran\n\n\ntfx-cli build tasks upload .\\VersionAssemblies\n\n\nI got a success, and so now I could test it in a build!\n\nTesting a Windows Build\n\nTesting the windows build was fairly simple. I opened up an existing hosted build and replaced the PowerShell task that called my original PowerShell version assemblies task, and added in a brand new shiny “VersionAssemblies” task:\n\n\n\nThe run worked perfectly too – I was able to see the version change in the build output. Just a tip – setting “system.debug” to “true” in the build variables caused the task to log verbose.\n\n\n\nTesting a Linux build using Docker\n\nNow I wanted to test the task in a Linux build. I’ve installed a couple of Ubuntu vms before, so I was prepared to spin one up when I came across an excellent post by my friend and fellow ALM MVP Rene van Osnabrugge. Rene shows how you can quickly spin up a cross-platform build agent in a docker container – and even provides the Dockerfile to be able to do it in 1 line! The timing was perfect – I downloaded Docker Toolbox and installed a docker host (I couldn’t get the Hyper-V provider to work, so I had to resort to VirtualBox), then grabbed Rene’s Dockerfile and in no time at all I had a build agent on Ubuntu ready to test!\n\nHere’s my x-plat agent in the default pool:\n\n\n\n\nNote the Agent.OS “capability”. In order to target this agent, I’m going to add it as a demand for the build:\n\n\n\n\nHere’s the successful run:\n\n\n\nI committed, pushed to Github and now I can rest my weary brain!\n\nConclusion\n\nCreating a custom task is, if not simple, at least easy. The agent architecture has been well thought out and overall custom task creation is a satisfying process, both for PowerShell and for Node. I look forward to seeing what custom tasks start creeping out of the woodwork. Hopefully task designers will follow Microsoft’s lead and make them open source.\n\nHappy building!\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/developing-a-custom-build-vnext-task-part-2/"
    },{
      
      "title": "Developing a Custom Build vNext Task: Part 1",
      "date": "2015-08-20 22:34:07 +0000",
      
      "content": "I love the new build engine in VSO / TFS 2015. You can get pretty far with the out of the box tasks, but there are cases where a custom task improves the user experience. The “Microsoft” version of this is SonarQube integration – you can run the SonarQube MSBuild Runner by using a “Command Line” task and calling the exe. However, there are two tasks on the Microsoft Task Github repo that clean up the experience a little – SonarQube PreBuild and SonarQube PostTest. A big benefit of the tasks is that they actually “wrap” the exe within the task, so you don’t need to install the runner on the build machine yourself.\n\nOne customization I almost always make in my customers’ build processes is to match binary versions to the build number. In TFS 2012, this required a custom windows workflow task – a real pain to create and maintain. In 2013, you could enable it much more easily by invoking a PowerShell script. The same script can be invoked in Build vNext by using a PowerShell task.\n\nThe only down side to this is that the script has to be in source control somewhere. If you’re using TFVC, then this isn’t a problem, since all your builds (within a Team Project Collection) can use the same script. However, for Git repos it’s not so simple – you’re left with dropping the script into a known location on all build servers or committing the script to each Git repo you’re building. Neither option is particularly appealing. However, if we put the script “into” a custom build task for Build vNext, then we don’t have to keep the script anywhere else!\n\nTL;DR\n\nI want to discuss creating a task in some detail, so I’m splitting this into two posts. This post will look at scaffolding a task and then customizing the manifest and PowerShell implementation. In the next post I’m going to show the node implementation (along with some info on developing in TypeScript and VS Code) and how to upload the task.\n\nIf you just want the task, you can get the source at this repo.\n\nCreate a Custom Task\n\nIn order to create a new task, you need to supply a few things: a (JSON) manifest file, an icon and either a PowerShell or Node script (or both). You can, of course, create these by hand – but there’s an easier way to scaffold the task: tfx-cli. tfx-cli is a cross-platform command line utility that you can use to manage build tasks (including creating, deleting, uploading and listing). You’ll need to install both node and npm before you can install tfx-cli.\n\ntfx login\n\nOnce tfx-cli is installed, you should be able to run “tfx” and see the help screen.\n\n\n\n\nYou could authenticate each time you want to perform a command, but it will soon get tedious. It’s far better to cache your credentials.\n\nFor VSO, it’s simple. Log in to VSO and get a Personal Access Token (pat). When you type “tfx login” you’ll be prompted for your VSO url and your pat. Easy as pie.\n\nFor TFS 2015, it’s a little more complicated. You need to first enable basic authentication on your TFS app tier’s IIS. Then you can log in using your windows account (note: the tfx-cli team is working on ntfs authentication, so this is just a temporary hack).\n\nHere are the steps to enable basic auth on IIS:\n\n\n  Open Server Manager and make sure that the Basic Auth feature is installed (under the Security node)\n\n  If you have to install it, then you must reboot the machine before continuing\n  Open IIS and find the “Team Foundation Server” site and expand the node. Then click on the “tfs” app in the tree and double-click the “Authentication” icon in the “Features” view to open the authentication settings for the app.\n\n  Enable “Basic Authentication” (note the warning!)\n\n  Restart IIS\n\n\nDANGER WILL ROBINSON, DANGER! This is insecure since the passwords are sent in plaintext. You may want to enable https so that the channel is secure.\n\ntfx build tasks create\n\nOnce login is successful, you can run “tfx build tasks create” – you’ll be prompted for some basic information, like the name, description and author of the task.\n\n\n&gt;&gt; tfx build tasks createCopyright Microsoft CorporationEnter short name &gt; VersionAssembliesEnter friendly name &gt; Version AssembliesEnter description &gt; Match version assemblies to build numberEnter author &gt; Colin Dembovsky\n\n\nThat creates a folder (with the same name as the “short name”) that contains four files:\n\n\n  task.json – the json manifest file\n  VersionAssemblies.ps1 – the PowerShell implementation of the task\n  VersionAssemblies.js – the node implementation of the task\n  icon.png – the generic icon for the task\n\n\nCustomizing the Task Manifest\n\nThe first thing you’ll want to do after getting the skeleton task is edit the manifest file. Here you’ll set things like:\n\n\n  demands – a list of demands that must be present on the agent in order to run the task\n  visibility – should be “Build” or “Release” or both, if the task can be used in both builds and releases\n  version – the version number of your task\n  minimumAgentVersion – the minimum agent version this task requires\n  instanceNameFormat – this is the string that appears in the build tasks list once you add it to a build. It can be formatted to use any of the arguments that the task uses\n  inputs – input variables\n  groups – used to group input variables together\n  execution – used to specify the entry points for either Node or PowerShell (or both)\n  helpMarkDown – the markdown that is displayed below the task when added to a build definition\n\n\nInputs and Groups\n\nThe inputs all have the following properties:\n\n\n  name – reference name of the input. This is the name of the input that is passed to the implementation scripts, so choose wisely\n  type – type of input. Types include “pickList”, “filePath” (which makes the control into a source folder picker) and “string”\n  label – the input label that is displayed to the user\n  defaultValue – a default value (if any)\n  required – true or false depending on whether the input is mandatory or not\n  helpMarkDown – the markdown that is displayed when the user clicks the info icon next to the input\n  groupName – specify the name of the group (do not specify if you want the input to be outside a group)\n\n\nThe groups have the following format:\n\n\n  name – the group reference name\n  displayName – the name displayed on the UI\n  isExpanded – set to true for an open group, false for a closed group\n\n\nAnother note: the markdown needs to be on a single line (since JSON doesn’t allow multi-line values) – so if your help markdown is multi-line, you’ll have to replace line breaks with ‘\\n’.\n\nOf course, browsing the tasks on the Microsoft vso-agent-tasks repo lets you see what types are available, how to structure the files and so on.\n\nVersionAssembly Manifest\n\nFor the version assembly task I require a couple of inputs:\n\n\n  The path to the root folder where we start searching for files\n  The file pattern to match – any file in the directory matching the pattern should have the build version replaced\n  The regex to use to extract a version number from the build number (so if the build number is MyBuild_1.0.0.3, then we need regex to get 1.0.0.3)\n  The regex to use for the replacement in the files – I want this under advanced, since most of the time this is the same as the regex specified previously\n\n\nI also need the build number – but that’s an environment variable that I will get within the task scripts (as we’ll see later).\n\nHere’s the manifest file:\n\n{\n  \"id\": \"5b4d14d0-3868-11e4-a31d-3f0a2d8202f4\",\n  \"name\": \"VersionAssemblies\",\n  \"friendlyName\": \"Version Assemblies\",\n  \"description\": \"Updates the version number of the assemblies to match the build number\",\n  \"author\": \"Colin Dembovsky (colinsalmcorner.com)\",\n  \"helpMarkDown\": \"## Settings\\nThe task requires the following settings:\\n\\n1. **Source Path** : path to the sources that contain the version number files (such as AssemblyInfo.cs).\\n2. **File Pattern** : file pattern to search for within the `Source Path`. Defaults to 'AssemblyInfo.*'\\n3. **Build Regex Pattern** : Regex pattern to apply to the build number in order to extract a version number. Defaults to `\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+`.\\n4. **(Optional) Regex Replace Pattern**: Use this if the regex to search for in the target files is different from the Build Regex Pattern.\\n\\n## Using the Task\\nThe task should be inserted before any build tasks.\\n\\nAlso, you must customize the build number format (on the General tab of the build definition) in order to specify a format in such a way that the `Build Regex Pattern` can extract a build number from it. For example, if the build number is `1.0.0$(rev:.r)`, then you can use the regex `\\\\d+\\\\.\\\\d+\\\\.\\\\d\\\\.\\\\d+` to extract the version number.\\n\",\n  \"category\": \"Build\",\n  \"visibility\": [\n    \"Build\"\n  ],\n  \"demands\": [],\n  \"version\": {\n    \"Major\": \"0\",\n    \"Minor\": \"1\",\n    \"Patch\": \"1\"\n  },\n  \"minimumAgentVersion\": \"1.83.0\",\n  \"instanceNameFormat\": \"Version Assemblies using $(filePattern)\",\n  \"groups\": [\n    {\n      \"name\": \"advanced\",\n      \"displayName\": \"Advanced\",\n      \"isExpanded\": false\n    }\n  ],\n  \"inputs\": [\n    {\n      \"name\": \"sourcePath\",\n      \"type\": \"filePath\",\n      \"label\": \"Source Path\",\n      \"defaultValue\": \"\",\n      \"required\": true,\n      \"helpMarkDown\": \"Path in which to search for version files (like AssemblyInfo.* files).\" \n    },\n    {\n      \"name\": \"filePattern\",\n      \"type\": \"string\",\n      \"label\": \"File Pattern\",\n      \"defaultValue\": \"AssemblyInfo.*\",\n      \"required\": true,\n      \"helpMarkDown\": \"File filter to replace version info. The version number pattern should exist somewhere in the file.\"\n    },\n    {\n      \"name\": \"buildRegex\",\n      \"type\": \"string\",\n      \"label\": \"Build Regex Pattern\",\n      \"defaultValue\": \"\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+\",\n      \"required\": true,\n      \"helpMarkDown\": \"Regular Expression to extract version from build number. This is also the default replace regex (unless otherwise specified in Advanced settings).\"\n    },\n    {\n      \"name\": \"replaceRegex\",\n      \"type\": \"string\",\n      \"label\": \"Regex Replace Pattern\",\n      \"defaultValue\": \"\",\n      \"required\": false,\n      \"helpMarkDown\": \"Regular Expression to replace with in files. Leave blank to use the Build Regex Pattern.\",\n      \"groupName\": \"advanced\"\n    }\n  ],\n  \"execution\": {\n    \"Node\": {\n      \"target\": \"versionAssemblies.js\",\n      \"argumentFormat\": \"\"\n    },  \n    \"PowerShell\": {\n      \"target\": \"$(currentDirectory)\\\\VersionAssemblies.ps1\",\n      \"argumentFormat\": \"\",\n      \"workingDirectory\": \"$(currentDirectory)\"\n    }\n  }\n}\n\n\nThe PowerShell Script\n\nSince I am more proficient in PowerShell that in Node, I decided to tackle the PowerShell script first. Also, I have a script that does this already! You can see the full script in my Github repo – but here’s the important bit – the parameters declaration:\n\n[CmdletBinding(DefaultParameterSetName = 'None')]\nparam(\n    [string][Parameter(Mandatory=$true)][ValidateNotNullOrEmpty()] $sourcePath,\n    [string][Parameter(Mandatory=$true)][ValidateNotNullOrEmpty()] $filePattern,\n    [string][Parameter(Mandatory=$true)][ValidateNotNullOrEmpty()] $buildRegex,\n    [string]$replaceRegex,\n    [string]$buildNumber = $env:BUILD_BUILDNUMBER\n)\n\n\nNotes:\n\n\n  Line 3-5: these are the mandatory inputs. The name of the argument is the same as the name property of the inputs from the manifest file\n  Line 6: the optional input (again with the name matching the input name in the manifest)\n  Line 7: the build number is passed into the execution context as a predefined variable which is set in the environment, which I read here\n\n\nWhile any of the predefined variables can be read anywhere in the script, I like to put the value as the default for a parameter. This makes debugging the script (executing it outside of the build environment) so much easier, since I can invoke the script and pass in the value I want to test with (as opposed to first setting an environment variable before I call the script).\n\nOnce I had the inputs (and the build number) I just pasted the existing script. I’ve included lots of “Write-Verbose –Verbose” calls so that if you set “system.debug” to “true” in your build variables, the task spits out some diagnostics. Write-Host calls end up in the console when the build is running.\n\nWrap up\n\nIn this post I covered how to use tfx-cli to scaffold a task, then customize the manifest and implement a PowerShell script.\n\nIn the next post I’ll show you how to write the node implementation of the task, using TypeScript and VS Code. I’ll also show you how to upload the task and use it in a build.\n\nHappy customizing!\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/developing-a-custom-build-vnext-task-part-1/"
    },{
      
      "title": "Continuous Deployment with Docker and Build vNext",
      "date": "2015-09-18 16:59:39 +0000",
      
      "content": "I really like the idea of Docker. If you’re unfamiliar with Docker, then I highly recommend Nigel Poulton’s Docker Deep Dive course on Pluralsight. Containers have been around for quite a while in the Linux world, but Microsoft is jumping on the bandwagon with Windows Server Containers too. This means that getting to grips with containers is a good idea – I think it’s the way of the future.\n\ntl;dr\n\nIf you’re just after the task, then go to my Github repo. You can get some brief details in the section below titled “Challenge 2: Publishing to Docker (a.k.a The Publish Build Task)”. If you want the full story read on!\n\nDeploying Apps to Containers\n\nAfter hacking around a bit with containers, I decided to see if I could deploy some apps into a container manually. Turns out it’s not too hard. You need to have (at least) the Docker client, some code and a Dockerfile. Then you can just call a “docker build” (which creates an image) and then “docker run” to deploy an instance of the image.\n\nOnce I had that working, I wanted to see if I could bundle everything up into a build in Build vNext. That was a little harder to do.\n\nEnvironment and Tools\n\nYou can run a Docker host in Azure but I wanted to be able to work locally too. So here is how I set up my local environment (on my Surface Pro 3):\n\n\n  I enabled Hyper-V (you can also use VirtualBox) so that I can run VMs\n  I installed Docker Toolbox (note: Docker Toolbox bundles VirtualBox – so you can use that if you don’t have or want Hyper-V, but otherwise, don’t install it)\n  I then created a Docker host using “docker-machine create”. I used the hyper-v driver. This creates a Tiny Core Linux Docker host running the boot2docker image.\n  I set my environment variables to default to my docker host settings\n  I could now execute “docker” commands – a good command to sanity check your environment is “docker info”\n\n\nAside: PowerShell to Set Docker Environment\n\nI love using PowerShell. If you run “docker env” you get some settings that you could just “cat” to your profile (if you’re in Unix). However, the commands won’t work in PowerShell. So I created a small function that I put into my $PROFILE that I can run whenever I need to do any Docker stuff. Here it is:\n\nfunction Set-DockerEnv {\n    Write-Host \"Getting docker environment settings\" -ForegroundColor Yellow\n    docker-machine env docker | ? { $_.Contains('export') } | % { $_.Replace('export ', '') } | `\n        ConvertFrom-Csv -Delimiter \"=\" -Header \"Key\",\"Value\" | % { \n            [Environment]::SetEnvironmentVariable($_.Key, $_.Value)\n            Write-Host \"$($_.Key) = $($_.Value)\" -ForegroundColor Gray\n        }\n    Write-Host \"Done!\" -ForegroundColor Green\n}\n\n\nNow I can just run “Set-DockerEnv” whenever I need to set the environment.\n\nVS Tools for Docker\n\nSo I have a Docker environment locally – great. Now I need to be able to deploy something into a container! Since I (mostly) use Visual Studio 2015, I installed the VS Tools for Docker extension. Make sure you follow the install instructions carefully – the preview toolset is a bit picky. I wanted to play with Windows containers, but for starters I was working with Unix containers, so I needed some code that could run on Unix. Fortunately, ASP.NET 5 can! So I did a File –&gt; New Project and created an ASP.NET 5 Web application (this is a boilerplate MVC 6 application). Once I had the project created, I right-clicked the project and selected “Publish” to see the publish page. You’ll see the “Docker Containers” target:\n\n\n\n\nYou can select an Azure Docker VM if you have one – in my case I wanted to deploy locally, so I checked “Custom Docker Host” and clicked OK. I entered in the server url for my local docker host (tcp://10.0.0.19:2376) and left all the rest as default. Clicking “Validate Connection” however, failed. After some trial and error, I realized that the default directory for certificates for the “docker-machine” command I used is different to the default directory the VS Tools for Docker expects. So I just supplied additional settings for “Auth Options” and voila – I could now validate the connection:\n\n\n\n\nHere are the settings for “Auth Options” in full:\n\n--tls --tlscert=c:\\users\\colin\\.docker\\machine\\certs\\cert.pem --tlskey=c:\\users\\colin\\.docker\\machine\\certs\\key.pem\n\n\nI specifically left the Dockerfile setting to (auto generate) to see what I would get. Here’s what VS generated:\n\nFROM microsoft/aspnet:1.0.0-beta6\n\nADD . /app\n\nWORKDIR /app\n\nENTRYPOINT [\"./kestrel\"]\n\n\nNotes:\n\n\n  The FROM tells Docker which image to base this container on – it’s defaulting to the official image for ASP.NET 5 from Docker hub (the public Docker image repository)\n  ADD is copying all the files in the current directory (.) to a folder on the container called “/app”\n  WORKDIR is changing directory into “/app”\n  ENTRYPOINT tells Docker to run this command every time a container based on this image is fired up\n\n\nAside: Retargeting OS\n\nOnce you’ve generated the Dockerfile, you need to be careful if you want to deploy to a different OS (specifically Windows vs non-Windows). Rename the Dockerfile (in the root project directory) to “Docker.linux” or something and then clear the Dockerfile setting. VS will then auto generate a Dockerfile for deploying to Windows containers. Here’s the Windows flavored Dockerfile, just for contrast:\n\nFROM windowsservercore\n\nADD . /app\n\nWORKDIR /app\n\nENTRYPOINT [\"cmd.exe\", \"/k\", \"web.cmd\"]\n\n\nVS is even smart enough to nest your Dockerfiles in your solution!\n\n\n\n\nSo I could now publishing successfully from VS. Next up: deploying from Team Build!\n\nDocker Publish from Team Build vNext\n\n(Just to make things a little simple, I going to use “build” interchangeably with “build vNext” or even “team build”. I’ve switched over completely from the old XAML builds – so should you).\n\nIf you’ve looked at the build tasks on VSO, you’ll notice that there is a “Docker” task:\n\n\n\n\nIt’s a little unsatisfying, to be blunt. You can (supposedly) deploy a docker image – but there’s no way to get your code into the image (“docker build” the image) in the first place. Secondly, there doesn’t appear to be any security or advanced settings anywhere. Clicking “More Information” takes you to a placeholder markdown file – so no help there. Admittedly the team are still working on the “official” Docker tasks – but I didn’t want to wait!\n\nPrep: PowerShell Docker Publishing from the console\n\nTaking a step back and delving into the files and scripts that VS Tools for Docker generated for me, I decided to take a stab at deploying using the PowerShell script in the PublishProfiles folder of my solution. I created a publish profile called “LocalDocker”, and sure enough VS had generated 3 files: the pubxml file (settings), a PowerShell publish file and a shell script publish file.\n\n\n\n\nTo invoke the script, you need 3 things:\n\n\n  The path to the files that are going to be published\n  The path to the pubxml file (contains the settings for the publish profile)\n  (Optional) A hashtable of overrides for your settings\n\n\nI played around with the PowerShell script in my console – the first major flaw in the script is that it assumes you have already “packed” the project. So you first have to invoke msbuild with some obscure parameters. Only then can you invoke the Publish script. Also, the publish script does some hocus pocus with assuming that the script name and the pubxml file name are the same and working out the Dockerfile location is also a bit voodoo. It works nicely when you’re publishing from VS – but I found it not to be very “build friendly”.\n\nI tried it in build vNext anyway. I managed to invoke the “LocalDocker-publish.ps1” file, but could not figure out how to pass a hashtable to a PowerShell task (to override settings)! Besides, even if it worked, there’d be a lot of typing and you have to know what the keys are for each setting. Enter the custom build task!\n\nThe way I saw it, I had to:\n\n\n  Compile the solution in such a way that it can be deployed into a Docker container\n  Create a custom task that could invoke the PowerShell publish script, either from a pubxml file or some settings (or a combination)\n\n\nChallenge 1: Building ASP.NET 5 Apps in Build vNext\n\nBuilding ASP.NET 5 applications in build vNext isn’t as simple as you would think. I managed to find part of my answer in this post. You have to ensure that the dnvm is installed and that you have the correct runtimes. Then you have to invoke “dnu restore” on all the projects (to install all the dependencies). That doesn’t get you all the way though – you need to also “dnu pack” the project.\n\nFortunately, you can invoke a simple script to get the dnvm and install the correct runtime. Then you can fight with supply some parameters to msbuild that tell it to pack the project for you. This took me dozens of iterations to get right – and even when I got it right, the “kestrel” command in my containers was broken. For a long time I thought that my docker publish task was broken – turns out I could fix it with a pesky msbuild argument. Be that as it may, my pain is your gain – hence this post!\n\nYou need to add this script to your source repo and invoke it in your build using a PowerShell task (note this is modified from the original in this post):\n\nparam (\n    [string]$srcDir = $env.BUILD_SOURCESDIRECTORY\n)\n\n# bootstrap DNVM into this session.\n&amp;amp;{$Branch='dev';iex ((new-object net.webclient).DownloadString('https://raw.githubusercontent.com/aspnet/Home/dev/dnvminstall.ps1'))}\n\n# load up the global.json so we can find the DNX version\n$globalJsonFile = (Get-ChildItem -Path $srcDir -Filter \"global.json\" -Recurse | Select -First 1).FullName\n$globalJson = Get-Content -Path $globalJsonFile -Raw -ErrorAction Ignore | ConvertFrom-Json -ErrorAction Ignore\n\nif($globalJson)\n{\n    $dnxVersion = $globalJson.sdk.version\n}\nelse\n{\n    Write-Warning \"Unable to locate global.json to determine using 'latest'\"\n    $dnxVersion = \"latest\"\n}\n\n# install DNX\n# only installs the default (x86, clr) runtime of the framework.\n# If you need additional architectures or runtimes you should add additional calls\n&amp;amp; $env:USERPROFILE\\.dnx\\bin\\dnvm install $dnxVersion -r coreclr\n&amp;amp; $env:USERPROFILE\\.dnx\\bin\\dnvm install $dnxVersion -Persistent\n\n # run DNU restore on all project.json files in the src folder including 2&amp;gt;1 to redirect stderr to stdout for badly behaved tools\nGet-ChildItem -Path $srcDir -Filter project.json -Recurse | ForEach-Object {\n    Write-Host \"Running dnu restore on $($_.FullName)\"\n    &amp;amp; dnu restore $_.FullName 2&amp;gt;1\n}\n\n\nNotes:\n\n\n  Lines 9,10: I had to change the way that the script searched for the “global.json” (where the runtime is specified). So I’m using BUILD_SOURCESDIRECTORY which the build engine passes in.\n  Line 25: I added the coreclr (since I wanted to deploy to Linux)\n  Lines 29-32: Again I changed the search path for the “dnu” commands\n\n\nJust add a PowerShell task into your build (before the VS Build task) and browse your repo to the location of the script:\n\n\n\n\nNow you’re ready to call msbuild. In the VS Build task, set the solution to the xproj file (the project file for the ASP.NET project you want to publish). Then add the following msbuild arguments:\n\n/t:Build,FileSystemPublish /p:IgnoreDNXRuntime=true /p:PublishConfiguration=$(BuildConfiguration) /p:PublishOutputPathNoTrailingSlash=$(Build.StagingDirectory)\n\n\nYou’re telling msbuild:\n\n\n  Run the build and FileSystemPublish targets\n  Ignore the DXN runtime (this fixed my broken kestrel command)\n  Use the $(BuildConfiguration) setting as the configuration (Debug, Release etc.)\n  Publish (dnu pack) the site to the staging directory\n\n\nThat now gets you ready to publish to Docker – hooray!\n\nChallenge 2: Publishing to Docker (a.k.a The Publish Build Task)\n\nA note on the build agent machine: the agent must have access to the docker client, as well as your certificate files. You can supply the paths to the certificate files in the Auth options for the task, but you’ll need to make sure the docker client is installed on the build machine.\n\nNow that we have some deployable code, we can finally turn our attention to publishing to Docker. I wanted to be able to specify a pubxml file, but then override any of the settings. I also wanted to be able to publish without a pubxml file. So I created a custom build task. The task has 3 internal components:\n\n\n  The task.json – this specifies all the parameters (including the pubxml path, the path to the “packed” solution, and all the overrides)\n  A modified version of the publish PowerShell script that VS Tools for Docker generated when I created a Docker publish profile\n  A PowerShell script that takes the supplied parameters, creates a hashtable, and invokes the publish PowerShell script\n\n\nThe source code for the task is on Github. I’m not going to go through all the details here. To get the task, you’ll need to do the following:\n\n\n  Clone the repo (git clone https://github.com/colindembovsky/cols-agent-tasks.git)\n  Install node and npm\n  Install tfx-cli (npm install –g tfx-cli)\n  Login (tfx login)\n  Upload (tfx build tasks upload pathToDockerPublish)\n\n\nThe pathToDockerPublish is the path to Tasks/DockerPublish in the repo.\n\nOnce you’ve done that, you can then add a Docker Publish task. Set the path to your pubxml (if you have one), the path to the Dockerfile you want to use and set the Pack Output Path to $(Build.StagingDirectory) (or wherever the code you want to deploy to the container is). If you don’t have a pubxml file, leave “Path to Pubxml” empty – you’ll have to supply all the other details. If you have a pubxml, but want to overwrite some settings, just enter the settings in accordingly. The script will take the pubxml setting unless you supply a value in an override.\n\n\n\n\nIn this example, I’m overriding the docker image name (using the build number as the tag) and specifying “Build only” as false. That means the image will be built (using “docker build”) and a container will be spun up. Set this value to “true” if you just want to build the image without deploying a container. Here are all the settings:\n\n\n  Docker Server Url – url of your docker host\n  Docker image name – name of the image to build\n  Build Only – true to just run “docker build” – false if you want to execute “docker run” after building\n  Host port – the port to open on the host\n  Container port – the port to open on the container\n  Run options – additional arguments passed to the “docker run” command\n  App Type – can be empty or “Web”. Only required for ASP.NET applications (sets the server.urls setting)\n  Create Windows Container – set to “true” if you’re targeting a Windows docker host\n  Auth Options – additional arguments to supply to docker commands (for example –tlscert)\n  Remove Conflicting Containers – removes containers currently running on the same port when set to “true”\n\n\nSuccess\n\nOnce the build completes, you’ll be able to see the image in “docker images”.\n\n\n\n\nIf you’ve set “build only” to false you’ll be able to access your application!\n\n\n\n\nHappy publishing!\n",
      "categories": [],
      "tags": ["devops","build"],
      
      "collection": "posts",
      "url": "/continuous-deployment-with-docker-and-build-vnext/"
    },{
      
      "title": "Docker DevOps",
      "date": "2015-11-23 16:02:06 +0000",
      
      "content": "Recently I attended the MVP Summit in Redmond. This is an annual event where MVPs from around the world converge on Microsoft to meet with each other and various product teams. It’s a highlight of the year (and one of the best benefits of being an MVP).\n\n\n\n\nThe ALM MVPs have a tradition – we love to hear what other MVPs have been doing, so we have a pre-Summit session where we get 20 minute slots to share anything that’s interesting. This year I did a slide ware chat entitle Docker DevOps. It was just a collection of thoughts that I have on what Docker means for DevOps. I’d like to put some of those thoughts down in this post.\n\nDocker Means Containers\n\nDocker isn’t actually a technology per se. It’s just a containerization manager that happened to be at the right place at the right time – it’s made containers famous.\n\nContainer technology has been around for a fairly long time – most notably in the Linux kernel. Think of containers as the evolution of virtualization. When you have a physical server, it can be idle a lot of the time. So virtualization became popular, allowing us to create several virtual machines (VMs) on a single server. Apps running on the VM don’t know they’re on a VM – the VM has abstracted the physical hardware. Now most developers and IT Pros take virtualization for granted.\n\nContainers take the abstraction deeper – they abstract the OS too. Containers are running instances of images. The base layer of the image is typically a lightweight OS – only the bare essentials needed to run an app. Typically that means no UI or anything else that isn’t strictly needed. Images are also immutable. Under the hood, when you change an image, you actually create a differencing layer on top of the current layer. Containers also share layers – for example, if two containers have an ubuntu14.04 base layer, and then one has nginx and another has mySQL, there’s only one physical copy of the ubuntu14.04 image on disk. Shipping containers means just shipping the different top layer, which makes them easily portable.\n\nWindows Containers\n\nSo what about Windows containers? Windows Server 2016 TP 4 (the latest release at the time of this article) has support for Windows containers – the first OS from Microsoft to support containerization. There are two flavors of Windows container – Windows Server containers and Hyper-V containers. Windows Server container processes are visible on the host, while Hyper-V containers are completely “black box” as far as the host is concerned – that makes the HyperV containers “more secure” than Windows Server containers. You can switch the mode at any time.\n\nWindows container technology is still in its infancy, so there are a few rough edges, but it does show that Microsoft is investing in container technology. Another glaring sign is the fact that you can already create Docker hosts in Azure (both for Windows and Linux containers). Microsoft is also actively working on open-source Docker.\n\nWhat Containers Mean For You\n\nSo what does it all mean for you? Here’s the rub – just like you’ve probably not installed a physical server for some years because of virtualization, I predict that pretty soon you won’t even install and manage VMs anymore. You’ll have a “cloud of hosts” somewhere (you won’t care where) and have the ability to spin up containers to your heart’s content. In short, it’s the way of the future.\n\nSo here are some things you need to be thinking about if you want to ride the wave of the future:\n\n\n  Microservices\n  Infrastructure as Code\n  Immutable machines\n  Orchestration\n  Docker Repositories\n  It works on my machine\n\n\nMicroservices\n\nThe most important architectural change that containers bring is microservices. In order to use containers effectively, you have to (re-)architect your applications into small, loosely coupled services (each deployed into its own container). This makes each individual service simpler, but moves quite a bit of complexity into managing the services. Coordinating all these microservices is a challenge. However, I believe that the complexity at the management level is – well, more manageable. If done correctly, microservices can be deployed without much (or any) impact to other services, so you can isolate issues, deploy smaller units more frequently and gain scalability in the parts of the overall application that require it, as and when they require it (this is the job of the orchestration engine – something I’ll talk to later). This is much better than having to deploy an entire monolithic application every time.\n\nSo what about networking between the containers? Turns out that Docker is pretty good at managing how containers talk to each other (via Docker Swarm and Docker Compose). Each container must define which ports it exposes (if any). You can also link containers, so that they can communicate with each other. Furthermore, you have to explicitly define a “mapping” between the container ports and the host ports in order for the container to be exposed outside its host machine. So you have tight control over the surface area of each container (or group of containers). But it’s another thing to manage.\n\nInfrastructure as Code\n\nWhen you create a Docker image, you specify it in a Dockerfile. The Dockerfile contains instructions in text that tell the Docker engine how to build up an image. The starting layer typically the (minimal) OS. Then follow instructions to install dependencies that the top app layers will need. Finally, the app itself is added.\n\nSpecifying your containers in this manner forces you to express your infrastructure as code. This is a great practice, whether you’re doing it for Docker or not. After you’ve described your infrastructure as code, you can automate building the infrastructure – so Infrastructure as Code is a building block for automation. Automation is good – it allows rapid and reliable deployment, which means better quality, faster. It does mean that you’re going to have to embrace DevOps – and have both developers and operations (or better yet your DevOps team) work together to define and manage the infrastructure code. In this brave new world, no-one is installing OSs or anything else using GUIs. Script it baby, script it!\n\nImmutable Machines\n\nContainers are essentially immutable. Under the hood, if you change a container, you actually freeze the current top layer (so that it’s immutable) and add a new “top layer” with the changes (this is enabled by Union File Systems). In fact, if you do it correctly, you should never have a reason to change a container once it’s out of development. If you really do need to change something (or say, deploy some new code for your app within the container), you actually throw away the existing container and create a new one. Don’t worry though – Docker is super efficient – which means that you won’t need to rebuild the entire image from scratch – the interim layers are stored in the Docker engine, so Docker is smart enough to just use the common layers again and just create a new differencing layer for the new image.\n\nBe that as it may, there is a shift in thinking about containers in production. They should essentially be viewed as immutable. Which means that your containers have to be stateless. That obviously won’t work for databases or any other persistent data. So Docker has the concept of data volumes, which are special directories that can be accessed (and shared) by containers but that are outside the containers themselves. This means you have to really think about where the data are located for containers and where they live on the host (since they’re outside the containers). Migrating or upgrading data is a bit tricky with so many moving parts, so it’s something to think about carefully.\n\nOrchestration\n\nSo let’s imagine that you’ve architected an application composed of several microservices that can be deployed independently. You can spin them all up on a single machine and then – hang on, a single machine? Won’t that hit resource limitations pretty quickly? Yes it will. And what about the promise of scale – that if a container comes under pressure I can just spin another instance (container) up and voila – I’ve scaled out? Won’t that depend on how much host resources are available? Right again.\n\nThis is where tools like Docker Swarm come into play. Docker Swarm allows you to create and access a pool of Docker hosts. Ok, that’s great for deploying apps. But what about monitoring the resources available? And wouldn’t it be nice if the system could auto-scale? Enter Apache Mesos and Mesosphere (there are other products in this space too). Think of Mesos as a distributed kernel. It aggregates a bunch of machines – be they physical, virtual or cloud – into what appears to be a single machine that you can program against. Mesosphere is then a layer on top of Mesos that further abstracts, allowing much easier consumption and use of the Datacenter OS (dcos), which enables highly available, highly automated systems. Mesos uses containers natively, so Docker works in Mesos and Mesosphere. If you’re going to build scalable apps, then you are going to need an orchestration engine like Mesosphere. And it runs in Azure too!\n\nDocker Repositories\n\nDocker enables you to define a container (or image) using a Dockerfile. This file can be shared via some code repository. Then developers can code against that container (by building it locally) and when it’s ready for production, Ops can pull the file down and build the container. Sounds like a great way to share and automate! Docker repositories allow you to share Dockerfiles in exactly this manner. There are public repos, like DockerHub, and you can of course create (or subscribe) to private repos. This means that you get to share base images from official partners (for example, if you need nginx, no need to build it yourself – just pull down the official image from DockerHub that the nginx guys themselves have built). It also means that you have a great mechanism for moving code from dev to QA to Production. Just share the images in a public (or private) repo, and if a tester wants to test they can just spin up a container or two for themselves. The containers run exactly the same wherever they are run, so it could be the developer’s laptop, in a Dev/Test lab or in Production. And since only the delta’s are actually moved around (common images are shared) it’s quick and efficient to share code in this manner.\n\nIt Works on My Machine\n\n\n“It works on my machine!” The classic exclamation heard by developer’s world over every time a bug is filed. And we all laugh since we know that between your dev environment and Production lie a whole slew of differences. Except that now, since the containers run the same wherever they are run, if it works in the developer’s container, it works in the Prod container.\n\nOf course there are ways the containers may differ – for example, most real-world containers will have environment variables that have different values in different environments. But containers actually allow “It works on my machine” to become a viable statement once more.\n\nConclusion\n\nContainers are the way of the future. You want to make sure that you’re getting on top of containers early (as in now) so that you don’t get left behind. Start re-architecting your application into microservices, and start investigating hosting options, starting with Docker and Docker Compose and moving towards dcos like Mesosphere. And be proud, once more, to say, “It works on my machine!”\n\nHappy containering!\n",
      "categories": [],
      "tags": ["devops","cloud"],
      
      "collection": "posts",
      "url": "/docker-devops/"
    },{
      
      "title": "WebDeploy, Configs and Web Release Management",
      "date": "2015-12-02 19:18:38 +0000",
      
      "content": "It’s finally here – the new web-based Release Management (WebRM). At least, it’s here in preview on VSTS (formerly VSO) and should hopefully come to TFS 2015 in update 2.\n\nI’ve blogged frequently about Release Management, the “old” WPF tool that Microsoft purchased from InCycle (it used to be called InRelease). The tool was good in some ways, and horrible in others – but it always felt like a bit of a stop-gap while Microsoft implemented something truly great – which is what WebRM is!\n\nOne of the most common deployment scenarios is deploying web apps – to IIS or to Azure. I blogged about using the old tool along with WebDeploy here. This post is a follow-on – how to use WebDeploy and WebRM correctly.\n\nFirst I want to outline a problem with the out-of-the-box Tasks for deploying web apps. Then I’ll talk about how to tokenize the build package ready for multi-environment deployments, and finally I’ll show you how to create a Release Definition.\n\nAzure Web App Deployment Task Limitations\n\nIf you create a new Release Definition, there is an “Azure Web App Deployment” Task. Why not just use that to deploy web apps? There are a couple of issues with this Task:\n\n\n  You can’t use it to deploy to IIS\n  You can’t manage different configurations for different environments (with the exception of connection strings)\n\n\nThe Task is great in that it uses a predefined Azure Service Endpoint, which abstracts credentials away from the deployment. However, the underlying implementation invokes an Azure PowerShell cmdlet Publish-AzureWebsiteProject. This cmdlet works – as long as you don’t intend to change any configuration except the connection strings. Have different app settings in different environments? You’re hosed. Here’s the Task UI in VSTS:\n\n\n\n\nThe good:\n\n\n  You select the Azure subscription from the drop-down – no messing with passwords\n  You can enter a deployment slot\n\n\nThe bad:\n\n\n  You have to select the zip file for the packaged site – no place for handling configs\n  Additional arguments – almost impossible to figure out what to put here. You can use this to set connection strings if you’re brave enough to figure it out\n\n\nThe ugly:\n\n\n  Web App Name is a combo-box, but it’s never populated, so you have to type the name yourself (why is it a combo-box then?)\n\n\nIn short, this demo’s nicely, but you’re not really going to use it for any serious deployments – unless you’ve set the app settings on the slots in the Azure Portal itself. Perhaps this will work for you – but if you change a setting value (or add a new setting) you’re going to have to manually update the slot using the Portal. Not a great automation story.\n\nConfig Management\n\nSo besides not being able to use the Task for IIS deployments, your biggest challenge is config management. Which is ironic, since building a WebDeploy package actually handles the config well – it places config into a SetParameters.xml file. Unfortunately the Task (because it is calling Publish-AzureWebsiteProject under the hood) only looks for the zip file – it ignores the SetParameters file.\n\nSo I got to thinking – and I stole an idea from Octopus Deploy: what if the deployment would just automagically replace any config setting value with any correspondingly named variable defined in the Release Definition for the target Environment? That would mean you didn’t have to edit long lists of arguments at all. Want a new value? Just add it to the Environment variables and the deployment takes care of it for you.\n\nThe Solution\n\nThe solution turned out to be fairly simple:\n\nFor the VS Solution:\n\n\n  Add a parameters.xml file to your Website project for any non-connecting string settings you want to manage, using tokens for values\n  Create a publish profile that inserts tokens for the website name and any db connection strings\n\n\nFor the Build:\n\n\n  Configure a Team Build to produce the WebDeploy package (and cmd and SetParameters files) using the publish profile\n  Configure the Build to upload the zip and supporting files as the output\n\n\nFor the Release:\n\n\n  Write a script to do the parameter value substitution (replacing tokens with actual values defined in the target Environment) into the SetParameters file\n  Invoke the cmd to deploy the Website\n\n\nOf course, the “parameter substituting script” has to be checked into the source repo and also included as a build output in order for you to use it in the Release.\n\nCreating a Tokenized WebDeploy Package in a Team Build\n\nGood releases start with good packages. Since the same package is going to be deployed to multiple environments, you cannot “hardcode” any config settings into the package. So you have to create the package in such a way that it has tokens for any config values that the Release pipeline will replace with Environment specific values at deployment time. In my previous WebDeploy and Release Management post, I explain how to add the parameters.xml file and how to create a publish profile to do exactly that. That technique stays exactly the same as far as the VS solution goes.\n\nHere’s my sample parameters.xml file for this post:\n\n&amp;lt;!--?xml version=\"1.0\" encoding=\"utf-8\" ?--&amp;gt;\n&amp;lt;parameters&amp;gt;\n  &amp;lt;parameter name=\"CoolKey\" description=\"The CoolKey setting\" defaultvalue=\" __CoolKey__\" tags=\"\"&amp;gt;\n    &amp;lt;parameterentry kind=\"XmlFile\" scope=\"\\\\web.config$\" match=\"/configuration/appSettings/add[@key='CoolKey']/@value\"&amp;gt;\n    &amp;lt;/parameterentry&amp;gt;\n  &amp;lt;/parameter&amp;gt;\n&amp;lt;/parameters&amp;gt;\n\n\nNote how I’m sticking with the double-underscore pre- and post-fix as the token, so the value (token) for CoolKey is __CoolKey__.\n\nOnce you’ve got a parameters.xml file and a publish profile committed into your source repo (Git or TFVC – either one works fine), you’re almost ready to create a Team Build (vNext Build). You will need the script that “hydrates” the parameters from the Environment variables. I’ll cover the contents of that script shortly – let’s assume for now that you have a script called “Replace-SetParameters.ps1” checked into your source repo along with your website. Here’s the structure I use:\n\n\n\n\nCreate a new Build Definition – select Visual Studio Build as the template to start from. You can then configure whatever you like in the build, but you have to do 3 things:\n\n\n  Configure the MSBuild arguments as follows in the “Visual Studio Build” Task:\n  /p:DeployOnBuild=true /p:PublishProfile=Release /p:PackageLocation=”$(build.StagingDirectory)”\n  The name of the PublishProfile is the same name as the pubxml file in your solution\n  The package location is set to the build staging directory\n\n  Configure the “Copy and Publish Build Artifacts” Task to copy the staging directory to a server drop:\n\n  Add a new “Publish Build Artifact” Task to copy the “Replace-SetParameters.ps1” script to a server drop called “scripts”:\n\n\n\nI like to version my assemblies so that my binary versions match my build number. I use a custom build Task to do just that. I also run unit tests as part of the build. Here’s my entire build definition:\n\n\n\n\nOnce the build has completed, the Artifacts look like this:\n\n\n\n\nHere’s what the SetParameters file looks like if you open it up:\n\n&amp;lt;?xml version=\"1.0\" encoding=\"utf-8\"?&amp;gt;\n&amp;lt;parameters&amp;gt;\n  &amp;lt;setParameter name=\"IIS Web Application Name\" value=\" __SiteName__\" /&amp;gt;\n  &amp;lt;setParameter name=\"CoolKey\" value=\" __CoolKey__\" /&amp;gt;\n  &amp;lt;setParameter name=\"EntityDB-Web.config Connection String\" value=\" __EntityDB__\" /&amp;gt;\n&amp;lt;/parameters&amp;gt;\n\n\nThe tokens for SiteName and EntityDB both come from my publish profile – the token for CoolKey comes from my parameters.xml file.\n\nNow we have a package that’s ready for Release!\n\nFilling in Token Values\n\nYou can see how the SetParameters file contains tokens. We will eventually define values for each token for each Environment in the Release Definition. Let’s assume that’s been done already – then how does the release pipeline perform the substitution? Enter PowerShell!\n\nWhen you execute PowerShell in a Release, any Environment variables you define in the Release Definition are created as environment variables that the script can access. So I wrote a simple script to read in the SetParameters file, use Regex to find any tokens and replace the tokens with the environment variable value. Of course I then overwrite the file. Here’s the script:\n\nparam(\n    [string]$setParamsFilePath\n)\nWrite-Verbose -Verbose \"Entering script Replace-SetParameters.ps1\"\nWrite-Verbose -Verbose (\"Path to SetParametersFile: {0}\" -f $setParamsFilePath)\n\n# get the environment variables\n$vars = gci -path env:*\n\n# read in the setParameters file\n$contents = gc -Path $setParamsFilePath\n\n# perform a regex replacement\n$newContents = \"\";\n$contents | % {\n    $line = $_\n    if ($_ -match \"__(\\w+)__\") {\n        $setting = gci -path env:* | ? { $_.Name -eq $Matches[1] }\n        if ($setting) {\n            Write-Verbose -Verbose (\"Replacing key {0} with value from environment\" -f $setting.Name)\n            $line = $_ -replace \"__(\\w+)__\", $setting.Value\n        }\n    }\n    $newContents += $line + [Environment]::NewLine\n}\n\nWrite-Verbose -Verbose \"Overwriting SetParameters file with new values\"\nsc $setParamsFilePath -Value $newContents\n\nWrite-Verbose -Verbose \"Exiting script Replace-SetParameters.ps1\"\n\n\nNotes:\n\n\n  Line 2: The only parameter required is the path to the SetParameters file\n  Line 8: Read in all the environment variables – these are populated according to the Release Definition\n  Line 11: Read in the SetParameters file\n  Line 15: Loop through each line in the file\n  Line 17: If the line contains a token, then:\n  Line 18-22: Find the corresponding environment variable, and if there is one, replace the token with the value\n  Line 27: Overwrite the SetParameters file\n\n\nCaveats: note, this can be a little bit dangerous since the environment variables that are in scope include more than just the ones you define in the Release Definition. For example, the environment includes a “UserName” variable, which is set to the build agent user name. So if you need to define a username variable, make sure you name it “WebsiteUserName” or something else that’s going to be unique.\n\nCreating the Release Definition\n\nWe now have all the pieces in place to create a Release Definition. Each Environment is going to execute (at least) 2 tasks:\n\n\n  PowerShell – to call the Replace-SetParameters.ps1 script\n  Batch Script – to invoke the cmd file to publish the website\n\n\nThe PowerShell task is always going to be exactly the same – however, the Batch Script arguments are going to change slightly depending on if you’re deploying to IIS or to Azure.\n\nI wanted to make sure this technique worked for IIS as well as for Azure (both deployment slots and “real” sites). So in this example, I’m deploying to 3 environments: Dev, Staging and Production. I’m using IIS for dev, to a staging deployment slot in Azure for Staging and the “real” Azure site for Production.\n\nHere are the steps to configure the Release Definition:\n\n\n  Go to the Release hub in VSTS and create a new Release Definition. Select “Empty” to start with an empty template.\n  Enter a name for the Release Definition and change “Default Environment” to Dev\n\n  Click “Link to a Build Definition” and select the build you created earlier:\n\n  Click “+ Add Tasks” and add a PowerShell Task:\n  For the “Script filename”, browse to the location of the Replace-SetParameters.ps1 file:\n\n  For the “Arguments”, enter the following:\n  -setParamsFilePath $(System.DefaultWorkingDirectory)\\CoolWebApp\\drop\\CoolWebApp.SetParameters.xml\n  Of course you’ll have to fix the path to set it to the correct SetParameters file – $(System.DefaultWorkingDirectory) is the root of the Release downloads. Then there is a folder with the name of the Build (e.g. CoolWebApp), then the artifact name (e.g. drop), then the path within the artifact source.\n  Click “+ Add Tasks” and add a Batch Script Task:\n  For the “Script filename”, browse to the location of the WebDeploy cmd file:\n\n  Enter the correct arguments (discussed below).\n  Configure variables for the Dev environment by clicking the ellipses button on the Environment tile and selecting “Configure variables”\n  Here you add any variable values you require for your web app – these are the values that you tokenized in the build:\n\n  Azure sites require a username and password – I’ll cover those shortly.\n\n\nThe Definition should now look something like this:\n\n\n\nCmd Arguments and Variables\n\nFor IIS, you don’t need username and password for the deployments. This means you’ll need to configure the build agent to run as an identity that has permissions to invoke WebDeploy. The SiteName variable is going to be the name of the website in IIS plus the name of your virtual application – something like “Default Web Site/cool-webapp”. Also, you’ll need to configure the Agent on the Dev environment to be an on-premise agent (so select an on-premise queue) since the hosted agent won’t be able to deploy to your internal IIS servers.\n\nFor Azure, you’ll need the website username and password (which you can get by downloading the Publish profile for the site from the Azure Portal). They’ll need to be added as variables in the environment, along with another variable called “WebDeploySiteName” (which is required only if you’re using deployment slots). The SiteName is going to be the name of the site in Azure. Of course you’re going to “lock” the password field to make it a secret. You can use the Hosted agent for Environments that deploy to Azure.\n\nHere are the 2 batch commands – the first is for local deployment to IIS, the 2nd for deployment to Azure:\n\n\n  /Y /M:http://$(WebDeploySiteName)/MsDeployAgentService\n  /Y /M:https://$(WebDeploySiteName).scm.azurewebsites.net:443/msdeploy.axd /u:$(AzureUserName) /p:$(AzurePassword) /a:Basic\n\n\nFor IIS deployments, you can set WebDeploySiteName to be the name or IP of the target on-premises server. Note that you’ll have to have WebDeploy remote agent running on the machine, with the appropriate permissions for the build agent identity to perform the deployment.\n\nFor Azure, the WebDeploySiteName is of the form “siteName[-slot]”. So if you have a site called “MyWebApp”, and you just want to deploy to the site, then WebDeploySiteName will be “MyWebApp”. If you want to deploy to a slot (e.g. Staging), then WebDeploySiteName must be set to “MyWebApp-staging”. You’ll also need to set the SiteName to the name of the site in Azure (“MyWebApp” for the site, “MyWebApp__slot” for a slot – e.g. “MyWebApp__staging”). Finally, you’ll need “AzureUserName” and “AzurePassword” to be set (according to the publish settings for the site).\n\nCloning Staging and Production Environments\n\nOnce you’re happy with the Dev Environment, clone it to Staging and update the commands and variables. Then repeat for Production. You’ll now have 3 Environments in the Definition:\n\n\n\n\nAlso, if you click on “Configuration”, you can see all the Environment variables by clicking “Release variables” and selecting “Environment Variables”:\n\n\n\n\nThat will open a grid so you can see all the variables side-by-side:\n\n\n\n\nNow you can ensure that you’ve set each Environment’s variables correctly. Remember to set approvals on each environment as appropriate!\n\n2 More Tips\n\nIf you want to trigger the Release every time the linked Build produces a new package, then click on Triggers and enable “Continuous Deployment”.\n\nYou can get the Release number to reflect the Build package version. Click on General and change the Release Name format to:\n\n\n$(Build.BuildNumber)-$(rev:r)\n\n\nNow when you release 1.0.0.8, say, your release will be “1.0.0.8-1”. If you trigger a new release with the same package, it will be numbered “1.0.0.8-2” and so on.\n\nConclusion\n\nWebRM is a fantastic evolution of Release Management. It’s much easier to configure Release Definitions, to track logs to see what’s going on and to configure deployment Tasks – thanks to the fact that the Release agent is the same as the Build agent. As far as WebDeploy goes, I like this technique of managing configuration – I may write a custom Build Task that bundles the PowerShell and Batch Script into a single task – that will require less argument “fudging” and bundle the PowerShell script so you don’t have to have it in your source repo. However, the process is not too difficult to master even without a custom Task, and that’s pleasing indeed!\n\nHappy releasing!\n",
      "categories": [],
      "tags": ["releasemanagement","build"],
      
      "collection": "posts",
      "url": "/webdeploy-configs-and-web-release-management/"
    },{
      
      "title": "Config Per Environment vs Tokenization in Release Management",
      "date": "2015-12-15 16:37:49 +0000",
      
      "content": "In my previous post I experimented with WebDeploy to Azure websites. My issue with the out-of-the-box Azure Web App Deploy task is that you can specify the WebDeploy zip file, but you can’t specify any environment variables other than connection strings. I showed you how to tokenize your configuration and then use some PowerShell to get values defined in the Release to replace the tokens at deploy time. However, the solution still felt like it needed some more work.\n\nAt the same time that I was experimenting with Release Management in VSTS, I was also writing a Hands On Lab for Release Management using the PartsUnlimited repo. While writing the HOL, I had some debate with the Microsoft team about how to manage environment variables. I like a clean separation between build and deploy. To achieve that, I recommend tokenizing configuration, as I showed in my previous post. That way the build produces a single logical package (this could be a number of files, but logically it’s a single output) that has tokens instead of values for environment config. The deployment process then fills in the values at deployment time. The Microsoft team were advocating hard-coding environment variables and checking them into source control – a la “infrastructure as code”. The debate, while friendly, quickly seemed to take on the the feel of an unwinnable debate like “Git merge vs rebase”. I think having both techniques in your tool belt is good, allowing you to select the one which makes sense for any release.\n\nConfig Per Environment vs Tokenization\n\nThere are then (at least) two techniques for handling configuration. I’ll call them “config per environment” and “tokenization”.\n\nIn “config per environment”, you essentially hard-code a config file per environment. At deploy time, you overwrite the target environment config with the config from source control. This could be an xcopy operation, but hopefully something a bit more intelligent – like an ARM Template param.json file. When you define an ARM template, you define parameters that are passed to the template when it is executed. You can also then define a param.json file that supplies the parameters. For example, look at the FullEnvironmentSetup.json and FullEnvironmentSetup.param.json file in this folder of the PartsUnlimited repo. Here’s the param.json file:\n\n{\n    \"$schema\": \"http://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#\",\n    \"contentVersion\": \"1.0.0.0\",\n    \"parameters\": {\n        \"WebsiteName\": {\n            \"value\": \"\"\n        },\n        \"PartsUnlimitedServerName\": {\n            \"value\": \"\"\n        },\n        \"PartsUnlimitedHostingPlanName\": {\n            \"value\": \"\"\n        },\n        \"CdnStorageAccountName\": {\n            \"value\": \"\"\n        },\n        \"CdnStorageContainerName\": {\n            \"value\": \"\"\n        },\n        \"CdnStorageAccountNameForDev\": {\n            \"value\": \"\"\n        },\n        \"CdnStorageContainerNameForDev\": {\n            \"value\": \"\"\n        },\n        \"CdnStorageAccountNameForStaging\": {\n            \"value\": \"\"\n        },\n        \"CdnStorageContainerNameForStaging\": {\n            \"value\": \"\"\n        }\n    }\n}\n\n\nYou can see how the parameters match the parameters defined in the template json file. In this case, since the repo is public, the values are just empty strings – but you can imagine how you could define “dev.param.json” and “staging.param.json” and so on – each environment gets its own param.json file. Then at deploy time, you specify to the release which param.json file to use for that environment in the Deploy Azure Resource Group task.\n\nI’m still not sure I like hard-coding values and committing them to source control. The Microsoft team argued that this is “config as code” – but I still think that defining values in Release Management constitutes config as code, even if the code isn’t committed into source control. I’m willing to concede if you’re deploying to Azure using ARM – but I don’t think too many people are at present. Also, there’s the issue of sensitive information going to source control – in this case, the template actually requires a password field (not defined in the param file) – are you going to hardcode usernames/passwords into source control? And even if you do, if you just want to change a value, you need to create a new build since there’s no way to use the existing build – which is probably not what you want!\n\nLet’s imagine you’re deploying your web app to IIS instead of Azure. How do you manage your configuration in that case? “Use config transformations!” you cry. The problem – as I pointed out in my previous post – is that if you have a config transform for each environment, you have to build a package for each environment, since the transformation occurs at build time, not at deploy time. Hence my preference for a single transform that inserts tokens into the WebDeploy package at build time that can be filled in with actual values at deploy time. This is what I call “tokenization”.\n\nSo when do you use config-per-environment and when do you use tokenization? I think that if you’ve got ARM templates, use config-per-environment. It’s powerful and elegant. However, even if you’re using ARM, if you have numerous environments, and environment configs change frequently, you may want to opt for tokenization. When you use config-per-environment, you’ll have to queue a new build to get the new config files into the drop that the release is deploying – while tokenization lets you change the value in Release Management and re-deploy an existing package. So if you prefer not to rebuild your binaries just to change an environment variable, then use tokenization. Also, if you don’t want to store usernames/passwords or other sensitive data in source control, then tokenization is better – sensitive information can be masked in Release Management. Of course you could do a combination – storing some config in source code and then just using Release Management for defining sensitive values.\n\nDocker Environment Variables\n\nAs an aside, I think that Docker encourages tokenization. Think about how you wouldn’t hard-code config into the Dockerfile – you’d “assume” that certain environment variables are set. Then when you run an instance of the image, you would specify the environment variable values as part of the run command. This is (conceptually anyway) tokenization – the image has “placeholders” for the config that are “filled in” at deploy time. Of course, nothing stops you from specifying a Dockerfile per environment, but it would seem a bit strange to do so in the context of Docker.\n\nYou, dear reader, will have to decide which is better for yourself!\n\nNew Custom Build Tasks – Replace Tokens and Azure WebDeploy\n\nSo I still like WebDeploy with tokenization – but the PowerShell-based solution I hacked out in my previous post still felt like it could use some work. I set about seeing if I could wrap the PowerShell scripts into custom Tasks. I also felt that I could improve on the arguments passed to the WebDeploy cmd file – specifically for Azure Web Apps. Why should you download the Web App publishing profile manually if you can specify credentials to the Azure subscription as a Service Endpoint? Surely it would be possible to suck down the publishing profile of the website automatically? So I’ve created two new build tasks – Replace Tokens and Azure WebDeploy.\n\nReplace Tokens Task\n\nI love how Octopus Deploy automatically replaces web.config keys if you specify matching environment variables in a deployment project. I did something similar in my previous post with some PowerShell. The Replace Tokens task does exactly that – using some Regex, it will replace any matching token with the environment variable (if defined) in Release Management. It will work nicely on the WebDeploy SetParams.xml file, but could be used to replace tokens on any file you want. Just specify the path to the file (and optionally configure the Regex) and you’re done. This task is implemented in node, so it’ll work on any platform that the VSTS agent can run on.\n\nAzure WebDeploy Task\n\nI did indeed manage to work out how to get the publishing username and password of an Azure website from the context of an Azure subscription. So now you drop a “Replace Tokens” task to replace tokens in the SetParams.xml file, and then drop an Azure WebDeploy task into the Release. This looks almost identical to the out-of-the-box “Azure Web App Deployment” task except that it will execute the WebDeploy command using the SetParams.xml file to override environment variables.\n\nUsing the Tasks\n\nI tried the same hypothetical deployment scenario I used in my previous post – I have a website that needs to be deployed to IIS for Dev, to a staging deployment slot in Azure for staging, and to the production slot for Production. I wanted to use the same tokenized build that I produced last time, so I didn’t change the build at all. Using my two new tasks, however, made the Release a snap.\n\nDev Environment\n\nHere’s the definition in the Dev environment:\n\n\n\n\nYou can see the “Replace Tokens” task – I just specified the path to the SetParams.xml file as the “Target File”. The environment variables look like this:\n\n\n\n\nNote how I define the app setting (CoolKey), the connection string (EntityDB) and the site name (the IIS virtual directory name of the website). The “Replace Tokens” path finds the corresponding tokens and replaces them with the values I’ve defined.\n\nTo publish to IIS, I can just use the “Batch Script” task:\n\n\n\n\nI specify the path to the cmd file (produced by the build) and then add the arguments “/Y” to do the deployment (as opposed to a what-if) and use the “/M” argument to specify the IIS server I’m deploying to. Very clean!\n\nStaging and Production Environments\n\nFor the staging environment, I use the same “Replace Tokens” task. The variables, however, look as follows:\n\n\n\n\nThe SiteName variable has been removed. This is because the “Azure WebDeploy” task will work out the site name internally before invoking WebDeploy.\n\nHere’s what the Azure WebDeploy task looks like in Staging:\n\n\n\n\nThe parameters are as follows:\n\n\n  Azure Subscription – the Azure subscription Service Endpoint – this sets the context for the execution of this task\n  Web App Name – the name of the Web App in Azure\n  Web App Location – the Azure region that the site is in\n  Slot – the deployment slot (leave empty for production slot)\n  Web Deploy Package Path – the path to the webdeploy zip, SetParams.xml and cmd files\n\n\nInternally, the task connects to the Azure subscription using the Endpoint credentials. It then gets the web app object (via the name) and extracts the publishing username/password and site name, taking the slot into account (the site name is different for each slot). It then replaces the SiteName variable in the SetParametes.xml file before calling WebDeploy via the cmd (which uses the zip and the SetParameters.xml file). Again, this looks really clean.\n\nThe production environment is the same, except that the Slot is empty, and the variables have production values.\n\nIIS Web Application Deployment Task\n\nAfter my last post, a reader tweeted me to ask why I don’t use the out-of-the-box IIS Web Application Deployment task. The biggest issue I have with this task is that it uses WinRM to remote to the target machine and then invokes WebDeploy “locally” in the WinRM session. That means you have to install and configure WinRM on the target machine before deploying. On the plus side, it does allow you to specify the SetParameters.xml file and even override values at deploy time. It can work against Azure Web Apps too. You can use it if you wish – just remember to use the “Replace Tokens” task before to get environment variables into your SetParameters.xml file!\n\nConclusion\n\nWhichever method you prefer – config per environment or tokenization – Release Management makes your choice a purely philosophical debate. Due to its customizable architecture, there’s not too much technical difference between the methods when it comes to defining the Release Definition. That, to my mind, assures me that Release Management in VSTS is a fantastic tool.\n\nSo make your choice and happy releasing!\n",
      "categories": [],
      "tags": ["devops","build"],
      
      "collection": "posts",
      "url": "/config-per-environment-vs-tokenization-in-release-management/"
    },{
      
      "title": "Building VS 2015 Setup Projects in Team Build",
      "date": "2016-01-13 05:13:20 +0000",
      
      "content": "Remember when Visual Studio had a setup project template? And then it was removed? Then you moved to WiX and after learning it for 3 months and still being confused, you just moved to Web Apps?\n\nWell everyone complained about the missing setup project templates and MS finally added it back in as an extension. Which works great if you build out of Visual Studio – but what about automated builds? Turns out they don’t understand the setup project, so you have to do some tweaking to get it to work.\n\nSetup Project Options\n\nThere are a couple of options if you’re going to use setup projects.\n\n\n  ClickOnce. This is a good option if you don’t have a deployment solution that can deploy new versions of your application (like System Center or the like). It requires fudging on the builds to get versioning to work in some automated fashion. At least it’s free.\n  WiX. Free and very powerful, but really hard to learn and you end up programming in XML – which is a pain. However, if you need your installer to do “extra” stuff (like create a database during install) then this is a good option. Automation is also complicated because you have to invoke Candle.exe and Light.exe to “build” the WiX project.\n  VS Setup Projects. Now that they’re back in VS, you can use these projects to create installers. You can’t do too much crazy stuff – this just lays down the exe’s and gets you going. It’s easy to maintain, but you need to tweak the build process to build these projects. Also free.\n  InstallShield and other 3rd party paid installer products. These are typically powerful, but expensive. Perhaps the support you get is worth the price, but you’ll have to decide if the price is worth the support and other features you don’t get from the other free solutions.\n\n\nTweaking Your Build Agent\n\nYou unfortunately won’t be able to build setup projects on the Hosted build agent because of these tweaks. So if you’ve got a build agent, here’s what you have to do:\n\n\n  Install Visual Studio 2015 on the build machine.\n  Install the extension onto your build machine.\n  Configure the build agent service to run under a known user account (not local service, but some user account on the machine).\n  Apply a registry hack – you have to edit HKCU\\SOFTWARE\\Microsoft\\VisualStudio\\14.0_Config\\MSBuild\\EnableOutOfProcBuild to have a DWORD of 0 (I didn’t have the key, so I just added it). If you don’t do this step, then you’ll probably get an obscure error like this: “ERROR: An error occurred while validating.  HRESULT = ‘8000000A’”\n  Customize the build template (which I’ll show below).\n\n\nIt’s fairly nasty, but once you’ve done it, your builds will work without users having to edit the project file or anything crazy.\n\nCustomizing the Build Definition\n\nYou’ll need to configure the build to compile the entire solution first, and then invoke Visual Studio to create the setup package.\n\nLet’s walk through creating a simple build definition to build a vdproj.\n\n\n  Log in to VSTS or your TFS server and go to the build hub. Create a new build definition and select the Visual Studio template. Select the source repo and set the default queue to the queue that your build agent is connected to.\n  Just after the Visual Studio Build task, add a step and select the “Command Line” task from the Utility section.\n  Enter the path to devenv.com for the Tool parameter (this is typically “C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\devenv.com”).\n  The arguments have the following format: solutionPath /build configuration projectPath\n  solutionPath is the path to the solution file\n  configuration is the config (debug, release etc.)\n  projectPath is the path to the vdproj file\n  Finally, expand the “Advanced” group and set the working folder to the path of the sln file and check the “Fail on Standard Error” checkbox.\n\n\nHere’s an example:\n\n\n\n\nFor reference, here’s how my source is structured:\n\n\n\n\nYou can then publish the setup exe or msi if you need to. You can run tests or scripts or anything else during the build (for ease I delete the unit test task in the above example).\n\nI now have a successful build:\n\n\n\n\nAnd the msi is in my drop, ready to be deployed in Release Management:\n\n\n\n\nHappy setup building!\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/building-vs-2015-setup-projects-in-team-build/"
    },{
      
      "title": "Staging Servers Must Die – Or Must They?",
      "date": "2016-01-29 17:12:37 +0000",
      
      "content": "Edith Harbaugh published a though-provoking post called Staging Servers Must Die with the byline “so continuous delivery may live.” She asserts something which I’d never really considered before: that separate, cascading Dev, QA, Staging and Prod environments is a hangover from Waterfall development.\n\nAgility = No Build or Staging?\n\nHarbaugh makes some bold assertions about what she calls “DevOps 2.0”. First, she states that teams should ditch the concept of build (which she calls antiquated). Developers should be checking source into their mainline and and deploying immediately to Prod – with feature flags. The flag of a new feature being deployed defaults to “off for everyone” – no need to keep staging in sync with Prod, and no delay. The QA team are then given access to the feature, then beta customers, and slowly the number of users with access to the feature is increased until everyone has it and the feature is “live”.\n\nShe calls out four problems with cascading environments. The first one is time: she argues that a pipeline of environments slows delivery since builds have to be queued and then progressively moved through the pipeline. Secondly, staging environments increase costs since they require infrastructure. Thirdly, she says that the effectiveness of staging environments is moot since they can almost never reproduce production exactly. Finally, she recounts bad experiences where she needed users to test on staging servers, and users continually logged into Prod instead of Staging (or vice-versa) and so the effectiveness of having a staging environment became eclipsed user confusion.\n\nFeature Flags\n\nI think that Harbaugh’s view of feature flags may be a tad biased, since she is the CEO of a LaunchDarkly, a product that allows developers to introduce and manage feature flags. Still, feature flags are a great solution to some of the challenges she lists. However, feature flags are hard to code and manage (that’s why she has a product that helps teams manage it).\n\nLaunchDarkly is a really neat idea – in your code, you call an API that queries LaunchDarkly to determine if this feature is on for this user. Then you can manage which users have which features outside the app in LaunchDarkly – great for A/B testing or releasing features to beta customers and so on.\n\nFeature flags always sound great in theory, but how do you manage database schema differences? How do you fix a bug (what bug?) – do you need a feature flag for the bug fix? What about load testing a new feature – do you do that against Prod?\n\nAgility\n\nSo are feature flags and ditching builds and staged environments the way to increase agility and progress to “DevOps 2.0”? It may be in some cases, but I don’t think so. Automated deployment doesn’t make you DevOps – DevOps is far more that just that.\n\nHere are my thoughts on what you should be thinking about in your DevOps journey.\n\nMicroservices\n\nYou may be able to go directly to microservices, but even if you can’t (and in some cases you probably shouldn’t), you should be thinking about breaking large, monolithic applications into smaller, loosely-coupled components. Besides better architecture, isolating components allows you deployment granularity. That is, you can deploy a component of your application without having to deploy the entire application. This makes for much faster cycles, since teams that complete functionality in one component can deploy immediately without waiting for teams that are working on other components to be ready to deploy. Smaller, more frequent, asynchronous deployments are far better than large, infrequent, synchronized deployments.\n\nAutomated Builds with Automated Testing\n\nThis has always seemed so fundamental to me – I battle to understand why so many dev teams do not have builds and unit tests. This is one of my biggest disagreements with Harbaugh – when a developer checks in, the code should trigger a build that not only compiles, but goes through a number of quality checks. The most non-negotiable is unit testing with coverage analysis – that way you have some measure of code quality. Next, consider static code analysis, and better yet, integration with SonarQube or some other technical debt management system.\n\nEvery build should produce metrics about the quality of your code – tests passed/failed, coverage percentage, maintainability indexes and so on. You should know these things about your code – deploying directly to production (even with feature switches) bypasses any sort of quality analysis on your code.\n\nYour build should also produce a deployable package – that is environment agnostic. You should be able to deploy your application to any environment, and have the deployment process take care of environment specific configuration.\n\nBeyond unit testing, you should be creating automated integration tests. These should be running on an environment (we’ll discuss that shortly) so that you’re getting quality metrics back frequently. These tests typically take longer to run than unit tests, so they should at least be run on a schedule if you don’t want them running on each check-in. Untested code should never be deployed to production – that means you’re going to have to invest into keeping your test suites sharp – treat your test code as “real” code and help it to help you!\n\nAutomated Deployment with Infrastructure As Code\n\nHarbaugh does make a good point – that cascading dev/test/staging type pipelines originate in Waterfall. I constantly try to get developers to separate branch from environment in their minds – it’s unfortunate that we have dev/test/prod branches and dev/test/prod environments – that makes developers think that the code on a branch is the code in the environment. This is almost never the case – I usually recommend a dev/prod branching structure and let the build track which code is in which environment (with proper versioning and labeling of course).\n\nSo we should repurpose our cascading environments – call them integration and load or something appropriate if you want to. You need somewhere to run all these shiny tests you’ve invested in. And go Cloud – pay as you use models mean that you don’t have to have hardware idling – you’ll get much more efficient usage of environments that are spun up/down as you need them. However, if you’re spinning environments up and down, you’ll need to use Infrastructure as Code in some form to automate the deployment and configuration of your infrastructure – ARM Templates, DSC scripts and the like.\n\nYou’ll then also need a tool for managing deployments in the pipeline – for example, Release Management. Release Management allows you to define tasks – that can deploy build outputs or run tests or do whatever you want to – in a series of environments. You can automate the entire pipeline (stopping when tests fail) or insert manual approval points. You can then configure triggers, so when a new build is available the pipeline automatically triggers for example. Whichever tool you use though, you’ll need a way to monitor what builds are where in which pipelines. And you can of course deploy directly to Production when it is appropriate to do so, so the pipeline won’t slow critical bugfixes if you don’t want it to.\n\nLoad Testing\n\nSo what about load and scale testing? Doing this via feature switches is almost impossible if you don’t want to bring your production environment to a grinding halt. If you’re frequently doing this, then consider replication of your databases so that you always have an exact copy of production that you can load test against. Of course, most teams can use a subset of prod and extrapolate results – so you’ll have to decide if matching production exactly is actually necessary for load testing.\n\nHaving large enough datasets should suffice – load testing should ideally be a relative operation. In other words, you’re not testing for an absolute number, like how many requests per second your site can handle. Rather, you should be base lining and comparing runs. Execute load test on current code to set a base line, then implement some performance improvement, then re-run the tests. You now compare the two runs to see if your tweaks were effective. This way, you don’t necessarily need an exact copy of production data or environments – you just need to run the tests with the same data and environment so that comparisons make sense.\n\nA/B Testing\n\nOf course feature switches can be manipulated and managed in such as way as to enable A/B testing – having some users go to “version A” and some to “version B” of your application. It’s still possible to do A/B testing without deploying to production – for example, using deployment slots in Azure. In an Azure site, you’d create a staging slot on your production site. The staging slot can have the same config as your production slot or have different config, so it could point to production databases if necessary. Then you’d use Traffic Manager to divert some percentage of traffic to the staging slot until you’re happy (which the users will be unaware of – they go to the production URL and are none the wiser that they’ve been redirected to the staging slot). Then just swap the slots – instant deployment, no data loss, no confusion.\n\nConclusion\n\nStaging environments shouldn’t die – they should be repurposed, rising like a Phoenix out of the ashes of Waterfall’s cremation. Automated builds with solid automated testing (which requires staging infrastructure) should be what you’re aiming for. That way, you can deploy into production quickly with confidence, something that’s hard to do if you deploy directly to production, even with feature switches.\n\nHappy staging!\n",
      "categories": [],
      "tags": ["devops"],
      
      "collection": "posts",
      "url": "/staging-servers-must-die-or-must-they/"
    },{
      
      "title": "DevOps is a Culture, Not a Team",
      "date": "2016-02-18 22:33:28 +0000",
      
      "content": "This post was originally posted on our Northwest Cadence blog – but I feel it’s a really important post, so I’m cross-posting it here!\n\nI recently worked at a customer that had created a DevOps team in addition to their Ops and Development teams. While I appreciated the attempt to improve their processes, inwardly I was horrified. Just as DevOps is not a product, I think it is bad practice to create a DevOps team. DevOps is a culture and a mindset that should pervade every member of your organization – beyond even developers and operations.\n\nWhat is DevOps?\n\nSo how do you define DevOps? Donovan Brown, a DevOps product manager at Microsoft, defines DevOps succinctly as follows: DevOps is the union of people, process, and products to enable continuous delivery of value to end users. Unfortunately, since the name is an amalgamation of development and operations, most organizations get developers and ops to collaborate, and then boldly declare, “We do DevOps!”\n\nWider than Dev and Ops\n\nHowever, true DevOps is a culture that should involve everyone that is involved in delivery of value to end users. This means that business should understand their role in DevOps. Testers and Quality Assurance (QA) should understand their role in DevOps. Project Management Offices (PMOs), Human Resources (HR) and any other part of the organization that touches on the flow of value should be aware of their role in DevOps.\n\nThat’s why creating a DevOps team is a fundamentally bad decision. It distances people outside the “DevOps” team from being involved in the culture of DevOps. If there’s a DevOps team, and I’m not on it, why should I worry about DevOps? In the same manner, DevOps that is confined solely to dev and ops is indicative of the culture not pervading the organization. To fully benefit from DevOps, the entire organization needs to embrace the mindset and culture of DevOps.\n\nDevOps Values\n\nWhat then is a DevOps culture? What values should be upheld by people as they improve their processes and utilize tools to aid in implementing DevOps practices? Here are a few:\n\n\n  Whatever we do should ultimately deliver value to the end users\n  This is absolutely key to good DevOps – everyone, from stakeholders to developers, to testers and ops should be thinking of how to deliver value. If you can’t ship it, it’s not delivered value – so fix it until you can deliver.\n  There’s no such thing as a DevOps Hero\n  DevOps is not the domain of a single individual or team. Everyone needs to buy in to the culture, and everyone needs to own it. And we need to build a culture of “team”, an ubuntu for value, within the entire organization.\n  If we touch it, we own it\n  If developers hand off their code to testers, then they ultimately assume “someone else” will check their code. If a developer is responsible for the after hours support calls, they’re more likely to ensure good quality. Of course enlisting the help of some testers will help that effort!\n  We should examine everything we do for efficiencies\n  Sometimes we need to step back and examine why we do certain things. Before we automated our deployments, we needed a change control board to wade through pages of “installation instructions”. Now we’ve automated deployments – so we do we still need the documentation or the checkpoint? We could go faster if we removed the “legacy” processes.\n  We should be allowed to experiment\n  Will automated deployment help us deliver value faster? Perhaps yes, perhaps no. We’ll never find out if we never have the permission (and time) to experiment. And we can learn from failed experiments too – so we should value experimentation\n\n\nEveryone has a responsibility in DevOps\n\nDevOps is more than just developers and ops getting together and automating a dew things. While this is certainly a fast-track to better DevOps, the DevOps mindset has to widen to include other teams not traditionally associated with DevOps – like HR, PMOs and Testers.\n\nHuman Resources (HR)\n\nHuman Resources should be looking for people who are passionate about delivering value. DevOps culture is built faster when people have passion and care about their end users. Don’t just inspect a candidates technical competency – get a feel for how much of a team player they are and how much they will take responsibility for what they do. Don’t hire people who just want to clock in and out and do as little as possible. Also, you may have to “trim the fat” – get rid of positions that are not delivering value.\n\nA further boost for developing DevOps culture is the right working environment. Make your workplace a place people love – but make sure they don’t burn out too! Force them to go home and decompress with friends and family. If your teams are always working overtime, it’s an indication that something isn’t right. Find and improve the erroneous practice so that your team members can have a life – this will reinforce the passion and loyalty they have to delivering value.\n\nPMOs (Project Management Offices)\n\nThe PMO needs to rethink in many areas – especially in utilization. Most PMOs strive to make sure that every team member is running at 100% utilization. However, there are problems with this approach:\n\n\n  Humans don’t multitask\n  Humans don’t multitask – they switch quickly. However, unlike computers that can switch context perfectly to give the illusion of multitasking, humans have fallible memories. Switching costs, since our memories are not perfect. If there are no gaps in our schedules, we will inevitably run late since we don’t usually account for the cost of switching\n  No time for thinking, discussion and experimenting\n  If you’re at 100% utilization, you inevitably feel like you don’t have time to think. You can’t get involved in discussions with other team members about how to best solve a problem. And you can’t reflect on what is working and what is not. You certainly won’t have time to experiment. Over the long run, this will hamper delivery of value, since you won’t be innovating.\n  High utilization increases wait-time\n  The busier a resource is, the longer you have to wait to access it. A simple formula proves this – wait time = % busy / % idle. So if a resource is 50% utilized, wait time is 50/50 = 1 unit. If that same resource is 90% utilized, the wait time is 90/10 = 9 units. So you have to wait 9 times longer to access a resource that’s busy 90% of the time than when it’s busy 50% of the time. Long wait times means longer cycle times and lower throughput.\n\n\nPMOs need to embrace the innovative nature of DevOps – and that means giving team members time in their schedules. And it means embracing uncertainty – don’t be afraid to trust the team.\n\nTesters\n\nAs Infrastructure as Code, Continuous Integration (CI) and Continuous Deployment (CD) speed the delivery time, testers need to jump in and start automating their testing efforts. In fact, just as I think that a DevOps team is a bad idea, I think that a Testing team is just as bad. Testers should be part of the development/operations team, not a separate entity. And traditional “manual” testers need to beef up on their automation skills, since manual testing becomes a bottleneck in the delivery pipeline. Remember, testers that “find bugs” are not thinking DevOps – testers that aim to automate their tests so that results are faster and more accurate are thinking about real quality improvement – and that means they’re thinking about delivering value to the end users.\n\nConclusion\n\nDevOps is not a team or a product – it is a culture that needs to pervade everyone in the organization. Everyone – from HR to PMOs to Testers, not just developers and ops – needs to embrace DevOps values, making sure that value is being delivered continually to their end users.\n",
      "categories": [],
      "tags": ["devops","alm"],
      
      "collection": "posts",
      "url": "/devops-is-a-culture-not-a-team/"
    },{
      
      "title": "AppInsights Analytics in the Real World",
      "date": "2016-03-31 18:57:35 +0000",
      
      "content": "Ever since Application Insights (AppInsights) was released, I’ve loved it. Getting tons of analytics about site usage, performance and diagnostics – pretty much for free – makes adding Application Performance Monitoring (APM) to you application a no-brainer. If you aren’t using AppInsights, then you really should be.\n\nAPM is the black sheep of DevOps – most teams are concentrating on getting continuous integration and deployment and release management, which are critical pillars of DevOps. But few teams are taking DevOps beyond deployment into APM, which is also fundamental to successful DevOps. AppInsights is arguably the easiest, least-friction method of quickly and easily getting real APM into your applications. However, getting insights from your AppInsights data has not been all that easy up until now.\n\nApplication Insights Analytics\n\nA few days ago Brian Harry wrote a blog post called Introducing Application Insights Analytics. Internally, MS was using a tool called Kusto to do log analytics for many systems – including Visual Studio Team Services (VSTS) itself. (Perhaps Kusto is a reference perhaps to the naval explorer Jacques Cousteau – as in, Kusto lets you explore the oceans of data?) MS then productized their WPF Kusto app into web-based Application Insights Analytics. App Insights Analytics adds phenomenal querying and visualizations onto AppInsights telemetry, allowing you to really dig into the data AppInsights logs. Later on I’ll show you some really simple queries that we use to analyze our usage data.\n\nBrian goes into detail about how fast the Application Insights Analytics engine is – and he should know since they process terrabytes worth of telemetry. Our telemetry is nowhere near that large, so performance of the query language isn’t that big a deal for us. What is a big deal is the analytics and visualizations that the engine makes possible.\n\nIn this post I want to show you how to get AppInsights into a real world application. Northwest Cadence has a Knowledge Library application and in order to generate tracing diagnostics and usage telemetry, we added AppInsights. We learned some lessons about AppInsights on the way, and here are some of our lessons-learned.\n\nConfiguring AppInsights\n\nWe have 4 sites that we deploy the same code to – there are 2 production sites, Azure Library and Knowledge Library, and each has a dev environment too. By default the AppInsights key is configured in ApplicationInsights.config. We wanted to have a separate AppInsights instance for each site, so we created 4 in Azure. Now we had the problem of where to set the key so that each site logs to the correct AppInsights instance.\n\nServer-side telemetry is easy to configure. Add an app setting called “AIKey” in the web.config. In a startup method somewhere, you make a call to the Active TelemetryConfig:\n\n\nMicrosoft.ApplicationInsights.Extensibility.TelemetryConfiguration.Active.InstrumentationKey = WebConfigurationManager.AppSettings[\"AIKey\"];\n\n\nThis call then sets the AIKey for all serve-side telemetry globally. But what about client side?\n\nFor that we added a static getter to a class like this:\n\nprivate static string aiKey;\npublic static string AIKey\n{\n    get\n    {\n        if (string.IsNullOrEmpty(aiKey))\n        {\n            aiKey = WebConfigurationManager.AppSettings.Get(\"AIKey\");\n        }\n        return aiKey;\n    }\n}\n\n\nIn the master.cshtml file, we added the client-side script for AppInsights and made a small modification to get the key injected in instead of hard-coded:\n\n&amp;lt;script type=\"text/javascript\"&amp;gt;\n    var appInsights=window.appInsights||function(config){\n        function s(config){t[config]=function(){var i=arguments;t.queue.push(function(){t[config].apply(t,i)})}}var t={config:config},r=document,f=window,e=\"script\",o=r.createElement(e),i,u;for(o.src=config.url||\"//az416426.vo.msecnd.net/scripts/a/ai.0.js\",r.getElementsByTagName(e)[0].parentNode.appendChild(o),t.cookie=r.cookie,t.queue=[],i=[\"Event\",\"Exception\",\"Metric\",\"PageView\",\"Trace\"];i.length;)s(\"track\"+i.pop());return config.disableExceptionTracking||(i=\"onerror\",s(\"_\"+i),u=f[i],f[i]=function(config,r,f,e,o){var s=u&amp;amp;&amp;amp;u(config,r,f,e,o);return s!==!0&amp;amp;&amp;amp;t[\"_\"+i](config,r,f,e,o),s}),t\n    }({\n        instrumentationKey: \"@Easton.Web.Helpers.Utils.AIKey\"\n    });\n\n    window.appInsights=appInsights;\n    appInsights.trackPageView();\n&amp;lt;/script&amp;gt;\n\n\nYou can see how we’re using Razor syntax to get the AIKey static property value for the instrumentationKey value.\n\nThe next thing we wanted was to set the application version (assembly version) and site type (either KL for “Knowledge Library” or Azure for “Azure Library”). Perhaps this is a bit overkill since we have 4 separate AppInsights instances anyway, but if we decide to consolidate at some stage we can do so and preserve partitioning in the data.\n\nSetting telemetry properties for every log entry is a little harder – there used to be an IConfigurationInitializer interface, but it seems it was deprecated. So we implemented an ITelmetryInitializer instance:\n\npublic class AppInsightsTelemetryInitializer : ITelemetryInitializer\n{\n    string appVersion = GetApplicationVersion();\n    string siteType = GetSiteType();\n\n    private static string GetSiteType()\n    {\n        return WebConfigurationManager.AppSettings[\"SiteType\"];\n    }\n\n    private static string GetApplicationVersion()\n    {\n        return typeof(AppInsightsTelemetryInitializer).Assembly.GetName().Version.ToString();\n    }\n\n    public void Initialize(ITelemetry telemetry)\n    {\n        telemetry.Context.Component.Version = appVersion;\n        telemetry.Context.Properties[\"siteType\"] = siteType;\n    }\n}\n\n\nIn order to tell AppInsights to use the initializer, you need to add an entry to the ApplicationInsights.config file:\n\n&amp;lt;TelemetryInitializers&amp;gt;\n  ...\n  &amp;lt;Add Type=\"Easton.Web.AppInsights.AppInsightsTelemetryInitializer, Easton.Web\"/&amp;gt;\n&amp;lt;/TelemetryInitializers&amp;gt;\n\n\nNow the version and siteType properties are added to every server-side log. Of course we could add additional “global” properties using the same code if we needed more.\n\nTracing\n\nLast week we had an issue with our site – there’s a signup process in which we generate an access code and customers then enter the access code and enable integration with their Azure Active Directory so that their users can authenticate against their AAD when logging into our site. Customers started reporting that the access code “wasn’t found”. The bug turned out to be the fact that a static variable on a base class is shared across all child instances too – so our Azure Table data access classes were pointing to the incorrect tables (We fixed the issue using a curiously recurring generic base class – a study for another day) but the issue had us stumped for a while.\n\nInitially I thought, “I can debug this issue quickly – I have AppInsights on the site so I can see what’s going on.” Turns out that there wasn’t any exception for the issue – the data access searched for an entity and couldn’t find it, so it reported the “access code not found” error that our customers were seeing. I didn’t have AppInsights tracing enabled – so I immediately set about adding it.\n\nFirst, you install the Microsoft.ApplicationInsights.TraceListener package from NuGet. Then you can pepper your code with trace calls to System.Diagnostics.Trace – each one is sent to AppInsights by the TraceListener.\n\nWe decided to create an ILogger interface and a base class that just did a call to System.Diagnostics.Trace. Here’s a snippet:\n\npublic abstract class BaseLogger : ILogger\n{\n    public virtual void TraceError(string message)\n    {\n        Trace.TraceError(message);\n    }\n\n    public virtual void TraceError(string message, params object[] args)\n    {\n        Trace.TraceError(message, args);\n    }\n\n    public virtual void TraceException(Exception ex)\n    {\n        Trace.TraceError(ex.ToString());\n    }\n\n    // ... TraceInformation and TraceWarning methods same as above\n\n    public virtual void TraceCustomEvent(string eventName, IDictionary&amp;lt;string, string&amp;gt; properties = null, IDictionary&amp;lt;string, double&amp;gt; metrics = null)\n    {\n        var propertiesStr = \"\";\n        if (properties != null)\n        {\n            foreach (var key in properties.Keys)\n            {\n                propertiesStr += string.Format(\"{0}{1}{2}\", key, properties[key], Environment.NewLine);\n            }\n        }\n\n\n        var metricsStr = \"\";\n        if (metrics != null)\n        {\n            foreach (var key in metrics.Keys)\n            {\n                metricsStr += string.Format(\"{0}{1}{2}\", key, metrics[key], Environment.NewLine);\n            }\n        }\n\n        Trace.TraceInformation(\"Custom Event: {0}{1}{2}{1}{3}\", eventName, Environment.NewLine, propertiesStr, metricsStr);\n    }\n}\n\n\nThe TraceInformation and TraceError methods are pretty straightforward – the TraceCustomEvent was necessary to enable custom telemetry. Using the logger to add tracing and exception logging is easy. We inject an instance of our AppInsightsLogger (more on this later) and then we can use it to log. Here’s an example of our GET videos method (we use NancyFx which is why this is an indexer method):\n\nGet[\"/videos\"] = p =&amp;gt;\n{\n    try\n    {\n        logger.TraceInformation(\"[/Videos] Returning {0} videos\", videoManager.Videos.Count);\n        return new JsonResponse(videoManager.Videos, new EastonJsonNetSerializer());\n    }\n    catch (Exception ex)\n    {\n        logger.TraceException(ex);\n        throw ex;\n    }\n};\n\n\nCustom Telemetry\n\nOut of the box you get a ton of great logging in AppInsights – page views (including browser type, region, language and performance) and server side requests, exceptions and performance. However, we wanted to start doing some custom analytics on usage. Our application is multi-tenant, so we wanted to track the tenantId as well as the user. We want to track each time a user views a video so we can see which users (across which tenants) are accessing which videos. Here’s the call we make to log that a user has accessed a video:\n\n\nlogger.TraceCustomEvent(\"ViewVideo\", new Dictionary&lt;string, string&gt;() { { \"TenantId\", tenantId }, { \"User\", userId }, { \"VideoId\", videoId } });\n\n\nThe method in the AppInsightsLogger is as follows:\n\npublic override void TraceCustomEvent(string eventName, IDictionary&amp;lt;string, string&amp;gt; properties = null, IDictionary&amp;lt;string, double&amp;gt; metrics = null)\n{\n    AppInsights.TrackEvent(eventName, properties, metrics);\n}\n\n\nPretty simple.\n\nAnalytics Queries\n\nNow that we’re getting some telemetry, including requests and custom events, we can start to query. Logging on to the Azure Portal I navigate to the AppInsights instance and click on the Analytics button in the toolbar:\n\n\n\n\nThat will open the AppInsights Analytics page. Here I can start querying my telemetry. There are several “tables” that you can query – requests, traces, exceptions and so on. If I want to see the performance percentiles of my requests in 1 hour bins for the last 7 days, I can use this query which calculates the percentiles and then renders to a time chart:\n\n\n\n\nrequests\n| where timestamp &amp;gt;= ago(7d)\n| summarize percentiles(duration,50,95,99) by bin (timestamp, 1h)\n| render timechart\n\n\nThe query syntax is fairly “natural” though I did have to look at these help docs to get to grips with the language.\n\nSweet!\n\nYou can even join the tables. Here’s an example from Brian Harry’s post that correlates exceptions and requests:\n\n\n\n\nrequests\n| where timestamp &amp;gt; ago(2d)\n| where success == \"False\"\n| join kind=leftouter (\n    exceptions\n    | where timestamp &amp;gt; ago(2d)\n) on operation_Id\n| summarize exceptionCount=count() by operation_Name\n| order by exceptionCount asc\n\n\nNote that I did have some trouble with the order by direction – it could be a bug (this is still in preview) or maybe I just don’t understand the ordering will enough.\n\nHere are a couple of queries against our custom telemetry:\n\n\n\n\ncustomEvents\n| where timestamp &amp;gt; ago(7d)\n| where name == \"ValidateToken\"\n| extend user = tostring(customDimensions.User), tenantId = tostring(customDimensions.TenantId)\n| summarize logins = dcount(user) by tenantId, bin(timestamp, 1d)\n| order by logins asc\n\n\nAgain, the ordering direction seems odd to me.\n\nI love the way that the customDimensions (which is just a json snippet) is directly addressable. Here’s what the json looks like for our custom events:\n\n\n\n\nYou can see how the “siteType” property is there because of our ITelemetryInitializer.\n\nVisualizations\n\nAfter writing a couple queries, we can then add a visualization by adding a render clause. You’ve already seen the “render timechart“ above – but there’s also piechart, barchart and table. Here’s a query that renders a stacked bar chart showing user views (per tenant) in hourly bins:\n\ncustomEvents\n| where timestamp &amp;gt;= ago(7d)\n| extend user = tostring(customDimensions.User), videoId = tostring(customDimensions.VideoId), tenantId = tostring(customDimensions.TenantId)\n| summarize UserCount = dcount(user) by tenantId, bin (timestamp, 1h)\n| render barchart\n\n\n\n\n\nThis is just scratching the surface, but I hope you get a feel for what this tool can bring out of your telemetry.\n\nExporting Data to PowerBI\n\nThe next step is to make a dashboard out of the queries that we’ve created. You can export to Excel, but for a more dynamic experience, you can also export to PowerBI. I was a little surprised that when I clicked “Export to PowerBI” I got a text file. Here’s the same bar chart query exported to PowerBI:\n\n/*\nThe exported Power Query Formula Language (M Language ) can be used with Power Query in Excel \nand Power BI Desktop. \nFor Power BI Desktop follow the instructions below: \n 1) Download Power BI Desktop from https://powerbi.microsoft.com/en-us/desktop/ \n 2) In Power BI Desktop select: 'Get Data' -&amp;gt; 'Blank Query'-&amp;gt;'Advanced Query Editor' \n 3) Paste the M Language script into the Advanced Query Editor and select 'Done' \n*/\n\n\nlet\nSource = Json.Document(Web.Contents(\"https://management.azure.com/subscriptions/someguid/resourcegroups/rg/providers/microsoft.insights/components/app-insights-instance/api/query?api-version=2014-12-01-preview\", \n[Query=[#\"csl\"=\"customEvents| where timestamp &amp;gt;= ago(7d)| extend user = tostring(customDimensions.User), videoId = tostring(customDimensions.VideoId), tenantId = tostring(customDimensions.TenantId)| summarize UserCount = dcount(user) by tenantId, bin (timestamp, 1h)| render barchart\"]])),\nSourceTable = Record.ToTable(Source), \nSourceTableExpanded = Table.ExpandListColumn(SourceTable, \"Value\"), \nSourceTableExpandedValues = Table.ExpandRecordColumn(SourceTableExpanded, \"Value\", {\"TableName\", \"Columns\", \"Rows\"}, {\"TableName\", \"Columns\", \"Rows\"}), \nRowsList = SourceTableExpandedValues{0}[Rows], \nColumnsList = SourceTableExpandedValues{0}[Columns],\nColumnsTable = Table.FromList(ColumnsList, Splitter.SplitByNothing(), null, null, ExtraValues.Error), \nColumnNamesTable = Table.ExpandRecordColumn(ColumnsTable, \"Column1\", {\"ColumnName\"}, {\"ColumnName\"}), \nColumnsNamesList = Table.ToList(ColumnNamesTable, Combiner.CombineTextByDelimiter(\",\")), \nTable = Table.FromRows(RowsList, ColumnsNamesList), \nColumnNameAndTypeTable = Table.ExpandRecordColumn(ColumnsTable, \"Column1\", {\"ColumnName\", \"DataType\"}, {\"ColumnName\", \"DataType\"}), \nColumnNameAndTypeTableReplacedType1 = Table.ReplaceValue(ColumnNameAndTypeTable,\"Double\",Double.Type,Replacer.ReplaceValue,{\"DataType\"}), \nColumnNameAndTypeTableReplacedType2 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType1,\"Int64\",Int64.Type,Replacer.ReplaceValue,{\"DataType\"}), \nColumnNameAndTypeTableReplacedType3 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType2,\"Int32\",Int32.Type,Replacer.ReplaceValue,{\"DataType\"}), \nColumnNameAndTypeTableReplacedType4 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType3,\"Int16\",Int16.Type,Replacer.ReplaceValue,{\"DataType\"}), \nColumnNameAndTypeTableReplacedType5 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType4,\"UInt64\",Number.Type,Replacer.ReplaceValue,{\"DataType\"}), \nColumnNameAndTypeTableReplacedType6 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType5,\"UInt32\",Number.Type,Replacer.ReplaceValue,{\"DataType\"}), \nColumnNameAndTypeTableReplacedType7 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType6,\"UInt16\",Number.Type,Replacer.ReplaceValue,{\"DataType\"}), \nColumnNameAndTypeTableReplacedType8 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType7,\"Byte\",Byte.Type,Replacer.ReplaceValue,{\"DataType\"}), \nColumnNameAndTypeTableReplacedType9 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType8,\"Single\",Single.Type,Replacer.ReplaceValue,{\"DataType\"}), \nColumnNameAndTypeTableReplacedType10 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType9,\"Decimal\",Decimal.Type,Replacer.ReplaceValue,{\"DataType\"}), \nColumnNameAndTypeTableReplacedType11 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType10,\"TimeSpan\",Duration.Type,Replacer.ReplaceValue,{\"DataType\"}), \nColumnNameAndTypeTableReplacedType12 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType11,\"DateTime\",DateTimeZone.Type,Replacer.ReplaceValue,{\"DataType\"}),\nColumnNameAndTypeTableReplacedType13 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType12,\"String\",Text.Type,Replacer.ReplaceValue,{\"DataType\"}),\nColumnNameAndTypeTableReplacedType14 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType13,\"Boolean\",Logical.Type,Replacer.ReplaceValue,{\"DataType\"}), \nColumnNameAndTypeTableReplacedType15 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType14,\"SByte\",Logical.Type,Replacer.ReplaceValue,{\"DataType\"}), \nColumnNameAndTypeTableReplacedType16 = Table.SelectRows(ColumnNameAndTypeTableReplacedType15, each [DataType] is type), \nColumnNameAndTypeList = Table.ToRows(ColumnNameAndTypeTableReplacedType16), \nTypedTable = Table.TransformColumnTypes(Table, ColumnNameAndTypeList) \nin\nTypedTable\n\n\nAh, so I’ll need PowerBI desktop. No problem. Download it, open it and follow the helpful instructions in the comments at the top of the file:\n\n\n\n\nNow I can create visualizations, add custom columns – do whatever I would normally do in PowerBI.\n\nOne thing I did want to do was fix up the nasty “tenantId”. This is a guid which is the Partition Key for an Azure Table that we use to store our tenants. So I just added a new Query to the report to fetch the tenant data from the table. Then I was able to create a relationship (i.e. foreign key) that let me use the tenant name rather than the nasty guid in my reports:\n\n\n\n\nHere’s what the relationship looks like for the “Users Per Tenant Per Hour Query”:\n\n\n\n\nOnce I had the tables in, I could create reports. Here’s a performance report:\n\n\n\n\nOne tip – when you add the “timestamp” property, PowerBI defaults to a date hierarchy (Year, Quarter, Month, Day). To use the timestamp itself, you can just click on the field in the axis box and select “timestamp” from the values:\n\n\n\n\nHere’s one of our usage reports:\n\n\n\n\nAnd of course, once I’ve written the report, I can just upload it to PowerBI to share with the team:\n\n\n\n\nLook ma – it’s the same report!\n\nConclusion\n\nIf you’re not doing APM, then you need to get into AppInsights. If you’re already using AppInsigths, then it’s time to move beyond logging telemetry to actually analyzing telemetry and gaining insights from your applications using AppInights Analytics.\n\nHappy analyzing!\n",
      "categories": [],
      "tags": ["appinsights","devops"],
      
      "collection": "posts",
      "url": "/appinsights-analytics-in-the-real-world/"
    },{
      
      "title": "Continuous Deployment of Service Fabric Apps using VSTS (or TFS)",
      "date": "2016-04-28 19:41:34 +0000",
      
      "content": "Azure’s Service Fabric is breathtaking – the platform allows you to create truly “born in the cloud” apps that can really scale. The platform takes care of the plumbing for you so that you can concentrate on business value in your apps. If you’re looking to create cloud apps, then make sure you take some time to investigate Service Fabric.\n\nPublishing Service Fabric Apps\n\nUnfortunately, most of the samples (like this getting started one or this more real-world one) don’t offer any guidance around continuous deployment. They just wave hands and say, “Publish from Visual Studio” or “Publish using PowerShell”. Which is all well and good – but how do you actually do proper DevOps with ServiceFabric Apps?\n\nPublishing apps to Service Fabric requires that you package the app and then publish it. Fortunately VSTS allows you to fairly easily package the app in an automated build and then publish the app in a release.\n\nThere are two primary challenges to doing this:\n\n\n  Versioning. Versioning is critical to Service Fabric apps, so your automated build is going to have to know how to version the app (and its constituent services) correctly\n  Publishing – new vs upgrade. The out-of-the-box publish script (that you get when you do a File-&gt;New Service Fabric App project) needs to be invoked differently for new apps as opposed to upgrading existing apps. In the pipeline, you want to be able to publish the same way – whether or not the application already exists. Fortunately a couple modifications to the publish script do the trick.\n\n\nFinally, the cluster should be created or updated on the fly during the release – that’s what the ARM templates do.\n\nTo demonstrate a Service Fabric build/release pipeline, I’m going to use a “fork” of the original VisualObjects sample from the getting started repo (it’s not a complete fork since I just wanted this one solution from the repo). I’ve added an ARM template project to demonstrate how to create the cluster using ARM during the deployment and then I’ve added two publishing profiles – one for Test and one for Prod. The ARM templates and profiles for both Test and Prod are exactly the same in the repo – in real life you’ll have a beefier cluster in Prod (with different application parameters) than you will in test, so the ARM templates and profiles are going to look different. Having two templates and profiles gives you the idea of how to separate environments in the Release, which is all I want to demonstrate.\n\nThis entire flow works on TFS as well as VSTS, so I’m just going to show you how to do this using VSTS. I’ll call out differences for TFS when necessary.\n\nGetting the Code\n\nThe easiest way is to just fork this repo on Github. You can of course clone the repo, then push it to a VSTS project if you prefer. For this post I’m going to use code that I’ve imported into a VSTS repo. If you’re on TFS, then it’s probably easiest to clone the repo and push it to your TFS server.\n\nSetting up the Build\n\nUnfortunately the Service Fabric SDK isn’t installed on the hosted agent image in VSTS, so you’ll have to use a private agent. Make sure the Service Fabric SDK is installed on the build machine. Use this help doc to get the bits.\n\nThe next thing you’ll need is my VersionAssemblies custom build task. I’ve bundled it into a VSTS marketplace extension. If you’re on VSTS, just click “Install” – if you’re on TFS, you’ll need to download it and upload it. You’ll only be able to do this on TFS 2015 Update 2 or later.\n\nNow go to your VSTS account and navigate to the Code hub. Create a new Build definition using the Visual Studio template. Select the appropriate source repo and branch (I’m just going to use master) and select the queue with your private agent. Select Continuous Integration to queue the build whenever a commit is pushed to the repo:\n\n\n\n\nChange the name of the build – I’ve called mine “VisualObjects”. Go to the General tab and change the build number format to be\n\n\n1.0$(rev:.r)\n\n\nThis will give the build number 1.0.1, then 1.0.2, 1.0.3 and so on.\n\nNow we want to change the build so that it will match the ApplicationTypeVersion (from the application manifest) and all the Service versions within the ServiceManifests for each service within the application. So click “Add Task” and add two “VersionAssembly” tasks. Drag them to the top of the build (so that they are the first two tasks executed).\n\nConfigure the first one as follows:\n\n\n\n\nConfigure the second one as follows:\n\n\n\n\nThe first task finds the ApplicationManifest.xml file and replaces the version with the build number. The second task recursively finds all the ServiceManifest.xml files and then also replaces the version number of each service with the build number. After the build, the application and service versions will all match the build number.\n\nThe next 3 tasks should be “NuGet Installer”, “Visual Studio Build” and “Visual Studio Test”. You can leave those as is.\n\nAdd a new “Visual Studio Build” task and place it just below the test task. Configure the Solution parameter to the path of the .sfproj in the solution (src/VisualObjects/VisualObjects/VisualObjects.sfproj). Make the MSBuild Arguments parameter “/t:Package). Finally, add $(BuildConfiguration) to the Configuration parameter. This task invokes Visual Studio to package the Service Fabric app:\n\n\n\n\nNow you’ll need to do some copying so that we get all the files we need into the artifact staging directory, ready for publishing. Add a couple “Copy” tasks to the build and configure them as follows:\n\n\n\n\nThis copies the Service Fabric app package to the staging directory.\n\n\n\n\nThis copies the Scripts folder to the staging directory (we’ll need this in the release to publish the app).\n\n\n\n\nThese tasks copy the Publish Profiles and ApplicationParameters files to the staging directory. Again, these are needed for the release.\n\nYou’ll notice that there isn’t a copy task for the ARM project – that’s because the ARM project automagically puts its output into the staging directory for you when building the solution.\n\nYou can remove the Source Symbols task if you want to – it’s not going to harm anything if it’s there. If you really want to keep the symbols you’ll have to specify a network share for the symbols to be copied to.\n\nFinally, make sure that your “Publish Build Artifacts” task is configured like this:\n\n\n\n\nOf course you can also choose a network folder rather than a server drop if you want. The tasks should look like this:\n\n\n\n\nRun the build to make sure that it’s all happy. The artifacts folder should look like this:\n\n\n\nSetting up the Release\n\nNow that the app is packaged, we’re almost ready to define the release pipeline. There’s a decision to make at this point: to ARM or not to ARM. In order to create the Azure Resource Group containing the cluster from the ARM template, VSTS will need a secure connection to the Azure subscription (follow these instructions). This connection is service principal based, so you need to have an AAD backing your Azure subscription and you need to have permissions to add new applications to the AAD (being an administrator or co-admin will work – there may be finer-grained RBAC roles for this, I’m not sure). However, if you don’t have an AAD backing your subscription or can’t create applications, you can manually create the cluster in your Azure subscription. Do so now if you’re going to create the cluster(s) manually (one for Test, one for Prod).\n\nTo create the release definition, go to the Release hub in VSTS and create a new (empty) Release. Select the VisualObjects build as the artifact link and set Continuous Deployment. This will cause the release to be created as soon as a build completes successfully. (If you’re on TFS, you will have to create an empty Release and then link the build in the Artifacts tab). Change the name of the release to something meaningful (I’ve called mine VisualObjects, just to be original).\n\nChange the name of the first environment to “Test”. Edit the variables for the environment and add one called “AdminPassword” and another called “ClusterName”. Set the admin password to some password and padlock it to make it a secret. The name that you choose for the cluster is the DNS name that you’ll use to address your cluster. In my case, I’ve selected “colincluster-test” which will make the URL of my cluster “colincluster-test.eastus.cloudapp.azure.com”.\n\n\n\nCreate or Update the Cluster\n\nIf you created the cluster manually, skip to the next task. If you want to create (or update) the cluster as part of the deployment, then add a new “Azure Resource Group Deployment” task to the Test environment. Set the parameters as follows:\n\n\n  Azure Connection Type: Azure Resource Manager\n  Azure RM Subscription: set this to the SPN connection you created from these instructions\n  Action: Create or Update Resource Group\n  Resource Group: a name for the resource group\n  Location: the location of your resource group\n  Template: brows to the TestServiceFabricClusterTemplate.json file in the drop using the browse button (…)\n  Template Parameters: brows to the TestServiceFabricClusterTemplate.parameters.json file in the drop using the browse button (…)\n  Override Template Parameters: set this to -adminPassword (ConvertTo-SecureString ‘$(AdminPassword)’ -AsPlainText -Force) –dnsName $(ClusterName)\n\n\nYou can override any other parameters you need to in the Override parameters setting. For now, I’m just overriding the clusterName and adminPassword parameters.\n\n\n\nReplace Tokens\n\nThe Service Fabric profiles contain the cluster connection information. Since you could be creating the cluster on the fly, I’ve tokenized the connection setting in the profile files as follows:\n\n&amp;lt;?xml version=\"1.0\" encoding=\"utf-8\"?&amp;gt;\n&amp;lt;PublishProfile xmlns=\"http://schemas.microsoft.com/2015/05/fabrictools\"&amp;gt;\n  &amp;lt;!-- ClusterConnectionParameters allows you to specify the PowerShell parameters to use when connecting to the Service Fabric cluster.\n       Valid parameters are any that are accepted by the Connect-ServiceFabricCluster cmdlet.\n       \n       For a remote cluster, you would need to specify the appropriate parameters for that specific cluster.\n         For example: &amp;lt;ClusterConnectionParameters ConnectionEndpoint=\"mycluster.westus.cloudapp.azure.com:19000\" /&amp;gt;\n\n       Example showing parameters for a cluster that uses certificate security:\n       &amp;lt;ClusterConnectionParameters ConnectionEndpoint=\"mycluster.westus.cloudapp.azure.com:19000\"\n                                    X509Credential=\"true\"\n                                    ServerCertThumbprint=\"0123456789012345678901234567890123456789\"\n                                    FindType=\"FindByThumbprint\"\n                                    FindValue=\"9876543210987654321098765432109876543210\"\n                                    StoreLocation=\"CurrentUser\"\n                                    StoreName=\"My\" /&amp;gt;\n\n  --&amp;gt;\n  &amp;lt;!-- Put in the connection to the Prod cluster here --&amp;gt;\n  &amp;lt;ClusterConnectionParameters ConnectionEndpoint=\" __ClusterName__.eastus.cloudapp.azure.com:19000\" /&amp;gt;\n  &amp;lt;ApplicationParameterFile Path=\"..\\ApplicationParameters\\TestCloud.xml\" /&amp;gt;\n  &amp;lt;UpgradeDeployment Mode=\"Monitored\" Enabled=\"true\"&amp;gt;\n    &amp;lt;Parameters FailureAction=\"Rollback\" Force=\"True\" /&amp;gt;\n  &amp;lt;/UpgradeDeployment&amp;gt;\n&amp;lt;/PublishProfile&amp;gt;\n\n\nYou can see that there is a __ClusterName__ token (the highlighted line). You’ve already defined a value for cluster name that you used in the ARM task. Wouldn’t it be nice if you could simply replace the token called __ClusterName__ with the value of the variable called ClusterName? Since you’ve already installed the Colin’s ALM Corner Build and Release extension from the marketplace, you get the ReplaceTokens task as well, which does exactly that! Add a ReplaceTokens task and set it as follows:\n\n\n\n\nIMPORTANT NOTE! The templates I’ve defined are not secured. In production, you’ll want to secure your clusters. The connection parameters then need a few more tokens like the ServerCertThumbprint and so on. You can also make these tokens that the ReplaceTokens task can substitute. Just note that if you make any of them secrets, you’ll need to specify the secret values in the Advanced section of the task.\n\nDeploying the App\n\nNow that we have a cluster and we have a profile that can connect to the cluster, and we have a package ready to deploy, we can invoke the PowerShell scrip to deploy! Add a “Powershell Script” task and configure it as follows:\n\n\n  Type: File Path\n  Script filename: browse to the Deploy-FabricApplication.ps1 script in the drop folder (under drop/SFPackage/Scripts)\n  Arguments: Set to -PublishProfileFile ../PublishProfiles/TestCloud.xml -ApplicationPackagePath ../Package\n\n\nThe script needs to take at least the PublishProfile path and then the ApplicationPackage path. These paths are relative to the Scripts folder, so expand Advanced and set the working folder to the Scripts directory:\n\n\n\n\nThat’s it! You can now run the release to deploy it to the Test environment. Of course you can add other tasks (like Cloud Load Tests etc.) and approvals. Go wild.\n\nChanges to the OOB Deploy Script\n\nI mentioned earlier that this technique has a snag: if the release creates the cluster (or you’ve created an empty cluster manually) then the Deploy script will fail. The reason is that the profile includes an &lt;UpgradeDeployment&gt; tag that tells the script to upgrade the app. If the app exists, the script works just fine – but if the app doesn’t exist yet, the deployment will fail. So to work around this, I modified the OOB script slightly. I just query the cluster to see if the app exists, and if it doesn’t, the script calls the Publish-NewServiceFabricApplication cmdlet instead of the Publish-UpgradedServiceFabricApplication. Here are the changed lines:\n\n$IsUpgrade = ($publishProfile.UpgradeDeployment -and $publishProfile.UpgradeDeployment.Enabled -and $OverrideUpgradeBehavior -ne 'VetoUpgrade') -or $OverrideUpgradeBehavior -eq 'ForceUpgrade'\n\n# check if this application exists or not\n$ManifestFilePath = \"$ApplicationPackagePath\\ApplicationManifest.xml\"\n$manifestXml = [Xml] (Get-Content $ManifestFilePath)\n$AppTypeName = $manifestXml.ApplicationManifest.ApplicationTypeName\n$AppExists = (Get-ServiceFabricApplication | ? { $_.ApplicationTypeName -eq $AppTypeName }) -ne $null\n\nif ($IsUpgrade -and $AppExists)\n\n\nLines 1 to 185 of the script are original, (I show line 185 as the first line of this snippet). The if statement alters slightly to take the $AppExists into account – the remainder of the script is as per the OOB script.\n\n\n\n\nNow that you have the Test environment, you can clone it to the Prod environment. Change the parameter values (and the template and profile paths) to make the prod-specific and you’re done! One more tip: if you change the release name format (under the general tab) to\n\n\n$(Build.BuildNumber)-$(rev:r)\n\n\nthen you’ll get the build number as part of the release number.\n\nHere you can see my cluster with the Application Version matching the build number:\n\n\n\n\nSweet! Now I can tell which build was used for my application right from my cluster!\n\nSee the Pipeline in Action\n\nA fun demo to do is to deploy the app and then open up the VisualObjects url – that will be at clustername.eastus.cloudapp.azure.com:8082/VisualObjects (where clustername is the name of your cluster). When you see the bouncing triangles.\n\nThen you can edit\n\n\nsrc/VisualObjects/VisualObjects.ActorService/VisualObjectActor.cs\n\n\nin Visual Studio or in the Code hub in VSTS. Look around line 50 for\n\n\nvisualObject.Move(false);\n\n\nand change it to\n\n\nvisualObject.Move(true)\n\n\n. This will cause the triangle to start rotating. Commit the change and push it to trigger the build and the release. Then monitor the Service Fabric UI to see the upgrade trigger (from the release) and watch the triangles to see how they are upgraded in the Service Fabric rolling upgrade.\n\nConclusion\n\nService Fabric is awesome – and creating a build/release pipeline for Service Fabric apps in VSTS is a snap thanks to an amazing build/release engine – and some cool custom build tasks!\n\nHappy releasing!\n",
      "categories": [],
      "tags": ["devops","cloud","build"],
      
      "collection": "posts",
      "url": "/continuous-deployment-of-service-fabric-apps-using-vsts-or-tfs/"
    },{
      
      "title": "Updating XAML Release Builds after Upgrading Release Management Legacy from 2013 to 2015",
      "date": "2016-05-17 21:13:18 +0000",
      
      "content": "You need to get onto the new Release Management (the web-based one) in VSTS or TFS 2015 Update 2. The new version is far superior to the old version for numerous reasons – it uses the new Team Build cross-platform agent, has a much simpler UI for designing releases, has better logging etc. etc.\n\nHowever, I know that lots of teams are invested in Release Management “legacy”. Over the weekend I helped a customer upgrade their TFS servers from 2013 to 2015.2.1. Part of this included upgrading their Release Management Server from 2013 to 2015. This customer has been using Release Management since it was still InRelease! They have a large investment in their current release tools, so they need it to continue working so that they can migrate over time.\n\nThe team also trigger releases in Release Management from their XAML builds. Unfortunately, their builds started breaking once we upgraded the Release Management client on the build servers. The build error was something like: “Invalid directory”. (Before upgrading the client, the release step failed saying that the build service needed to be set up as a Service User – which it was. This error is misleading – it’s an indication that you need to upgrade the RM Client on the build machine).\n\nUpgrading XAML Build Definitions\n\nIt turns out that the Release Management XAML templates include a step that reads the registry to obtain the location of the Release Management client binaries. This registry key has changed from RM 2013 to 2015, so you have two options:\n\n\n  If you used the older ReleaseGitTemplate.12.xaml or ReleaseTfvcTemplate12.xaml files from RM 2013, then you can replace them with the updated release management templates that ship with Release Management client (find them in \\Program Files (x86)\\ Microsoft Visual Studio 14.0\\ReleaseManagement\\bin)\n  If you customized your own templates (or customized the RM 2013 templates), you need to update your release template\n\n\nFortunately updating existing templates to work with the new RM client is fairly trivial. Here are the steps:\n\n\n  Check out your existing XAML template\n  Open it in Notepad (or using the XML editor in VS)\n  Find the task with DisplayName “Get the Release Management install directory”. One of the arguments is a registry key – it will be something like HKEY_LOCAL_MACHINE\\Software\\Microsoft\\ReleaseManagement\\12.0\\Client. Replace this key with this value: *HKEY_LOCAL_MACHINE\\Software\\WOW6432Node\\Microsoft\\ReleaseManagement\\14.0\\Client*\n  The task just below is for finding the x64 directory – you can do the same replacement in this task.\n  Commit your changes and checkin\n  Build and release\n  Party\n\n\nThanks to Jesse Arens for this great find!\n\nOn a side note – the ALM Rangers have a project that will help you port your “legacy” RM workflows to the new web-based releases. You can find it here.\n\nHappy releasing! (Just move off XAML builds and Release Management legacy as soon as possible – for your own sanity!)\n",
      "categories": [],
      "tags": ["releasemanagement"],
      
      "collection": "posts",
      "url": "/upgrading-release-management-legacy-from-2013-to-2015/"
    },{
      
      "title": "DotNet Core, VS 2015, VSTS and Docker",
      "date": "2016-07-20 20:52:30 +0000",
      
      "content": "I unashamedly love Docker. Late last year I posted some thoughts I had on Docker DevOps. In this post I’m going to take a look at Docker DevOps using DotNet Core 1.0.0, Docker Tools for Visual Studio, Docker for Windows and VSTS.\n\nJust before I continue – I’m getting tired of typing “DotNet Core 1.0.0” so for the rest of this post when I say “.NET Core” I mean “DotNet Core 1.0.0” (unless otherwise stated!).\n\nHighlights\n\nFor those of you that just want the highlights, here’s a quick summary:\n\n\n  .NET Core does indeed run in a Docker container\n  You can debug a .Net Core app running in a container from VS\n  You can build and publish a DotNet Core app to a docker registry using VSTS Build\n  You can run a Docker image from a registry using VSTS Release Management\n  You can get frustrated by the lack of guidance and the amount of hacking required currently\n\n\nSo what’s the point of this anyway? Well I wanted to know if I could create the following workflow:\n\n\n  Docker for Windows as a Docker host for local dev\n  Visual Studio with the Docker Tools for VS for local debugging within a container\n  VSTS for building (and publishing) a Docker image with an app\n  VSTS for releasing (running) an image\n\n\nThis is a pretty compelling workflow, and I was able to get it running relatively easily. One of the biggest frustrations was the lack of documentation and the immaturity of some of the tooling.\n\nGrab a cup of coffee (or tea or chai latte – or if it’s after hours, a good IPA) and I’ll take you on my journey!\n\nDocker Host in Azure\n\nI started my journey from this post in article: Deploy ASP.NET Core 1.0 apps to a Docker Container (aka the VSTS Docker article). While certainly helpful, there are some issue with this article. Firstly, it’s designed for ASP.NET Core 1.0.0-rc1-update1 and not the RTM release (1.0.0). This mainly had some implications for the Dockerfile, but wasn’t too big an issue. The bigger issue is that it’s a little ambiguous in places, and the build instructions were quite useless. We’ll get to that later.\n\nAfter skimming the article, I decided to first stand up a Docker host in Azure and create the service connections that VSTS requires for performing Docker operations. Then, I figured, I’d be ready to start coding and I’ll have a host to deploy to.\n\nImmediately I hit a strange limitation – the Docker image in Azure can only be created using “classic” and not “Resource Group” mode. I ended up deciding that wasn’t too big a deal, but it’s still frustrating that the official image isn’t on the latest tech within Azure.\n\nThe next challenge was getting Docker secured. I followed the VSTS Docker articles link to instructions on how to protect the daemon socket. I generated the ssh keys without too much fuss. However, I ran into issues ensuring that the Docker daemon starts with the keys! The article doesn’t tell you how to do that (it tells you how to start Docker manually), so I had to scratch around a bit. I found that you could set the daemon startup options by editing /etc/default/docker, so I opened it up and edited the DOCKER_OPTS to look like this:\n\n\nDOCKER_OPTS=\"--tlsverify --tlscacert=/var/docker/ca.pem --tlscert=/var/docker/server-cert.pem --tlskey=/var/docker/server-key.pem -H=0.0.0.0:2376”\n\n\nOf course I copied the pem files to /var/docker. I then restarted the Docker service.\n\nIt didn’t work. After a long time of hacking, searching, sacrificing chickens and anything else I could think of to help, I discovered that the daemon ignores the /etc/default/docker file altogether! Perhaps it’s just the Azure VM and linux distro I’m on? Anyway, I had to edit the /etc/systemd/system/docker.service file. I changed the\n\n\nExecStart\n\n\ncommand and added an\n\n\nEnviromentFile\n\n\ncommand in the\n\n\n[Service]\n\n\nsection as follows:\n\n\nEnvironmentFile=/etc/default/docker\n\n\nExecStart=/usr/bin/docker daemon $DOCKER_OPTS\n\n\nNow when I restart the service (using\n\n\nsudo service restart docker\n\n\n) the daemon starts correctly and is protected with the keys.\n\nI could now run docker commands on the machine itself. However, I couldn’t run commands from my local machine (which is running Docker for Windows) because:\n\n\nError response from daemon: client is newer than server (client API version: 1.24, server API version: 1.23)\n\n\nI tried in vain to upgrade the Docker engine on the server, but could not for the life of me do it. The apt packages are on 1.23, and so eventually I gave up. I can run Docker commands by ssh-ing to the host machine if I really need to, so while irritating, it wasn’t a show-stopper.\n\n.NET Core and Docker in VS\n\nNow that I (finally) had a Docker host configured, I installed Docker Tools for Visual Studio onto my Visual Studio 2015.3. I also installed the .NET Core 1.0 SDK. I then did a File-&gt;New-&gt;Project and created an ASP.NET project – you know, the boilerplate one. I then followed the instructions from the VSTS article and right-clicked the project and selected “Add-&gt;Docker support”. This created a DockerTask.ps1 file, a Dockerfile (and Dockerfile.debug) and some docker-compose yml files. Great! However, nothing worked straight away (argh!) so I had to start debugging the scripts.\n\nI kept getting this error:\n\n\n.\\DockerTask.ps1 : The term '.\\DockerTask.ps1' is not recognized as the name of a cmdlet, function, script file, or operable program.\n\n\n After lots of hacking, I finally found that there is a path issue somewhere. I opened up the Properties\\Docker.targets file and edited the &lt;DockerBuildCommand&gt;: I changed “.\\DockerTask.ps1” to the full path – c:\\projects\\docker\\TestApp\\src\\TestApp\\DockerTask.ps1. I did the same for the &lt;DockerCleanCommand&gt;. This won’t affect the build, but other developers who share this code will have to have the same path structure for this to work. Gah!\n\n\nNow the command was being executed, but I was getting this error:\n\n\nNo machine name(s) specified and no “default” machines exist\n\n\n. I again opened the DockerTask.ps1 script. It’s checking for a machine name to invoke\n\n\ndocker-machine\n\n\ncommands, but it’s only supposed to do this if the Docker machine name is specified. For Docker for Windows, you don’t have to use\n\n\ndocker-machine\n\n\n, so the script makes provision for this by assuming Docker for Windows if the machine name is empty. At least, that’s what it’s supposed to do. For some reason, this line in the script is evaluating to true, even when $Machine was set to ‘’ (empty string):\n\nif (![System.String]::IsNullOrWhiteSpace($Machine))\n\n\nSo I commented out the entire if block since I’ve got Docker for Windows and don’t need it to do and\n\n\ndocker-machine\n\n\ncommands.\n\nNow at least the build operation was working, and I could see VS creating an image in my local Docker for Windows:\n\n\n\n\nNext I tried debugging an app in the container. No dice. The issue seemed to be that the container couldn’t start on port 80. Looking at the Dockerfile and the DockerTask.ps1 files, I saw that the port is hard-coded to 80. So I changed the port to 5000 (making it a variable in the ps1 script and an ARG in my Dockerfile). Just remember that you have a Dockerfile.debug as well – and that the ports are hard-coded in the docker-compose.yml and docker-compose.debug.yml files too. The image name is also hardcoded all over the place to “username/appname”. I tried to change it, but ended up reverting back. This only affects local dev, so I don’t really care that much.\n\nAt this point I could get the container to run in Release, so I knew Docker was happy. However, I couldn’t debug. I was getting this error:\n\n\n\n\nAgain a bit of googling led me to enable volume sharing in Docker for Windows (which is disabled by default). I clicked the moby in my task bar, opened the Docker settings and enabled volume sharing on my c drive:\n\n\n\n\nDebugging then actually worked – the script starts up a container (based on the image that gets created when you build) and attaches the remote debugger. Pretty sweet now that it’s working!\n\n\n\n\nIn the above image you can see how I’m navigating to the About page (the url is http://docker:5000) and VS is spewing logging into the console showing the server (in the container) responding to the request).\n\nOne more issue – the clean command wasn’t working. I kept getting this error:\n\n\nThe handle could not be duplicated during redirection of handle 1.\n\n\n Turns out some over-eager developer had the following line in function Clean() in the DockerTask.ps1 file:\n\n\nInvoke-Expression \"cmd /c $shellCommand `\"*&amp;gt;&amp;amp;1`\"\" | Out-Null\n\n\nI changed\n\n\n*&gt;&amp;1\n\n\nto\n\n\n2&gt;&amp;1\n\n\nlike this:\n\nInvoke-Expression \"cmd /c $shellCommand `\"2&amp;gt;&amp;amp;1`\"\" | Out-Null\n\n\nAnd now the clean was working great.\n\nSo I could get an ASP.NET Core 1.0 app working in VS in a container (with some work). Now for build and release automation in VSTS!\n\nBuild and Release in VSTS\n\nIn order to execute Docker commands during build or release in VSTS, you need to install the Docker extension from the marketplace. Once you’ve installed it, you’ll get some new service endpoint types as well as a Docker task for builds and releases. You need two connections: one to a Docker registry (like DockerHub) and one for a Docker host. Images are built on the Docker host and published to the registry during a build. Then an image can be pulled from the registry and run on the host during a release. So I created a new private DockerHub repo (using the account that I created on DockerHub to get access to Docker for Windows). This info I used to create the Docker registry endpoint. Next I copied all the keys I created on my Azure Docker host and created a service endpoint for my Docker host. The trick here was the URL – initially I had “http://my-docker-host.cloudapp.net:2376” but that doesn’t work – it has to be “tcp://my-docker-host.cloudapp.net:2376”.\n\nThe cool thing about publishing to the repo is that you can have any number of hosts pull the image to run it!\n\nNow I had the endpoints ready for build/deploy. I then added my solution to a Git repo and pushed to VSTS. Here’s the project structure:\n\n\n\n\nI then set up a build. In the VSTS Docker article, they suggest just two Docker tasks: the first with a “Build” action and the second with a “Push” action. However, I think this is meant to copy the source to the image and have the image do a dotnet restore – else how it work? However, I wanted the build to do the dotnet restore and publish (and test) and then just have the output bundled into the Docker image (as well as uploaded as a build drop). So I had to include two “run command” tasks and a publish build artifacts task. Here’s what my build definition ended up looking like:\n\n\n\n\nThe first two commands are fairly easy – the trick is setting the working directory (to the folder containing the project) and the correct framework and runtimes for running inside a Docker container:\n\n\n\n\nYou’ll see that I output the published site to $(Build.ArtifactStagingDirectory)/site/app which is important for the later Docker commands.\n\nI also created two variables (the values of which I got from the DockerTask.ps1 script) for this step:\n\n\n\n\nFor building the Docker image, I specified the following arguments:\n\n\n\n\nI use the two service endpoints I created earlier and set the action to “Build an Image”. I then specify the path to the Dockerfile – initially I browsed to the location in the src folder, but I want the published app so I changed this to the path in the artifact staging directory (otherwise Docker complains that the Dockerfile isn’t within the context). I then specify a repo/tag name for the Image Name, and use the build number for the version. Finally, the context is the folder which contains the “app” folder – the Dockerfile needs to be in this location. This location is used as the root for any Dockerfile COPY commands.\n\nNext step is publishing the image – I use the same endpoints, change the action to “Push an image” and specify the same repo/tag name:\n\n\n\n\nNow after running the build, I can see the image in my DockerHub repo (you can see how the build number and tag match):\n\n\n\n\nNow I could turn to the release. I have a single environment release with a single task:\n\n\n\n\nI named the ARG for the port in my Dockerfile APP_PORT, so I make sure it’s set to 5000 in the “Environment Variables” section. The example I followed had the HOST_PORT specified as well – I left that in, though I don’t know if it’s necessary. I linked the release to the build, so I can use the $(Build.BuildNumber) to specify which version (tag) of the container this release needs to pull.\n\nInitially the release failed while attempting to download the drops. I wanted the drops to enable deployment of the build somewhere else (like Azure webapps or IIS), so this release doesn’t need them. I configured this environment to “Skip artifact download”:\n\n\n\n\nLo and behold, the release worked after that! Unfortunately, I couldn’t browse to the site (connection refused). After a few moments of thinking, I realized that the Azure VM probably didn’t allow traffic on port 5000 – so I headed over to the portal and added an new endpoint (blegh – ARM network security groups are so much better):\n\n\n\n\nAfter that, I could browse to the ASP.NET Core 1.0 App that I developed in VS, debugged in Docker for Windows, source controlled in Git in VSTS, built and pushed in VSTS build and released in VSTS Release Management. Pretty sweet!\n\n\n\nConclusion\n\nThe Docker workflow is compelling, and having it work (nearly) out the box for .NET Core is great. I think teams should consider investing into this workflow as soon as possible. I’ve said it before, and I’ll say it again – containers are the way of the future! Don’t get left behind – start learning Docker today and skill up for the next DevOps wave – especially if you’re embarking on .NET Core dev!\n\nHappy Dockering!\n",
      "categories": [],
      "tags": ["devops","docker"],
      
      "collection": "posts",
      "url": "/dotnet-core-vs-2015-vsts-and-docker/"
    },{
      
      "title": "Running the New DotNet Core VSTS Agent in a Docker Container",
      "date": "2016-07-29 01:12:06 +0000",
      
      "content": "This week I finally got around to updating my VSTS extension (which bundle x-plat VersionAssembly and ReplaceTokens tasks) to use the new vsts-task-lib, which is used by the new DotNet Core vsts-agent. One of the bonuses of the new agent is that it can run in a DotNet Core Docker container! Since I am running Docker for Windows, I can now (relatively) easily spin up a test agent in a container to run test – a precursor to running the agent in a container as the de-facto method of running agents!\n\nAll you need to do this is a Dockerfile with a couple of commands that do the following:\n\n\n  Install Git\n  Create a non-root user and switch to it (since the agent won’t run as root)\n  Copy the agent tar.gz file and extract it\n  Configure the agent to connect it to VSTS\n\n\nPretty simple.\n\nThe Dockerfile\n\nLet’s take a look at the Dockerfile (which you can find here in Github) for an agent container:\n\nFROM microsoft/dotnet:1.0.0-core\n\n# defaults - override them using --build-arg\nARG AGENT_URL=://github.com/Microsoft/vsts-agent/releases/download/v2.104.0/vsts-agent-ubuntu.14.04-x64-2.104.0.tar.gz\nARG AGENT_NAME=docker\nARG AGENT_POOL=default\n\n# you must supply these to the build command using --build-arg\nARG VSTS_ACC\nARG PAT\n\n# install git\n#RUN apt-get update &amp;amp;&amp;amp; apt-get -y install software-properties-common &amp;amp;&amp;amp; apt-add-repository ppa:git-core/ppa\nRUN apt-get update &amp;amp;&amp;amp; apt-get -y install git\n\n# create a user\nRUN useradd -ms /bin/bash agent\nUSER agent\nWORKDIR /home/agent\n\n# download the agent tarball\n#RUN curl -Lo agent.tar.gz $AGENT_URL &amp;amp;&amp;amp; tar xvf agent.tar.gz &amp;amp;&amp;amp; rm agent.tar.gz\nCOPY *.tar.gz .\nRUN tar xzf *.tar.gz &amp;amp;&amp;amp; rm -f *.tar.gz\nRUN bin/Agent.Listener configure --url https://$VSTS_ACC.visualstudio.com --agent $AGENT_NAME --pool $AGENT_POOL --acceptteeeula --auth PAT --token $PAT --unattended\n\nENTRYPOINT ./run.sh\n\n\nNotes:\n\n\n  Line 1: We start with the DotNet Core 1.0.0 image\n  Lines 4-6: We create some arguments and set defaults\n  Lines 9-10: We create some args that don’t have defaults\n  Line 14: Install Git\n  This installs Git 2.1.4 from the official Jesse packages. We should be installing Git 2.9, but the only way to install it from a package source is to add a package source (line 13, which I commented out). Unfortunately apt-add-repository is inside the package software-properties-common, which introduces a lot of bloat to the container which I decided against. The VSTS agent will work with Git 2.1.4 (at least at present) so I was happy to leave it at that.\n  Line 17: create a user called agent\n  Line 18: switch to the agent user\n  Line 19: switch to the agent home directory\n  Line 23: Use this to download the tarball as part of building the container. Do it if you have enough bandwidth. I ended up downloading the tarball and putting it in the same directory as the Dockerfile and using Line 24 to copy it to the container\n  Line 24: Extract the tarball and then delete it\n  Line 25: Run the command to configure the agent in an unattended mode. This uses the args supplied through the file or from the docker build command to correctly configure the agent.\n  Line 27: Set an entrypoint – this is the command that will be executed when you run the container.\n\n\nPretty straightforward. To build the image, just cd to the Dockerfile folder and download the agent tarball (from here) if you’re going to use Line 23 (otherwise if you use Line 22, just make sure Line 4 has the latest release URL for Ubuntu 14.04 or use the AGENT_URL arg to supply it when building the image). Then run the following command:\n\ndocker build . --build-arg VSTS_ACC=myVSTSAcc --build-arg PAT=abd64… --build-arg AGENT_POOL=docker –t colin/agent\n\n\n\n  Mandatory: VSTS_ACC (which is the 1st part of your VSTS account URL – so for https://myVSTSAcc.visualstudio.com the VSTS_ACC is myVSTSAcc.\n  Mandatory: PAT – your Personal Auth Token\n  Optional: AGENT_POOL – the name of the agent pool you want the agent to register with\n  Optional: AGENT_NAME – the name of the agent\n  Optional: AGENT_URL – the URL to the Ubuntu 14.04 agent (if using Line 22)\n  The –t is the tag argument. I use colin/agent.\n\n\nThis creates a new image that is registered with your VSTS account!\n\nNow that you have an image, you can simply run it whenever you need your agent:\n\n&amp;gt; docker run -it colin/agent:latest\n\nScanning for tool capabilities.\nConnecting to the server.\n2016-07-28 17:56:57Z: Listening for Jobs\n\n\nAfter the docker run command, you should see the agent listening for jobs.\n\nGotcha – Self-Updating Agent\n\nOne issue I did run into is that I had downloaded agent 2.104.0. When the first build runs, the agent checks to see if there’s a new version available. In my case, 2.104.1 was available, so the agent updated itself. It also restarts – however, if it’s running in a container, when the agent stops, the container stops. The build fails with this error message:\n\n\nThe job has been abandoned because agent docker did not renew the lock. Ensure agent is running, not sleeping, and has not lost communication with the service.\n\n\nRunning the container again starts it with the older agent again, so you get into a loop. Here’s how to break the loop:\n\n\n  Run docker run -it –entrypoint=/bin/bash colin/agent:latest\n  This starts the container but just creates a prompt instead of starting the agent\n  In the container, run “./run.sh”. This will start the agent.\n  Start a build and wait for the agent to update. Check the version in the capabilities pane in the Agent Queue page in VSTS. The first build will fail with the above “renew lock” error.\n  Run a second build to make sure the agent is working correctly.\n  Now exit the container (by pressing Cntr-C and then typing exit).\n  Commit the container to a new image by running docker commit –change=’ENTRYPOINT ./run.sh’ &lt;containerId&gt; (you can get the containerId by running docker ps)\n  Now when you run the container using docker run –it colin/agent:latest your agent will start and will be the latest version. From there on, you’re golden!\n\n\nConclusion\n\nOverall, I was happy with how (relatively) easy it was to get an agent running in a container. I haven’t yet tested actually compiling a DotNet Core app – that’s my next exercise.\n\nHappy Dockering!\n",
      "categories": [],
      "tags": ["docker","build"],
      
      "collection": "posts",
      "url": "/running-the-new-dotnet-core-vsts-agent-in-a-docker-container/"
    },{
      
      "title": "Parallel Testing in a Selenium Grid with VSTS",
      "date": "2016-07-29 01:24:24 +0000",
      
      "content": "There are several different types of test – unit tests, functional test, load tests and so on. Generally, unit tests are the easiest to implement and have a high return on investment. Conversely, UI automation tests tend to be incredibly fragile, hard to maintain and don’t always deliver a huge amount of value. However, if you carefully design a good UI automation framework (especially for web testing) you can get some good mileage.\n\nEven if you get a good framework going, you’re going to want to find ways of executing the tests in parallel, since they can take a while. Enter Selenium Grid.  Selenium is a “web automation framework”. Under the hood, Selenium actually runs as a server which accepts ReST commands – those commands are wrapped in the Selenium client, so you usually see these HTTP commands. However, since the server is capable of driving a browser via HTTP, you can run tests remotely – that is, have tests run on one machine that execute over the wire to a browser on another machine. This allows you to scale your test infrastructure (by adding more machines). This is exactly what Selenium Grid does for you.\n\nMy intention with this post isn’t to show how you can create Selenium tests. Rather, I’ll show you some ins and outs of running Selenium tests in parallel in a Grid in a VSTS build/release pipeline.\n\nComponents for Testing in a Grid\n\nThere are a few moving parts we’ll need to keep track of in order for this to work:\n\n\n  The target site\n  The tests\n  The Selenium Hub (the master that coordinates the Grid)\n  The Selenium Nodes (the machines that are going to be executing the tests)\n  Selenium drivers\n  VSTS Build\n  VSTS Release\n\n\nThe cool thing about a Selenium grid is that, from a test perspective, you only need to know about the Selenium Grid hub. The nodes register with the hub and await commands to execute (i.e. run tests). The tests themselves just target the hub: “Hey Hub, I’ve got this test I want you to run for me on a browser with these capabilities…”. The hub then finds a node that meets the required capabilities and executes the tests remotely (via HTTP) on the node. Pretty sweet. This means you can scale the grid out without having to modify the tests at all.\n\nAgain lifting Selenium’s skirts we’ll see that a Selenium Node receives an instruction (like “use Chrome and navigate to google.com”). The node uses a driver for each browser (Firefox doesn’t have a driver, since Selenium knows how to drive it “natively”) to drive the browser. So when you configure a grid, you need to configure the Selenium drivers for each browser you want to test with on the grid (and by configure I mean copy to a folder).\n\nSetting Up a Selenium Grid\n\nIn experimenting with the grid, I decided to set up a two-machine Grid in Azure. Selenium Server (used to run the Hub and the Nodes) is a java application, so it’ll run wherever you can run Java. So I spun up a Resource Group with two VMs (Windows 2012 R2) and installed Java (and added the bin folder to the Path), Chrome and Firefox. I then downloaded the Selenium Server jar file, the IE driver and the Chrome driver (you can see the instructions on installing here). I put the IE and Chrome drivers into c:\\selenium\\drivers on both machines.\n\nI wanted one machine to be the Hub and run Chrome/Firefox tests, and have the other machine run IE/Firefox tests (yes, Nodes can run happily on the same machine or even on the same machine as the Hub). There are a myriad of options you can specify when you start a Hub or Node, so I scripted a general config that I thought would work as a default case.\n\nTo start the Hub, I created a one-line bat file in c:\\selenium\\server folder (where I put the server jar file):\n\njava -jar selenium-server-standalone-2.53.1.jar -role hub\n\n\nThis command starts up the Hub using port 4444 (the default) on the machine. Don’t forget to open the firewall for this port!\n\nConfiguring the nodes took a little longer to work out. The documentation is a bit all over the place (and ambiguous) so I eventually settled on putting some config in a JSON file and some I pass in to the startup command. Here’s the config JSON I have for the IE node:\n\n{\n  \"capabilities\":\n  [\n    {\n      \"browserName\": \"firefox\",\n      \"platform\": \"WINDOWS\",\n      \"maxInstances\": 1\n    },\n    {\n      \"browserName\": \"internet explorer\",\n      \"platform\": \"WINDOWS\",\n      \"maxInstances\": 1\n    }\n  ],\n  \"configuration\":\n  {\n    \"nodeTimeout\": 120,\n    \"nodePolling\": 2000,\n    \"timeout\": 30000\n  }\n}\n\n\nThis is only the bare minimum of config that you can specify – there are tons of other options that I didn’t need to bother with. All I wanted was for the Node to be able to run Firefox and IE tests. In a similar vein I specify the config for the Chrome node:\n\n{\n  \"capabilities\": [\n    {\n      \"browserName\": \"firefox\",\n      \"platform\": \"WINDOWS\",\n      \"maxInstances\": 1\n    },\n    {\n      \"browserName\": \"chrome\",\n      \"platform\": \"WINDOWS\"\n    }\n  ],\n  \"configuration\":\n  {\n    \"nodeTimeout\": 120,\n    \"nodePolling\": 2000,\n    \"timeout\": 30000\n  }\n}\n\n\nYou can see this is almost identical to the config of the IE node, except for the second browser type.\n\nI saved these files as ieNode.json and chromeNode.json in c:\\selenium\\server\\configs respectively.\n\nThen I created a simple PowerShell script that would let me start a node:\n\nparam(\n    $port,\n    [string]$hubHost,\n    $hubPort = 4444,\n\n    [ValidateSet('ie', 'chrome')]\n    [string]$browser,\n\n    [string]$driverPath = \"configs/drivers\"\n)\n\n$hubUrl = \"http://{0}:{1}/grid/register\" -f $hubHost, $hubPort\n$configFile = \"./configs/{0}Node.json\" -f $browser\n\njava -jar selenium-server-standalone-2.53.1.jar -role node -port $port -nodeConfig $configFile -hub $hubUrl -D\"webdriver.chrome.driver=$driverPath/chromedriver.exe\" -D\"webdriver.ie.driver=$driverPath/IEDriverServer.exe\"\n\n\nSo now I can run the following command on the Hub machine to start a node:\n\n.\\startNode.ps1 -port 5555 -hubHost localhost -browser chrome -driverPath c:\\selenium\\drivers\n\n\nThis will start a node that can run Chrome/Firefox tests using drivers in the c:\\selenium\\server\\drivers path running on port 5555. On the other machine, I copied the same files and just ran this command:\n\n.\\startNode.ps1 -port 5556 -hubHost 10.4.0.4 -browser ie -driverPath c:\\selenium\\drivers\n\n\nThis time the node isn’t on the same machine as the Hub, so I used the Azure vNet internal IP address of the Hub – I also specified I want IE/Firefox tests to run on this node.\n\nOf course I made sure that all these files are in source control!\n\nAgain I had to make sure that the ports I specify were allowed in the Firewall. I just created a single Firewall rule to allow TCP traffic on ports 4444, 5550-5559 on both machines.\n\n\n\n\nI also opened those ports in the Azure network security group that both machines’ network cards are connected to:\n\n\n\n\nNow I can browse to the Selenium console of my Grid:\n\n\n\n\nNow my hub is ready to run tests!\n\nWriting Tests for Grid Execution\n\nThe Selenium Grid is capable of running tests in parallel, spreading tests across the grid. Spoiler alert: it doesn’t run tests in parallel. I can hear you now thinking, “What?!? You just said it can run tests in parallel, and now you say it can’t!”. Well, the grid can spread as many tests as you throw at it – but you have to parallelize the tests yourself!\n\nIt turns out that you can do this in Visual Studio 2015 and VSTS – but it’s not pretty. If you open up the Test Explorer Window, you’ll see an option to “Run Tests In Parallel” in the toolbar (next to the Group By button):\n\n\n\n\nAgain I hear you thinking: “Just flip the switch! Easy!” Whoa, slow down, Dear Reader – it’s not that easy. You have to consider the unit of parallelization. In other words – what does it mean to “Run Tests In Parallel”? Well, Visual Studio runs different assemblies in parallel. Which means that you have to have at least two test projects (assemblies) in order to get any benefit.\n\nIn my case I had two tests that I wanted to run against three browsers – IE, Chrome and Firefox. Of course if you have several hundred tests, you probably have them in different assemblies already – hopefully grouped by something meaningful. In my case I chose to group the tests by browser. Here’s what I ended up doing:\n\n\n  Create a base (abstract) class that contains the Selenium test methods (the actual test code)\n  Create three additional projects – one for each browser type – that contains a class that derives from the base class\n  Run in parallel\n\n\nIt’s a pity really – since the Selenium libraries abstract the test away from the actual browser. That means you can run the same test against any browser that you have a driver for! However, since we’re going to run tests in a Hub, we need to use a special driver called a RemoteWebDriver. This driver is going to connect the test to the hub using “capabilities” that we define (like what browser to run in).\n\nLet’s consider an example test. Here’s a test I created to check the Search functionality of my website:\n\nprotected void SearchTest()\n{\n    driver.Navigate().GoToUrl(baseUrl + \"/\");\n    driver.FindElement(By.Id(\"search-box\")).Clear();\n    driver.FindElement(By.Id(\"search-box\")).SendKeys(\"tire\");\n\n    driver.FindElement(By.Id(\"search-link\")).Click();\n\n    // check that there are 3 results\n    Assert.AreEqual(3, driver.FindElements(By.ClassName(\"list-item-part\")).Count);\n}\n\n\nThis is a pretty simple test – and as I mentioned before, this post isn’t about the Selenium tests themselves as much as it is about running the tests in a build/release pipeline – so excuse the simple nature of the test code. However, I have to show you some code in order to show you how I got the tests running successfully in the grid.\n\nYou can see how the code assumes that there is a “driver” object and that it is instantiated? There’s also a “baseUrl” object. Both of these are essential to running tests in the grid: the driver is an instantiated RemoteWebDriver object that connects us to the Hub, while baseUrl is the base URL of the site we’re testing.\n\nThe base class is going to instantiate a RemoteWebDriver for each test (in the test initializer). Each child (test) class is going to specify what capabilities the driver should be instantiated with. The driver constructor needs to know the URL of the grid hub as well as the capabilities required for the test. Here’s the constructor and test initializer in the base class:\n\npublic abstract class PartsTests\n{\n    private const string defaultBaseUrl = \"http://localhost:5001\";\n    private const string defaultGridUrl = \"http://10.0.75.1:4444/wd/hub\";\n\n    protected string baseUrl;\n    protected string gridUrl;\n\n    protected IWebDriver driver;\n    private StringBuilder verificationErrors;\n    protected ICapabilities capabilities;\n    public TestContext TestContext { get; set; }\n\n    public PartsTests(ICapabilities capabilities)\n    {\n        this.capabilities = capabilities;\n    }\n\n    [TestInitialize]\n    public void SetupTest()\n    {\n        if (TestContext.Properties[\"baseUrl\"] != null) //Set URL from a build\n        {\n            baseUrl = TestContext.Properties[\"baseUrl\"].ToString();\n        }\n        else\n        {\n            baseUrl = defaultBaseUrl;\n        }\n        Trace.WriteLine($\"BaseUrl: {baseUrl}\");\n\n        if (TestContext.Properties[\"gridUrl\"] != null) //Set URL from a build\n        {\n            gridUrl = TestContext.Properties[\"gridUrl\"].ToString();\n        }\n        else\n        {\n            gridUrl = defaultGridUrl;\n        }\n        Trace.WriteLine($\"GridUrl: {gridUrl}\");\n\n        driver = new RemoteWebDriver(new Uri(gridUrl), capabilities);\n        verificationErrors = new StringBuilder();\n    }\n\n    [TestCleanup]\n    public void Teardown()\n    {\n        try\n        {\n            driver.Quit();\n        }\n        catch (Exception)\n        {\n            // Ignore errors if unable to close the browser\n        }\n        Assert.AreEqual(\"\", verificationErrors.ToString());\n    }\n\n    ...\n}\n\n\nThe constructor takes an ICapabilities object which will allow us to specify how we want the test run (or at least which browser to run against). We hold on to these capabilities. The SetupTest() method then reads the “gridUrl” and the “baseUrl” from the TestContext properties (defaulting values if none are present). Finally it created a new RemoteWebDriver using the gridUrl and capabilities. The Teardown() method calls the driver Quit() method, which closes the browser (ignoring errors) and checks that there are no verification errors. Pretty standard stuff.\n\nSo how do we pass in the gridUrl and baseUrl? To do that we need a runsettings file – this sets the value of the parameters in the TestContext object.\n\nI added a new XML file to the base project called “selenium.runsettings” with the following contents:\n\n&amp;lt;?xml version=\"1.0\" encoding=\"utf-8\"?&amp;gt;\n&amp;lt;RunSettings&amp;gt;\n  &amp;lt;TestRunParameters&amp;gt;\n    &amp;lt;Parameter name=\"baseUrl\" value=\"http://localhost:5001\" /&amp;gt;\n    &amp;lt;Parameter name=\"gridUrl\" value=\"http://localhost:4444/wd/hub\" /&amp;gt;\n  &amp;lt;/TestRunParameters&amp;gt;\n&amp;lt;/RunSettings&amp;gt;\n\n\nAgain I’m using default values for the values of the parameters – this is how I debug locally. Note the “/wd/hub” on the end of the grid hub URL.\n\nNow I can set the runsettings file in the Test menu:\n\n\n\n\nSo what about the child test classes? Here’s what I have for the Firefox tests:\n\n[TestClass]\npublic class FFTests : PartsTests\n{\n    public FFTests()\n        : base(DesiredCapabilities.Firefox())\n    {\n    }\n\n    [TestMethod]\n    [TestCategory(\"Firefox\")]\n    public void Firefox_AddToCartTest()\n    {\n        AddToCartTest();\n    }\n\n    [TestMethod]\n    [TestCategory(\"Firefox\")]\n    public void Firefox_SearchTest()\n    {\n        SearchTest();\n    }\n}\n\n\nI’ve prepended the test name with Firefox_ (you’ll see why when we run the tests in the release pipeline). I’ve also added the [TestClass] and [TestMethod] attributes as well as [TestCategory]. This is using the MSTest framework, but the same will work with nUnit or xUnit too. Unfortunately the non-MSTest frameworks don’t have the TestContext, so you’re going to have to figure out another method of providing the baseUrl and gridUrl to the test. The constructor is using a vanilla Firefox capability for this test – you can instantiate more complex capabilities if you need them here.\n\nJust for comparison, here’s my code for the ChromeTests file:\n\n[TestClass]\npublic class ChromeTests : PartsTests\n{\n    public ChromeTests()\n        : base(DesiredCapabilities.Chrome())\n    {\n    }\n\n    [TestMethod]\n    [TestCategory(\"Chrome\")]\n    public void Chrome_AddToCartTest()\n    {\n        AddToCartTest();\n    }\n\n    [TestMethod]\n    [TestCategory(\"Chrome\")]\n    public void Chrome_SearchTest()\n    {\n        SearchTest();\n    }\n}\n\n\nYou can see it’s almost identical except for the class name prefix, the [TestCategory] and the capabilities in the constructor.\n\nHere’s my project layout:\n\n\n\n\nAt this point I can run tests (in parallel) against my “local” grid (I started a hub and two nodes locally to test). Next we need to put all of this into a build/release pipeline.\n\nCreating a Build\n\nYou could run the tests during the build, but I wanted my UI tests to be run against a test site that I deploy to, so I felt it more appropriate to run the tests in the release. Before we get to that, we have to build the application (and test code) so that it’s available in the release.\n\nI committed all the code to source control in VSTS and created a build definition. Here’s what the tasks look like:\n\n\n\n\nThe first three steps are for building the application and the solution – I won’t bore you with the details. Let’s look at the next five steps though:\n\nThe first three “Copy Files” steps copy the binaries for the three test projects I want (one for IE, for Chrome and Firefox):\n\n\n\n\nIn each case I’m copying the compiled assemblies of the test project (from say test/PartsUnlimitedSelenium.Chrome\\bin$(BuildConfiguration) to $(Build.ArtifactStagingDirectory)\\SeleniumTests.\n\nThe fourth copy task copies the runsettings file:\n\n\n\n\nThe final task publishes the $(Build.ArtifactStagingDirectory) to the server:\n\n\n\n\nAfter running the build, I have the following drop:\n\n\n\n\nThe “site” folder contains the webdeploy package for my site – but the important bit here is that all the test assemblies (and the runsettings file) are in the SeleniumTests folder.\n\nRunning Tests in the Release\n\nNow that we have the app code (the site) and the test assemblies in a drop location, we’re ready to define a Release. In the release for Dev I have the following steps:\n\n\n\n\nI have all the steps that I need to deploy the application (in this case I’m deploying to Azure). Again, that’s not the focus of this post. The Test Assemblies task is the important step to look at here:\n\n\n\n\nIt turns out to be pretty straightforward. I just make sure that “Test Assembly” includes all the assemblies I want to execute – remember you need at least two in order for “Run In Parallel” to have any effect. For Filter Criteria I’ve excluded IE tests – IE tests seem to fail for all sorts of arbitrary reasons that I couldn’t work out – you can leave this empty (or put in a positive expression) if you want to only run certain tests. I specify the path to the runsettings file, and then in “Override TestRun Parameters” I specify the gridUrl and baseUrl that I want to test in this particular environment. I’ve used variables that I define on the environment so that I can clone this for other environments if I need to.\n\nNow when I release, I see that the tests run as part of the release. Clicking on the Tests tab I see the test results. I changed the Outcome filter to show Passed tests and configured the columns to show the “Date started” and “Date completed”. Sure enough I can see that the tests are running in parallel:\n\n\n\n\nNow you can see why I wanted to add the prefix to the test names – this lets me see exactly which browsers are behaving and which aren’t (ahem, IE).\n\nFinal Thoughts\n\nRunning Selenium Tests in a Grid in VSTS is possible –  there are a few hacks required though. You need to create multiple assemblies in order to take advantage of the grid scalability, and this can lead to lots of duplicated and error-prone code (for example when I initially created the Firefox tests, I copied the Chrome class and forgot to change the prefix and [TestCategory] which lead to interesting results). There are probably other ways of dividing your tests into multiple assemblies, and then you could pass the browser in as a Test Parameter and have multiple runs – but then the runs wouldn’t be simultaneous across browsers. A final gotcha is that the runsettings only work for MSTest – if you’re using another framework, chances are you’ll end up creating a json file that you read when the tests start.\n\nYou can see that there are challenges whichever way you slice it up. Hopefully the work the test team is doing in VSTS/TFS will improve this story at some stage.\n\nFor now, happy Grid testing!\n",
      "categories": [],
      "tags": ["testing","devops"],
      
      "collection": "posts",
      "url": "/parallel-testing-in-a-selenium-grid-with-vsts/"
    },{
      
      "title": "DacPac Change Report Task for VSTS Builds",
      "date": "2016-08-05 22:57:24 +0000",
      
      "content": "Most development requires working against some kind of database. Some teams choose to use Object Relational Mappers (ORMs) like Entity Framework. I think that should be the preferred method of dealing with databases (especially code-first), but there are times when you just have to work with a database schema.\n\nRecently I had to demo ReadyRoll in VSTS. I have to be honest that I don’t like the paradigm of ReadyRoll – migration-based history seems like a mess compared to model-based history (which is the approach that SSDT takes). That’s a subject for another post (some day) or a discussion over beers. However, there was one thing that I really liked – the ability to preview database changes in a build. The ReadyRoll extension on the VSTS marketplace allows you to do just that.\n\nSo I stole the idea and made a task that allows you to see SSDT schema changes from build to build.\n\nUsing the Task\n\nLet’s consider the scenario: you have an SSDT project in source control and you’re building the dacpac in a Team Build. What the task does is allow you to see what’s changed from one build to the next. Here’s what you need to do:\n\n\n  Install Colin’s ALM Corner Build Tasks Extension from the VSTS Marketplace\n  Edit the build definition and go to Options. Make sure “Allow Scripts to Access OAuth Token” is checked, since the task requires this. (If you forget this, you’ll see 403 errors in the task log).\n  Make sure that the dacpac you want to compare is being published to a build drop.\n  Add a “DacPac Schema Compare” task\n\n\nThat’s it! Here’s what the task looks like:\n\n\n\n\nEnter the following fields:\n\n\n  The name of the drop that your dacpac file is going to be published to. The task will look up the last successful build and download the drop in order to get the last dacpac as the source to compare.\n  The name of the dacpac (without the extension). This is typically the name of the SSDT project you’re building.\n  The path to the compiled dacpac for this build – this is the target dacpac path and is typically the bin folder of the SSDT project.\n\n\nNow run your build. Once the build completes, you’ll see a couple new sections in the Build Summary:\n\n\n\n\nThe first section shows the schema changes, while the second shows a SQL-CMD file so you can see what would be generated by SqlPackage.exe.\n\nNow you can preview schema changes of your SSDT projects between builds! As usual, let me know here, on Twitter or on Github if you have issues with the task.\n\nHappy building!\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/dacpac-change-report-task-for-vsts-builds/"
    },{
      
      "title": "Load Balancing DotNet Core Docker Containers with nginx",
      "date": "2016-08-06 14:15:13 +0000",
      
      "content": "Yes, I’ve been playing with Docker again – no big surprise there. This time I decided to take a look at scaling an application that’s in a Docker container. Scaling and load balancing are concepts you have to get your head around in a microservices architecture!\n\nAnother consideration when load balancing is of course shared memory. Redis is a popular mechanism for that (and since we’re talking Docker I should mention that there’s a Docker image for Redis) – but for this POC I decided to keep the code very simple so that I could see what happens on the networking layer. So I created a very simple .NET Core ASP.NET Web API project and added a single MVC page that could show me the name of the host machine. I then looked at a couple of load balancing options and started hacking until I could successfully (and easily) load balance three Docker container instances of the service.\n\nThe Code\n\nThe code is stupid simple – for this POC I’m interested in configuring the load balancer more than anything, so that’s ok. Here’s the controller that we’ll be hitting:\n\nnamespace NginXService.Controllers\n{\n    public class HomeController : Controller\n    {\n        // GET: /&amp;lt;controller&amp;gt;/\n        public IActionResult Index()\n        {\n            // platform agnostic call\n            ViewData[\"Hostname\"] = Environment.GetEnvironmentVariable(\"COMPUTERNAME\") ??\n                Environment.GetEnvironmentVariable(\"HOSTNAME\");\n\n            return View();\n        }\n    }\n}\n\n\nGetting the hostname is a bit tricky for a cross-platform app, since *nix systems and windows use different environment variables to store the hostname. Hence the ?? code.\n\nHere’s the View:\n\n@{\n    &amp;lt;h1&amp;gt;Hello World!&amp;lt;/h1&amp;gt;\n    &amp;lt;br/&amp;gt;\n\n    &amp;lt;h3&amp;gt;Info&amp;lt;/h3&amp;gt;\n    &amp;lt;p&amp;gt;&amp;lt;b&amp;gt;HostName:&amp;lt;/b&amp;gt; @ViewData[\"Hostname\"]&amp;lt;/p&amp;gt;\n    &amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Time:&amp;lt;/b&amp;gt; @string.Format(\"{0:yyyy-MM-dd HH:mm:ss}\", DateTime.Now)&amp;lt;/p&amp;gt;\n}\n\n\nI had to change the Startup file to add the MVC route. I just changed the\n\n\napp.UseMvc()\n\n\nline in the\n\n\nConfigure()\n\n\nmethod to this:\n\napp.UseMvc(routes =&amp;gt;\n{\n    routes.MapRoute(\n        name: \"default\",\n        template: \"{controller=Home}/{action=Index}/{id?}\");\n});\n\n\nFinally, here’s the Dockerfile for the container that will be hosting the site:\n\nFROM microsoft/dotnet:1.0.0-core\n\n# Set the Working Directory\nWORKDIR /app\n\n# Configure the listening port\nARG APP_PORT=5000\nENV ASPNETCORE_URLS http://*:$APP_PORT\nEXPOSE $APP_PORT\n\n# Copy the app\nCOPY . /app\n\n# Start the app\nENTRYPOINT dotnet NginXService.dll\n\n\nPretty simple so far.\n\nProxy Wars: HAProxy vs nginx\n\nAfter doing some research it seemed to me that the serious contenders for load balancing Docker containers boiled down to HAProxy and nginx (with corresponding Docker images here and here). In the end I decided to go with nginx for two reasons: firstly, nginx can be used as a reverse proxy, but it can also serve static content, while HAProxy is just a proxy. Secondly, the nginx website is a lot cooler – seemed to me that nginx was more modern than HAProxy (#justsaying). There’s probably as much religious debate about which is better as there is about git rebase vs git merge. Anyway, I picked nginx.\n\nConfiguring nginx\n\nI quickly pulled the image for nginx (\n\n\ndocker pull nginx\n\n\n) and then set about figuring out how to configure it to load balance three other containers. I used a Docker volume to keep the config outside the container – that way I could tweak the config without having to rebuild the image. Also, since I was hoping to spin up numerous containers, I turned to docker-compose. Let’s first look at the nginx configuration:\n\nworker_processes 1;\n\nevents { worker_connections 1024; }\n\nhttp {\n\n    sendfile on;\n\n    # List of application servers\n    upstream app_servers {\n\n        server app1:5000;\n        server app2:5000;\n        server app3:5000;\n\n    }\n\n    # Configuration for the server\n    server {\n\n        # Running port\n        listen [::]:5100;\n        listen 5100;\n\n        # Proxying the connections\n        location / {\n\n            proxy_pass http://app_servers;\n            proxy_redirect off;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Host $server_name;\n\n        }\n    }\n}\n\n\nThis is really a bare-bones config for nginx. You can do a lot in the config. This config does a round-robin load balancing, but you can also configure least_connected, provide weighting for each server and more. For the POC, there are a couple of important bits:\n\n\n  Lines 10-16: this is the list of servers that nginx is going to be load balancing. I’ve used aliases (app1, app2 and app3, all on port 5000) which we’ll configure through docker-compose shortly.\n  Lines 22-23: the nginx server itself will listen on port 5100.\n  Line 26, 28: we’re passing all traffic on to the configured servers.\n\n\nI’ve saved this config to a file called nginx.conf and put it into the same folder as the Dockerfile.\n\nConfiguring the Cluster\n\nTo configure the whole cluster (nginx plus three instances of the app container) I use the following docker-compose yml file:\n\nversion: '2'\n\nservices:\n  app1:\n    image: colin/nginxservice:latest\n  app2:\n    image: colin/nginxservice:latest\n  app3:\n    image: colin/nginxservice:latest\n\n  nginx:\n    image: nginx\n    links:\n     - app1:app1\n     - app2:app2\n     - app3:app3\n    ports:\n     - \"5100:5100\"\n    volumes:\n     - ./nginx.conf:/etc/nginx/nginx.conf\n\n\nThat’s 20 lines of code to configure a cluster – pretty sweet! Let’s take a quick look at the file:\n\n\n  Lines 4-9: Spin up three containers using the image containing the app (that I built separately, since I couldn’t figure out how to build and use the same image multiple times in a docker-compose file).\n  Line 12: Spin up a container based on the stock nginx image.\n  Lines 13-16: Here’s the interesting bit: we tell docker to create links between the nginx container and the other containers, aliasing them with the same names. Docker creates internal networking (so it’s not exposed publically) between the containers. This is very cool – the nginx container can reference app1, app2 and app3 (as we did in the nginx config file) and docker takes care of figuring out the IP addresses on the internal network.\n  Line 18: map port 5100 on the nginx container to an exposed port 5100 on the host (remember we configured nginx to listen on the internal 5100 port).\n  Line 20: map the nginx.conf file on the host to /etc/nginx/nginx.conf within the container.\n\n\nNow we can simply run\n\n\ndocker-compose up\n\n\nto run the cluster!\n\n\n\n\nYou can see how docker-compose pulls the logs into a single stream and even color-codes them!\n\nThe one thing I couldn’t figure out was how to do a\n\n\ndocker build\n\n\non an image and use that image in another container within the docker-compose file. I could just have three\n\n\nbuild\n\n\ndirectives, but that felt a bit strange to me since I wanted to supply build args for the image. So I ended up doing the\n\n\ndocker build\n\n\nto create the app image and then just using the image in the docker-compose file.\n\nLet’s hit the index page and then refresh a couple times:\n\n\n\n\nYou can see in the site (the hostname) as well as in the logs how the containers are round-robining:\n\n\n\nConclusion\n\nLoad balancing containers with nginx is fairly easy to accomplish. Of course the app servers don’t need to be running .NET apps – nginx doesn’t really care, since it’s just directing traffic. However, I was pleased that I could get this working so painlessly.\n\nHappy load balancing!\n",
      "categories": [],
      "tags": ["docker"],
      
      "collection": "posts",
      "url": "/load-balancing-dotnet-core-docker-containers-with-nginx/"
    },{
      
      "title": "Using Release Management to Manage Ad-Hoc Deployments",
      "date": "2016-10-19 12:08:56 +0000",
      
      "content": "Release Management (RM) is awesome – mostly because it works off the amazing cross platform build engine. Also, now that pricing is announced, we know that it won’t cost an arm and a leg!\n\nWhen I work with customers to adopt RM, I see two kinds of deployments: repeatable and ad-hoc. RM does a great job at repeatable automation – that is, it is great at doing the same thing over and over. But what about deployments that are different in some way every time? Teams love the traceability that RM provides – not just the automation logs, but also the approvals. It would be great if you could track ad-hoc releases using RM.\n\nThe Problem\n\nThe problem is that RM doesn’t have a great way to handle deployments that are slightly different every time. Let’s take a very typical example: ad-hoc SQL scripts. Imagine that you routinely perform data manipulation on a production database using SQL scripts. How do you audit what script was run and by whom? “We can use RM!” I hear you cry. Yes you can – but there are some challenges.\n\nAh-hoc means (in this context) different every time. That means that the script you’re running is bound to change every time you run the release. Also, depending on how dynamic you want to go, even the target servers could change – sometimes you’re executing against server A, sometimes against server B, sometimes both. “Just make the script name or server name a variable that you can change at queue time,” I hear you say. Unfortunately, unlike builds, you can’t specify parameter values at queue time. You could create a release in draft and then edit the variables for that run of the release, but this isn’t a great experience since you’re bound to forget things – and you’ll have to do this every time you start a release.\n\nA Reasonable Solution\n\nI was at a customer who were trying to convert to RM from a home-grown deployment tool. Besides “repeatable” deployments their tool was handling several hundred ad-hoc deployments a month, so they had to decide whether or not to keep the ad-hoc deployments in the home-grown tool or migrate to RM. So I mailed the Champs List – a mailing list direct to other MVPs and the VSTS Product Group in Microsoft (being an MVP has to have some benefits, right?) – and asked them what they do for ad-hoc deployments. It turns out that they use ad-hoc deployments to turn feature switches on and off, and they run their ad-hoc deployments with RM – and while I didn’t get a lot of detail, I did get some ideas.\n\nI see three primary challenges for ad-hoc release definitions:\n\n\n  What to execute\n\n\n\n  Where does the Release get the script to execute? You could create a network share and put a script in there called “adhoc.sql” and get the release to execute that script every time it runs. Tracking changes is then a challenge – but we’re developers and already know how to track changes, right? Yes: source control. So source control the script – that way every time the release runs, it gets the latest version of the script and runs that. Now you can track what executed and who changed the script. And you can even perform code-review prior to starting the release – bonus!\n\n\n\n  What to execute\n\n\n\n  Is there an echo here? Well no – it’s just that if the release is executing the same script every time, there’s a danger that it could well – execute the same script. That means you have to either write your script defensively – that is, in such a manner that it is idempotent (has the same result no matter how many times you run it) or you have to keep a record of whether or not the script has been run before, say using a table in a DB or an Azure table or something. I favor idempotent scripts, since I think it’s a good habit to be in when you’re automating stuff anyway – so for SQL that means doing “if this record exists, skip the following steps” kind of logic or using MERGE INTO etc.\n\n\n\n  Where to execute\n\n\n\n  Are you executing against the same server every time or do the targets need to be more dynamic? There are a couple of solutions here – you could have a text doc that has a list of servers, and the release definition reads in the file and then loops, executing the script against the target servers one by one. This is dynamic, but dangerous – what if you put in a server that you don’t mean to? Or you could create an environment per server (if you have a small set of servers this is ok) and then set each environment to manual deployment (i.e. no trigger). Then when you’re ready to execute, you create the release, which just sits there until you explicitly tell it which environment (server) to deploy to.\n\n\nRecommended Steps\n\nWhile it’s not trivial to set up an ad-hoc deployment pipeline in RM, I think it’s feasible. Here’s what I’m going to start recommending to my customers:\n\n\n  Create a Git repository with a well-defined script (or root script)\n  Create a Release that has a single artifact link – to the Git repo you just set up, on the master branch\n  Create an environment per target server. In that environment, you’re conceptually just executing the root script (this could be more complicated depending on what you do for ad-hoc deployments). All the server credentials etc. are configured here so you don’t have to do them every time. You can also configure approvals if they’re required for ad-hoc scripts. Here’s an example where a release definition is targeting (potentially) ServerA, ServerB and/or ServerC. This is only necessary if you have a fixed set of target servers and you don’t always know which server you’re going to target:\n\n  Here I’ve got an example of copying a file (the root script, which is in a Git artifact link) to the target server and then executing the script using the WinRM SQL task. These tasks are cloned to each server – of course the server name (and possibly credentials) are different for each environment – but you only have to set this up once.\n  Configure each environment to have a manual trigger (under deployment conditions). This allows you to select which server (or environment) you’re deploying to for each instance of the release:\n\n  Enable a branch policy on the master branch so that you have to create a Pull Request (PR) to get changes into master. This forces developers to branch the repo, modify the script and then commit and create a PR. At that point you can do code review on the changes before queuing up the release.\n  When you’ve completed code review on the PR, you then create a release. Since all the environments are set to manual trigger, you now can go and manually select which environment you want to deploy to:\n\n  Here you can see how the status on each environment is “Not Deployed”. You can now use the deploy button to manually select a target. You can of course repeat this if you’re targeting multiple servers for this release.\n\n\nConclusion\n\nWith a little effort, you can set up an ad-hoc release pipeline. This gives you the advantages of automation (since the steps and credentials etc. are already set up) as well as auditability and accountability (since you can track changes to the scripts as well as any approvals). How do you, dear reader, handle ad-hoc deployments? Sound off in the comments!\n\nHappy releasing!\n",
      "categories": [],
      "tags": ["releasemanagement"],
      
      "collection": "posts",
      "url": "/using-release-management-to-manage-ad-hoc-deployments/"
    },{
      
      "title": "End to End Walkthrough: Deploying Web Applications Using Team Build and Release Management",
      "date": "2016-10-22 02:32:19 +0000",
      
      "content": "I’ve posted previously about deploying web applications using Team Build and Release Management (see Config Per Environment vs Tokenization in Release Management and WebDeploy, Configs and Web Release Management). However, reviewing those posts recently at customers I’ve been working with, I’ve realized that these posts are a little outdated, you need pieces of both to form a full picture and the scripts that I wrote for those posts are now encapsulated in Tasks in my marketplace extension. So in this post I’m going to do a complete end-to-end walkthrough of deploying web applications using Team Build and Release Management. I’ll be including handling configs – arguably the hardest part of the whole process.\n\nOverview\n\nLet’s start with an overview of the process. I like to think of three distinct “areas” – source control, build and release. Conceptually you have tokenized config in source control (more on how to do this coming up). Then you have a build that takes in the source control and produces a WebDeploy package – a single tokenized package that is potentially deployable to multiple environments. The build should not have to know about anything environment specific (such as the correct connection string for Staging or Production, for example). Then release takes in the package and (conceptually) performs two steps: inject environment values for the tokens, and then deploy using WebDeploy. Here’s a graphic (which I’ll refer to as the Flow Diagram) of the process:\n\n\n\n\nI’ve got some details in this diagram that we’ll cover shortly, so don’t worry about the details for now. The point is to clearly separate responsibilities (especially for build and release): build compiles source code, runs tests and performs static code analysis and so on. It shouldn’t have to know anything about environments – the output of the build is a single package with “holes” for environment values (tokens). Release will take in this single package and deploy it to each environment, plugging environment values into the holes as it goes. This guarantees that the bits you test in Dev and Staging are the same bits that get deployed to Prod!\n\nDeep Dive: Configuration\n\nLet’s get down into the weeds of configuration. If you’re going to produce a single package from build, then how should you handle config? Typically you have (at least) a connection string that you want to be different for each environment. Beyond that you probably have appSettings as well. If the build shouldn’t know about these values when it’s creating the package, then how do you manage config? Here are a some options:\n\n\n  Create a web.config for each environment in source control\n\n\n\n  Pros: All your configs are in source control in their entirety\n  Cons: Lots of duplications – and you have to copy the correct config to the correct environment; requires a re-build to change config values in release management\n\n\n\n  Create a config transform for each environment\n\n\n\n  Pros: Less duplication, and you have all the environment values in source control\n  Cons: Requires a project (or solution) config per environment, which can lead to config bloat; requires that you create a package per environment during build; requires a re-build to change config values in release management\n\n\n\n  Tokenize using a single transform and parameters.xml\n\n\n\n  Pros: No duplication; enables a single package that can be deployed to multiple environments; no rebuild required to change config values in release management\n  Cons: Environment values aren’t in source control (though they’re in Release Management); learning curve\n\n\nFurthermore, if you’re targeting Azure, you can use the same techniques as targeting IIS, or you can use Azure Resource Manager (ARM) templates to manage your configuration. This offloads the config management to the template and you assume that the target Azure Web App is correctly configured at the time you run WebDeploy.\n\nHere’s a decision tree to make this a bit easier to digest:\n\n\n\n\nLet’s walk through it:\n\n\n  If you’re deploying to Azure, and using ARM templates, just make sure that you configure the settings correctly in the template (I won’t cover how to do this in this post)\n  If you’re deploying to IIS (or you’re deploying to Azure and don’t have ARM templates or just want to manage config in the same manner as you would for IIS), you should create a single publish profile using right-click-&gt;Publish (on the web application) called “Release”. This should target the release configuration and you should tokenize the connection strings in the wizard (details coming up)\n  Next, if you have appSettings, you’ll have to create a parameters.xml file (details coming up)\n  Commit to source control\n\n\nFor the remainder of this post I’m going to assume that you’re deploying to IIS (or to Azure and handling config outside of an ARM template).\n\nCreating a Publish Profile\n\nSo what is this publish profile and why do you need it? The publish profile enables you to:\n\n\n  provide a single transform (via the Web.release.config) that makes your config release-ready (removing the debug compilation property, for example)\n  tokenize the connection strings\n\n\nTo create the profile, right-click your web application project and select “Publish…”. Then do the following:\n\n\n  Select “Custom” to create a new custom profile. Name this “Release” (you can name this anything, but you’ll need to remember the name for the build later)\n\n  On the Connection page, change the Publish method to “Web Deploy Package”. Type anything you want for the filename and leave the site name empty. Click Next.\n\n  On the Settings page, select the configuration you want to compile. Typically this is Release – remember that the name of the configuration here is how the build will know which transform to apply. If you set this to Release, it will apply Web.Release.config – if you set it to Debug it will apply Web.Debug.Release. Typically you want to specify Release here since you’re aiming to get this site into Prod (otherwise why are you coding at all?) and you probably don’t want debug configuration in Prod!\n  You’ll see a textbox for each connection string you have in your Web.config. You can either put a single token in or a tokenized connection string. In the example below, I’ve used a single token (“__AppDbContextStr__”) for the one connection string and a tokenized string (“Server=__DbServer__;Initial Catalog=__DbName__;User Name=__DbUser__;Password=__DbPassword__”) for the other (just so you can see the difference). I’m using double underscore pre- and post-fix for the tokens:\n\n  Now click “Close” (don’t hit publish). When prompted to save the profile, select yes. This creates a Release.pubxml file in the Properties folder (the name of the file is the name of the profile you selected earlier):\n\n    Creating a parameters.xml File\n  \n\n\nThe publish profile takes care of the connection strings – but you will have noticed that it doesn’t ask for values for appSettings (or any other configuration) anywhere. In order to tokenize anything in your web.config other than connection strings, you’ll need to create a parameters.xml file (yes, it has to be called that) in the root of your web application project. When the build runs, it will use this file to expose properties for you to tokenize (it doesn’t actually transform the config at build time).\n\nHere’s a concrete example: in my web.config, I have the following snippet:\n\n&amp;lt;appSettings&amp;gt;\n  ...\n  &amp;lt;add key=\"Environment\" value=\"debug!\" /&amp;gt;\n&amp;lt;/appSettings&amp;gt;\n\n\nThere’s an appSetting key called “Environment” that has the value “debug!”. When I run or debug out of Visual Studio, this is the value that will be used. If I want this value to change on each environment I target for deployment, I need to add a parameters.xml file to the root of my web application with the following xml:\n\n&amp;lt;?xml version=\"1.0\" encoding=\"utf-8\" ?&amp;gt;\n&amp;lt;parameters&amp;gt;\n  &amp;lt;parameter name=\"Environment\" description=\"doesn't matter\" defaultvalue=\" __Environment__\" tags=\"\"&amp;gt;\n    &amp;lt;parameterentry kind=\"XmlFile\" scope=\"\\\\web.config$\" match=\"/configuration/appSettings/add[@key='Environment']/@value\"&amp;gt;\n    &amp;lt;/parameterentry&amp;gt;\n  &amp;lt;/parameter&amp;gt;\n&amp;lt;/parameters&amp;gt;\n\n\nLines 3-6 are repeated for each parameter I want to configure. Let’s take a deeper look:\n\n\n  parameter name (line 3) – by convention it should be the name of the setting you’re tokenizing\n  parameter description (line 3) – totally immaterial for this process, but you can use it if you need to. Same with tags.\n  parameter defaultvalue (line 3) – this is the token you want injected – notice the double underscore again. Note that this can be a single token or a tokenized string (like the connection strings above)\n  parameterentry match (line 4) – this is the xpath to the bit of config you want to replace. In this case, the xpath says in the “configuration” element, find the “appSetting” element, then find the “add” element with the key property = ‘Environment’ and replace the value parameter with the defaultvalue.\n\n\nHere you can see the parameters.xml file in my project:\n\n\n\n\nTo test your transform, right-click and publish the project using the publish profile (for this you may want to specify a proper path for the Filename in the Connection page of the profile). After a successful publish, you’ll see 5 files. The important files are the zip file (where the bits are kept – this is all the binary and content files, no source files) and the SetParameters.xml file:\n\n\n\n\nOpening the SetParameters.xml file, you’ll see the following:\n\n&amp;lt;?xml version=\"1.0\" encoding=\"utf-8\"?&amp;gt;\n&amp;lt;parameters&amp;gt;\n  &amp;lt;setParameter name=\"IIS Web Application Name\" value=\"Default Web Site/WebDeployMe_deploy\" /&amp;gt;\n  &amp;lt;setParameter name=\"Environment\" value=\" __Environment__\" /&amp;gt;\n  &amp;lt;setParameter name=\"DefaultConnection-Web.config Connection String\" value=\" __AppDbContextStr__\" /&amp;gt;\n  &amp;lt;setParameter name=\"SomeConnection-Web.config Connection String\" value=\"Server= __DbServer__ ;Initial Catalog= __DbName__ ;User Name= __DbUser__ ;Password= __DbPassword__\" /&amp;gt;\n&amp;lt;/parameters&amp;gt;\n\n\nYou’ll see the tokens for the appSetting (Environment, line 4) and the connection strings (lines 5 and 6). Note how the tokens live in the SetParameters.xml file, not in the web.config file! In fact, if you dive into the zip file and view the web.config file, you’ll see this:\n\n&amp;lt;connectionStrings&amp;gt;\n  &amp;lt;add name=\"DefaultConnection\" connectionString=\"$(ReplacableToken_DefaultConnection-Web.config Connection String_0)\" providerName=\"System.Data.SqlClient\" /&amp;gt;\n  &amp;lt;add name=\"SomeConnection\" connectionString=\"$(ReplacableToken_SomeConnection-Web.config Connection String_0)\" providerName=\"System.Data.SqlClient\" /&amp;gt;\n&amp;lt;/connectionStrings&amp;gt;\n&amp;lt;appSettings&amp;gt;\n  ...\n  &amp;lt;add key=\"Environment\" value=\"debug!\" /&amp;gt;\n&amp;lt;/appSettings&amp;gt;\n\n\nYou can see that there are placeholders for the connection strings, but the appSetting is unchanged from what you have in your web.config! As long as your connection strings have placeholders and your appSettings are in the SetParameters.xml file, you’re good to go – don’t worry, WebDeploy will still inject the correct values for your appSettings at deploy time (using the xpath you supplied in the parameters.xml file).\n\nDeep Dive: Build\n\nYou’re now ready to create the build definition. There are some additional build tasks which may be relevant – such as creating dacpacs from SQL Server Data Tools (SSDT) projects to manage database schema changes – that are beyond the scope of this post. As for the web application itself, I like to have builds do the following:\n\n\n  Version assemblies to match the build number (optional, but recommended)\n  Run unit tests, code analysis and other build verification tasks\n  Create the WebDeploy package\n\n\nTo version the assemblies, you can use the VersionAssemblies task from my build tasks extension in the marketplace. You’ll need the ReplaceTokens task for the release later, so just install the extension even if you’re not versioning. To show the minimum setup required to get the release working, I’m skipping unit tests and code analysis – but this is only for brevity. I highly recommend that unit testing and code analysis become part of every build you have.\n\nOnce you’ve created a build definition:\n\n\n  Click on the General tab and change the build number format to 1.0.0$(rev:.r). This makes the first build have the number 1.0.0.1, the second build 1.0.0.2 etc.\n  Add a VersionAssemblies task as the first task. Set the Source Path to the folder that contains the projects you want to version (typically the root folder). Leave the rest defaulted.\n\n  Leave the NuGet restore task as-is (you may need to edit the solution filter if you have multiple solutions in the repo)\n  On the VS Build task, edit the MSBuild Arguments parameter to be /p:DeployOnBuild=true /p:PublishProfile=Release /p:PackageLocation=$(build.artifactstagingdirectory)\n  This tells MSBuild to publish the site using the profile called Release (or whatever name you used for the publish profile you created) and place the package in the build artifact staging directory\n\n  Now you should put in all your code analysis and test tasks – I’m omitting them for brevity\n  The final task should be to publish the artifact staging directory, which at this time contains the WebDeploy package for your site\n\n\n\nRun the build. When complete, the build drop should contain the site zip and SetParameters.xml file (as well as some other files):\n\n\n\n\nYou now have a build that is potentially deployable to multiple environments.\n\nDeep Dive: Release\n\nIn order for the release to work correctly, you’ll need to install some extensions from the Marketplace. If you’re targeting IIS, you need to install the IIS Web App Deployment Using WinRM Extension. For both IIS and Azure deployments, you’ll need the ReplaceTokens task from my custom build tasks extension.\n\nThere are a couple of ways you can push the WebDeploy package to IIS:\n\n\n  Use the IIS Web App Deployment using WinRM task. This is fairly easy to use, but requires that you copy the zip and SetParameters files to the target server deploying.\n  Use the cmd file that gets generated with the zip and SetParameters files to deploy remotely. This requires you to know the cmd parameters and to have the WebDeploy remote agent running on the target server.\n\n\nI recommend the IIS task generally – unless for some or other reason you don’t want to open up WinRM.\n\nSo there’s some configuration required on the target IIS server:\n\n\n  Install WebDeploy\n  Install WebDeploy Remote Agent – for using the cmd. Note: if you install via Web Platform Installer you’ll need to go to Programs and Features and Modify the existing install, since the remote agent isn’t configured when installing WebDeploy via WPI\n  Configure WinRM – for the IIS task. You can run “winrm quickconfig” to get the service started. If you need to deploy using certificates, then you’ll have to configure that too (I won’t cover that here)\n  Firewall – remember to open ports for WinRM (5895 or 5986 for default WinRM HTTP or HTTPS respectively) or 8172 for the WebDeploy remote agent (again, this is the default port)\n  Create a service account that has permissions to copy files and “do stuff” in IIS – I usually recommend that this user account be a local admin on the target server\n\n\nOnce you’ve done that, you can create the Release Definition. Create a new release and specify the build as the primary artifact source. For this example I’m using the IIS task to deploy (and create the site in IIS – this is optional). Here are the tasks you’ll need and their configs:\n\n\n  Replace Tokens Task\n  Source Path – the path to the folder that contains the SetParameters.xml file within the drop\n  Target File Pattern – set to *.SetParameters.xml\n\n  Windows Machine File Copy Task\n  Copy the drop folder (containing the SetParameters.xml and website zip files) to a temp folder on the target server. Use the credentials of the service account you created earlier. I recommend using variables for all the settings.\n\n  (Optional) WinRM – IIS Web App Management Task\n  Use this task to create (or update) the site in IIS, including the physical path, host name, ports and app pool. If you have an existing site that you don’t want to mess with, then skip this task.\n\n  WinRM – IIS Web App Deployment Task\n  This task takes the (local) path of the site zip file and the SetParameters.xml file and deploys to the target IIS site.\n\n  You can supply extra WebDeploy args if you like – there are some other interesting switches under the MSDeploy Additional Options section.\n\n\nFinally, open the Environment variables and supply name/value pairs for the values you want injected. In the example I’ve been using, I’ve got a token for Environment and then I have a tokenized connection string with tokens for server, database, user and password. These are the variables that the Replace Tokens task uses to inject the real environment-specific values into the SetParameters file (in place of the tokens). When WebDeploy runs, it transforms the web.config in the zip file with the values that are in the SetParameters.xml file. Here you can see the variables:\n\n\n\n\nYou’ll notice that I also created variables for the Webserver name and admin credentials that the IIS and Copy Files tasks use.\n\nYou can of course do other things in the release – like run integration tests or UI tests. Again for brevity I’m skipping those tasks. Also remember to make the agent queue for the environment one that has an agent that can reach the target IIS server for that environment. For example I have an agent queue called “webdeploy” with an agent that can reach my IIS server:\n\n\n\n\nI’m now ready to run the deployment. After creating a release, I can see that the tasks completed successfully! Of course the web.config is correct on the target server too.\n\n\n\nDeploying to Azure\n\nAs I’ve noted previously if you’re deploying to Azure, you can put all the configuration into the ARM template (see an example here – note how the connection strings and Application Insights appSettings are configured on the web application resource). That means you don’t need the publish profile or parameters.xml file. You’ll follow exactly the same process for the build (just don’t specify the PublishProfile argument). The release is a bit easier too – you first deploy the resource group using the Azure Deployment: Create or Update Resource Group task like so:\n\n\n\n\nYou can see how I override the template parameters – that’s how you “inject” environment specific values.\n\nThen you use a Deploy AzureRM Web App task (no need to copy files anywhere) to deploy the web app like so:\n\n\n\n\nI specify the Azure Subscription – this is an Azure ARM service endpoint that I’ve preconfigured – and then the website name and optionally the deployment slot. Here I am deploying to the Dev slot – there are a couple extensions in the marketplace that allow you to swap slots (usually after you’ve smoke-tested the non-prod slot to warm it up and ensure it’s working correctly). This allows you to have zero downtime. The important bit here is the Package or Folder argument – this is where you’ll specify the path to the zip file.\n\nOf course if you don’t have the configuration in an ARM template, then you can just skip the ARM deployment task and run the Deploy AzureRM Web App task. There is a parameter called SetParameters file (my contribution to this task!) that allows you to specify the SetParameters file. You’ll need to do a Replace Tokens task prior to this to make sure that environment specific values are injected.\n\nFor a complete walkthrough of deploying a Web App to Azure with an ARM template, look at this hands-on-lab.\n\nConclusion\n\nOnce you understand the pieces involved in building, packaging and deploying web applications, you can fairly easily manage configuration without duplicating yourself – including connection strings, appSettings and any other config – using a publish profile and a parameters.xml file. Then using marketplace extensions, you can build, version, test and package the site. Finally, in Release Management you can inject environment specific values for your tokens and WebDeploy to IIS or to Azure.\n\nHappy deploying!\n",
      "categories": [],
      "tags": ["releasemanagement","build"],
      
      "collection": "posts",
      "url": "/end-to-end-walkthrough-deploying-web-applications-using-team-build-and-release-management/"
    },{
      
      "title": "Managing Config for .NET Core Web App Deployments with Tokenizer and ReplaceTokens Tasks",
      "date": "2016-10-26 10:20:20 +0000",
      
      "content": "Last week I posted an end-to-end walkthrough about how to build and deploy web apps using Team Build and Release Management – including config management. The post certainly helps you if you’re on the .NET 4.x Framework – but what about deploying .NET Core apps?\n\nThe Build Once Principle\n\nIf you’ve ever read any of my blogs you’ll know I’m a proponent of the “build once” principle. That is, your build should be taking source code and (after testing and code analysis etc.) producing a single package that can be deployed to multiple environments. The biggest challenge with a “build once” approach is that it’s non-trivial to manage configuration. If you’re building a single package, how do you deploy it to multiple environments when the configuration is different on those environments? I present a solution in my walkthrough – use a publish profile and a parameters.xml file to tokenize the configuration file during build. Then replace the tokens with environment values at deploy time. I show you how to do that starting with the required source changes, how the build works and finally how to craft your release definition for token replacements and deployment.\n\nAppSettings.json\n\nHowever, .NET Core apps are a different kettle of fish. There is no web.config file (by default). If you File-&gt;New Project and create a .NET Core web app, you’ll get an appsettings.json file. This is the “new” web.config if you will. If you then go to the .NET Core documentation, you’ll see that you can create multiple configuration files using “magic” names like appsettings.dev.json and appsettings.prod.json (these are loaded up during Startup.cs). I understand the appeal of this approach, but to me it feels like having multiple web.config files which you replace at deployment time (like web.dev.config and web.prod.config). I’m not even talking about config transforms – just full config files that you keep in source control and (conceptually) overwrite during deployment. So you’re duplicating code – which is bad juju.\n\nI got to thinking about how to handle configuration for .NET Core apps, and after mulling it over and having a good breakfast chat fellow MVP Scott Addie, I thought about tokenizing the appsettings.json file. If I could figure out a clean way to tokenize the file at build time, then I could use my existing ReplaceTokens task (part of my marketplace extension) during deploy time to fill in environment specific values. Unfortunately there’s no config transform for JSON files, so I decided to create a Tokenizer task that could read in a JSON file and then auto-replace values with tokens (based on the object hierarchy).\n\nTokenizer Task\n\nTo see this in action, I created a new .NET Core Web App in Visual Studio. I then added a custom config section. I ended up with an appsettings.json file that looks as follows:\n\n{\n  \"ConnectionStrings\": {\n    \"DefaultConnection\": \"Server=(localdb)\\\\mssqllocaldb;Database=aspnet-WebApplication1-26e8893e-d7c0-4fc6-8aab-29b59971d622;Trusted_Connection=True;MultipleActiveResultSets=true\"\n  },\n  \"Tricky\": {\n    \"Gollum\": \"Smeagol\",\n    \"Hobbit\": \"Frodo\"\n  },\n  \"Logging\": {\n    \"IncludeScopes\": false,\n    \"LogLevel\": {\n      \"Default\": \"Debug\",\n      \"System\": \"Information\",\n      \"Microsoft\": \"Information\"\n    }\n  }\n}\n\n\nLooking at this config, I can see that I might want to change the ConnectionStrings.DefaultConnection as well as the Tricky.Gollum and Tricky.Hobbit settings (yes, I’m reading the Lord of the Rings – I’ve read it about once a year since I was 11). I may want to change Logging.LogLevel.Default too.\n\nSince the file is JSON, I figured I could create a task that reads the file in and then walks the object hierarchy, replacing values with tokens as it goes. But I realized that you may not want to replace every value in the file, so the task would have to take an explicit include (for only replacing certain values) or exclude list (for replacing all but certain values).\n\nI wanted the appsettings file to look like this once the tokenization had completed:\n\n{\n  \"ConnectionStrings\": {\n    \"DefaultConnection\": \" __ConnectionStrings.DefaultConnection__\"\n  },\n  \"Tricky\": {\n    \"Gollum\": \" __Tricky.Gollum__\",\n    \"Hobbit\": \" __Tricky.Hobbit__\"\n  },\n  \"Logging\": {\n    \"IncludeScopes\": false,\n    \"LogLevel\": {\n      \"Default\": \" __Logging.LogLevel.Default__\",\n      \"System\": \"Information\",\n      \"Microsoft\": \"Information\"\n    }\n  }\n}\n\n\nYou can see the tokens on the highlighted lines.\n\nAfter coding for a while on the plane (#RoadWarrior) I was able to create a task for tokenizing a JSON file (perhaps in the future I’ll make more file types available – or I’ll get some Pull Requests!). Having recently added unit tests for my Node tasks, I was able to bang this task out rather quickly.\n\nThe Build Definition\n\nWith my shiny new Tokenize task, I was ready to see if I could get the app built and deployed. Here’s what my build definition looks like:\n\n\n\n\nThe build tasks perform the following operations:\n\n\n  Run dotnet with argument “restore” (restores the package dependencies)\n  Tokenize the appsettings.json file\n  At this point I should have Test, Code Annalysis etc. – I’ve omitted these quality tasks for brevity\n  Run dotnet with arguments “publish src/CoreWebDeployMe –configuration $(BuildConfiguration) –output $(Build.ArtifactStagingDirectory)/Temp” (I’m publishing the folder that contains my .NET Core web app with the BuildConfiguration and placing the output in the Build.ArtifactStagingDirectory/Temp folder)\n  Zip the published folder (the zip task comes from this extension)\n  Remove the temp folder from the staging directory (since all the files I need are now in the zip)\n  Upload the zip as a build drop\n\n\nThe Tokenize task is configured as follows:\n\n\n\n\nLet’s look at the arguments:\n\n\n  Source Path – the path containing the file(s) I want to tokenize\n  File Pattern – the mini-match pattern for the file(s) within the Source Path I want to tokenize\n  Tokenize Type – I only have json for now\n  IncludeFields – the list of properties in the json file that I want the Tokenizer to tokenize\n  ExcludeFields – I could have used a list of properties I wanted to exclude from tokenization here instead of using the Include Fields property\n\n\nOnce the build completes, I now have a potentially deployable .NET Core web application with a tokenized appsettings file. I could have skipped the zip task and just uploaded the site unzipped, but uploading lots of little files takes longer than uploading a single larger file. Also, I was thinking about the deployment – downloading a single larger file (I guessed) was going to be faster than downloading a bunch of smaller files.\n\nThe Release\n\nI was expecting to have to unzip the zip file, replace the tokens in the appsettings.json file and then re-zip the file before invoking WebDeploy to push the zip file to Azure. However, the AzureRM WebDeploy task recently got updated, and I noticed that what used to be “Package File” was now “Package File or Folder”. So the release turned out to be really simple:\n\n\n  Unzip the zip file to a temp folder using an inline PowerShell script (why is there no complementary Unzip task from the Trackyon extension?)\n  Run ReplaceTokens on the appsettings.json file in the temp folder\n  Run AzureRM WebDeploy using the temp folder as the source folder\n\n\n\nHere’s how I configured the PowerShell task:\n\n\n\n\nThe script takes in the sourceFile (the zip file) as well as the target path (which I set to a temp folder in the drop folder):\n\nparam(\n  $sourceFile,\n  $targetPath)\n\nExpand-Archive -Path $sourceFile -DestinationPath $targetPath -Force\n\n\nMy first attempt deployed the site – but the ReplaceTokens task didn’t replace any tokens. After digging a little I figured out why – the default regex pattern – __(\\w+)__ – doesn’t work when the token name have periods in them. So I just updated the regex to __(\\w+[.\\w+]*)__ (which reads “find double underscore, followed by a word, followed by a period and word repeated 0 or more times, ending with double underscore”.\n\n\n\n\nThat got me closer – one more change I had to make was replacing the period with underscore in the variable names on the environment:\n\n\n\n\nOnce the ReplaceTokens task was working, the Deploy task was child’s play:\n\n\n\n\nI just made sure that the “Package or Folder” was set to the temp path where I unzipped the zip file in the first task. Of course at this point the appsettings.json now contains real environment-specific values instead of tokens, so WebDeploy can go and do its thing.\n\nConclusion\n\nIt is possible to apply the Build Once principle to .NET Core web applications, with a little help from my friends Tokenizer and ReplaceTokens in the build and release respectively. I think this approach is fairly clean – you get to avoid duplication in source code, build a single package and deploy to multiple environments. Of course my experimentation is available to your for free from the tasks in my marketplace extension! Sound off in the comments if you think this is useful (or horrible)…\n\nHappy releasing!\n",
      "categories": [],
      "tags": ["releasemanagement","build"],
      
      "collection": "posts",
      "url": "/managing-config-for-net-core-web-app-deployments-with-tokenizer-and-replacetokens-tasks/"
    },{
      
      "title": "You Suck: Or, How to Process Criticism",
      "date": "2016-11-01 11:11:25 +0000",
      
      "content": "\n\nRecently I received some criticism from a customer. Sometimes I find it difficult to process criticism – I justify or argue or dismiss. Some of that is my personality – I like to be right! Part of that is the fact that I strive for excellence, so when I’m told I missed the mark, it can feel like I’m being told I’m a failure. You, dear reader, probably strive for perfection – but let’s face facts: we’re not perfect. If you’re like me and you have a difficult time receiving criticism, then this post is for you – hopefully you can learn something from how I process.\n\nI’m Not Perfect\n\nThis one’s tough. My natural instinct when receiving criticism is to justify. For example, the criticism might be, “You didn’t finish when you said you would.” My inclination is to retort: “Well, you weren’t clear enough on what you wanted,” or something like that. However, the most critical key to successfully processing criticism is to remain teachable – and that means acknowledging that I missed the mark. I have to tell myself not to argue, not to justify. I have to take a step back and see the situation from the other perspective.\n\nI Have Blind Spots\n\nThat leads to the second critical principle – I have blind spots. No matter how much I stare in the mirror to gel my hair to perfection, I still can’t see what’s going on with that stubborn crown on the back of my head! Even if I’m prone to introspection and self-improvement, I’m going to miss stuff. About me. If I reject criticism outright, I’ll never get a chance to see into those blind spots. I have to let criticism be a catalyst to stepping back and honestly assessing what I said or did from someone else’s perspective. I can only improve if there’s something to work on – so I have to let criticism surface things that I can work on.\n\nI Am Not Defined By a Moment\n\nThis is a big one for me – I can tend to take criticism hard, so it becomes overwhelming. I have to realize that even if I blow it, that moment (or engagement) doesn’t define me. I’m more than this one moment. I may have gotten this thing wrong, but I get a lot of things right too! Remembering previous moments where I got things right helps me process moments when I get things wrong.\n\nI Can’t Always Win\n\nSometimes, no matter how hard I try, I can’t win. Someone is going to be disappointed in something I did or said. Most of the time I don’t set out to disappoint, but life happens. Expectations aren’t clear, or are just different, or communication fails. Things beyond my control happen. I have admit that I lost a round – as long as I get up and keep on going!\n\nLearning is a Team Sport\n\nSometimes criticism is deserved. Sometimes it isn’t. And sometimes it’s hard to tell the difference. I make sure I surround myself with people that know and love me – that way, when I’m criticized I have a team I can go to. I like to make my team diverse – my colleagues of course, but also my friends and family. Even if the criticism is work-related, sometimes having a “personal” perspective can help process a “professional” issue. I also make sure I get someone who’s more experienced than me who can mentor me through a situation.\n\nOften criticism has some meat and some bones. Take the meat, spit out the bones. My team helps me to sort the meat from the bones. They help me to keep things in perspective.\n\nMake it Right\n\nFinally, if it’s appropriate to do so, make it right. Sometimes I can take some criticism and just improve, learn and get better. Sometimes I may need to make things right. My team helps me figure out “action items” – things I can do to improve, but also things that I can do to make it right. This doesn’t always apply, but I like to look for things to do or say that will make things right. Although doing this without justifying myself is challenging for me!\n\nConclusion\n\nUnless you’re particularly reclusive, you’ll get criticized at some point. Learning how to embrace and deal with criticism is an important skill to learn. If you use it as a chance to learn and improve, and surround yourself with people who can coach and encourage you, you can process criticism positively and become better!\n\nHappy learning!\n\n* Image by innoxiuss used under Creative Commons\n",
      "categories": [],
      "tags": ["alm"],
      
      "collection": "posts",
      "url": "/you-suck-or-how-to-process-criticism/"
    },{
      
      "title": "DevOps Drives Better Architecture–Part 1 of 2",
      "date": "2017-02-19 05:05:42 +0000",
      
      "content": "(Read part 2 here)\n\nI haven’t blogged for a long while – it’s been a busy few months!\n\nOne of the things I love about being a DevOps consultant is that I have to be technically proficient – I can’t help teams develop best practices if I don’t know the technology to at least a reasonable depth – but I also get to be a catalyst for change. I love the cultural dynamics of DevOps. After all, as my friend Donovan Brown says, “DevOps is the union of people, processes and tools…”. When you involve people, then you get to watch (or, in my case, influence) culture. And it fascinates me.\n\nI only recently read Continuous Delivery by Jez Humble and David Farley. I was pleased at how much of their insights I’ve been advocating “by instinct” over my years of ALM and DevOps consulting. Reading their book sparked the thoughts that I’ll put into this two-part post.\n\nThis part will introduce the thought that DevOps and architecture are symbiotic – good architecture makes for good DevOps, and good DevOps drives good architecture. Ill look at Builds and Source Control in particular. In part 2, I’ll discuss infrastructure as code, database design, automated testing and monitoring and how they relate to DevOps and vice versa.\n\nTools, tools, tools\n\nOver the past few months, the majority of my work has been to help teams implement Build/Release Pipelines. This seems inevitable to me given the state of DevOps in the market in general – most teams have made (or are in the process of making) a shift to agile, iterative frameworks for delivering their software. As they get faster, they need to release more frequently. And if they’ve got manual builds and deployments, the increasing frequency becomes a frustration because they can’t seem to deploy fast enough (or consistently enough). So teams are starting to want to automate their build/release flows.\n\nIt’s natural, therefore, to immediately look for a tool to help automation. And for a little help from your friends at Northwest Cadence to help you do it right!\n\nOf course my tool of choice for build/release pipelines is Visual Studio Team Services (VSTS) or Team Foundation Server (TFS) for a number of reasons:\n\n\n  The build agent is cross platform (it’s built on .NET Core, so runs wherever .NET Core runs)\n  The build agent is also the release agent\n  The build agent can run tests\n  The task-based system has a good Web-based UI, allowing authoring from wherever you have a browser\n  The logging is great – allowing fast debugging of build issues\n  Small custom logic can easily be handled with inline scripts\n  If you can script it, the agent can do it – whether it’s bat, ps1 or sh\n  Extensions are fairly easy to create\n  There is a large and thriving marketplace for extensions\n\n\nGood Architecture Means Easier DevOps\n\nInevitably implementing build automation impacts how you organize your source control. And implementing a release pipeline impacts how you test. And implementing continuous deployment impacts IT, since there’s suddenly a need to be able to spin up and configure and tear down environments on the fly. I love seeing this progression – but it’s often painful for the teams I’m working with. Why? Because teams start realizing that if their architecture was better, it would make other parts of the DevOps pipeline far easier to implement.\n\nFor example, if you start automating releases, pretty soon you start wanting to run automated tests since your tests start becoming the bottleneck to delivery. At this point, if you’ve used good architectural principles like interfaces and inversion of control, writing unit tests is far easier. If you haven’t, you have a far harder time writing tests.\n\nGood architecture can make DevOps easier for you and your team. We’ve been told to do these things, and often we’ve found reasons not to do them (“I don’t have time to make an interface for everything!” or “I’ll refactor that class to make it more testable in the next sprint” etc. etc.). Hopefully I can show you how these architectural decisions, if done with DevOps in mind, will not only make your software better but help you to implement better DevOps, more easily!\n\nThe Love Triangle: Source Control, Branches and Builds\n\nI really enjoy helping teams implement their first automated builds. Builds are so foundational to good DevOps – and builds tend to force teams to reevaluate their code layout (structure), dependencies and branching strategy.\n\nMost of the time, the teams have their source code in some sort of source control system. Time and time again, the teams that have a good structure and simple branching strategies have a far easier time getting builds to work well.\n\nUnfortunately, most repositories I look at are not very well structured. Or the branches represent environments so you see MAIN/DEV/PROD (which is horrible even though most teams don’t know why – read on if this is you). Or they have checked binaries into source control instead of using a package manager like NuGet. Or they have binary references instead of project references.\n\nAnyway, as we get the build going, we start to uncover potential issues most teams don’t even know they have (like missing library references or conflicting package versions). After some work and restructuring, we manage to get a build to compile. Whoop!\n\nBranching\n\nAfter the initial elation and once the party dies down, we take a look at the branching strategy. “We need a branch for development, then we promote to QA, and once QA signs off we promote to PROD. So we need branches for each of these environments, right?” This is still a very pervasive mindset. However, DevOps – specifically release pipelines – should operate on a simple principle: build once, deploy many times. In other words, the bits that you deploy to DEV should be the same bits that you deploy to QA and then to PROD. Don’t just take my work for it: read Continuous Delivery – the authors emphasize this over and over again!. You can’t do that if you have to merge and build each time you want to promote code between environments. So how do you track what code is where and promote parallel development?\n\nBuilds and Versioning\n\nI advocate for a master/feature branch strategy. That is, you have your stable code on master and then have multiple feature branches (1 to n at any one time) that developers work on. Development is done on the feature branch and then merged into master via Pull Request when it’s ready. At that point, a build is queued which versions the assemblies and tags the source with the version (which is typically the build number).\n\nThat’s how you keep track of what code is where – by versions and tags that your build keeps the keys for. That way, you can do hotfixes directly onto master even if you’ve already merged code that is in the pipeline and not yet in production. For example, say you have 1.0.0.6 in prod and you merge some code in for a new feature. The build kicks in and produces version 1.0.0.7 which gets automatically deployed to the DEV environment for integration testing. While that’s going on, you get a bug report from PROD. Oh no! We’ve already merged in code that isn’t yet in PROD, so how do we fix it on master?!?\n\nIt’s easy – we know that 1.0.0.6 is in PROD, so we branch the code using tag 1.0.0.6 (which the build tagged in the repo when it ran the 1.0.0.6 build). We then fix the issue in the branch build off of this branch. A new build – 1.0.0.8. We take a quick look at this and fast-track it through until it’s deployed and business can continue. In the meantime, we can abandon the 1.0.0.7 build that’s currently in the deployment pipeline. We merge the hotfix branch back to the master and do a new build – 1.0.0.9 that now has the hotfix as well as the new feature. No sweat.\n\n“Hang on. Pull Requests? Feature branches? That sounds like Git.” Yes, it does. If you’re not on Git, then you’d better have a convincing reason not to be. You can do a lot of this with TFVC, but it’s just harder. So just get to Git. And as a side benefit, you’ll get a far richer code review experience (in the form of Pull Requests) so your quality is likely to improve. And merging is easier. And you can actually cherry-pick. I could go on and on, but there’s enough Git bigots out there that I don’t need to add my voice too. But get to Git. Last word. Just Do It.\n\nSmall Repos, Microservices and Package Management\n\nSo you’re on Git and you have a master/feature branch branching strategy. But you have multiple components or layers and you need them to live together for compilation, so you put them all into one repo, right? Wrong. You need to separate out your components and services into numerous small repos. Each repo should have Continuous Integration (CI) on it. This change forces teams to start decomposing their monolithic apps into shared libraries and microservices. “Wait – what? I need to get into microservices to get good DevOps?” I hear you yelling. Well, another good DevOps principle is releasing small amounts of change often. And if everything is mashed together in a giant repo, it’s hard to do that. So you need to split up your monoliths into smaller components that can be independently released. Yet again, good architecture (loose coupling, strict service boundaries) promotes good DevOps – or is it DevOps finally motivating you to Do It Right™ like you should have all along?\n\nYou’ve gone ahead and split out shared code and components. But now your builds don’t work because your common code (your internal libraries) are suddenly in different repos. Yes, you’re going to need some package management tool. Now, as a side benefit, teams can opt-in to changes in common libraries rather than being forced to update project references. This is a great example of how good DevOps influences good development practices! Even if you just use a file share as a NuGet source (you don’t necessarily need a full-blown package management system) you’re better off.\n\nConclusion\n\nIn this post, we’ve looked at how good source control structure, branching strategies, loosely coupled architectures and package management can make DevOps easier. Or perhaps how DevOps pushes you to improve all of these. As I mentioned, good architecture and DevOps are symbiotic, feeding off each other (for good or bad). So make sure it’s for good! Now go and read part 2 of this post.\n\nHappy architecting!\n",
      "categories": [],
      "tags": ["devops"],
      
      "collection": "posts",
      "url": "/devops-drives-better-architecturepart-1-of-2/"
    },{
      
      "title": "DevOps Drives Better Architecture–Part 2 of 2",
      "date": "2017-02-19 05:06:13 +0000",
      
      "content": "In part 1 I introduced some thoughts as to how good architecture makes DevOps easier. And how good DevOps drives better architecture – a symbiotic relationship. I discussed how good source control structure, branching strategies, loosely coupled architectures and package management can make DevOps easier. In this post I’ll share some thoughts along the same lines for infrastructure as code, database design and management, monitoring and test automation.\n\nInfrastructure as Code\n\nLet’s say you get your builds under control, you’re versioning, you get your repos sorted and you get package management in place. Now you’re starting to produce lots and lots of builds. Unfortunately, a build doesn’t add value to anyone until it’s in production! So you’ll need to deploy it. But you’ll need infrastructure to deploy to. So you turn to IT and fill in the forms and wait 2 weeks for the servers…\n\nEven if you can quickly spin up tin (or VMs more likely) or you’re deploying to PaaS, you still need to handle configuration. You need to configure your servers (if you’re on IaaS) or your services (on PaaS). You can’t afford to do this manually each time you need infrastructure. You’re going to need to be able to automatically spin up (and down) resources when you need them.\n\nSpinning Up From Scratch\n\nI was recently at a customer that were using AWS VMs for their DevTest infrastructure. We were evaluating if we could replicate their automated processes in Azure. The problem was they hadn’t scripted creation of their environments from scratch – they would manually configure a VM until it was “golden” and then use that as a base image for spinning up instances. Now that we were wanting to change their cloud host, they couldn’t do it easily because someone would have to spin up an Azure VM and manually configure it. If they had rather used the principle of scripting and automating configuration, we could have used the existing scripts to quickly spin up test machines on any platform or host. Sometimes you don’t know you need something until you actually need it – so do the right things early and you’ll be better off in the long run. Get into the habit of configuring via code rather than via UI.\n\nDeploy Whole Components – Always\n\nIn Continuous Delivery, Humble and Farley argue that it’s easier to deploy your entire system each time than trying to figure out what the delta is and only deploy that. If you craft your scripts and deployments so that they are idempotent, then this shouldn’t be a problem. Try to prefer declarative scripting (such as PowerShell DSC) over imperative scripting (like pure PowerShell). Not only is it easier to “read” the configuration, but the system can check if a component is in the required state, and if it is, just “no-op”. Make sure your scripts work irrespective of the initial state of the system.\n\nIf you change a single class, should you just deploy that assembly? It’s far easier to deploy the entire component (be that a service or a web app) than trying to work out what changed and what need to be deployed. Tools can also help here – web deploy, for example, only deploys files that are different. You build the entire site and it calculates at deployment time what the differences are. Same with SDDT for database schema changes.\n\nOf course, getting all the requirements and settings correct in a script in source control is going to mean that you need to cooperate and team up with the IT guys (and gals). You’re going to need to work together to make sure that you’re both happy with the resulting infrastructure. And that’s good for everyone.\n\nGood Database Design and Management\n\nWhere does your logic live?\n\nHow does DevOps influence database design and management? I used to work for a company where the dev manager insisted that all our logic be in stored procedures. “If we need to make a change quickly,” he reasoned, “then we don’t need to recompile, we can just update the SP!” Needless to say, our code was virtually untestable, so we just deployed and hoped for the best. And spent a lot of time debugging and fighting fires. It wasn’t pretty.\n\nStored procedures are really hard to test reliably. And they’re hard to code and debug. So you’re better off leaving your database to store data. Placing logic into component or services lets you test the logic without having to spin up databases with test data – using mocks or fakes or doubles lets you abstract away where the data is stored and test the logic of your apps. And that makes DevOps a lot easier since you can test a lot more during the build phase. And the earlier you find issues (builds are “earlier” than releases) the lest it costs to fix them and the easier it is to fix them.\n\nManaging Schema\n\nWhat about schema? Even if you don’t have logic in your database in the form of stored procedures, you’re bound to change the schema at some stage. Don’t do it using manual scripts. Start using SQL Server Data Tools (SSDT) for managing your schema. Would you change the code directly on a webserver to implement new features? Of course not – you want to have source control and testing etc. So why don’t we treat databases the same way? Most teams seem happy to “just fix it on the server” and hope they can somehow replicate changes made on the DEV databases to QA and PROD. If that’s you – stop it! Get your schema into an SSDT project and turn off DDL-write permissions so that they only way to change a database schema is to change the project, commit and let the pipeline make the change.\n\nThe advantage of this approach is that you get a full history (in source control) of changes made to your schema. Also, sqlpackage calculates the diff at deployment time between your model and the target database and only updates what it needs to to make the database match the model. Idempotent and completely uncaring as to the start state of the database. Which means hassle free deployments.\n\nAutomated Testing\n\nI’ve already touched on this topic – using interfaces and inversion of control makes your code testable, since you can easily mock out external dependencies. Each time you have code that interacts with an external system (be it a database or a web API) you should abstract it as an interface. Not only does this uncouple your development pace from the pace of the external system, it allows you to much more easily test your application by mocking/stubbing/faking the dependency. Teams that have well-architected code are more likely to test their code since the code is easier to test! And tested code produces fewer defects, which means more time delivering features rather than fighting fires. Once again, good architecture is going to ease your DevOps!\n\nOnce you’ve invested in unit testing, you’ll want to start doing some integration testing. This requires code to be deployed to environments so that it can actually hit the externals systems. If everything is a huge monolithic app, then as tests fail you won’t know why they failed. Smaller components will let you more easily isolate where issues occur, leading to faster mean time to detection (MTTD). And if you set up so that you can deploy components independently (since they’re loosely coupled, right!) then you can recover quickly, leading to faster mean time to recovery (MTTR).\n\nYou’ll want to have integration tests that operate “headlessly”. Prefer API calls and HTTP requests over UI tests since UI tests are notoriously hard to create correctly and tend to be fragile. However, if you do get to UI tests, then good architecture can make a big difference here too. Naming controls uniquely means UI test frameworks can find them more easily (and faster) so that UI testing is faster and more reliable. The point surfaces again that DevOps is making you think about how you structure even your UI!\n\nMonitoring\n\nUnfortunately, very few teams that I come across have really good monitoring in place. This is often the “forgotten half-breed” of the DevOps world – most teams get source code right, test right and deploy right – and then wash their hands. “Prod isn’t my responsibility – I’m a dev!” is a common culture. However, good monitoring means that you’re able to more rapidly diagnose issues, which is going to save you time and effort and keep you delivering value (debugging is not delivering value). So you’ll need to think about how to monitor your code, which is going to impact on your architecture.\n\nLogging is just monitoring 1.0. What about utilization? How do you monitor how much resources your code is consuming? And how do you know when to spin up more resources for peak loads? Can you even do that – or do your web services require affinity? Ensuring that your code can run on 1 or 100 servers will make scaling a lot easier.\n\nBut beyond logging and performance monitoring, there’s a virtually untapped wealth of what I call “business monitoring” that very few (if any) teams seem to take advantage of. If you’re developing an e-commerce app, how can you monitor what product lines are selling well? And can you correlate user profiles to spending habits? The data is all there – if you can tap into it. Application Insights, coupled with analytics and PowerBI can empower a new level of insight that your business didn’t even know existed. DevOps (which included monitoring) will drive you to architect good “business monitoring” into your apps.\n\nBuild and Release Responsibilities\n\nOne more nugget that’s been invaluable for successful pipelines: know what builds do and what releases do. Builds should take source code (and packages) as inputs, run quality checks such as code analysis and unit testing, and produce packaged binaries as outputs. These packages should be “environment agnostic” – that is, they should not need to know about environments or be tied to environments. Similarly your builds should not need connections strings or anything like that since the testing that occurs during a build should be unit tests that are fast and have no external dependencies.\n\nThis means that you’ll have to have packages that have “holes” in them where environment values can later be injected. Or you may decide to use environment variables altogether and have no configuration files. However you do it, architecting configuration correctly (and fit for purpose, since there can be many correct ways) will make deployment far easier.\n\nReleases need to know about environments. After all, they’re going to be deploying to the environments, or perhaps even spinning them up and configuring them. This is where your integration and functional tests should be running, since some infrastructure is required.\n\nConclusion\n\nGood architecture makes good DevOps a lot easier to implement – and good DevOps feeds back into improving the architecture of your application as well as your processes. The latest trend of “shift left” means you need to be thinking about more than just solving a problem in code – you need to be thinking beyond just coding. Think about how the code you’re writing is going to be tested. And how it’s going to be configured on different environments. And how it’s going to be deployed. And how you’re going to spin up the infrastructure you need to run it. And how you’re going to monitor it.\n\nThe benefits, however, of this “early effort” will pay off many times over in the long run. You’ll be faster, leaner and meaner than ever. Happy DevOps architecting!\n",
      "categories": [],
      "tags": ["devops"],
      
      "collection": "posts",
      "url": "/devops-drives-better-architecturepart-2-of-2/"
    },{
      
      "title": "Running Selenium Tests in Docker using VSTS Release Management",
      "date": "2017-03-10 00:43:25 +0000",
      
      "content": "The other day I was doing a POC to run some Selenium tests in a Release. I came across some Selenium docker images that I thought would be perfect – you can spin up a Selenium grid (or hub) container and then join as many node containers as you want to (the node container is where the tests will actually run). The really cool thing about the node containers is that the container is configured with a browser (there are images for Chrome and Firefox) meaning you don’t have to install and configure a browser or manually run Selenium to join the grid. Just fire up a couple containers and you’re ready to test!\n\nThe source code for this post is on Github.\n\nHere’s a diagram of the components:\n\n\n\nThe Tests\n\nTo code the tests, I use Selenium WebDriver. When it comes to instantiating a driver instance, I use the RemoteWebDriver class and pass in the Selenium Grid hub URL as well as the capabilities that I need for the test (including which browser to use) – see line 3:\n\nprivate void Test(ICapabilities capabilities)\n{\n    var driver = new RemoteWebDriver(new Uri(HubUrl), capabilities);\n    driver.Navigate().GoToUrl(BaseUrl);\n    // other test steps here\n}\n\n[TestMethod]\npublic void HomePage()\n{\n    Test(DesiredCapabilities.Chrome());\n    Test(DesiredCapabilities.Firefox());\n}\n\n\nLine 4 includes a setting that is specific to the test – in this case the first page to navigate to.\n\nWhen running this test, we need to be able to pass the environment specific values for the HubUrl and BaseUrl into the invocation. That’s where we can use a runsettings file.\n\nTest RunSettings\n\nThe runsettings file for this example is simple – it’s just XML and we’re just using the TestRunParameters element to set the properties:\n\n&amp;lt;?xml version=\"1.0\" encoding=\"utf-8\" ?&amp;gt;\n&amp;lt;RunSettings&amp;gt;\n    &amp;lt;TestRunParameters&amp;gt;\n        &amp;lt;Parameter name=\"BaseUrl\" value=\"http://bing.com\" /&amp;gt;\n        &amp;lt;Parameter name=\"HubUrl\" value=\"http://localhost:4444/wd/hub\" /&amp;gt;\n    &amp;lt;/TestRunParameters&amp;gt;\n&amp;lt;/RunSettings&amp;gt;\n\n\nYou can of course add other settings to the runsettings file for the other environment specific values you need to run your tests. To test the setting in VS, make sure to go to Test-&gt;Test Settings-&gt;Select Test Settings File and browse to your runsettings file.\n\nThe Build\n\nThe build is really simple – in my case I just build the test project. Of course in the real world you’ll be building your application as well as the test assemblies. The key here is to ensure that you upload the test assemblies as well as the runsettings file to the drop (more on what’s in the runsettings file later). The runsettings file can be uploaded using two methods: either copy it using a Copy Files task into the artifact staging directory – or you can mark the file’s properties in the solution to “Copy Always” to ensure it’s copied to the bin folder when you compile. I’ve selected the latter option.\n\nHere’s what the properties for the file look like in VS:\n\n\n\n\nHere’s the build definition:\n\n\n\nThe Docker Host\n\nIf you don’t have a docker host, the fastest way to get one is to spin it up in Azure using the Azure CLI – especially since that will create the certificates to secure the docker connection for you! If you’ve got a docker host already, you can skip this section – but you will need to know where the certs are for your host for later steps.\n\nHere are the steps you need to take to do that (I did this all in my Windows Bash terminal):\n\n\n  Install node and npm\n  Install the azure-cli using “npm install –g azure-cli”\n  Run “azure login” and log in to your Azure account\n  Don’t forget to set your subscription if you have more than one\n  Create an Azure Resource Group using “azure group create &lt;name&gt; &lt;location&gt;”\n  Run “azure vm image list –l westus –p Canonical” to get a list of the Ubuntu images. Select the Urn of the image you want to base the VM on and store it – it will be something like “Canonical:UbuntuServer:16.04-LTS:16.04.201702240”. I’ve saved the value into $urn for the next command.\n  Run the azure vm docker create command – something like this:\nazure vm docker create --data-disk-size 22 --vm-size \"Standard_d1_v2\" --image-urn $urn --admin-username vsts --admin-password $password --nic-name \"cd-dockerhost-nic\" --vnet-address-prefix \"10.2.0.0/24\" --vnet-name \"cd-dockerhost-vnet\" --vnet-subnet-address-prefix \"10.2.0.0/24\" --vnet-subnet-name \"default\" --public-ip-domain-name \"cd-dockerhost\"  --public-ip-name \"cd-dockerhost-pip\" --public-ip-allocationmethod \"dynamic\" --name \"cd-dockerhost\" --resource-group \"cd-docker\" --storage-account-name \"cddockerstore\" --location \"westus\" --os-type \"Linux\" --docker-cert-cn \"cd-dockerhost.westus.cloudapp.azure.com\"\n\n\nHere’s the run from within my bash terminal:\n\n\n\n\nHere’s the result in the Portal:\n\n\n\n\nOnce the docker host is created, you’ll be able to log in using the certs that were created. To test it, run the following command:\n\n\ndocker -H $dockerhost --tls info\n\n\n\n\nI’ve included the commands in a fish script here.\n\nThe docker-compose.yml\n\nThe plan is to run multiple containers – one for the Selenium Grid hub and any number of containers for however many nodes we want to run tests in. We can call docker run for each container, or we can be smart and use docker-compose!\n\nHere’s the docker-compose.yml file:\n\nhub:\n  image: selenium/hub\n  ports:\n    - \"4444:4444\"\n  \nchrome-node:\n  image: selenium/node-chrome\n  links:\n    - hub\n\nff-node:\n  image: selenium/node-firefox\n  links:\n    - hub\n\n\nHere we define three containers – named hub, chrome-node and ff-node. For each container we specify what image should be used (this is the image that is passed to a docker run command). For the hub, we map the container port 4444 to the host port 4444. This is the only port that needs to be accessible outside the docker host. The node containers don’t need to map ports since we’re never going to target them directly. To connect the nodes to the hub, we simple use the links keyword and specify the name(s) of the containers we want to link to – in this case, we’re linking both nodes to the hub container. Internally, the node containers will use this link to wire themselves up to the hub – we don’t need to do any of that plumbing ourselves - really elegant!\n\nThe Release\n\nThe release requires us to run docker commands to start a Selenium hub and then as many nodes as we need. You can install this extension from the marketplace to get docker tasks that you can use in build/release. Once the docker tasks get the containers running, we can run our tests, passing in the hub URL so that the Selenium tests hit the hub container, which will distribute the tests to the nodes based on the desired capabilities. Once the tests complete, we can optionally stop the containers.\n\nDefine the Docker Endpoint\n\nIn order to run commands against the docker host from within the release, we’ll need to configure a docker endpoint. Once you’ve installed the docker extension from the marketplace, navigate to your team project and click the gear icon and select Services. Then add a new Docker Host service, entering your certificates:\n\n\n\nDocker VSTS Agent\n\nWe’re almost ready to create the release – but you need an agent that has the docker client installed so that it can run docker commands! The easiest way to do this – is to run the vsts agent docker image on your docker host. Here’s the command:\n\ndocker -H $dockerhost --tls run --env VSTS_ACCOUNT=$vstsAcc --env VSTS_TOKEN=$pat --env VSTS_POOL=docker -it microsoft/vsts-agent\n\n\nI am connecting this agent to a queue called docker – so I had to create that queue in my VSTS project. I wanted a separate queue because I want to use the docker agent to run the docker commands and then use the hosted agent to run the tests – since the tests need to run on Windows. Of course I could have just created a Windows VM with the agent and the docker bits – that way I could run the release on the single agent.\n\nThe Release Definition\n\nCreate a new Release Definition and start from the empty template. Set the build to the build that contains your tests so that the tests become an artifact for the release. Conceptually, we want to spin up the Selenium containers for the test, run the tests and then (optionally) stop the containers. You also want to deploy your app, typically before you run your tests – I’ll skip the deployment steps for this post. You can do all three of these phases on a single agent – as long as the agent has docker (and docker-compose) installed and VS 2017 to run tests. Alternatively, you can do what I’m doing and create three separate phases – the docker commands run against a docker-enabled agent (the VSTS docker image that I we just got running) while the tests run off a Windows agent. Here’s what that looks like in a release:\n\n\n\n\nHere are the steps to get the release configured:\n\n\n  Create a new Release Definition and rename the release by clicking the pencil icon next to the name\n  Rename “Environment 1” to “Test” or whatever you want to call the environment\n  Add a “Run on agent” phase (click the dropdown next to the “Add Tasks” button)\n  Set the queue for that phase to “docker” (or whatever queue you are using for your docker-enabled agents)\n\n  In this phase, add a “Docker-compose” task and configure it as follows:\n\n  Change the action to “Run service images” (this ends up calling docker-compose up)\n  Uncheck Build Images and check Run in Background\n  Set the Docker Host Connection\n  In the next phase, add tasks to deploy your app (I’m skipping these tasks for this post)\n  Add a VSTest task and configure it as follows:\n\n  I’m using V2 of the Test Agent task\n  I update the Test Assemblies filter to find any assembly with UITest in the name\n  I point the Settings File to the runsettings file\n  I override the values for the HubUrl and BaseUrl using environment variables\n  Click the ellipses button on the Test environment and configure the variables, using the name of your docker host for the HubUrl (note also how the port is the port from the docker-compose.yml file):\n\n  In the third (optional) phase, I use another Docker Compose task to run docker-compose down to shut down the containers\n\n  This time set the Action to “Run a Docker Compose command” and enter “down” for the Command\n  Again use the docker host connection\n\n\nWe can now queue and run the release!\n\nMy release is successful and I can see the tests in the Tests tab (don’t forget to change the Outcome filter to Passed – the grid defaults this to Failed):\n\n\n\nSome Challenges\n\nDocker-compose SSL failures\n\nI could not get the docker-compose task to work using the VSTS agent docker image. I kept getting certificate errors like this:\n\n\nSSL error: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:581)\n\n\nI did log an issue on the VSTS Docker Tasks repo, but I’m not sure if this is a bug in the extension or the VSTS docker agent. I was able to replicate this behavior locally by running docker-compose. What I found is that I can run docker-compose successfully if I explicitly pass in the ca.pem, cert.pem and key.pem files as command arguments – but if I specified them using environment variables, docker-compose failes with the SSL error. I was able to run docker commands successfully using the Docker tasks in the release – but that would mean running three commands (assuming I only want three containers) in the pre-test phase and another three in the post-test phase to stop each container. Here’s what that would look like:\n\n\n\n\nYou can use the following commands to run the containers and link them (manually doing what the docker-compose.yml file does):\n\n\nrun -d -P --name selenium-hub selenium/hub\n\n\nrun -d --link selenium-hub:hub selenium/node-chrome\n\n\nrun -d --link selenium-hub:hub selenium/node-firefox\n\n\nTo get the run for this post working, I just ran the docker-compose from my local machine (passing in the certs explicitly) and disabled the Docker Compose task in my release.\n\nEDIT (3/9/2017): I figured out the issue I was having: when I created the docker host I wasn’t specifying a CN for the certificates. The default is *, which was causing my SSL issues. When I configured the CN correctly using\n\n–docker-cert-cn” “cd-dockerhost.westus.cloudapp.azure.com”, everything worked nicely.\n\nRunning Tests in the Hosted Agent\n\nI also could not get the test task to run successfully using the hosted agent – but it did run successfully if I used a private windows agent. This is because at this time VS 2017 is not yet installed on the hosted agent. Running tests from the hosted agent will work just fine once VS 2017 is installed onto it.\n\nPros and Cons\n\nThis technique is quite elegant – but there are pros and cons.\n\nPros:\n\n\n  Get lots of Selenium nodes registered to a Selenium hub to enable lots of parallel testing (refer to my previous blog on how to run tests in parallel in a grid)\n  No config required – you can run tests on the nodes as-is\n\n\nCons:\n\n\n  Only Chrome and Firefox tests supported, since there are only docker images for these browsers. Technically you could join any node you want to to the hub container if you wanted other browsers, but at that point you may as well configure the hub outside docker anyway.\n\n\nConclusion\n\nI really like how easy it is to get a Selenium grid up and running using Docker. This should make testing fast – especially if you’re running tests in parallel. Once again VSTS makes advanced pipelines easy to tame!\n\nHappy testing!\n",
      "categories": [],
      "tags": ["docker","releasemanagement"],
      
      "collection": "posts",
      "url": "/running-selenium-tests-in-docker-using-vsts-release-management/"
    },{
      
      "title": "Easy Config Management when Deploying Azure Web Apps from VSTS",
      "date": "2017-03-18 05:51:34 +0000",
      
      "content": "A good DevOps pipeline should utilize the principle of build once, deploy many times. In fact, I’d go so far as to say it’s essential for a good DevOps pipeline. That means that you have to have a way to manage your configuration in such a way that the package coming out of the build process is tokenized somehow so that when you release to different environments you can inject environment-specific values. Easier said that done – until now.\n\nDoing it the Right but Hard Way\n\nCurrently I’ve recommended that you use WebDeploy to do this. You define a publish profile to handle connection string and a parameters.xml file to handle any other config you want to tokenize during build. This produces a WebDeploy zip file along with a (now tokenized) SetParameters.xml file. Then you use the ReplaceTokens task from my VSTS build/release task pack extension and inject the environment values into the SetParameters.xml file before invoking WebDeploy. This works, but it’s complicated. You can read a full end to end walkthrough in this post.\n\nDoing it the Easy Way\n\nA recent release to the Azure Web App deploy task in VSTS has just dramatically simplified the process! No need for parameters.xml or publish profiles at all.\n\nMake sure your build is producing a WebDeploy zip file. You can read my end to end post on how to add the build arguments to the VS Build task – but now you don’t have to specify a publish profile. You also don’t need a parameters.xml in the solution. The resulting zip file will deploy (by default) with whatever values you have in the web.config at build time.\n\nHere’s what I recommend:\n\n\n/p:DeployOnBuild=true /p:WebPublishMethod=Package /p:PackageAsSingleFile=true /p:SkipInvalidConfigurations=true /p:PackageLocation=\"$(build.artifactStagingDirectory)\"\n\n\nYou can now just paste that into the build task:\n\n\n\n\nYou can see the args (in the new build UI). This tells VS to create the WebDeploy zip and put it into the artifact staging directory. The Publish Artifact Drop task uploads anything that it’s the artifact staging directory (again, by default) – which at the time it runs should be the WebDeploy files.\n\nThe Release\n\nHere’s where the magic comes in: drop in an Azure App Service Deploy task. Set it’s version to 3.*(preview). You’ll see a new section called “File Transforms &amp; Variable Substitution Options”. Just enable the “XML Override substitution”.\n\n\n\n\nThat’s it! Except for defining the values we want to use for the said substitution. To do this, open the web.config and look at your app setting keys or connection string names. Create a variable that matches the name of the setting and enter a value. In my example, I have Azure B2C so I need a key called “ida:Tenant” so I just created a variable with that name and set the value for the DEV environment. I did the same for the other web.config variables:\n\n\n\n\nNow you can run your release!\n\nChecking the Web.Config Using Kudu\n\nOnce the release had completed, I wanted to check if the value had been set. I opened up the web app in the Azure portal, but there were no app settings defined there. I suppose that makes sense – the substitutions are made onto the web.config itself. So I just opened the Kudu console for the web app and cat’ed the web.config by typing “cat Web.config”. I could see that the environment values had been injected!\n\n\n\nConclusion\n\nIt’s finally become easy to manage web configs using the VSTS Azure Web App Deploy task. No more publish profiles, parameters.xml files, SetParameters.xml files or token replacement. It’s refreshingly clean and simple. Good job VSTS team!\n\nI did note that there is also the possibility of injecting environment-specific values into a json file – so if you have .NET CORE apps, you can easily inject values at deploy time.\n\nHappy releasing!\n",
      "categories": [],
      "tags": ["releasemanagement"],
      
      "collection": "posts",
      "url": "/easy-config-management-when-deploying-azure-web-apps-from-vsts/"
    },{
      
      "title": "New Task: Tag Build or Release",
      "date": "2017-05-02 12:49:12 +0000",
      
      "content": "I have a build/release task pack in the marketplace. I’ve just added a new task that allows you to add tags to builds or releases in the pipeline, inspired by my friend and fellow MVP Rene van Osnabrugge’s excellent post.\n\nHere are a couple of use cases for this task:\n\n\n  You want to trigger releases, but only for builds on a particular branch with a particular tag. This trigger only works if the build is tagged during the build. So you could add a TagBuild task to your build that is only run conditionally (for example for buildreason = Pull Request). Then if the condition is met, the tag is set on the build and the release will trigger in turn, but only for builds that have the tag set.\n\n  You want to tag a build from a release once a release gets to a certain environment. For example, you can add a TagBuild task and tag the primary build once all the integration tests have passed in the integration environment. That way you can see which builds have passed integration tests simply by querying the tags.\n\n\n\nOf course you can use variables for the tags – so you could tag the build with the release(s) that have made it to prod by specifying $(Release.ReleaseNumber) as the tag value.\n\nThere are of course a ton of other use cases!\n\nTag Types\n\nYou can see the tag type matrix for the “tag type” (which can be set to Build or Release) in the docs.\n\nConclusion\n\nLet me know if you have issues or feedback. Otherwise, happy taggin’!\n",
      "categories": [],
      "tags": ["development","releasemanagement"],
      
      "collection": "posts",
      "url": "/new-task-tag-build-or-release/"
    },{
      
      "title": "Testing in Production: Routing Traffic During a Release",
      "date": "2017-05-10 07:13:27 +0000",
      
      "content": "DevOps is a journey that every team should at least have started by now. Most of the engagements I have been on in the last year or so have been in the build/release automation space. There are still several practices that I think teams must invest in to remain competitive – unit testing during builds and integration testing during releases are crucial foundations for more advanced DevOps, which I’ve blogged about (a lot) before. However, Application Performance Monitoring (APM) is also something that I believe is becoming more and more critical to successful DevOps teams. And one application of monitoring is hypothesis driven development.\n\nHypothesis Driven Development using App Service Slots\n\nThere are some prerequisites for hypothesis driven development: you need to have metrics that you can measure (I highly, highly recommend using Application Insights to gather the metrics) and you have to have a hypothesis that you can quickly test. Testing in production is the best way to do this – but how do you manage that?\n\nIf you’re deploying to Azure App Services, then it’s pretty simple: create a deployment slot on the Web App that you can deploy the “experimental” version of your code to and divert a small percentage of traffic from the real prod site to the experimental slot. Then monitor your metrics. If you’re happy, swap the slots, instantly promoting the experiment. If it does not work, then you’ve failed fast – and you can back out.\n\nSounds easy. But how do you do all of that in an automated pipeline? Well, you can already deploy to a slot using VSTS and you can already swap slots using OOB tasks. What’s missing is the ability to route a percentage of traffic to a slot.\n\nRoute Traffic Task\n\nTo quote Professor Farnsworth, “Good news everyone!” There is now a VSTS task in my extension pack that allows you to configure a percentage of traffic to a slot during your release – the Route Traffic task. To use it, just deploy the new version of the site to a slot and then drop in a Route Traffic task to route a percentage of traffic to the staging site. At this point, you can approve or reject the experiment – in both cases, take the traffic percentage down to 0 to the slot (so that 100% traffic goes to the production slot) ad then if the experiment is successful, swap the slots.\n\nWhat He Said – In Pictures\n\nTo illustrate that, here’s an example. In this release I have DEV and QA environments (details left out for brevity), and then I’ve split prod into Prod-blue, blue-cleanup and Prod-success. There is a post-approval set on Prod-blue. For both success and failure of the experiment, approve the Prod-blue environment. At this stage, blue-cleanup automatically runs, turning the traffic routing to 0 for the experimental slot. Then Prod-success starts, but it has a pre-approval set that you can approve only if the experiment is successful: it swaps the slots.\n\nHere is the entire release in one graphic:\n\n\n\n\nIn Prod-blue, the incoming build is deployed to the “blue” slot on the web app:\n\n\n\n\nNext, the Route Traffic task routes a percentage of traffic to the blue slot (in this case, 23%):\n\n\n\n\nIf you now open the App Service in the Azure Portal, click on “Testing in Production” to view the traffic routing:\n\n\n\n\nNow it’s time to monitor the two slots to check if the experiment is successful. Once you’ve determined the result, you can approve the Prod-blue environment, which automatically triggers the blue-cleanup environment, which updates the traffic routing to route 0% traffic to the blue slot (effectively removing the traffic route altogether).\n\n\n\n\nThen the Prod-success environment is triggered with a manual pre-deployment configured – reject to end the experiment (if it failed) or approve to execute the swap slot task to make the experimental site production.\n\n\n\n\nWhew! We were able to automate an experiment fairly easily using the Route Traffic task!\n\nConclusion\n\nUsing my new Route Traffic task, you can easily configure traffic routing into your pipeline to conduct true A/B testing. Happy hypothesizing!\n",
      "categories": [],
      "tags": ["releasemanagement","devops"],
      
      "collection": "posts",
      "url": "/testing-in-production-routing-traffic-during-a-release/"
    },{
      
      "title": "Aurelia, Azure and VSTS",
      "date": "2017-06-10 00:05:22 +0000",
      
      "content": "I am a huge fan of Aurelia – and that was even when I was working with it in the beta days. I recently had to do some development to display d3 graphs, and needed a simple SPA app. Of course I decided to use Aurelia. During development, I was again blown away by how well thought out Aurelia is – and using some new (to me) tooling, the experience was super. In this post I’ll walk through the tools that I used as well as the build/release pipeline that I set up to host the site in Azure.\n\nTools\n\nHere are the tools that I used:\n\n\n  aurelia-cli to create the project, scaffold and install components, build and run locally\n  VS Code for frontend editing, with a great Aurelia extension\n  Visual Studio 2017 for coding/running the API\n  TypeScript for the Aurelia code\n  Karma (with phantomJS) and Istanbul for frontend testing and coverage\n  .NET Core for the Aurelia host as well as for an API\n  Azure App Services to host the web app\n  VSTS for Git source control, build and release\n\n\nThe Demo App and the Challenges\n\nTo walk through the development process, I’m going to create a stupid-simple app. This isn’t a coding walkthrough per se – I want to focus on how to use the tooling to support your development process. However, I’ll demonstrate the challenges as well as the solutions, hopefully showing you how quickly you can get going and do what you do best – code!\n\nThe demo app will be an Aurelia app with just a REST call to an API. While it is a simple app, I’ll walk through a number of important development concepts:\n\n\n  Creating a new project\n  Configuring VS Code\n  Installing components\n  Building, bundling and running the app locally\n  Handling different configs for different environments\n  Automated build, test and deployment of the app\n\n\nCreating the DotNet Projects\n\nThere are some prerequisites to getting started, so I installed all of these:\n\n\n  nodejs\n  npm\n  dotnet core\n  aurelia-cli\n  VS Code\n  VS 2017\n\n\nOnce I had the prereqs installed, I created a new empty folder (actually I cloned an empty Git repo – if you don’t clone a repo, remember to git init). Since I wanted to peg the dotnet version, I created a new file called global.json:\n\n{\n  \"sdk\": {\n    \"version\": \"1.0.4\"\n  }\n}\n\n\nI also created a .gitignore (helpful tip: if you open the folder in Visual Studio and use Team Explorer-&gt;Settings-&gt;Repository Settings, you can create a default .gitignore and .gitattributes file).\n\nThen I created a new dotnet webapi project to “host” the Aurelia app in a folder called frontend and another dotnet project to be the API in a folder called API:\n\n\n\n\nThe commands are:\n\nmkdir frontend\ncd frontend\ndotnet new webapi\ncd ..\nmkdir API\ncd API\ndotnet new webapi\n\n\nI then opened the API project in Visual Studio. Pressing save prompted me to create a solution file, which I did in the API folder. I also created an empty readme.txt file in the wwwroot folder (I’ll explain why when we get to the build) and changed the Launch URL in the project properties to “api/values”:\n\n\n\n\nWhen I press F5 to debug, I see this:\n\n\n\nCreating the Aurelia Project\n\nI was now ready to create the Aurelia skeleton. The last time I used Aurelia, there was no such thing as the aurelia-cli – so it was a little bumpy getting started. I found using the cli and the project structure it creates for building/bundling made development smooth as butter. So I cd’d back to the frontend folder and ran the aurelia-cli command to create the Aurelia project:\n\n\nau new --here\n\n\n. The “\n\n\n--here\n\n\n” is important because it tells the aurelia-cli to create the project in this directory without creating another subdirectory. A wizard then walked me through some choices: here are my responses:\n\n\n  Target platform: .NET Core\n  Transpiler: TypeScript\n  Template: With minimum minification\n  CSS Processor: Less\n  Unit testing: Yes\n  Install dependencies: Yes\n\n\nThat created the Aurelia project for me and installed all of the nodejs packages that Aurelia requires. Once the install completed, I was able to run by typing “au run”:\n\n\n\n\nWhoop! The skeleton is up, so it’s time to commit!\n\nYou can find the repo I used for this post here. There are various branches – start is the the start of the project up until now – in other words, the absolute bare skeleton of the project.\n\nConfiguring VS Code\n\nNow that I have a project structure, I can start coding. I’ve already got Visual Studio for the API project, which I could use for the frontend editing, but I really like doing nodejs development in VS Code. So I open up the frontend folder in VS Code.\n\nI’ve also installed some VS Code extensions:\n\n\n  VSCode Great Icons – makes the icons in the file explorer purdy (don’t forget to configure your preferences after you install the extension!)\n  TSLint – lints my TypeScript as I code\n  aurelia – palette commands and html intellisense\n\n\nConfiguring TSLint\n\nThere is already an empty tslint.json file in the root of the frontend project. Once you’ve installed the VS Code TSLint extension, you’ll see lint warnings in the status bar: though you have to first configure which rules you want to run. I usually start by extending the tslint:latest rules. Edit the tslint.json file to look like this:\n\n{\n  \"extends\": [\"tslint:latest\"],\n  \"rules\": {\n    \n  }\n}\n\n\nNow you’ll see some warnings and green squigglies in the code:\n\n\n\n\nI don’t care about the type of quotation marks (single or double) and I don’t care about alphabetically ordering my imports, so I override those rules:\n\n{\n  \"extends\": [\"tslint:latest\"],\n  \"rules\": {\n    \"ordered-imports\": [\n      false\n    ],\n    \"quotemark\": [\n      false\n    ]\n  }\n}\n\n\nOf course you can put whatever ruleset you want into this file – but making a coding standard for your team that’s enforced by a tool rather than in a wiki or word doc is a great practice! A helpful tip is that if you edit the json file in VS Code you get intellisense for the rules – and you can see the name of the rule in the warnings window.\n\nInstalling Components\n\nNow we can use the aurelia cli (au) to install components. For example, I want to do some REST calls, so I want to install the fetch-client:\n\n\nau install aurelia-fetch-client whatwg-fetch\n\n\nThis not only adds the package, but amends the aurelia.json manifest file (in the aurelia_project folder) so that the aurelia-fetch-client is bundled when the app is “compiled”. I also recommend installing whatwg-fetch which is a fetch polyfill. Let’s create a new class which is a wrapper for the fetch client:\n\nimport { autoinject } from 'aurelia-framework';\nimport { HttpClient } from 'aurelia-fetch-client';\n\nconst baseUrl = \"http://localhost:1360/api\";\n\n@autoinject\nexport class ApiWrapper {\n    public message = 'Hello World!';\n    public values: string[];\n\n    constructor(public client: HttpClient) {\n\t\tclient.configure(config =&amp;gt; {\n\t\t\tconfig\n\t\t\t\t.withBaseUrl(baseUrl)\n\t\t\t\t.withDefaults({\n\t\t\t\t\theaders: {\n\t\t\t\t\t\tAccept: 'application/json',\n\t\t\t\t\t},\n\t\t\t\t});\n\t\t});\n\t}\n}\n\n\nNote that (for now) we’re hard-coding the baseUrl. We’ll address config shortly.\n\nWe can now import in the ApiWrapper (via injection) and call the values method:\n\nimport { autoinject } from 'aurelia-framework';\nimport { ApiWrapper } from './api';\n\n@autoinject\nexport class App {\n  public message = 'Hello World!';\n  public values: string[];\n\n  constructor(public api: ApiWrapper) {\n    this.initValues();\n  }\n\n  private async initValues() {\n    try {\n      this.values = await this.api.client.fetch(\"/values\")\n        .then((res) =&amp;gt; res.json());\n    } catch (ex) {\n      console.error(ex);\n    }\n  }\n}\n\n\nHere’s the updated html for the app.html page:\n\n&lt;template&gt;\n  &lt;h1&gt;${message}&lt;/h1&gt;\n  &lt;h3&gt;Values&lt;/h3&gt;\n  &lt;ul&gt;\n    &lt;li repeat.for=\"val of values\"&gt;${val}&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/template&gt;\n\n\nNothing too fancy – but shown here to be complete. I’m not going to make a full app here, since that’s not the goal of this post.\n\nFinally, we need to enable CORS on the Web API (since it does not allow CORS by default). Add the Microsoft.AspNet.Cors package to the API project and then add the services.AddCors() and app.UseCors() lines (see this snippet):\n\npublic void ConfigureServices(IServiceCollection services)\n{\n  // Add framework services.\n  services.AddMvc();\n\tservices.AddCors();\n}\n\n// This method gets called by the runtime. Use this method to configure the HTTP request pipeline.\npublic void Configure(IApplicationBuilder app, IHostingEnvironment env, ILoggerFactory loggerFactory)\n{\n  loggerFactory.AddConsole(Configuration.GetSection(\"Logging\"));\n  loggerFactory.AddDebug();\n\n  app.UseCors(p =&amp;gt; p.AllowAnyOrigin().AllowAnyMethod());\n  app.UseMvc();\n}\n\n\nNow we can get this when we run the project (using “au run”):\n\n\n\n\nIf you’re following along in the repo code, the changes are on the “Step1-AddFetch” branch.\n\nRunning Locally\n\nRunning locally is trivial. I end up with Visual Studio open and pressing F5 to run the backend API project – the frontend project is just as trivial. In VSCode, with the frontend folder open, just hit ctrl-shift-p to bring up the command palette and then type/select “au run –watch” to launch the frontend “build”. This transpiles the TypeScript to JavaScript, compiles Less (or SASS) to css, bundles and minifies all your html, compiled css and JavaScript into a single app-bundle.js in wwwroot\\scripts. It also minifies and bundles Aurelia and its dependencies into vendor-bundle.js, using the settings from the aurelia.json file. It’s a lot of work, but Aurelia takes care of it all for you – just run “au run” to do all that stuff and launch a server. If you add the –watch parameter, the process watches your source files (html, Less, TypeScript) and automatically recompiles everything and refreshes the browser automagically using browsersync. It’s as smooth as butter!\n\nConfig Management\n\nAttempt 1 – Using environment.ts\n\nLet’s fix up the hard-coded base URL for the api class. Aurelia does have the concept of “environments” – you can see that by looking in the src\\environment.ts file. You would be tempted to change the values of that file, but you’ll see that if you do, the contents get overwritten each time Aurelia compiles. Instead, open up the aurelia-project\\environments folder, where you’ll see three environment files – dev, stage and prod.ts. To change environment, just enter “au run –env dev” to get the dev environment or “au run –env prod” to get the prod environment. (Unfortunately you can’t change the environment using VSCode command palette, so you have to run the run command from a console or from the VSCode terminal).\n\nLet’s edit the environments to put the api base URL there instead of hard-coding it:\n\nexport default {\n  apiBaseUrl: \"http://localhost:64705/api\",\n  debug: true,\n  testing: true,\n};\n\n\nOf course we add the apiBaseUrl property to the stage and prod files too!\n\nWith that change, we can simply import the environment and use the value of the property in the api.ts file:\n\nimport { autoinject } from 'aurelia-framework';\nimport { HttpClient } from 'aurelia-fetch-client';\nimport environment from './environment';\n\n@autoinject\nexport class ApiWrapper {\n    public message = 'Hello World!';\n    public values: string[];\n\n    constructor(public client: HttpClient) {\n        client.configure(config =&amp;gt; {\n            config\n                .withBaseUrl(environment.apiBaseUrl)\n                .withDefaults({\n                    headers: {\n                        Accept: 'application/json',\n                    },\n                });\n        });\n    }\n}\n\n\nThe important changes are on line 2 (reading in the environment settings) and line 13 (using the value). Now we can run for different environments. If you’re following along in the repo code, the changes are on the “Step2-EnvTsConfig” branch.\n\nAttempt 2 – Using a Json File\n\nThere’s a problem with the above approach though – if we have secrets (like access tokens or keys) then we don’t want them checked into source control. Also, when we get to build/release, we want the same build to go to multiple environments – using environment.ts means we have to build once for each environment and then select the correct package for the corresponding environment – it’s nasty. Rather, we want to be able to configure the environment settings during a release. This puts secret information in the release tool instead of source control, which is much better, and allows a single build to be deployed to any number of environments.\n\nUnfortunately, it’s not quite so simple (at first glance). The environment.ts file is bundled into app-bundle.js, so there’s no way to inject values at deploy time, unless you want to monkey with the bundle itself. It would be much better to take a leaf out of the .NET CORE playbook and set up a Json config file. Fortunately, there’s an Aurelia plugin that allows you to do just that! Conveniently, it’s called aurelia-configuration.\n\nRun “au install aurelia-configuration” to install the module.\n\nNow (by convention) the config module looks for a file called “config\\config.json”. So in the src folder, add a new folder called config and add a new file into the config folder called config.json:\n\n{\n      \"api\": {\n            \"baseUri\": \"http://localhost:12487/api\"\n      }\n}\n\n\nWe can then inject the AureliaConfiguration class into our classes and call the get() method to retrieve a variable value. Let’s change the api.ts file again:\n\nimport { autoinject } from 'aurelia-framework';\nimport { HttpClient } from 'aurelia-fetch-client';\nimport { AureliaConfiguration } from 'aurelia-configuration';\n\n@autoinject\nexport class ApiWrapper {\n    public message = 'Hello World!';\n    public values: string[];\n\n    constructor(public client: HttpClient, private aureliaConfig: AureliaConfiguration) {\n        client.configure(config =&amp;gt; {\n            config\n                .withBaseUrl(aureliaConfig.get(\"api.baseUri\"))\n                .withDefaults({\n                    headers: {\n                        Accept: 'application/json',\n                    },\n                });\n        });\n    }\n}\n\n\nLine 3 has us importing the type, line 10 has the constructor arg for the autoinjection and we get the value on line 13.\n\nWe also have to tell Aurelia to use the config plugin. Open main.ts and add the plugin code (line 8 below):\n\nimport {Aurelia} from 'aurelia-framework';\nimport environment from './environment';\n\nexport function configure(aurelia: Aurelia) {\n  aurelia.use\n    .standardConfiguration()\n    .feature('resources')\n    .plugin('aurelia-configuration');\n  ...\n\n\nThere’s one more piece to this puzzle: the config.json file doesn’t get handled anywhere, so running the program won’t work. We need to tell the Aurelia bundler that it needs to add in the config.json file and publish it to the wwwroot folder. To do that, we can add in a copyFiles target onto the aurelia.json settings file:\n\n{\n  \"name\": \"frontend\",\n  \"type\": \"project:application\",\n  \"platform\": {\n    ...\n  },\n  ...\n  \"build\": {\n    \"targets\": [\n     ...\n    ],\n    \"loader\": {\n      ...\n    },\n    \"options\": {\n      ...\n    },\n    \"bundles\": [\n      ...\n    ],\n    \"copyFiles\": {\n      \"src/config/*.json\": \"wwwroot/config\"\n    }\n  }\n}\n\n\nAt the bottom of the file, just after the build.bundles settings, we add the copyFiles target. The config.json file is now copied to the wwwroot/config folder when we build, ready to be read at run time! If you’re following along in the repo code, the changes are on the “Step3-JsonConfig” branch.\n\nTesting\n\nAuthoring the Tests\n\nOf course the API project would require tests – but doing .NET testing is fairly simple and there’s a ton of guidance on how to do that. I was more interested in testing the frontend (Aurelia) code with coverage results.\n\nWhen I created the frontend project, Aurelia created a test stub project. If you open the test folder, there’s a simple test spec in unit\\app.spec.ts:\n\nimport {App} from '../../src/app';\n\ndescribe('the app', () =&amp;gt; {\n  it('says hello', () =&amp;gt; {\n    expect(new App().message).toBe('Hello World!');\n  });\n});\n\n\nWe’ve changed the App class, so this code won’t compile correctly. Now we need to pass an ApiWrapper to the App constructor. And if we want to construct an ApiWrapper, we need an AureliaConfiguration instance as well as an HttpClient instance. We’re going to want to mock the API calls that the frontend makes, so let’s stub out a mock implementation of HttpClient. I add a new class in src\\test\\unit\\utils\\mock-fetch.ts:\n\nimport { HttpClient } from 'aurelia-fetch-client';\n\nexport class HttpClientMock extends HttpClient {\n}\n\n\nWe’ll flesh this class out shortly. For now, it’s enough to get an instance of HttpClient for the ApiWrapper constructor. What about the AureliaConfiguration instance? Fortunately, we can create (and even configure) one really easily:\n\nlet aureliaConfig = new AureliaConfiguration();\naureliaConfig.set(\"api.baseUri\", \"http://test\");\n\n\nWe add the “api.BaseUri” key since that’s the value that the ApiWrapper reads from the configuration object. We can now flesh out the remainder of our test:\n\nimport {App} from '../../src/app';\nimport {ApiWrapper} from '../../src/api';\nimport {HttpClientMock} from './utils/mock-fetch';\nimport {AureliaConfiguration} from 'aurelia-configuration';\n\ndescribe('the app', () =&amp;gt; {\n  it('says hello', async done =&amp;gt; {\n    // arrange\n    let aureliaConfig = new AureliaConfiguration();\n    aureliaConfig.set(\"api.baseUri\", \"http://test\");\n\n    const client = new HttpClientMock();\n    client.setup({\n      data: [\"testValue1\", \"testValue2\", \"testValue3\"],\n      headers: {\n        'Content-Type': \"application/json\",\n      },\n      url: \"/values\",\n    });\n    const api = new ApiWrapper(client, aureliaConfig);\n\n    // act\n    let sut: App;\n    try {\n      sut = new App(api);\n    } catch (e) {\n      console.error(e);\n    }\n\n    // assert\n    setTimeout(() =&amp;gt; {\n      expect(sut.message).toBe('Hello World!');\n      expect(sut.values.length).toBe(3);\n      expect(sut.values).toContain(\"testValue1\");\n      expect(sut.values).toContain(\"testValue2\");\n      expect(sut.values).toContain(\"testValue3\");\n      done();\n    }, 10);\n  });\n});\n\n\nNotes:\n\n\n  Lines 13-19: configure the mock fetch response (we’ll see the rest of the mock HttpClient class shortly)\n  Line 20: instantiate a new ApiWrapper\n  Lines 23-28: call the App constructor\n  Lines 31-38: we wrap the asserts in a timeout since the App constructor calls an async method (perhaps there’s a better way to do this?)\n\n\nLet’s finish off the test code by looking at the mock-fetch class:\n\nimport { HttpClient } from 'aurelia-fetch-client';\nexport class HttpClientMock extends HttpClient {\n}\n\nexport interface IMethodConfig {\n    url: string;\n    method?: string;\n    status?: number;\n    statusText?: string;\n    headers?: {};\n    data?: {};\n};\n\nexport class HttpClientMock extends HttpClient {\n    private config: IMethodConfig[] = [];\n\n    public setup(config: IMethodConfig) {\n        this.config.push(config);\n    }\n\n    public async fetch(input: Request | string, init?: RequestInit) {\n        let url: string;\n        if (typeof input === \"string\") {\n            url = input;\n        } else {\n            url = input.url;\n        }\n\n        // find the matching setup method\n        let methodConfig: IMethodConfig;\n        methodConfig = this.config.find(c =&amp;gt; c.url === url);\n        if (!methodConfig) {\n            console.error(`---MockFetch: No such method setup: ${url}`);\n            return Promise.reject(new Response(null,\n                {\n                    status: 404,\n                    statusText: `---MockFetch: No such method setup: ${url}`,\n                }));\n        }\n\n        // set up headers\n        let responseInit: ResponseInit = {\n            headers: methodConfig.headers || {},\n            status: methodConfig.status || 200,\n            statusText: methodConfig.statusText || \"\",\n        };\n\n        // get a unified request object\n        let request: Request;\n        if (Request.prototype.isPrototypeOf(input)) {\n            request = (&lt;request&gt; input);\n        } else {\n            request = new Request(input, responseInit || {});\n        }\n\n        // create a response object\n        let response: Response;\n        const data = JSON.stringify(methodConfig.data);\n        response = new Response(data, responseInit);\n\n        // resolve or reject accordingly\n        return response.status &amp;gt;= 200 &amp;amp;&amp;amp; response.status &amp;lt; 300 ?\n            Promise.resolve(response) : Promise.reject(response);\n    }\n}\n&lt;/request&gt;\n\n\nI won’t go through the whole class, but essentially you configure a mapping of routes to responses so that when the mock object is called it can return predictable data.\n\nWith those changes in place, we can run the tests using “au test”. This launches Chrome and runs the test. The Aurelia project did the heavy lifting to configure paths for the test runner (Karma) so that the tests “just work”.\n\nGoing Headless and Adding Reports and Coverage\n\nNow that we can run the tests in Chrome with results splashed to the console, we should consider how these tests would run in a build. Firstly, we want to produce a report file of some sort so that the build can save the results. We also want to add coverage. Finally, we want to run headless so that we can run this on an agent that doesn’t need access to a desktop to launch a browser!\n\nWe’ll need to add some development-time node packages to accomplish these changes:\n\nyarn add karma-phantomjs-launcher karma-coverage karma-tfs gulp-replace –dev\n\nWith those package in place, we can change the karma.conf.js file to use phantomjs (a headless browser) instead of Chrome. We’re also going to add in the test result reporter, coverage reporter and a coverage remapper. The coverage will report coverage on the JavaScript files, but we would ideally want coverage on the TypeScript files – that’s what the coverage remapper will do for us.\n\nHere’s the new karma.conf.js:\n\n'use strict';\nconst path = require('path');\nconst project = require('./aurelia_project/aurelia.json');\nconst tsconfig = require('./tsconfig.json');\n\nlet testSrc = [\n  { pattern: project.unitTestRunner.source, included: false },\n  'test/aurelia-karma.js'\n];\n\nlet output = project.platform.output;\nlet appSrc = project.build.bundles.map(x =&amp;gt; path.join(output, x.name));\nlet entryIndex = appSrc.indexOf(path.join(output, project.build.loader.configTarget));\nlet entryBundle = appSrc.splice(entryIndex, 1)[0];\nlet files = [entryBundle].concat(testSrc).concat(appSrc);\n\nmodule.exports = function(config) {\n  config.set({\n    basePath: '',\n    frameworks: [project.testFramework.id],\n    files: files,\n    exclude: [],\n    preprocessors: {\n      [project.unitTestRunner.source]: [project.transpiler.id],\n      'wwwroot/scripts/app-bundle.js': ['coverage']\n    },\n    typescriptPreprocessor: {\n      typescript: require('typescript'),\n      options: tsconfig.compilerOptions\n    },\n    reporters: ['progress', 'tfs', 'coverage', 'karma-remap-istanbul'],\n    port: 9876,\n    colors: true,\n    logLevel: config.LOG_INFO,\n    autoWatch: true,\n    browsers: ['PhantomJS'],\n    singleRun: false,\n    // client.args must be a array of string.\n    // Leave 'aurelia-root', project.paths.root in this order so we can find\n    // the root of the aurelia project.\n    client: {\n      args: ['aurelia-root', project.paths.root]\n    },\n\n    phantomjsLauncher: {\n      // Have phantomjs exit if a ResourceError is encountered (useful if karma exits without killing phantom)\n      exitOnResourceError: true\n    },\n\n    coverageReporter: {\n      dir: 'reports',\n      reporters: [\n        { type: 'json', subdir: 'coverage', file: 'coverage-final.json' },\n      ]\n    },\n\n    remapIstanbulReporter: {\n      src: 'reports/coverage/coverage-final.json',\n      reports: {\n        cobertura: 'reports/coverage/cobertura.xml',\n        html: 'reports/coverage/html'\n      }\n    }\n  });\n};\n\n\nNotes:\n\n\n  Line 25: add a preprocessor to instrument the code that we’re going to execute\n  Line 31: we add reporters to produce results files (tfs), coverage and remapping\n  Lines 45-48: we configure a catch-all to close phantomjs if something fails\n  Lines 50-55: we configure the coverage to output a Json coverage file\n  Lines 57-63: we configure the remapper so that we get TypeScript coverage results\n\n\nOne gotcha I had that I couldn’t find a work-around for: the html files that are generated showing which lines of code were hit is generated with incorrect relative paths and the src folder (with detailed coverage) generated outside the html report folder. Eventually, I decided that a simple replace and file move was all I needed, so I modified the test.ts task in the aurelia-project\\tasks folder:\n\n// hack to fix the relative paths in the generated mapped html report\nlet fixPaths = done =&amp;gt; {\n  let repRoot = path.join(__dirname, '../../reports/');\n  let repPaths = [\n    path.join(repRoot, 'src/**/*.html'),\n    path.join(repRoot, 'src/*.html'),\n  ];\n  return gulp.src(repPaths, { base: repRoot })\n        .pipe(replace(/(..\\/..\\/..\\/)(\\w)/gi, '../coverage/html/$2'))\n        .pipe(gulp.dest(path.join(repRoot)));\n};\n\nlet unit;\n\nif (CLIOptions.hasFlag('watch')) {\n  unit = gulp.series(\n    build,\n    gulp.parallel(\n      watch(build, onChange),\n      karma,\n      fixPaths\n    )\n  );\n} else {\n  unit = gulp.series(\n    build,\n    karma,\n    fixPaths\n  );\n}\n\n\nI add new tasks called “updateIndex” and “copySrc” that fix up the paths for me. Perhaps there’s a config setting for the remapper that will render this obsolete, but this was the best I could come up with.\n\nNow when you run “au test” you get a result file and coverage results for the TypeScript code all in the html folder with the correct paths. If you’re following along in the repo code, these changes are on the master branch (this is the final state of the demo code).\n\nAutomated Build and Test\n\nWe now have all the pieces in place to do a build. The build is fairly straightforward once you work out how to invoke the Arelia cli. Starting with a .NET Core Web App template, here is the definition I ended up with:\n\n\n\n\nHere are the task settings:\n\n\n  .NET Core Restore – use defaults\n  .NET Core Build\n  Change “Arguments” to –configuration $(BuildConfiguration) –version-suffix $(Build.BuildNumber)\n  The extra bit added is the version-suffix arg which produces binaries with the same version as the build number\n  npm install\n  Change “working folder” to frontend (this is the directory of the Aurelia project).\\node_modules\\aurelia-cli\\bin\\aurelia-cli.js test\n  Run command\n  Set “Tool” to node\n  Set “Arguments” to .\\node_modules\\aurelia-cli\\bin\\aurelia-cli.js test\n  Expand “Advanced” and set “Working folder” to frontend\n  This runs the tests and produces the test results and coverage results files\n  Run command\n  Set “Tool” to node\n  Set “Arguments” to .\\node_modules\\aurelia-cli\\bin\\aurelia-cli.js build –env prod\n  Expand “Advanced” and set “Working folder” to frontend\n  This does transpilation, minification and bundling so that we’re ready to deploy\n  Publish Test Results\n  Set “Test Result Format” to VSTest\n  Set “Test results files” to frontend/testresults/TEST*.xml\n  Set “Test run title” to Aurelia\n  Publish code coverage Results\n  Set “Code Coverage Tool” to Cobertura\n  Set “Summary File” to $(Build.SourcesDirectory)/frontend/reports/coverage/cobertura.xml\n  Set “Report Directory” to $(System.DefaultWorkingDirectory)/frontend/reports/coverage/html\n  .NET Core Publish\n  Make sure “Publish Web Projects” is checked – this is why I added a dummy readme file into the wwwroot folder of the API app, otherwise it’s not published as a web project\n  Set “Arguments” to –configuration $(BuildConfiguration) –output $(build.artifactstagingdirectory) –version-suffix $(Build.BuildNumber)\n  Make sure “Zip Published Projects” is checked\n  On the Options Tab\n  Set the build number format to 1.0.0$(rev:.r) to give the build number a 1.0.0.x format\n  Set the default agent queue to Hosted VS2017 (or you can select a private build agent with VS 2017 installed)\n\n\nNow when I run the build, I get test and coverage results in the summary:\n\n\n\n\nThe coverage files are there if you click the Code Coverage results tab, but there’s a problem with the css.\n\n\n\n\nThe &lt;link&gt; elements are stripped out of the html pages when the iFrame for the coverage results shows – I’m working with the product team to find a workaround for this. If you download the results from the Summary page and unzip them, you get the correct rendering.\n\nI can also see both web projects ready for deployment in the Artifacts tab:\n\n\n\n\nWe’re ready for a release!\n\nThe Release Definition\n\nI won’t put the whole release to Azure here – the key point to remember is the configuration. We’ve done the work to move the configuration into the config.json file for this very reason.\n\nOnce you’ve set up an Azure endpoint, you can add in an “Azure App Services Deploy” task. Select the subscription and app service and then change the “Package or folder” from “$(System.DefaultWorkingDirectory)/**/*.zip” to “$(System.DefaultWorkingDirectory)/drop/frontend.zip” (or API.zip) to deploy the corresponding site. To handle the configuration, you simply add “wwwroot/config/config.json” to the “JSON variable substitution”.\n\n\n\n\nNow we can define an environment variable for the substitution. Just add one with the full “JSON path” for the variable. In our case, we want “api.baseUri” to be the name and then put in whatever the corresponding environment value is:\n\n\n\n\nWe can repeat this for other variables if we need more.\n\nConclusion\n\nI really love the Aurelia framework – and with the solid Aurelia cli, development is a really good experience. Add to that simple build and release management to Azure using VSTS, and you can get a complete site skeleton with full CI/CD in half a day. And that means you’re delivering better software, faster – always a good thing!\n\nHappy Aurelia-ing!\n",
      "categories": [],
      "tags": ["development","alm"],
      
      "collection": "posts",
      "url": "/aurelia-azure-and-vsts/"
    },{
      
      "title": "DevOps with Kubernetes and VSTS: Part 2",
      "date": "2017-07-04 11:34:06 +0000",
      
      "content": "In Part 1 I looked at how to develop multi-container apps using Kubernetes (k8s) - and more specifically, minikube, which is a full k8s environment that runs a single node on a VM on your laptop. In that post I walk through cloning this repo (be sure to look at the docker branch) which contains two containers: a DotNet Core API container and a frontend SPA (Aurelia) container (also hosted as static files in a DotNet Core app). I show how to build the containers locally and get them running in minikube, taking advantage of ConfigMaps to handle configuration.\n\nIn this post, I will show you how to take the local development into CI/CD and walk through creating an automated build/release pipeline using VSTS. We’ll create an Azure Container Registry and Azure Container Services using k8s as the orchestration mechanism.\n\nI do recommend watching Nigel Poulton’s excellent Getting Started with Kubernetes PluralSight course and reading this post by Atul Malaviya from Microsoft. Nigel’s course was an excellent primer into Kubernetes and Atul’s post was helpful to see how VSTS and k8s interact - but neither course nor post quite covered a whole pipeline. How do you update your images in a CI/CD pipeline was a question not answered to my satisfaction. So after some experimentation, I am writing this post!\n\nCreating a k8s Environment using Azure Container Services\n\nYou can run k8s on-premises, or in AWS or Google Cloud. However, I think Azure Container Services makes spinning up an k8s cluster really straightforward. However, the pipeline I walk through in this post is cloud-host agnostic - it will work against any k8s cluster. We’ll also set up a private Container Registry in Azure, though once again, you can use any container registry you choose.\n\nTo spin up a k8s cluster you can use the portal, but the Azure CLI makes it a snap and you get to save the keys you’ll need to connect, so I’ll use that mechanism. I’ll also use Bash for Windows with kubectl, but any platform running kubectl and the Azure CLI will do.\n\nHere are the commands:\n\n# set some variables\nexport RG=\"cd-k8s\"\nexport clusterName=\"cdk8s\"\nexport location=\"westus\"\n# create a folder for the cluster ssh-keys\nmkdir cdk8s\n\n# login and create a resource group\naz login\naz group create --location $location --name $RG\n\n# create an ACS k8s cluster\naz acs create --orchestrator-type=kubernetes --resource-group $RG --name=$ClusterName --dns-prefix=$ClusterName --generate-ssh-keys --ssh-key-value ~/cdk8s/id_rsa.pub --location $location --agent-vm-size Standard_DS1_v2 --agent-count 2\n\n# create an Azure Container Registry\naz acr create --resource-group $RG --name $ClusterName --location $location --sku Basic --admin-enabled\n\n# configure kubectl\naz acs kubernetes get-credentials --name $ClusterName --resource-group $RG --file ~/cdk8s/kubeconfig --ssh-key-file ~/cdk8s/id_rsa\nexport KUBECONFIG=\"~/cdk8s/kubeconfig\"\n\n# test connection\nkubectl get nodes\nNAME STATUS AGE VERSION\nk8s-agent-96607ff6-0 Ready 17m v1.6.6\nk8s-agent-96607ff6-1 Ready 17m v1.6.6\nk8s-master-96607ff6-0 Ready,SchedulingDisabled 17m v1.6.6\n\n\nNotes:\n\n\n  Lines 2-4: create some variables\n  Line 6: create a folder for the ssh-keys and kubeconfig\n  Line 9: login to Azure (this prompts you to open a browser with the device login - if you don’t have an Azure subscription create a free one now!)\n  Line 10: create a resource group to house all the resources we’re going to create\n  Line 13: create a k8s cluster using the resource group we just created and the name we pass in; generate ssh-keys and place them in the specified folder; we want 2 agents (nodes) with the specified VM size\n  Line 16: create an Azure Container registry in the same resource group with admin access enabled\n  Line 19: get the credentials necessary to connect to the cluster using kubectl; use the supplied ssh-key and save the creds to the specified kubeconfig file\n  Line 20: tell kubectl to use this config rather than the default config (which may have other k8s clusters or minikube config)\n  Line 23: test that we can connect to the cluster\n  Lines 24-27: we are indeed connecting successfully!\n\n\nIf you open a browser and navigate to the Azure portal and then open your resource group, you’ll see how much stuff got created by the few preceding simple commands:\n\n\n\n\nDon’t worry - you’ll not need to manage these resources yourself. Azure and the k8s cluster manage them for you!\n\nNamespaces\n\nBefore we actually create the build and release for our container apps, let’s consider the promotion model. Typically there’s Dev-&gt;UAT-&gt;Prod or something similar. In the case of k8s, minikube is the local dev environment - and that’s great since this is a full k8s cluster on your laptop - so you get to run your code locally including using k8s “meta-constructs” such as configMaps. So what about UAT and Prod? You could spin up separate clusters, but that could end up being expensive. You can also share the prod cluster resources by leveraging namespaces. Namespaces in k8s can be security boundaries, but they can also be isolation boundaries. I can deploy new versions of my app to a dev namespace - and even though that namespace shares the resources of the prod namespace, it’s completely invisible, including its own IPs etc. Of course I shouldn’t load test in this configuration since loading the dev namespace is going to potentially steal resources from prod apps. This is conceptually similar to deployment slots in Azure App Services - they can be used to test apps lightly before promoting to prod.\n\nWhen you spin up a k8s cluster, besides kube-system and kube-public namespaces (which house the k8s pods) there is a “default” namespace. If you don’t specify otherwise, any services, deploymens or pods you create will go to this namespace. However, let’s create two additional namespaces: dev and prod. Here’s the yml:\n\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev\n---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: prod\n\n\nThis file contains the definitions for both namespaces. Run the apply command to create the namespaces. Once completed, you can list all the namespaces in the cluster:\n\nkubectl apply -f namespaces.yml\nnamespace \"dev\" created\nnamespace \"prod\" created\n\nkubectl get namespaces\nNAME STATUS AGE\ndefault Active 27m\ndev Active 20s\nkube-public Active 27m\nkube-system Active 27m\nprod Active 20s\n\n\nConfiguring the Container Registry Secret\n\nOne more piece of setup before we get to the code: when the k8s cluster is pulling images to run, we’re going to want it to pull from the Container Registry we just created. Access to this registry is secured since this is a private registry. So we need to configure a registry secret that we can just reference in our deployment yml files. Here are the commands:\n\naz acr credential show --name $ClusterName --output table\nUSERNAME PASSWORD PASSWORD2\n---------- -------------------------------- --------------------------------\ncdk8s some-long-key-1 some-long-key-2\n\nkubectl create secret docker-registry regsecret --docker-server=$ClusterName.azurecr.io --docker-username=$ClusterName --docker-password=&amp;lt;some-long-key-1&amp;gt; --docker-email=admin@azurecr.io\nsecret \"regsecret\" created\n\n\nThe first command uses az to get the keys for the admin user (the admin username is the same as the name of the Container registry: so I created cdk8s.azurecr.io and so the admin username is cdk8s). Pass in one of the keys (it doesn’t really matter which one) as the password. The email address is not used, so this can be anything. We now have a registry secret called “regsecret” that we can refer to when deploying to the k8s cluster. K8s will use this secret to authenticate to the registry.\n\nConfigure VSTS Endpoints\n\nWe now have the k8s cluster and container registry configured. Let’s add these endpoints to VSTS so that we can push containers to the registry during a build and perform commands against the k8s cluster during a release. The endpoints allow us to abstract away authentication so that we don’t need to store credentials in our release definitions directly. You can also restrict who can view/consume the endpoints using endpoint roles.\n\nOpen VSTS and navigate to a Team Project (or just create a new one). Go to the team project and click the gear icon to navigate to the settings hub for that Team Project. Then click Services. Click “+ New Services” and create a new Docker Registry endpoint. Use the same credentials you used to create the registry secret in k8s using kubectl:\n\n\n\n\nNext create a k8s endpoint. For the url, it will be https://$ClusterName.$location.cloudapp.azure.com (where clustername and location are the variables we used earlier to create the cluster). You’ll need to copy the entire contents of the ~/cdk8s/kubeconfig file (or whatever you called it) that was output when you ran the az acs kubernetes get-credential command into the credentials textbox:\n\n\n\n\nWe now have two endpoints that we can use in the build/release definitions:\n\n\n\nThe Build\n\nWe can now create a build that compiles/tests our code, creates docker images and pushes the images to the Container Registry, tagging them appropriately. Click on Build &amp; Release and then click on Builds to open the build hub. Create a new build definition. Select the ASP.NET Core template and click apply. Here are the settings we’ll need:\n\n\n  Tasks-&gt;Process: Set the name to something like k8s-demo-CI and select the “Hosted Linux Preview” queue\n  Options: change the build number format to “1.0.0$(rev:.r)” so that the builds have a 1.0.0.x format\n  Tasks-&gt;Get Sources: Select the Github repo, authorizing via OAuth or PAT. Select the AzureAureliaDemo and set the default branch to docker. You may have to fork the repo (or just import it into VSTS) if you’re following along.\n  Tasks-&gt;DotNet Restore - leave as-is\n  Tasks-&gt;DotNet Build - add “–version-suffix $(Build.BuildNumber)” to the build arguments to match the assembly version to the build number\n  Tasks-&gt;DotNet Test - disable this task since there are no DotNet tests in this solution (you can of course re-enable this task when you have tests)\n  Tasks-&gt;Add an “npm” task. Set the working folder to “frontend” and make sure the command is “install”\n  Tasks-&gt;Add a “Command line” task. Set the tool to “node”, the arguments to “node_modules/aurelia-cli/bin/aurelia-cli.js test” and the working folder to “frontend”. This will run Aurelia tests.\n  Tasks-&gt;Add a “Publish test results” task. Set “Test Results files” to “test*.xml” and “Search Folder” to “$(Build.SourcesDirectory)/frontend/testresults”. This publishes the Aurelia test results.\n  Tasks-&gt;Add a “Publish code coverage” task. Set “Coverage Tool” to “Cobertura”, “Summary File” to “$(Build.SourcesDirectory)/frontend/reports/coverage/cobertura.xml” and “Report Directory” to “$(Build.SourcesDirectory)/frontend/reports/coverage/html”. This publishes the Aurelia test coverage results.\n  Tasks-&gt;Add a “Command line” task. Set the tool to “node”, the arguments to “node_modules/aurelia-cli/bin/aurelia-cli.js build –env prod” and the working folder to “frontend”. This transpiles, processes and packs the Aurelia SPA app.\n  Tasks-&gt;DotNet Publish. Change the Arguments to “-c $(BuildConfiguration) -o publish” and uncheck “Zip Published Projects”\n  Tasks-&gt;Add a “Docker Compose” task. Set the “Container Registry Type” to “Azure Container Registry” and set your Azure subscription and container registry to the registry we created an endpoint for earlier. Set “Additional Docker Compose Files” to “docker-compose.vsts.yml”, the action to “Build service images” and “Additional Image Tags” to “$(Build.BuildNumber)” so that the build number is used as the tag for the images.\n  Clone the “Docker Compose” task. Rename it to “Push service images” and change the action to “Push service images”. Check the “Include Latest Tag” checkbox.\n  Tasks-&gt;Publish Artifact. Set both “Path to Publish” and “Artifact Name” to k8s. This publishes the k8s yml files so that they are available in the release.\n\n\nThe final list of tasks looks something like this:\n\n\n\n\nYou can now Save and Queue the build. When the build is complete, you’ll see the test/coverage information in the summary.\n\n\n\n\nYou can also take a look at your container registry to see the newly pushed service images, tagged with the build number.\n\n\n\nThe Release\n\nWe can now configure a release that will create/update the services we need. For that we’re going to need to manage configuration. Now we could just hard-code the configuration, but that could mean sensitive data (like passwords) would end up in source control. I prefer to tokenize any configuration and have Release Management keep the sensitive data outside of source control. VSTS Release Management allows you to create secrets for individual environments or releases or you can create them in reusable Variable Groups. You can also now easily integrate with Azure Key Vault.\n\nTo replace the tokens with environment-specific values, we’re going to need a task that can do token substitution. Fortunately, I’ve got a (cross-platform) ReplaceTokens task in Colin’s ALM Corner Build &amp; Release Tasks extension on the VSTS Marketplace. Click on the link to navigate to the page and click install to install the extension onto your account.\n\nFrom the build summary page, scroll down on the right hand side to the “Deployments” section and click the “Create release” link. You can also click on Releases and create a new definition from there. Start from an Empty template and select your team project and the build that you just completed as the source build. Check the “Continuous Deployment” checkbox to automatically trigger a release for every good build.\n\nRename the definition to “k8s” or something descriptive. On the “General” tab change the release number format to “$(Build.BuildNumber)-$(rev:r)” so that you can easily see the build number in the name of the release. Back on Environments, rename Environment 1 to “dev”. Click on the “Run on Agent” link and make sure the Deployment queue is “Hosted Linux Preview”. Add the following tasks:\n\n\n  Replace Tokens\n  Source Path: browse to the k8s folder\n  Target File Pattern: “*-release.yml”. This performs token replacement on any yml file with a name that ends in “-release.” There’s 3: back- and frontend service/deployment files and the frontend config file. This task finds the tokens in the file (with pre- and postfix __) and looks for variables with the same name. Each variable is replaced with its corresponding value. We’ll create the variables shortly.\n  Kubernetes Task 1 (apply frontend config)\n  Set the k8s connection to the endpoint you created earlier. Also set the connection details for the Azure Container Registry. This applies to all the Kubernetes tasks. Set the Command to “apply”, check the “Use Configuration Files” option and set the file to the k8s/app-demo-frontend-config-release.yml file using the file picker. Add “–namespace $(namespace)” to the arguments textbox.\n\n  Kubernetes Task 2 (apply backend service/deployment definition)\n  Set the same connection details for the k8s service and Azure Container Registry. This time, set “Secret Name” to “regsecret” (this is the name of the secret we created when setting up the k8s cluster, and is also the name we refer to for the imagePullSecret in the Deployment definitions). Check the “Force update secret” setting. This ensures that the secret value in k8s matches the key from Azure. You could also skip this option since we created the key manually.\n  Set the Command to “apply”, check the “Use Configuration Files” option and set the file to the k8s/app-demo-backend-release.yml file using the file picker. Add “–namespace $(namespace)” to the arguments textbox.\n\n  Kubernetes Task 3 (apply frontend service/deployment definition)\n  This is the same as the previous task except that the filename is k8s/app-demo-frontend-release.yml.\n  Kubernetes Task 4 (update backend image)\n  Set the same connection details for the k8s service and Azure Container Registry. No secret required here. Set the Command to “set” and specify Arguments as “image deployment/demo-backend-deployment backend=$(ContainerRegistry)/api:$(Build.BuildNumber) –record –namespace=$(namespace)”.\n  This updates the version (tag) of the container image to use. K8s will do a rolling update that brings new containers online and takes the old containers offline in such a manner that the service is still up throughout the bleed over.\n\n  Kubernetes Task 5 (update the frontend image)\n  Same as the previous task except the Arguments are “image deployment/demo-frontend-deployment frontend=$(ContainerRegistry)/frontend:$(Build.BuildNumber) –record –namespace=$(namespace)”\n  Click on the “…” button on the “dev” card and click Configure Variables. Set the following values:\n  BackendServicePort: 30081\n  FrontendServicePort: 30080\n  ContainerRegistry: &lt;your container reg&gt;.azurecr.io\n  namespace: $(Release.EnvironmentName)\n  AspNetCoreEnvironment: development\n  baseUri: http://$(BackendServiceIP)/api\n  BackendServiceIP: 10.0.0.1\n\n\n\nThis sets environment-specific values for all the variables in the yml files. The Replace Tokens task takes care of injecting into the files for us. Let’s take a quick look at one of the tokenized files (tokenized lines are highlighted):\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: demo-frontend-service\n  labels:\n    app: demo\nspec:\n  selector:\n    app: demo\n    tier: frontend\n  ports:\n    - protocol: TCP\n      port: 80\n      nodePort: __FrontendServicePort__\n  type: LoadBalancer\n---\napiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: demo-frontend-deployment\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: demo\n        tier: frontend\n    spec:\n      containers:\n        - name: frontend\n          image: __ContainerRegistry__ /frontend\n          ports:\n          - containerPort: 80\n          env:\n          - name: \"ASPNETCORE_ENVIRONMENT\"\n            value: \" __AspNetCoreEnvironment__\"\n          volumeMounts:\n            - name: config-volume\n              mountPath: /app/wwwroot/config/\n          imagePullPolicy: Always\n      volumes:\n        - name: config-volume\n          configMap:\n            name: demo-app-frontend-config\n      imagePullSecrets:\n        - name: regsecret\n\n\nA note on the value for BackendServiceIP: we use 10.0.0.1 as a temporary placeholder, since Azure will create an IP for this service when k8s spins up the backend service (you’ll see a public IP address in the resource group in the Azure portal). We will have to run this once to create the services and then update this to the real IP address so that the frontend service works correctly. We also use $(Release.EnvironmentName) as the value for namespace - so “dev” (and later “prod”) need to match the namespaces we created, including casing.\n\nIf the service/deployment and config don’t change, then the first 3 k8s tasks are essentially no-ops. Only the “set” commands are actually going to do anything. But this is great - since the service/deployment and config files can be applied idempotently! They change when they have to and don’t mess anything up when they don’t change - perfect for repeatable releases!\n\nSave the definition. Click “+ Release” to create a new release. Click on the release number (it will be something like 1.0.0.1-1) to open the release. Click on logs to see the logs.\n\n\n\n\nOnce the release has completed, you can see the deployment in the Kubernetes dashboard. To open the dashboard, execute the following command:\n\naz acs kubernetes browse -n $ClusterName -g $RG --ssh-key-file ~/cdk8s/id_rsa\n\nProxy running on 127.0.0.1:8001/ui\nPress CTRL+C to close the tunnel...\nStarting to serve on 127.0.0.1:8001\n\n\nThe last argument is the path to the SSH key file that got generated when we created the cluster - adjust your path accordingly. You can now open a browser to http://localhost:8001/ui. Change the namespace dropdown to “dev” and click on Deployments. You should see 2 successful deployments - each showing 2 healthy pods. You can also see the images that are running in the deployments - note the build number as the tag!\n\n\n\n\nTo see the services, click on Services.\n\n\n\n\nNow we have the IP address of the backend service, so we can update the variable in the release. We can then queue a new release - this time, the frontend configuration is updated with the correct IP address for the backend service (in this case 23.99.58.48). We can then browse to the frontend service IP address and see our service is now running!\n\n\n\nCreating Prod\n\nNow that we are sure that the dev environment is working, we can go back to the release and clone “dev” to “prod”. Make sure you specify a post-approval on dev (or a pre-approval on prod) so that there’s a checkpoint between the two environments.\n\n\n\n\nWe can then just change the node ports, AspNetCoreEnvironment and BackendServiceIP variables and we’re good to go! Of course we need to deploy once to the prod namespace before we see the k8s/Azure assigned IP address for the prod backend and then re-run the release to update the config.\n\n\n\n\nWe could also remove the nodePort from the definitions altogether and let k8s decide on a node port - but if it’s explicit then we know what port the service is going to run on within the cluster (not externally).\n\nI did get irritated having to specify “–namespace” for each command - so irritated, in fact, that I’ve created a Pull Request in the vsts-tasks Github repo to expose namespace as an optional UI element!\n\nEnd to End\n\nNow that we have the dev and prod environments set up in a CI/CD pipeline, we can make a change to the code. I’ll change the text below the version to “K8s demo” and commit the change. This triggers the build, creating a newer container image and running tests, which in turn triggers the release to dev. Now I can see the change in dev (which is on 1.0.0.3 or some newer version than 1.0.0.1), while prod is still on version 1.0.0.1.\n\n\n\n\nApprove dev in Release Management and prod kicks off - and a few seconds later prod is now also on 1.0.0.3.\n\nI’ve exported the json definitions for both the build and the release into this folder - you can attempt to import them (I’m not sure if that will work) but you can refer to them in any case.\n\nConclusion\n\nk8s shows great promise as a solid container orchestration mechanism. The yml infrastructure-as-code is great to work with and easy to version control. The deployment mechanism means you can have very minimal (if any) downtime when deploying and having access to configMaps and secrets makes the entire process secure. Using the Azure CLI you can create a k8s cluster and Azure Container registry with a couple simple commands. The VSTS integration through the k8s tasks makes setting up CI/CD relatively easy - all in all it’s a great development workflow. Throw in minikube as I described in Part 1 of this series, which gives you a full k8s cluster for local development on your laptop, and you have a great dev/CI/CD workflow.\n\nOf course a CI/CD pipeline doesn’t battle test the actual applications in production! I would love to hear your experiences running k8s in production - sound out in the comments if you have some experience of running apps in a k8s cluster in prod!\n\nHappy k8sing!\n",
      "categories": [],
      "tags": ["docker","devops"],
      
      "collection": "posts",
      "url": "/devops-with-kubernetes-and-vsts-part-2/"
    },{
      
      "title": "DevOps with Kubernetes and VSTS: Part 1",
      "date": "2017-07-04 11:37:20 +0000",
      
      "content": "If you’ve read my blog before, you’ll probably know that I am huge fan of Docker and containers. When was the last time you installed software onto bare metal? Other than your laptop, chances are you haven’t for a long time. Virtualization has transformed how we think about resources in the datacenter, greatly increasing the density and utilization of resources. The next evolution in density is containers - just what VMs are to physical servers, containers are to VMs. Soon, almost no-one will work against VMs anymore - we’ll all be in containers. At least, that’s the potential.\n\nHowever, as cool as containers are for packaging up apps, there’s still a lot of uncertainty about how to actually run containers in production. Creating a single container is a cool and satisfying experience for a developer, but how do you run a cluster and scale containers? How do you monitor your containers? How do you manage faults? This is where we enter the world of container orchestration.\n\nThis post will cover the local development experience with Kubernetes and minikube. Part 2 covers the CI/CD pipeline to a Kubernetes cluster in Azure.\n\nOrchestrator Wars\n\nThere are three popular container orchestration systems - Mesos, Kubernetes and Docker Swarm. I don’t want to go into a debate on which one you should go with (yet) - but they’re all conceptually similar.  They all work off configuration as code for spinning up lots of containers across lots of nodes. Kubernetes does have a couple features that I think are killer for DevOps: ConfigMaps, Secrets and namespaces.\n\nIn short, namespaces allow you to segregate logical environments in the same cluster - the canonical example is a DEV namespace where you can run small copies of your PROD environment for testing. You could also use namespaces for different security contexts or multi-tenancy. ConfigMaps (and Secrets) allow you to store configuration outside of your containers - which means you can have the same image running in various contexts without having to bake environment-specific code into the images themselves.\n\nKubernetes Workflow and Pipeline\n\nIn this post, I want to look at how you would develop with Kubernetes in mind. We’ll start by looking at the developer workflow and then move on to how the DevOps pipeline looks in the next post. Fortunately, having MiniKube (a one-node Kubernetes cluster that runs in a VM) means that you can develop against a fully features cluster on your laptop! That means you can take advantage of cluster features (like ConfigMaps) without having to be connected to a production cluster.\n\nSo what would the developer workflow look like? Something like this:\n\n\n  Develop code\n  Build image from Dockerfile or docker-compose files\n  Run service in MiniKube (which spins up containers from the images you just built)\n\n\nIt turns out that Visual Studio 2017 (and/or VS Code), Docker and MiniKube make this a really smooth experience.\n\nEventually you’re going to move to the DevOps pipeline - starting with a build. The build will take the source files and Dockerfiles and build images and push them to a private container registry. Then you’ll want to push configuration to a Kubernetes cluster to actually run/deploy the new images. It turns out that using Azure and VSTS makes this DevOps pipeline smooth as butter! That will be the subject of Part 2 - for now, we’ll concentrate on the developer workflow.\n\nSetting up the Developer Environment\n\nI’m going to focus on a Windows setup, but the same setup would apply to Mac or Linux environments as well. To set up a local development environment, you need to install the following:\n\n\n  Docker\n  Kubectl\n  MiniKube\n\n\nYou can follow the links and run the installs. I had a bit of trouble with MiniKube on HyperV - by default, MiniKube start (the command that creates the MiniKube VM) just grabs the first HyperV virtual network it finds. I had a couple, and the one that MiniKube grabbed was an internal network, which caused MiniKube to fail. I created a new virtual network called minikube in the HyperV console and made sure it was an external network. I then used the following command to create the MiniKube VM:\n\nc:\ncd \\\nminikube start --vm-driver hyperv --hyperv-virtual-switch minikube\n\n\nNote: I had to cd to c:\\ - if I did not, MiniKube failed to create the VM.\n\nMy external network if connected to my WiFi. That means when I join a new network, my minikube VM gets a new IP. Instead of having to update the kubeconfig each time, I just added a hosts entry in my hosts file (c:\\windows\\system32\\drivers\\etc\\hosts on Windows) using “&lt;IP&gt; kubernetes”, where IP is the IP address of the minikube VM - obtained by running “minikube ip”. To update the kubeconfig, run this command:\n\n\nkubectl config set-cluster minikube --server=\n\n\nhttps://kubernetes:8443\n\n\n --certificate-authority=c:/users/&lt;user&gt;/.minikube/ca.crt\n\n\nwhere &lt;user&gt; is your username, so that the cert points to the ca.crt file generated into your .minikube directory.\n\nNow if you join a new network, you just update the IP in the hosts file and your kubectl commands will still work. The certificate is generated for a hostname “kubernetes” so you have to use that name.\n\nIf everything is working, then you should get a neat response to “kubectl get nodes”:\n\nPS:\\&amp;gt; kubectl get nodes\nNAME STATUS AGE VERSION\nminikube Ready 11m v1.6.4\n\n\nTo open the Kubernetes UI, just enter “minikube dashboard” and a browser will launch:\n\n\n\n\nFinally, to “re-use” the minikube docker context, run the following command:\n\n\n&amp; minikube docker-env | Invoke-Expression\n\n\nNow you are sharing the minikube docker socket. Running “docker ps” will return a few running containers - these are the underlying Kubernetes system containers. It also means you can create images here that the minikube cluster can run.\n\nYou now have a 1-node cluster, ready for development!\n\nGet Some Code\n\nI recently blogged about Aurelia development with Azure and VSTS. Since I already had a couple of .NET Core sites, I thought I would see if I could get them running in a Kubernetes cluster. Clone this repo and checkout the docker branch. I’ve added some files to the repo to support both building the Docker images as well as specifying Kubernetes configuration. Let’s take a look.\n\nThe docker-compose.yml file specifies a composite application made up of two images: api and frontend:\n\nversion: '2'\n\nservices:\n  api:\n    image: api\n    build:\n      context: ./API\n      dockerfile: Dockerfile\n\n  frontend:\n    image: frontend\n    build:\n      context: ./frontend\n      dockerfile: Dockerfile\n\n\nThe Dockerfile for each service is straightforward: start from the ASP.NET Core 1.1 image, copy the application files into the container, expose port 80 and run “dotnet app.dll” (frontend.dll and api.dll for each site respectively) as the entry point for each container:\n\nFROM microsoft/aspnetcore:1.1\nARG source\nWORKDIR /app\nEXPOSE 80\nCOPY ${source:-obj/Docker/publish} .\nENTRYPOINT [\"dotnet\", \"API.dll\"]\n\n\nTo build the images, we need to dotnet restore, build and publish. Then we can build the images. Once we have images, we can configure a Kubernetes service to run the images in our minikube cluster.\n\nBuilding the Images\n\nThe easiest way to get the images built is to use Visual Studio, set the docker-compose project as the startup project and run. That will build the images for you. But if you’re not using Visual Studio, then you can build the images by running the following commands from the root of the repo:\n\ncd API\ndotnet restore\ndotnet build\ndotnet publish -o obj/Docker/publish\ncd ../frontend\ndotnet restore\ndotnet build\ndotnet publish -o obj/Docker/publish\ncd ..\ndocker-compose -f docker-compose.yml build\n\n\nNow if you run “docker images” you’ll see the minikube containers as well as images for the frontend and the api:\n\n\n\nDeclaring the Services - Configuration as Code\n\nWe can now define the services that we want to run in the cluster. One of the things I love about Kubernetes is that it pushes you to declare the environment you want rather than running a script. This declarative model is far better than an imperative model, and we can see that with the rise of Chef, Puppet and PowerShell DSC. Kubernetes allows us to specify the services we want exposed as well as how to deploy them. We can define various Kubernetes objects using a simple yaml file. We’re going to declare two services: an api service and a frontend service. Usually, the backend services won’t be exposed outside the cluster, but since the demo code we’re deploying is a single page app (SPA), we need to expose the api outside the cluster.\n\nThe services are rarely going to change - they specify what services are available in the cluster. However, the underlying containers (or in Kubernetes speak, pods) that make up the service will change. They’ll change as they are updated and they’ll change as we scale out and then back in. To manage the containers that “make up” the service, we use a construct known as a Deployment. Since the service and deployment are fairly tightly coupled, I’ve placed them into the same file, so that we have a frontend service/deployment file (k8s/app-demo-frontend-minikube.yml) and an api service/deployment file (k8s/app-demo-backend-minikube.yml). The service and deployment definitions could live separately too if you want. Let’s take a look at the app-demo-backend.yml file:\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: demo-backend-service\n  labels:\n    app: demo\nspec:\n  selector:\n    app: demo\n    tier: backend\n  ports:\n    - protocol: TCP\n      port: 80\n      nodePort: 30081\n  type: NodePort\n---\napiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: demo-backend-deployment\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: demo\n        tier: backend\n    spec:\n      containers:\n      - name: backend\n        image: api\n        ports:\n        - containerPort: 80\n        imagePullPolicy: Never\n\n\nNotes:\n\n\n  Lines 1 - 15 declare the service\n  Line 4 specified the service name\n  Line 8 - 10 specify the selector for this service. Any pod that has the labels app=demo and tier=frontend will be load balanced for this service. As requests come into the cluster that target this service, the service will know how to route the traffic to its underlying pods. This makes adding, removing or updating pods easy since all we have to do is modify the selector. The service will get a static IP, but the underlying pods will get dynamic IPs that will change as they move through their lifecycle. However, this is transparent to us, since we just target the service and all is good.\n  Line 14 - we want this service exposed on port 30081 (mapping to port 80 on the pods, as specified in line 13)\n  Line 15 - the type NodePort specifies that we want Kubernetes to give the service a port on the same IP as the cluster. For “real” clusters (in a cloud provider like Azure) we would change this to get an IP from the cloud host.\n  Lines 17 - 34 declare the Deployment that will ensure that there are containers (pods) to do the work for the service. If a pod dies, the Deployment will automatically start a new one. This is the construct that ensures the service is up and running.\n  Line 22 specifies that we want 2 instances of the container for this service at all times\n  Lines 26 and 27 are important: they must match the selector labels from the service\n  Line 30 specifies the name of the container within the pod (in this case we only have a single container in this pod anyway, which is generally what you want to do)\n  Line 31 specifies the name of the image to run - this is the same name as we specified in the docker-compose file for the backend image\n  Line 33 exposes port 80 on this container to the cluster\n  Line 34 specifies that we never want Kubernetes to pull the image since we’re going to build the images into the minikube docker context. In a production cluster, we’ll want to specify other policies so that the cluster can get updated images from a container registry (we’ll see that in Part 2).\n\n\nThe frontend definition for the frontend service is very similar - except there’s also some “magic” for configuration. Let’s take a quick look:\n\nspec:\n  containers:\n    - name: frontend\n      image: frontend\n      ports:\n      - containerPort: 80\n      env:\n      - name: \"ASPNETCORE_ENVIRONMENT\"\n        value: \"Production\"\n      volumeMounts:\n        - name: config-volume\n          mountPath: /app/wwwroot/config/\n      imagePullPolicy: Never\n  volumes:\n    - name: config-volume\n      configMap:\n        name: demo-app-frontend-config\n\n\nNotes:\n\n\n  Line 30: name the container in the pod\n  Line 31: specify the name of the image for this container - matching the name in the docker-compose file\n  Lines 34 - 36: an example of how to specify environment variables for a service\n  Lines 37 - 39: this is a reference to a volume mount (specified lower down) for mounting a config file, telling Kuberenetes where in the container file system to mount the file. In this case, Kubernetes will mount the volume with name “config-volume” to the path /app/wwwroot/config inside the container.\n  Lines 41 - 44: this specifies a volume - in this case a configMap volume to use for the configuration (more on this just below). Here we tell Kubernetes to create a volume called config-volume (referred to by the container volumeMount) and to base the data for the volume off a configMap with the name demo-app-frontend-config\n\n\nHandling Configuration\n\nWe now have a couple of container images and can start running them in minikube. However, before we start that, let’s take a moment to think a little about configuration. If you’ve ever heard me speak or read my blog, you’ll know that I am a huge proponent of “build once, deploy many times”. This is a core principle of good DevOps. It’s no different when you consider Kubernetes and containers. However, to achieve that you’ll have to make sure you have a way to handle configuration outside of your compiled bits - hence mechanisms like configuration files. If you’re deploying to IIS or Azure App Services, you can simply use the web.config (or for DotNet Core the appsettings.json file) and just specify different values for different environments. However, how do you do that with containers? The entire app is self-contained in the container image, so you can’t have different versions of the config file - otherwise you’ll need different versions of the container and you’ll be violating the build once principle.\n\nFortunately, we can use volume mounts (a container concept) in conjunction with secrets and/or configMaps (a Kubernetes concept). In essence, we can specify configMaps (which are essentially key-value pairs) or secrets (which are masked or hidden key-value pairs) in Kubernetes and then just mount them via volume mounts into containers. This is really powerful, since the pod definition stays the same, but if we have a different configMap we get a different configuration! We’ll see how this works when we deploy to a cloud cluster and use namespaces to separate dev and production environments.\n\nThe configMaps can also be specified using configuration as code. Here’s the configuration for our configMap:\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: demo-app-frontend-config\n  labels:\n    app: demo\n    tier: frontend\ndata:\n  config.json: |\n    {\n      \"api\": {\n        \"baseUri\": \"http://kubernetes:30081/api\"\n      }\n    }\n\n\nNotes:\n\n\n  Line 2: we specify that this is a configMap definition\n  Line 4: the name we can refer to this map by\n  Line 9: we’re specifying this map using a “file format” - the name of the file is “config.json”\n  Lines 10 - 14: the contents of the config file\n\n\nAside: Static Files Symlink Issue\n\nI did have one issue when mounting the config file using configMaps: inside the container the volume mount to /app/www/config/config.json ends up being a symlink. I got the idea of using a configMap in the container from this excellent post by Anthony Chu, in which he mounts an application.json file that the Startup.cs file can consume. Apparently he didn’t have any issues with the symlink in the Startup file. However, in the case of my demo frontend app, I am using a config file that is consumed by the SPA app - and that means, since it’s on the client side, the config file needs to be served from the DotNet Core app, just like the html or js files. No problem - we’ve already got a UseStaticFiles call in Startup, so that should just serve the file, right? Unfortunately, it doesn’t. At least, it only serves the first few bytes of the file.\n\nI took a couple of days to figure this out - there’s a conversation on Github you can read if you’re interested. In short, the symlink length is not the length of the file, but the length of the path to the file. The StaticFiles middleware reads FileInfo.Length bytes when the file is requested, but since the length isn’t the full length of the file, only the first few bytes were being returned. I was able to create a FileProvider that worked around the issue.\n\nRunning the Images in Kubernetes\n\nTo run the services we just created in minikube, we can just use kubectl to apply the configurations. Here’s the list of commands (the highlighted lines):\n\nPS:\\&amp;gt; cd k8s\nPS:\\&amp;gt; kubectl apply -f .\\app-demo-frontend-config.yml\nconfigmap \"demo-app-frontend-config\" created\n\nPS:\\&amp;gt; kubectl apply -f .\\app-demo-backend-minikube.yml\nservice \"demo-backend-service\" created\ndeployment \"demo-backend-deployment\" created\n\nPS:\\&amp;gt; kubectl apply -f .\\app-demo-frontend-minikube.yml\nservice \"demo-frontend-service\" created\ndeployment \"demo-frontend-deployment\" created\n\n\nAnd now we have some services! You can open the minikube dashboard by running “minikube dashboard” and check that the services are green:\n\n\n\n\nAnd you can browse to the frontend service by navigating to http://kubernetes:30080:\n\n\n\n\nThe list (value1 and value2) are values coming back from the API service - so the frontend is able to reach the backend service in minikube successfully!\n\nUpdating the Containers or Containers\n\nIf you update your code, you’re going to need to rebuild the container(s). If you update the config, you’ll have to re-run the “kubectl apply” command to update the configMap. Then, since we don’t need high-availability in dev, we can just delete the running pods and let the replication set restart them - this time with updated config and/or code. Of course in production we won’t do this - I’ll show you how to do rolling updates in the next post when we do CI/CD to a Kubernetes cluster.\n\nFor dev though, I get the pods, delete them all and then watch Kubernetes magically re-start the containers again (with new IDs) and voila - updated containers.\n\nPS:&amp;gt; kubectl get pods\nNAME READY STATUS RESTARTS AGE\ndemo-backend-deployment-951716883-fhf90 1/1 Running 0 28m\ndemo-backend-deployment-951716883-pw1r2 1/1 Running 0 28m\ndemo-frontend-deployment-477968527-bfzhv 1/1 Running 0 14s\ndemo-frontend-deployment-477968527-q4f9l 1/1 Running 0 24s\n\nPS:&amp;gt; kubectl delete pods demo-backend-deployment-951716883-fhf90 demo\n-backend-deployment-951716883-pw1r2 demo-frontend-deployment-477968527-bfzhv demo-frontend-deployment-477968527-q4f9l\npod \"demo-backend-deployment-951716883-fhf90\" deleted\npod \"demo-backend-deployment-951716883-pw1r2\" deleted\npod \"demo-frontend-deployment-477968527-bfzhv\" deleted\npod \"demo-frontend-deployment-477968527-q4f9l\" deleted\n\nPS:&amp;gt; kubectl get pods\nNAME READY STATUS RESTARTS AGE\ndemo-backend-deployment-951716883-4dsl4 1/1 Running 0 3m\ndemo-backend-deployment-951716883-n6z4f 1/1 Running 0 3m\ndemo-frontend-deployment-477968527-j2scj 1/1 Running 0 3m\ndemo-frontend-deployment-477968527-wh8x0 1/1 Running 0 3m\n\n\nNote how the pods get updated IDs - since they’re not the same pods! If we go to the frontend now, we’ll see updated code.\n\nConclusion\n\nI am really impressed with Kubernetes and how it encourages infrastructure as code. It’s fairly easy to get a cluster running locally on your laptop using minikube, which means you can develop against a like-for-like environment that matched prod - which is always a good idea. You get to take advantage of secrets and configMaps, just like production containers will use. All in all this is a great way to do development, putting good practices into place right from the start of the development process.\n\nHappy sailing! (Get it? Kubernetes = helmsman)\n",
      "categories": [],
      "tags": ["docker","development"],
      
      "collection": "posts",
      "url": "/devops-with-kubernetes-and-vsts-part-1/"
    },{
      
      "title": "Protecting a VSTS Web Hook with Basic Authentication",
      "date": "2017-07-15 03:42:54 +0000",
      
      "content": "VSTS supports service hooks like Slack, AppVeyor, Bamboo and a host of other ALM tools. You can also create your own hooks using a simple WebHooks API. There’s an example here. However, one thing that is missing from the sample is any kind of authentication.\n\nWhy care? Well, simply put - without authentication, anyone could trigger events to your event sink. That may or may not be a big deal, but I prefer to be secure by default.\n\nNow there are a couple of ways you could do auth - you could use AAD or OpenConnect and get a token and use that for the WebHook subscription. That would probably work, but VSTS won’t renew the token automatically (at least I don’t think it will) so you’ll have to update the webhook subscription manually every time the token expires.\n\nThe other way is to use Basic Auth. When you subscribe to a webhook in VSTS, you can pass a Basic username/password. The username and password are base64 encoded and added to a header for the requests. Assuming your using HTTPS (so that you don’t get man-in-the-middle attacks) you can use this for a relatively safe authentication method. Once you extract the username/password from the header, you can validate them however you want.\n\nIn this post I’ll cover how to create a Web Hook project that includes Basic Auth as well as logging to Application Insights. I’ll also cover how to debug and test your service using Postman.\n\nThe source code for this stub project is here so you can just grab that if you want to get going.\n\nCreating the Project\n\nInitially I wanted to create the project in .NET Core. However, I wanted to use a NuGet package and the package unfortunately only supports .NET 4.x. So I’ll just use that.\n\nOpen Visual Studio 2017 and click File-&gt;New Project and create a new ASP.NET Web Application. Select Web API from the project type dialog (oh how I love that you don’t have to do this in ASP.NET Core) and ensure you have “No authentication” (we’ll add Basic Auth shortly). This creates a new project and even includes Application Insights.\n\nAdding Packages\n\nWe’re going to add a few NuGet packages. Right-click the web project and add the following packages: Microsoft.AspNet.WebHooks.Receivers.VSTS (contains webhook handler abstract class and event payload classes) and Microsoft.ApplicationInsights.TraceListener (which will send Trace.WriteLines to AppInsights).\n\nOnce the packages are installed, you can (optionally) update all the packages. The project templates sometimes have older package versions, so I usually like to do this so that I’m on the latest NuGet package versions from the get go.\n\nAdding a WebHook Handler\n\nThis is the class that will do the work. Add a new folder called “Handlers” and create a new class called “VSTSHookHandler”.\n\nusing Microsoft.AspNet.WebHooks;\nusing Microsoft.AspNet.WebHooks.Payloads;\nusing System.Threading.Tasks;\nusing System.Diagnostics;\n\nnamespace vsts_webhook_with_auth.Handlers\n{\n\tpublic class VSTSHookHandler : VstsWebHookHandlerBase\n\t{\n\t\tpublic override Task ExecuteAsync(WebHookHandlerContext context, WorkItemCreatedPayload payload)\n\t\t{\n\t\t\tTrace.WriteLine($\"Event WorkItemCreated triggered for work item {payload.Resource.Id}\");\n\t\t\treturn base.ExecuteAsync(context, payload);\n\t\t}\n\t}\n}\n\n\nNotes:\n\n\n  Line 8: We inherit from VstsWebHookHandlerBase - this base class has abstract methods for all the VSTS service hook events that we can listen for.\n  Line 10: We override the async WorkItemCreated event - there are other events that you can override depending on what you need. Add as many overrides as you need. This method also gets the context and the payload for the event for us.\n  Line 12: We are writing log entries to Trace - because we’ve added the AppInsights TraceListener, these end up in AppInsights where we can search for particular messages.\n  Line 13: Here is where you will implement your logic to respond to the event. For this stub project, I just call the base method (which is essentially a no-op).\n\n\nAdding a BasicAuthHandler\n\nAdd a new class to the Handlers folder called BasicAuthHandler. You can get the full class here, but we only need to see the SendAsync method for our discussion:\n\nprotected override Task&amp;lt;HttpResponseMessage&amp;gt; SendAsync(HttpRequestMessage request, CancellationToken cancellationToken)\n{\n\tvar credentials = ParseAuthorizationHeader(request);\n\n\tif (credentials != null &amp;amp;&amp;amp; CredentialsAreValid(credentials))\n\t{\n\t\tvar identity = new BasicAuthenticationIdentity(credentials.Name, credentials.Password);\n\t\tThread.CurrentPrincipal = new GenericPrincipal(identity, null);\n\t\treturn base.SendAsync(request, cancellationToken);\n\t}\n\telse\n\t{\n\t\tvar response = request.CreateResponse(HttpStatusCode.Unauthorized, \"Access denied\");\n\t\tAddChallengeHeader(request, response);\n\t\treturn Task.FromResult(response);\n\t}\n}\n\n\nNotes:\n\n\n  Line 3: The ParseAuthorizationHeader() method extracts the username/password from the auth header\n  Line 5: We check that there are credentials and that they “are valid” (in this case that they match hard-coded values we’ll add to the web.config)\n  Lines 7,8: We add the auth details to the CurrentPrincipal, which has the effect of marking the request as “authenticated”\n  Line 9: We forward the request on to the remainder of the pipeline - which is the VSTSHookHandler class methods at this point\n  Lines 13-15: We handle the unauthorized scenario\n\n\nConfiguration\n\nWe can now add the handlers into the message processing pipeline. Open the Global.asax.cs file and modify the Application_Start() method by adding in the highlighted line (and resolving the namespace):\n\nprotected void Application_Start()\n{\n\tGlobalConfiguration.Configuration.MessageHandlers.Add(new BasicAuthenticationHandler());\n\n\tAreaRegistration.RegisterAllAreas();\n\tGlobalConfiguration.Configure(WebApiConfig.Register);\n\tFilterConfig.RegisterGlobalFilters(GlobalFilters.Filters);\n\tRouteConfig.RegisterRoutes(RouteTable.Routes);\n\tBundleConfig.RegisterBundles(BundleTable.Bundles);\n}\n\n\nOur auth handler is now configured to trigger on requests.\n\nNext, open up App_Start/WebApiConfig.cs and modify the Register() method with the highlighted line:\n\nconfig.Routes.MapHttpRoute(\n\tname: \"DefaultApi\",\n\trouteTemplate: \"api/{controller}/{id}\",\n\tdefaults: new { id = RouteParameter.Optional }\n);\n\nconfig.InitializeReceiveVstsWebHooks();\n\n\nThis registers the handler class to respond to VSTS events.\n\nFinally, open the web.config file and add the following appSetting keys:\n\n  &amp;lt;appSettings&amp;gt;\n\t...\n\t&amp;lt;add key=\"WebHookUsername\" value=\"vsts\"/&amp;gt;\n\t&amp;lt;add key=\"WebHookPassword\" value=\"P@ssw0rd\"/&amp;gt;\n\t&amp;lt;add key=\"MS_WebHookReceiverSecret_VSTS\" value=\"C8B7F962-2B5A-4973-81F3-8888D53CF86E\"/&amp;gt;\n  &amp;lt;/appSettings&amp;gt;\n\n\nNotes:\n\n\n  The MS_WebHookReceiverSecret_VSTS is a “code” that the VSTS webhook requires to be in the query params for the service call. This can be anything as long as it is longer than 32 and less than 128 chars. You can also have multiple codes. This code lets you handle different projects firing the same events - you can tie the code to a project so that you can do project specific logic.\n  WebHookUsername and WebHookPassword are hardcoded username/password for validation - you can ignore these if you need some other way to validate the values.\n\n\nConfiguring SSL in VS\n\nTo run the site using SSL from VS, you’ll need to enable that in the project properties. Click on the web project node (so that it is selected in the Solution Explorer). Then press F4 to open the properties pane (this is different to right-click -&gt; Properties). Change SSL Enabled to true. Make a note of the https URL.\n\n\n\n\nYou can now right-click the project and select Properties. Change the startup URL to the https URL so that you always get the SSL site.\n\nTesting from PostMan\n\nYou can now run the site. You’ll notice when you first run it that the cert is invalid (it’s self-signed). In order to get Postman working to test the webhooks, I had to read these instructions. In the end, I just opened the https URL in IE and imported the cert to Trusted Root Certification Authorities. I then shut down Chrome and restarted and all was good.\n\nI opened Postman and entered this url: https://localhost:44388/api/webhooks/incoming/vsts?code=C8B7F962-2B5A-4973-81F3-8888D53CF86E, changing the method to POST. I made sure that the code was the same value as the MS_WebHookReceiverSecret_VSTS key in my web.config. I then opened the docs page and grabbed the sample payload for the WorkItemCreated event and pasted this into the Body for the request. I updated the type to application/json (which adds a header).\n\n\n\n\nHitting Send returns a 401 - which is expected since we haven’t provided a username/password. Nice!\n\n\n\n\nNow let’s test adding the auth. Go back to Postman and click on Authorization. Change the Type to “Basic Auth” and enter the username/password that you hard-coded into your web.config. Click “Update Request” to add the auth header:\n\n\n\n\nNow press Send again. We get a 200!\n\n\n\n\nOf course you can now set breakpoints and debug normally. If you open AppInsights Viewer in VS you’ll see the traces - these will eventually make their way to AppInsights (remember to update your key when you create a real AppInsights resource in Azure).\n\n\n\nRegistering the Secure Hook in VSTS\n\nYou can now clean up the project a bit (remove the home controller etc.) and deploy the site to Azure (or your own IIS web server) using VSTS Build and Release Management (friends don’t let friends right-click Publish) and you’re ready to register the hook in VSTS. Open your VSTS Team Project and head to Configuration-&gt;Service Hooks. Add a new service hook. Enter the URL (including the code query param) for your service. Then just enter the same username/password you have in the website web.config and you’re good to go! Hit test to make sure it works.\n\n\n\nConclusion\n\nSecuring your WebHooks from VSTS isn’t all that hard - just add the BasicAuthHandler and configure the basic auth username/password in the WebHook subscription in VSTS. Now you can securely receive events from VSTS. I would really like to see the VSTS team update the NuGet packages to support .NET Core WebAPI, but the 4.x version is fine in the interim.\n\nHappy hooking!\n",
      "categories": [],
      "tags": ["development"],
      
      "collection": "posts",
      "url": "/protecting-a-vsts-web-hook-with-basic-authentication/"
    },{
      
      "title": "Configuring AAD Authentication to Azure SQL Databases",
      "date": "2017-08-22 06:03:40 +0000",
      
      "content": "Azure SQL is a great service - you get your databases into the cloud without having to manage all that nasty server stuff. However, one of the problems with Azure SQL is that you have to authenticate using SQL authentication - a username and password. However, you can also authenticate via Azure Active Directory (AAD) tokens. This is analogous to integrated login using Windows Authentication - but instead of Active Directory, you’re using AAD.\n\nThere are a number of advantages to AAD Authentication:\n\n\n  You no longer have to share logins since users log in with their AAD credentials, so auditing is better\n  You can manage access to databases using AAD groups\n  You can enable “app” logins via Service Principals\n\n\nIn order to get this working, you need:\n\n\n  To enable AAD authentication on the Azure SQL Server\n  A Service Principal\n  Add logins to the database granting whatever rights required to the service principal\n  Add code to get an auth token for accessing the database\n  If you’re using Entity Framework (EF), create a new constructor for your DbContext\n\n\nIn this post I’ll walk through creating a service principal, configuring the database for AAD auth, creating code for retrieving a token and configuring an EF DbContext for AAD auth.\n\nCreate a Service Principal\n\nAzure lets you configure service principals - these are like service accounts on an Active Directory. The advantage to this is that you can configure access to resources for the service and not have to worry about users leaving the org (or domain) and having to change creds and so on. Service principals get keys that can be rotated for better security too. You’ll need the service principal when you configure your app to connect to the database.\n\nYou can create a service principal using the portal or you can do it easily using:\n\n# Azure CLI 2.0\naz ad sp create-for-rbac --name CoolAppSP --password SomeStrongPassword\n\n# PowerShell\n# get the application we want a service principal for\n$app = Get-AzureRmADApplication -DisplayNameStartWith MyDemoApp\nNew-AzureRmADServicePrincipal -ApplicationId $app.ApplicationId -DisplayName CoolAppSP -Password SomeStrongPassword\n\n\nOf course you need to provide a proper strong password! Take a note of the servicePrincipalNames property - the one that looks like a GUID. We’ll need this later.\n\nConfiguring AAD on the Database\n\nIn order to use AAD against the SQL Server, you’ll need to configure an AAD admin (user or group) for the database. You can do this in the portal by browsing to the Azure SQL Server (not the database) and clicking “Active Directory Admin”. In the page that appears, click “Set Admin” and assign a user or group as the AAD admin.\n\nOnce you’ve done that, you need to grant Azure AD users (or groups) permissions in the databases (not the server). To do that you have to connect to the database using an Azure AD account. Open Visual Studio or SQL Server Management Studio and connect to the database as the admin (or a member of the admin group) using “Active Directory Password Authentication” or “Azure Directory Integrated Authentication” from the Authentication dropdown:\n\n\n\n\nIf you don’t see these options, then you’ll need to update your SQL Management Studio or SSDT. If you’re domain joined to the Azure Active Directory domain, you can use the integrated method - in my case my laptop isn’t domain joined so I used the password method. For username and password, I used my Azure AD (org account) credentials. Once you’re logged in and connected to the database, execute the following T-SQL:\n\nCREATE USER [CoolAppSP] FROM EXTERNAL PROVIDER\nEXEC sp_addrolemember 'db_owner', 'CoolAppSP'\n\n\nOf course you’ll use the name of the Service Principal you created earlier - the name for the Login is the same name as the service principal you created, or can be the email address of a specific user or group display name if you’re granting access to specific AAD users or groups so that they can access the db directly. And of course the role doesn’t have to be dbowner - it can be whatever role you need it to be.\n\nAuthenticating using the Service Principal\n\nThere are a couple of pieces we need in order to authenticate an application to the Azure SQL database using AAD credentials. The first is a token (it’s an OAuth token) that identifies the service principal. Secondly, we need to construct a database connection that uses the token to authenticate to the server.\n\nRetrieve a Token from AAD\n\nTo get a token, we’ll need to call Azure AD and request one. For this, you’ll need the Microsoft.IdentityModel.Clients.ActiveDirectory Nuget package.\n\nHere’s the code snippet I used to get a token from AAD:\n\npublic async Task&amp;lt;string&amp;gt; GetAccessTokenAsync(string clientId, string clientSecret, string authority, string resource, string scope)\n{\n\tvar authContext = new AuthenticationContext(authority, TokenCache.DefaultShared);\n\tvar clientCred = new ClientCredential(clientId, clientSecret);\n\tvar result = await authContext.AcquireTokenAsync(resource, clientCred);\n\n\tif (result == null)\n\t{\n\t\tthrow new InvalidOperationException(\"Could not get token\");\n\t}\n\n\treturn result.AccessToken;\n}\n\n\nNotes:\n\n\n  Line 1: We need some information in order to get the token: ClientId and ClientSecret are from the service principal. The authority, resource and scope will need to be passed in too (more on this later).\n  Line 3: We’re getting a token from the “authority” or tenant in Azure\n  Line 4: We create a new client credential using the id and secret of the “client” (in this case, the service principal)\n  Line 5: We get a token for this client onto the “resource”\n  Line 7: We throw if we don’t get a token back\n  Line 12: If we do get a token, return it to the caller\n\n\nThe client id is the “application ID” of the service principal (the guid in the servicePrincipalNames property of the service principal). To get the secret, log in to the portal and click in the Active Directory blade. Click on “App Registration” and search for your service principal. Click on the service principal to open it. Click on Keys and create a key - make a note of the key so that you can add this to configurations. This key is the clientSecret that the GetAccessToken method needs.\n\nFor authority, you’ll need to supply the URL to your Azure tenant. You can get this by running “az account show” (Azure CLI 2.0) or “Get-AzureRmSubscription” (PowerShell). Make a note of the tenantId of the subscription (it’s a GUID). Once you have that, the authority is simply “https://login.windows.net/{tenantId}”. The final piece of info required is the resource - for Azure SQL access, this is simply “https://database.windows.net/”. The scope is just empty string - for databases, the security is configured per user (using the role assignments on the DB you configured earlier). The authentication is done using Azure AD via the token - the database is doing authorization. In other words, Azure lets an Azure AD user in when they present a valid token - the database defines what the user can do once they’re in via roles.\n\nCreating a SQL Connection\n\nWe’ve now got a way to get a token - so we can create a SQL Connection to the database. Here’s a code snippet:\n\npublic async Task&amp;lt;SqlConnection&amp;gt; GetSqlConnectionAsync(string tenantId, string clientId, string clientSecret, string dbServer, string dbName)\n{\n\tvar authority = string.Format(\"https://login.windows.net/{0}\", tenantId);\n\tvar resource = \"https://database.windows.net/\";\n\tvar scope = \"\";\n\tvar token = await GetTokenAsync(clientId, clientSecret, authority, resource, scope);\n\n\tvar builder = new SqlConnectionStringBuilder();\n\tbuilder[\"Data Source\"] = $\"{dbServer}.database.windows.net\";\n\tbuilder[\"Initial Catalog\"] = dbName;\n\tbuilder[\"Connect Timeout\"] = 30;\n\tbuilder[\"Persist Security Info\"] = false;\n\tbuilder[\"TrustServerCertificate\"] = false;\n\tbuilder[\"Encrypt\"] = true;\n\tbuilder[\"MultipleActiveResultSets\"] = false;\n\n\tvar con = new SqlConnection(builder.ToString());\n\tcon.AccessToken = token;\n\treturn con;\n}\n\n\nNotes:\n\n\n  Line 1: All the info we need for the connection\n  Lines 3 - 5: Prepare the info for the call to get the token\n  Line 6: Get the access token\n  Lines 8-15: Prepare the SQL connection string to the Azure SQL database - tweak the properties (like Connect Timeout) appropriately.\n  Line 17: Create the connection\n  Line 18: Inject the token into the connection object\n\n\nYou’d now be able to use the connection just like you would any SqlConnection object.\n\nEntity Framework DataContext Changes\n\nIf you’re using Entity Framework for data access, you’ll notice there’s no obvious way to use the SqlConnection object that’s now configured to access the Azure SQL database. You’ll need to create a constructor on your DbContext:\n\npublic class CoolAppDataContext : DbContext\n{\n\tpublic CoolAppDataContext(SqlConnection con)\n\t\t: base(con, true)\n\t{\n\t\tDatabase.SetInitializer&amp;lt;CoolAppDataContext&amp;gt;(null);\n\t}\n\n\tpublic DbSet&amp;lt;Product&amp;gt; Products { get; set; }\n\n\t...\n}\n\n\nNotes:\n\n\n  Line 3: A constructor that accepts a SqlConnection object\n  Line 4: Call the base constructor method\n  Line 5: Override the initializer for the context’s Database object\n\n\nNow you can use the above methods to construct a SqlConnection to an Azure SQL database using AAD credentials and pass it in to the DbContext - and you’re good to go!\n\nConclusion\n\nConfiguring an application to use Azure AD credentials to connect to an Azure SQL database is straightforward once you have all the pieces in place. There’s some configuration you need to ensure is in place, but once it’s configured you can stop using SQL Authentication to access your cloud databases - and that’s a win!\n\nHappy connecting!\n",
      "categories": [],
      "tags": ["development","cloud"],
      
      "collection": "posts",
      "url": "/configuring-aad-authentication-to-azure-sql-databases/"
    },{
      
      "title": "A/B Testing with Azure Linux Web Apps for Containers",
      "date": "2017-10-14 09:43:18 +0000",
      
      "content": "I love containers. I’ve said before that I think they’re the future. Just as hardly anyone installs on tin any more since we’re so comfortable with Virtualization, I think that in a few years time hardly anyone will deploy VMs - we’ll all be on containers. However, container orchestration is still a challenge. Do you choose Kubernetes or Swarm or DCOS? (For my money I think Kubernetes is the way to go). But that means managing a cluster of nodes (VMs). What if you just want to deploy a single container in a useful manner?\n\nYou can do that now using Azure Container Instances (ACI). You can also host a container in an Azure Web App for Containers. The Web App for Containers is what I’ll use for this post - mostly because I already know how to do A/B testing with Azure Web Apps, so once the container is running then you get all the same paradigms as you would for “conventional” web apps - like slots, app settings in the portal etc.\n\nIn this post I’ll cover publishing a .NET Core container to Azure Web Apps using VSTS with an ARM template. However, since the hosting technology is “container” you can host whatever application you want - could be Java/TomCat or node.js or python or whatever. The A/B Testing principles will still apply.\n\nYou can grab the code for this demo from this Github repo.\n\nOverview of the Moving Parts\n\nThere are a couple of moving parts for this demo:\n\n\n  The source code. This is just a File-&gt;New Project .NET Core 2.0 web app. I’ve added a couple lines of code and Application Insights for monitoring - but other than that there’s really nothing there. The focus of this post is about how to A/B test, not how to make an app!\n  Application Insights - this is how you can monitor the web app to make sure\n  An Azure Container Registry (ACR). This is a private container repository. You can use whatever repo you want, including DockerHub.\n  A VSTS Build. I’ll show you how to set up a build in VSTS to build the container image and publish it to the ACR.\n  An ARM template. This is the definition of the resources necessary for running the container in Azure Web App for Containers. It includes a staging slot and Application Insights.\n  A VSTS Release. I’ll show you how to create a release that will spin up the web app and deploy the container. Then we’ll set up Traffic Manager to (invisibly) divert a percentage of traffic from the prod slot to the staging slot - this is the basis for A/B testing and the culmination of the all the other steps.\n\n\nThe Important Source Bits\n\nLet’s take a look at some of the important code files. Firstly, the Startup.cs file:\n\npublic void Configure(IApplicationBuilder app, IHostingEnvironment env)\n{\n\tvar aiKey = Environment.GetEnvironmentVariable(\"AIKey\");\n\tif (aiKey != null)\n\t{\n\t\tTelemetryConfiguration.Active.InstrumentationKey = aiKey;\n\t}\n\t...\n\n\nNotes:\n\n\n  Line 3: I read the environment variable “AIKey” to get the Application Insights key\n  Lines 4 - 7: If there is a key, then I set the key for the Application Insights config\n\n\nThe point here is that for configuration, I want to get values from the environment. This would include database connection strings etc. Getting them from the environment lets me specify them in the Web App appSettings so that I don’t have to know the values at build time - only at release time.\n\nLet’s look at the Dockerfile:\n\nFROM microsoft/aspnetcore:2.0\nARG source\nENV AIKey=\"11111111-2222-3333-4444-555555555555\"\n\nWORKDIR /app\nEXPOSE 80\nCOPY ${source:-obj/Docker/publish} .\nENTRYPOINT [\"dotnet\", \"DockerWebApp.dll\"]\n\n\nNotes:\n\n\n  Line 3 was the only thing I added to make the AIKey configurable as an environment variable\n\n\nFinally, let’s look at the ARM template that defines the resources. The file is too large to paste here and it’s Json so it’s not easy to read. You can have a look at the file yourself. The key points are:\n\n\n  a HostingPlan with kind = “linux” and properties.reserved = true. This creates a Linux app plan.\n  a Site with properties.siteConfig.appSettings that specify the DOCKER_REGISTRY_SERVER_URL, DOCKER_REGISTRY_SERVER_USERNAME, DOCKER_REGISTRY_SERVER_PASSWORD (using the listCredentials function) and AIKey. We do not specify the DOCKER_CUSTOM_IMAGE_NAME for reasons that will be explained later. Any other environment variables (like connection strings) could be specified here.\n  a staging slot (called blue) that has the same settings as the production slot\n  a slotconfignames resource that locks the DOCKER_CUSTOM_IMAGE_NAME so that the value is slot-sticky\n  an Application Insights resource - the key for this resource is referenced by the appSettings section for the site and the slot\n\n\nBuilding (and Publishing) Your Container\n\nAt this point we can look at the build. The build needs to compile, test and publish the .NET Core app and build a container. It then needs to publish the container to the ACR (or whatever registry you created).\n\nCreate a new build using the ASP.NET Core Web App template and then edit the steps to look as follows:\n\n\n\n\nMake sure you point the Source settings to the repo (you can create a new one and import mine from the Github URL if you want to try this yourself). I then changed the queue to Hosted Linux Preview and changed the name to something appropriate.\n\nOn the Options page I set the build number format to 1.0$(rev:.r) which gives a 1.0.0, 1.0.1, 1.0.2 etc. format for my build number.\n\nClick on the Build task and set the arguments to –configuration $(BuildConfiguration) /p:Version=$(Build.BuildNumber). This versions the assemblies to match the build number.\n\nClick on the Publish task and set Publish Web Apps, change the arguments to –configuration $(BuildConfiguration) –output $(Build.ArtifactStagingDirectory) /p:Version=$(Build.BuildNumber) and unselect Zip files. This puts the output files into the artifact staging directory and doesn’t zip them. It also publishes with a version number matching the build number:\n\n\n\n\nI deleted the Test task since I don’t have tests in this simple project - but you can of course add testing in before publishing.\n\nI then deleted the Publish build artifact tasks since this build won’t be publishing an artifact - it will be pushing a container image to my ACR.\n\nIn order to build the docker container image, I first need to put the Dockerfile at the correct location relative to the output. So I add a Copy Files task in and configure it to copy the Dockerfile to the artifact staging directory:\n\n\n\n\nNow I add a 2 Docker tasks: the has a Build an image Action. I set the ACR by selecting the settings from the dropdowns. I set the path to the Dockerfile and specify “DockerWebApp” as the source build argument (the Publish task will have places the compiled site and content into this folder in the artifact staging directory). I set Qualify the Image name to correctly tag the container with the ACR prefix and I include the Latest tag in the build (so the current build is always the Latest).\n\n\n\n\nThe 2nd Docker task has Action set to Publish an Image. I set the ACR like the Docker Build task. I also change the image name to $(Build.Repository.Name):$(Build.BuildNumber) instead of $(Build.Repository.Name):$(Build.BuildId) and I set the Latest tag.\n\n\n\n\nNow I can run the build. Lots of green! I can also see the image in my ACR in the Azure portal:\n\n\n\n\nWoot! We now have a container image that we can host somewhere.\n\nReleasing the Container\n\nNow that we have a container and an infrastructure template, we can define a release. Here’s what the release looks like:\n\n\n\n\nThere are 2 incoming artifacts: the build and the Git repo. The build doesn’t actually have any artifacts itself - I just set the build as the trigger mechanism. I specify a “master” artifact filter so that only master builds trigger this release. The Git repo is referenced for the deployment scripts (in this case just the ARM template). I started with an empty template and then changed the Release number format to $(Build.BuildNumber)-$(rev:r) in the Options page.\n\nThere are 3 environments: Azure blue, Azure prod and blue failed. These are all “production” environments - you could have Dev and Staging environments prior to these environments. However, I want to A/B test in production, so I’m just showing the “production environment” here.\n\nLet’s look at the Azure blue environment:\n\n\n\n\nThere are 3 tasks: Azure Resource Group Deployment (to deploy the ARM template), an Azure CLI command to deploy the correct container, and a Traffic Manager Route Traffic task. The Azure Resource Group Deployment task specifies the path to the ARM template and parameters files as well as the Azure endpoint for the subscription I want to deploy to. I specify a variable called $(RGName) for the resource group name and then override the parameters for the template using $(SiteName) for the name of the web app in Azure, $(ImageName) for the name of the container image, $(ACR) for the name of my ACR and $(ACRResourceGroup) for the name of the resource group that contains my ACR. Once this task has run, I will have the following resources in the resource group:\n\n\n\n\nLet’s take a quick look at the app settings for the site:\n\n\n\n\nAt this point the site (and slot) are provisioned, but they still won’t have a container running. For that, we need to specify which container (and tag) to deploy. The reason I can’t do this in the ARM template is because I want to update the staging slot and leave the prod slot on whatever container tag it is on currently. Let’s imagine I specified “latest” for the prod slot - then when we run the template, the prod slot will update, which I don’t want. Let’s say we specify latest for the blue slot - then the blue slot will update - but what version do we specify in the template for the prod slot? We don’t know that ahead of time. So to work around these issues, I don’t specify the container tag in the template - I use an Azure CLI command to update it after the template has deployed (or updated) all the other infrastructure and settings.\n\nTo deploy a container, we add an Azure CLI task and specify a simple inline script:\n\n\n\n\nHere I select version 1.* of the task (version 0.* doesn’t let you specify an inline script). I set the script to sleep for 30 seconds - if I don’t do this, then the site is still updating or something and the operation succeeds, but doesn’t work - it doesn’t actually update the image tag. I suspect that the update is async and so if you don’t wait for it to complete, then issuing another tag change succeeds but is ignored. It’s not pretty, but that’s the only workaround I’ve found. After pausing for a bit, we invoke the “az webapp config container set” command, specifying the site name, slot name, resource group name and image name. (If you’re running this phase on a Linux agent, use $1, $2 etc. for the args - if you’re running this on a Windows agent then specify the args using %1, %2 etc. in the script). Then I pass the arguments in - using $(SiteName), blue, $(RGName) and $(ImageName):$(Build.BuildNumber) for the respective arguments.\n\nIf you now navigate to the site in the Azure portal and click on the Docker Container tab on the slot, you’ll see the container settings:\n\n\n\n\nYou can see that the image name and version are specified.\n\nThe final task in this environment (a Route Traffic task from my Build and Release extension pack) adds a traffic manager rule - we divert 20% of traffic to the blue slot (unobtrusively to the client):\n\n\n\n\nThere’s a snag to this method: the first time you deploy, there’s nothing (yet) in the prod slot. This is just a first time condition - so after you run this environment for the very first time, navigate to the Azure portal and click on the site. Then swap the blue and prod slots. Now the prod slot is running the container and the blue slot is empty. Repeat the deployment and now both slots have the latest version of the container.\n\nLet’s go back to the release and look at the Azure prod environment. This has a pre-approval set so that someone has to approve the deployment to this environment. The Azure blue has a post-deployment approver so that someone can sign off on the release - approved if the experiment works, rejected if it’s not. The Azure prod environment triggers when the Azure blue environment is successful - all it has to do is swap the slots and reset the traffic router to reroute 100% of traffic to the prod slot:\n\n\n\n\nSo what do we do if the experiment fails? We can reject the Azure blue environment (so that the Azure prod environment doesn’t run). We then manually run the blue fail environment - this just resets the traffic to route 100% back to prod. It does not swap the slots:\n\n\n\n\nDon’t forget to specify values for the variables we used:\n\n\n\nRunning a Test\n\nSo imagine I have container version 1.0.43 in both slots. Now I make a change and commit and push - this triggers a build (I enabled CI) and we get version 1.0.45 (1.0.44 had an intermittent build failure). This triggers the release (I enabled CD) and now the blue slot has version 1.0.45 and 20% of traffic from the prod slot is going to the blue slot.\n\n\n\n\nLet’s navigate to the blue and prod slots and see them side-by-side:\n\n\n\n\nThe traffic routing is “sticky” to the user - so if a user navigates to the prod slot and gets diverted to the blue slot, then all requests to the site from the user go to the blue slot. Try opening some incognito windows and hitting the prod site - you’ll get the blue content 20% of the time! You can also force the routing using a query parameter - just tack “?x-ms-routing-name=blue” onto the end of any request and you’ll end up on the blue slot:\n\n\n\n\nNow you wait for users to generate traffic (or in my case, I totally fake client traffic - BWAHAHA!). So how do we know if the experiment is successful? We use Application Insights.\n\nApplication Insights Analytics\n\nLet’s click on the Azure portal and go to the App Insights resource for our web app. We can see some telemetry, showing traffic to the site:\n\n\n\n\nBut how do we know which requests went to the blue slot and how many went to the prod slot? It turns out that’s actually pretty simple. Click the Analytics button to launch App Insights Analytics. We enter a simple query:\n\n\n\n\nWe can clearly see that there is more usage of the 1.0.45 contact page. Yes, this metric is bogus - the point is to show you that you can slice by “Application_Version” and so you actually have metrics to determine if your new version (1.0.45) is better or worse that 1.0.43. Maybe it’s more traffic to a page. Maybe it’s more sales. Maybe it’s less exceptions or better response time - all of these metrics can be sliced by Application_Version.\n\nConclusion\n\nDeploying containers to Azure Web Apps for Containers is a great experience. Once you have the container running, you can use any Web App paradigm - such as Traffic Routing or appSettings - so it’s easy if you’ve ever done any Web App deployments before.\n\nThere are a couple of key practices that are critical to A/B testing: telemetry and unobtrusive routing.\n\nTelemetry is absolutely critical to A/B testing: if you can’t decide if A is better (or worse) than B, then there’s no point deploying both versions. Application Insights is a great tool for telemetry in general - but especially with A/B testing, since you can put some data and science behind your hypotheses. You don’t have to use AppInsights - but you do have to have some monitoring tool or framework in order to even contemplate A/B testing.\n\nThe other key is how you release the A and B sites or apps. Having traffic manager seamlessly divert customer traffic is an excellent way to do this since the customer is none the wiser about which version they are seeing - so they don’t have to change their URLs or anything obscure. You could also use LaunchDarkly or some other feature flag mechanism - as long as your users don’t have to change their usual way of accessing your app. This will give you “real” data. If users have to go to a beta site, they could change their behavior subconsciously. Maybe that isn’t a big deal - but at least prefer “seamless” routing between A and B sites before you explicitly tell users to navigate to a new site altogether.\n\nHappy testing!\n",
      "categories": [],
      "tags": ["docker","testing","devops"],
      
      "collection": "posts",
      "url": "/ab-testing-with-azure-linux-web-apps-for-containers/"
    },{
      
      "title": "Tips and Tricks for Complex IaaS Deployments Using VSTS Deployment Groups",
      "date": "2017-12-31 13:55:50 +0000",
      
      "content": "Recently I was working with a customer that was struggling with test environments. Their environments are complex and take many weeks to provision and configure - so they are generally kept around even though some of them are not frequently used. Besides a laborious, error-prone manual install and configuration process that usually takes over 10 business days, the team has to maintain all the clones of this environment. This means that at least two senior team members are required just to maintain existing dev and test environments as well as create new ones.\n\nUsing Azure ARM templates and VSTS Release Management with Deployment Groups, we were able to show how we could spin up the entire environment in just under two hours. That’s a 50x improvement in lead time! And it’s more consistent since the entire process is automated and the scripts are all source controlled so there’s auditability. This is a huge win for the team. Not only can they spin up an environment in a fraction of the time they are used to, they can now decommission environments that are not frequently used (some environments were only used twice a year). That means they have less maintenance to worry about. When they need an environment for a short time, they spin it up, used it and then throw it away. They’ve also disseminated “tribal knowledge” from a few team members’ heads to a button click - meaning anyone can create a new environment now.\n\nThis was my first time working with a larger scale provisioning and configuration project that uses Deployment Groups - and this post documents some of the lessons that we learned along the way.\n\nA Brief Glossary\n\nBefore we jump into the tips, I need to get some definitions out of the way. In VSTS, a Release Definition is made up of multiple Environments. Typically you see DEV, STAGE and PROD but you can have multiple “environments” that target the same set of machines.\n\n\n\n\nThe above VSTS release has three “environments”:\n\n\n  Infrastructure Provision\n  Runs an ARM template to provision VMs, VNets, Storage and any other infrastructure required for the environment\n  Infrastructure Config\n  Configure the OS of each machine, DNS and any other “low-level” settings\n  App Install\n  Install and configure the application(s)\n\n\nThis separation also allows you to run the “Infrastructure Provision” environment and then set it to a manual trigger and just trigger the config environment - particularly useful when you’re developing the pipeline, since you can skip environments that end up being no-ops but take a couple minutes to pass through.\n\nWithin an Environment, you can have 1..n phases. You specify tasks inside a phase - these are the smallest unit of work in the release.\n\n\n\n\nIn the above image, there are several phases within the “Infrastructure Config” environment. Each phase (in this case) is running a single task, but you can run as many tasks as you need for that particular phase.\n\nThere are three types of phases: agentless, agent-based or deployment-group based. You can think of agentless phases as phases that are executed on VSTS. Agent-based phases are executed on agent(s) in a build or release queue. Deployment Group phases are executed on all agents (with optional tag matching) within the specified Deployment Group. The agent for agent-based or deployment-group based is the same agent under the hood - the difference is that deployment group agents are only referenced through the Deployment Group while build/release agents are accessed through queues. You’d typically use agent queues for build servers or for “proxy servers” in releases (where the tasks are executing on the proxy but acting on other machines). Deployment Groups are used when you don’t know the machines ahead of time - like when you’re spinning up a set of machines in the cloud on demand. They also allow you to target multiple machines at the same time.\n\nThe VSTS Deployment agent joins a machine (this can be any machine anywhere that can connect to VSTS) to a Deployment Group. The agent is cross-platform (runs on DotNET Core) so it can run on practically any machine anywhere. It connects out to VSTS meaning you don’t need to open incoming firewall ports at all. The agent runs on the machine and so any scripts you write can execute locally - which simplifies configuration dramatically. Executing remote instructions is typically much harder to do - you have to think about your connection and security and so on. Executing locally is much easier.\n\nTL;DR - The Top 10 Tips and Tricks\n\nHere are my top 10 tips and tricks:\n\n\n  Spin Up Azure VMs with the VSTS Deployment Agent Extension\n  This allows you to configure everything else locally on each machine\n  Use Tagging for Parallelization and Specialization\n  Tagging the VSTS agent allows you to repeat the same actions on many machines in parallel and/or distinguish machines for unique actions\n  Use Phases to Start New Sessions\n  Each phase in an Environment gets a new session, which is useful in a number of scenarios\n  Update Your PowerShell PackageProviders and Install DSC Modules\n  If you’re using DSC, install the modules in a separate step to ensure that they are available when you run DSC scripts. You may need to update your Package Providers for this to work\n  Install Azure PowerShell and use the Azure PowerShell task\n  If you’re going to be doing any scripting to Azure, you can quickly install Azure PowerShell so that you can use the Azure PowerShell task\n  Use PowerShell DSC for OS Configuration\n  Configuring Windows Features, firewalls and so on is best done with PowerShell DSC\n  Use Plain PowerShell for Application Install and Config\n  Expressing application state can be challenging - so use “plain” PowerShell for application install and config\n  Attaching Data Disks in Azure VMs\n  If you add data disks in your ARM template, you still need to mount them in the OS of the VM\n  Configuring DNS on Azure VNets\n  If you create an Active Directory Domain Controller or DNS, you’ll need to do some other actions on the VNet too\n  Wait on machines when they reboot\n  If you reboot a machine and don’t pause, the subsequent deployment steps fail because the agent goes offline.\n\n\nIn the next section I’ll dig into each tip.\n\nTip 1: Spin Up Azure VMs with the VSTS Deployment Agent Extension\n\nYou can install the VSTS Deployment Agent (or just “the agent” for the remainder of this post) on any machine using a simple script. The script downloads the agent binary and configures it to connect the agent to your VSTS account and to the specified Deployment Group. However, if you’re spinning up machines by using an ARM template, you can also install the agent via the VSTS extension. In order to do this you need a Personal Access Token (or PAT), the name of the VSTS account, the name of the Deployment Group and optionally some tags to tag the agent with. Tags will be important when you’re distinguishing between machines in the same Deployment Group later on. You’ll need to create the Deployment Group in VSTS before you run this step.\n\nHere’s a snippet of an ARM template that adds the extension to the Deployment Group:\n\n{\n    \"name\": \"[parameters('settings').vms[copyIndex()].name]\",\n    \"type\": \"Microsoft.Compute/virtualMachines\",\n    \"location\": \"[resourceGroup().location]\",\n    \"apiVersion\": \"2017-03-30\",\n    \"dependsOn\": [\n      ...\n    ],\n    \"properties\": {\n      \"hardwareProfile\": {\n        \"vmSize\": \"[parameters('settings').vms[copyIndex()].size]\"\n      },\n      \"osProfile\": {\n        \"computerName\": \"[parameters('settings').vms[copyIndex()].name]\",\n        \"adminUsername\": \"[parameters('adminUsername')]\",\n        \"adminPassword\": \"[parameters('adminPassword')]\"\n      },\n      \"storageProfile\": {\n        \"imageReference\": \"[parameters('settings').vms[copyIndex()].imageReference]\",\n        \"osDisk\": {\n          \"createOption\": \"FromImage\"\n        },\n        \"dataDisks\": [\n            {\n                \"lun\": 0,\n                \"name\": \"[concat(parameters('settings').vms[copyIndex()].name,'-datadisk1')]\",\n                \"createOption\": \"Attach\",\n                \"managedDisk\": {\n                    \"id\": \"[resourceId('Microsoft.Compute/disks/', concat(parameters('settings').vms[copyIndex()].name,'-datadisk1'))]\"\n                }\n            }\n        ]\n      },\n      \"networkProfile\": {\n        \"networkInterfaces\": [\n          {\n            \"id\": \"[resourceId('Microsoft.Network/networkInterfaces', concat(parameters('settings').vms[copyIndex()].name, if(equals(parameters('settings').vms[copyIndex()].name, 'JumpBox'), '-nicpub', '-nic')))]\"\n          }\n        ]\n      }\n    },\n    \"resources\": [\n      {\n        \"name\": \"[concat(parameters('settings').vms[copyIndex()].name, '/TeamServicesAgent')]\",\n        \"type\": \"Microsoft.Compute/virtualMachines/extensions\",\n        \"location\": \"[resourceGroup().location]\",\n        \"apiVersion\": \"2015-06-15\",\n        \"dependsOn\": [\n          \"[resourceId('Microsoft.Compute/virtualMachines/', concat(parameters('settings').vms[copyIndex()].name))]\"\n        ],\n        \"properties\": {\n          \"publisher\": \"Microsoft.VisualStudio.Services\",\n          \"type\": \"TeamServicesAgent\",\n          \"typeHandlerVersion\": \"1.0\",\n          \"autoUpgradeMinorVersion\": true,\n          \"settings\": {\n            \"VSTSAccountName\": \"[parameters('vstsAccount')]\",\n            \"TeamProject\": \"[parameters('vstsTeamProject')]\",\n            \"DeploymentGroup\": \"[parameters('vstsDeploymentGroup')]\",\n            \"Tags\": \"[parameters('settings').vms[copyIndex()].tags]\"\n          },\n          \"protectedSettings\": {\n            \"PATToken\": \"[parameters('vstsPat')]\"\n          }\n        }\n      },\n      ...\n\n\nNotes:\n\n\n  The extension is defined in the highlighted lines\n  The “settings” section of the extension is where you specify the VSTS account name, team project name, deployment group name and comma-separated list of tags for the agent. You also need to supply a PAT that has access to join machines to the Deployment Group\n  You can also specify a “Name” property if you want the agent name to be custom. By default it will be machineName-DG (so if the machine name is WebServer, the agent will be named WebServer-DG.\n\n\nNow you have a set of VMs that are bare-boned but have the VSTS agent installed. They are now ready for anything you want to throw at them - and you don’t need to worry about ports or firewalls or anything like that.\n\nThere are some useful extensions and patterns for configuring VMs such as Custom Script or Join Domain. The problem with these scripts is that the link to the script has to be either in a public place or in a blob store somewhere, or they assume existing infrastructure. This can complicate deployment. Either you need to publish your scripts publically or you have to deal with uploading scripts and generating SAS tokens. So I recommend just installing the VSTS agent and let it do everything else that you need to do - especially since the agent will download artifacts (like scripts and build binaries) as a first step in any deployment phase.\n\nTip 2: Use Tagging for Parallelization and Specialization\n\nTags are really important for Deployment Groups. They let you identify machines or groups of machines within a Deployment Group. Let’s say you have a load balanced application with two webservers and a SQL server. You’d probably want identical configuration for the webservers and a completely different configuration for the SQL server. In this case, tag two machines with WEBSERVER and the other machine with SQL. Then you’ll define the tasks in the phase  - when the phase runs, it executes all the tasks on all the machines that match the filter - for example, you can target all WEBSERVER machines with a script to configure IIS. These will execute in parallel (you can configure it to work serially if you want to) and so you’ll only specify the tasks a single time in the definition and you’ll speed up the deployment.\n\n\n\n\nBe careful though: multiple tags use AND (not OR) logic. This means if you want to do something like join a domain on machines with WEBSERVER and SQL, you would think you could specify WEBSERVER, SQL as the tag filter in the phase tag filter. But since the tags are joined with an AND, you’ll see the phase won’t match any machines. So you’d have to add a NODE tag (or something similar) and apply it to both webservers and SQL machine and then target NODE for things you want to do on all the machines.\n\n\n\n\nThe above image shows the tag filtering on the Phase settings. Note too the parallelization settings.\n\nTip 3: Use Phases to Start New Sessions\n\nAt my customer we were using Windows 2012 R2 machines. However, we wanted to use PowerShell DSC for configuring the VMs and you need Windows Management Framework 5.0 to get DSC. So we executed a PowerShell task to upgrade the PowerShell to 5.x:\n\nif ($PSVersionTable.PSVersion.Major -lt 5) {\n    $powershell5Url = \"https://go.microsoft.com/fwlink/?linkid=839516\"\n    wget -Uri $powershell5Url -OutFile \"wmf51.msu\"\n    Start-Process .\\wmf51.msu -ArgumentList '/quiet' -Wait\n}\n\n\nNotes:\n\n\n  Line 1: This script checks the major version of the current PowerShell\n  Lines 2,3: If it’s less than 5, then the script downloads PowerShell 5.1 (the path to the installer can be update to whichever PowerShell version you need)\n  Line 4: The installer is invoked with the quiet parameter\n\n\nHowever, if we then called a task right after the update task, we’d still get the old PowerShell since all tasks within a phase are executed in the same session. We just added another phase with the same Deployment Group settings - the second phase started a new session and we got the upgraded PowerShell.\n\nThis doesn’t work for environment variables though. When you set machine environment variables, you have to restart the agent. The VSTS team are working on providing a task to do this, but for now you have to reboot the machine. We’ll cover how to do this in Tip 10.\n\nTip 4: Update Your PowerShell PackageProviders and Install DSC Modules\n\nYou really should be using PowerShell DSC to configure Windows. The notation is succinct and fairly easy to read and Windows plays nicely with DSC. However, if you’re using custom modules (like xNetworking) you have to ensure that the modules are installed. You can pre-install all the modules so that your scripts can assume the modules are already installed. To install modules you’ll need to update your Package Providers. Here’s how to do it:\n\nImport-Module PackageManagement\nInstall-PackageProvider -Name NuGet -MinimumVersion 2.8.5.201 -Force\n\n\nYou’ll need to start a new Phase in order to pick up the new packages. Then you’ll be able to install modules:\n\nInstall-Module -Name xActiveDirectory -Force\nInstall-Module -Name xNetworking -Force\nInstall-Module -Name xStorage -Force\nInstall-Module -Name xDSCDomainjoin -Force\nInstall-Module -Name xComputerManagement -Force\n\n\nNot all machines need all the modules, but this step is so quick I found it easier to just enumerate and install all the modules anyway. That way I know that any machine could run any DSC script I throw at it.\n\nTip 5: Install Azure PowerShell\n\nIf you’re going to do anything against Azure from the VMs (in our case we were downloading binaries from a blob store) then you’ll want to use the Azure PowerShell task. This task provides an authenticated context (via a preconfigured endpoint) so you don’t have to worry about adding passwords or anything to your script. However, for it to work, you’ll need to install Azure PowerShell. Again this must be a separate phase so that subsequent phases can make use of the Azure cmdlets. To do this simply add a PowerShell task and run this line of script:\n\n\nInstall-Module AzureRM -AllowClobber -Force\n\nTip 6: Use PowerShell DSC for OS Configuration\n\nOS configuration can easily be specified by describing the state: is IIS installed or not? Which other OS roles are installed? So DSC is the perfect tool for this kind of work. You can use a single DSC script to configure a group of machines (or nodes, in DSC) but since we have the VSTS agent you can simply write your scripts for each machine using “node localhost”. DSC script are also (usually) idempotent - so they work no matter what state the environment is in when the script executes. No messy if statements to check various conditions - DSC does it for you.\n\nWhen you’re doing DSC, you should first check if there is a “native” resource for your action - for example, configuring Windows Features uses the WindowsFeature resource. However, there are some custom actions you may want to perform. There are tons of extensions out there - we used xActiveDirectory to configure an Active Directory Domain Controller settings, for example.\n\nThere are times when you’ll want to do some custom work that there simply is no custom module for. In that case, you’ll need to use the Script resource. The script resource is composed of three parts: GetScript, TestScript and SetScript. GetScript is optional and should return the current state as an object if specified. TestScript should return a boolean - true for “the state is correct” or false for “the state is not correct”. If TestScript returns a false, then the SetScript is invoked. Here’s an example Script we wrote to configure SMB on a machine according to Security requirements:\n\nScript SMBConfig\n{\n\tGetScript = { @{ Result = Get-SmbServerConfiguration } }\n\tTestScript =\n\t{\n\t\t$config = Get-SmbServerConfiguration\n\t\t$needConfig = $config.EnableSMB2Protocol -and (-not ($config.EnableSMB1Protocol))\n\t\tif ($needConfig) {\n\t\t\t\tWrite-Host \"SMB settings are not correct.\" \n\t\t}\n\t\t$needConfig\n\t}\n\tSetScript =\n\t{\n\t\tWrite-Host \"Configuring SMB settings\" \n\t\tSet-SmbServerConfiguration -EnableSMB1Protocol $false -Force\n\t\tSet-SmbServerConfiguration -EnableSMB2Protocol $true -Force\n\t}\n}\n\n\nNotes:\n\n\n  Line 1: Specify the type of resource (Script) and a unique name\n  Line 3: The GetScript returns an hash table with a Result property that describes the current state - in this case, the SMB settings on the machine\n  Line 4: The start of the TestScript\n  Line 6: Query the SMB settings\n  Line 7: Determine if we need to configure anything or not - this is a check on the SMBProtocol states\n  Lines 8-10: Write a message if we do need to set state\n  Line 11: return the bool: true if the state is correct, false otherwise\n  Lines 16-17: correct the state of the machine - in this case, set protocols accordingly\n\n\nTip 7: Use Plain PowerShell for Application Install and Config\n\nExpressing application state and configuration as a DSC script can be challenging. I once wrote some DSC that could install SQL. However, I ended up using a Script resource - and the TestScript just checked to see if a SQL service was running. This check isn’t enough to determine if SQL features are installed according to some config.\n\nInstead of writing long Script resources, I just revert to “plain” PowerShell for app install and configuration. This is especially true for more complicated apps. Just make sure your script are idempotent - that is that they can run and succeed every time. For example, if you’re installing a service, you may want to first check to see if the service exists before running the installer (otherwise the installer may fail since the service already exists). This allows you to re-run scripts again if other scripts fail.\n\nTip 8: Attaching Data Disks in Azure VMs\n\nIf you’re creating data disks for your VMs, then you usually specify the size and type of disk in the ARM template. But even if you add a disk, you need to attach it in the OS. To do this, I used the xStorage DSC extension. This requires a disk number. When we started, the data disk was always disk 2. Later, we added Azure Disk Encryption - but this added another disk and so our disk numbers were off. We ended up needing to add some logic to determine the data disk number and pass that in as a parameter to the DSC configuration:\n\nConfiguration DiskConfig\n{\n\tparam\n\t(\n\t\t[Parameter(Mandatory)]\n\t\t[string]$dataDiskId\n\t)\n\n\t# import DSC Resources \n\tImport-DscResource -ModuleName PSDscResources\n\tImport-DscResource -ModuleName xStorage\n\n\tNode localhost\n\t{\n        LocalConfigurationManager\n\t\t{\n\t\t\tActionAfterReboot = 'ContinueConfiguration'\n\t\t\tConfigurationMode = 'ApplyOnly'\n\t\t\tRebootNodeIfNeeded = $true\n\t\t}\n\n        xWaitforDisk DataDisk\n        {\n            DiskId = $dataDiskId\n            RetryIntervalSec = 60\n            RetryCount = 3\n        }\n\n        xDisk FVolume\n        {\n            DiskId = $dataDiskId\n            DriveLetter = 'F'\n            FSLabel = 'Data'\n            DependsOn = \"[xWaitforDisk]DataDisk\"\n        }\n    }\n}\n\n# work out what disk number the data disk is on\n$dataDisks = Get-Disk -FriendlyName \"Microsoft Virtual Disk\" -ErrorAction SilentlyContinue\nif ($dataDisk -eq $null) {\n\t$dataDisk = Get-Disk -FriendlyName \"Microsoft Storage Space Device\" -ErrorAction SilentlyContinue\n}\n# filter to GPT partitions\n$diskNumber = 2\nGet-Disk | Out-Host -Verbose\n$dataDisk = Get-Disk | ? { $_.PartitionStyle -eq \"RAW\" -or $_.PartitionStyle -eq \"GPT\" }\nif ($dataDisk -eq $null) {\n\tWrite-Host \"Cannot find any data disks\"\n} else {\n\tif ($dataDisk.GetType().Name -eq \"Object[]\") {\n\t\tWrite-Host \"Multiple data disks\"\n\t\t$diskNumber = $dataDisk[0].Number\n\t} else {\n\t\tWrite-Host \"Found single data disk\"\n\t\t$diskNumber = $dataDisk.Number\n\t}\n}\nWrite-Host \"Using $diskNumber for data disk mounting\"\n\nDiskConfig -ConfigurationData .\\ConfigurationData.psd1 -dataDiskId \"$($diskNumber)\"\nStart-DscConfiguration -Wait -Force -Path .\\DiskConfig -Verbose \n\n\nNotes:\n\n\n  Lines 3-7: We specify that the script requires a dataDiskId parameter\n  Lines 11,12: Import the modules we need\n  Lines 23-28: Wait for disk with number $dataDiskId to be available (usually it was immediately anyway)\n  Lines 30-36: Mount the disk and assign drive letter F with a label of Data\n  Lines 41-43: Get potential data disks\n  Lines 46-59: Calculate the data disk number, defaulting to 2\n  Lines 62,63: Compile the DSC and the invoke the configuration manager to “make it so”\n\n\nTip 9: Configuring DNS on Azure VNets\n\nIn our example, we needed a Domain Controller to be on one of the machines. We were able to configure the domain controller using DSC. However, I couldn’t get the other machines to join the domain since they could never find the controller. Eventually I realized the problem was a DNS problem. So we added the DNS role to the domain controller VM. We also added a private static IP address for the domain controller so that we could configure the VNet accordingly. Here’s a snippet of the DSC script for this:\n\nWindowsFeature DNS\n{\n        Ensure = \"Present\"\n        Name = \"DNS\"\n        DependsOn = \"[xADDomain]ADDomain\"\n}\n\nxDnsServerAddress DnsServerAddress\n{\n        Address = '10.10.0.4', '127.0.0.1'\n        InterfaceAlias = 'Ethernet 2'\n        AddressFamily = 'IPv4'\n        DependsOn = \"[WindowsFeature]DNS\"\n}\n\n\nNotes:\n\n\n  Lines 1-6: Configure the DNS feature\n  Lines 8-14: Configure the network DNS NIC using the static private IP 10.10.0.4\n\n\nNow we needed to configure the DNS on the Azure VNet to use the domain controller IP address. We used this script:\n\nparam($rgName, $vnetName, $dnsAddress)\n$vnet = Get-AzureRmVirtualNetwork -ResourceGroupName $rgName -Name $vnetName\nif ($vnet.DhcpOptions.DnsServers[0] -ne $dnsAddress) {\n    $vnet.DhcpOptions.DnsServers = @($dnsAddress)\n    Set-AzureRmVirtualNetwork -VirtualNetwork $vnet\n}\n\n\nNotes:\n\n\n  Line 2: Get the VNet using the resource group name and VNet name\n  Line 3: Check if the DNS setting of the VNet is correct\n  Lines 4,5: If it’s not, then set it to the internal IP address of the DNS server\n\n\nThis script needs to run as an Azure PowerShell script task so that it’s already logged in to an Azure context (the equivalent of running Login-AzureRMAccount -ServicePrincipal). It’s sweet that you don’t have to provide any credentials in the script!\n\nNow that we’ve set the DNS on the VNet, we have to reboot every machine on the VNet (otherwise they won’t pick up the change). That brings us to the final tip.\n\nTip 10: Wait on Machines When They Reboot\n\nYou can easily reboot a machine by running this (plain) PowerShell:\n\n\nRestart-Machine -ComputerName localhost -Force\n\n\n. This is so simple that you can do it as an inline PowerShell task:\n\n\n\n\nRebooting the machine is easy: it’s waiting for it to start up again that’s more challenging. If you have a task right after the reboot task, the deployment fails since the agent goes offline. So you have to build in a wait. The simplest method is to add an agentless phase and add a Delay task:\n\n\n\n\nHowever, you can be slightly more intelligent if you poll the machine states using some Azure PowerShell:\n\nparam (\n    [string]$ResourceGroupName,\n    [string[]]$VMNames = @(),\n    $TimeoutMinutes = 2,\n    $DelaySeconds = 30\n)\n\nWrite-Host \"Delay for $DelaySeconds seconds.\"\nStart-Sleep -Seconds $DelaySeconds\n\nif($VMNames.Count -eq 0)\n{\n    $VMNames = (Get-AzureRmVm -ResourceGroupName $ResourceGroupName).Name\n    Write-Host \"Getting VM names.\"\n}\n\n$seconds = 10\n$desiredStatus = \"PowerState/running\"\n\nforeach($vmName in $VMNames)\n{\n    $timer = [Diagnostics.Stopwatch]::StartNew()\n    Write-Host \"Getting statuses of VMs.\"\n    $statuses = (Get-AzureRmVm -ResourceGroupName $ResourceGroupName -VMName $vmName -Status).Statuses\n    $status = $statuses | Where-Object { $_.Code -eq $desiredStatus }\n    while($status -eq $null -and ($timer.Elapsed.TotalMinutes -lt $TimeoutMinutes))\n    {\n        Write-Verbose \"Retrying in $($seconds) seconds.\"\n        Start-Sleep -Seconds $seconds\n        $statuses = (Get-AzureRmVm -ResourceGroupName $ResourceGroupName -VMName $vmName -Status).Statuses\n        $status = $statuses | Where-Object { $_.Code -eq $desiredStatus }\n    }\n\n    if($timer.Elapsed.TotalMinutes -ge $TimeoutMinutes)\n    {\n        Write-Error \"VM restart exceeded timeout.\"\n    }\n    else\n    {\n        Write-Host \"VM name $($vmName) has current status of $($status.DisplayStatus).\"\n    }\n}\n\n\nNotes:\n\n\n  The script requires a resource group name, an optional array of machine names (otherwise it will poll all the VMs in the resource group), a delay (defaulted to 30 seconds) and a timeout (defaulted to 2 minutes)\n  The script will delay for a small period (to give the machines time to start rebooting) and then poll them until they’re all running or the timeout is reached.\n\n\nThis script has to run in an Azure PowerShell task in an agent phase from either the Hosted agent or a private agent - you can’t run it in a Deployment Group phase since those machines are rebooting!\n\nConclusion\n\nDeployment Groups are very powerful - they allow you to dynamically target multiple machines and execute configuration in a local context. This makes complex environment provisioning and configuration much easier to manage. However, it’s always good to know limitations, gotchas and practical tips when designing a complex deployment workflow. Hopefully these tips and tricks make your life a bit easier.\n\nHappy deploying!\n",
      "categories": [],
      "tags": ["releasemanagement","devops"],
      
      "collection": "posts",
      "url": "/tips-and-tricks-for-complex-iaas-deployments-using-vsts-deployment-groups/"
    },{
      
      "title": "Using Linked ARM Templates with VSTS Release Management",
      "date": "2018-01-23 04:29:33 +0000",
      
      "content": "If you’ve ever had to create a complex ARM template, you’ll know it can be a royal pain. You’ve probably been tempted to split out your giant template into smaller templates that you can link to, only to discover that you can only link to a sub-template if the sub-template is accessible via some public URI. Almost all of the examples in the Template Quickstart repo that have links simply refer to the public Github URI of the linked template. But what if you want to refer to a private repo of templates?\n\nUsing Blob Containers\n\nThe solution is to use blob containers. You upload the templates to a private container in an Azure Storage Account and then create a SAS token for the container. Then you create the full file URI using the container URI and the SAS token. Sounds simple, right? Fortunately with VSTS Release Management, it actually is easy.\n\nAs an example, let’s look at this template that is used to create a VNet and some subnets. First we’ll look at the VNet template (the linked template) and then how to refer to it from a parent template.\n\nThe Child Template\n\n{\n  \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",\n  \"contentVersion\": \"1.0.0.0\",\n  \"parameters\": {\n    \"vnetName\": {\n      \"type\": \"string\"\n    },\n    \"vnetPrefix\": {\n      \"type\": \"string\"\n    },\n    \"subnets\": {\n      \"type\": \"object\"\n    }\n  },\n  \"variables\": {\n  },\n  \"resources\": [\n    {\n      \"name\": \"[parameters('vnetName')]\",\n      \"type\": \"Microsoft.Network/virtualNetworks\",\n      \"location\": \"[resourceGroup().location]\",\n      \"apiVersion\": \"2016-03-30\",\n      \"dependsOn\": [],\n      \"tags\": {\n        \"displayName\": \"vnet\"\n      },\n      \"properties\": {\n        \"addressSpace\": {\n          \"addressPrefixes\": [\n            \"[parameters('vnetPrefix')]\"\n          ]\n        }\n      }\n    },\n    {\n      \"apiVersion\": \"2015-06-15\",\n      \"type\": \"Microsoft.Network/virtualNetworks/subnets\",\n      \"tags\": {\n        \"displayName\": \"Subnets\"\n      },\n      \"copy\": {\n        \"name\": \"iterator\",\n        \"count\": \"[length(parameters('subnets').settings)]\"\n      },\n      \"name\": \"[concat(parameters('vnetName'), '/', parameters('subnets').settings[copyIndex()].name)]\",\n      \"location\": \"[resourceGroup().location]\",\n      \"dependsOn\": [\n        \"[parameters('vnetName')]\"\n      ],\n      \"properties\": {\n        \"addressPrefix\": \"[parameters('subnets').settings[copyIndex()].prefix]\"\n      }\n    }\n  ],\n  \"outputs\": {\n  }\n}\n\n\nNotes:\n\n\n  There are 3 parameters: the VNet name and prefix (strings) and then an object that contains the subnet settings\n  The first resource is the VNet itself - nothing complicated there\n  The second resource uses copy to create 0 or more instances. In this case, we’re looping over the subnets.settings array and creating a subnet for each element in that array, using copyIndex() as the index as we loop\n\n\nThere’s really nothing special here - using a copy is slightly more advanced, and the subnets parameter is a complex object. Otherwise, this is plain ol’ ARM json.\n\nThe Parent Template\n\nThe parent template has two things that are different from “normal” templates: it needs two parameters (containerUri and containerSasToken) that let it refer to the linked (child) template and it invokes the template by specifying a “Microsoft.Resources/deployments” resource type. Let’s look at an example:\n\n{\n  \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",\n  \"contentVersion\": \"1.0.0.0\",\n  \"parameters\": {\n    \"containerUri\": {\n      \"type\": \"string\"\n    },\n    \"containerSasToken\": {\n      \"type\": \"string\"\n    }\n  },\n  \"variables\": {},\n  \"resources\": [\n    {\n      \"apiVersion\": \"2017-05-10\",\n      \"name\": \"linkedTemplate\",\n      \"type\": \"Microsoft.Resources/deployments\",\n      \"properties\": {\n        \"mode\": \"incremental\",\n        \"templateLink\": {\n          \"uri\": \"[concat(parameters('containerUri'), '/Resources/vNet.json', parameters('containerSasToken'))]\",\n          \"contentVersion\": \"1.0.0.0\"\n        },\n        \"parameters\": {\n          \"vnetName\": { \"value\": \"testVNet\" },\n          \"vnetPrefix\": { \"value\": \"10.0.0.0/16\" },\n          \"subnets\": {\n            \"value\": {\n              \"settings\": [\n                {\n                  \"name\": \"subnet1\",\n                  \"prefix\": \"10.0.0.0/24\"\n                },\n                {\n                  \"name\": \"subnet2\",\n                  \"prefix\": \"10.0.1.0/24\"\n                }\n              ]\n            }\n          }\n        }\n      }\n    }\n  ],\n  \"outputs\": {}\n}\n\n\nNotes:\n\n\n  There are two parameters that pertain to the linked template: the containerUri and the SAS token\n  In the resources, there is a “Microsoft.Resources/deployment” resource - this is how we invoke the child template\n  In the templateLink, the URI is constructed by concatenating the containerUri, the path to the child template within the container, and the SAS token\n  Parameters are passed inline - note that even simple parameters look like JSON objects (see vNetName and vnetPrefix)\n\n\nInitially I tried to make the subnets object an array: but this blew up on the serialization. So I made an object called “settings” that is an array. So the subnets value property is an object called “settings” that is an array. You can look back at the child template to see how I dereference the object to get the values: to get the name of a subnet, I use “parameters(‘subnet’).settings[index].name” (where index is 0 or 1 or whatever). The copy uses the length() method to get the number of elements in the array and then I can use copyIndex() to get the current index within the copy.\n\nOf course the parent template can contain other resources - I just kept this example really simple to allow us to zoom in on the linking bits.\n\nSource Structure\n\nHere’s a look at how I laid out the files in the Azure Resource Group project:\n\n\n\n\nYou can see how the vNet.json (the child template) is inside a folder called “Resources”. I use that as the relative path when constructing the URI to the child template.\n\nThe Release Definition\n\nNow all the hard work is done! To get this into a release, we just create a storage account in Azure (that we can copy the templates to) and we’re good to go.\n\nNow create a new release definition. Add the repo containing the templates as a release artifact. Then in your environment, drop two tasks: Azure File Copy and Azure Resource Group Deployment. We configure the Azure File Copy task to copy all our files to the storage account into a container called templates. We also need to give the task two variable names: one for the containerUri and one for the SAS token:\n\n\n\n\nOnce this task has executed, the templates will be available in the (private) container with the same folder structure as we have in Visual Studio.\n\nOn the next task, we can select the parent template as the template to invoke. We can pass in any parameters that are needed - at the very least, we need the containerUri and SAS token, so we pass in the variables from the previous task using $() notation:\n\n\n\n\nNow we can run the release and voila - we’ll have a vNet with two subnets.\n\nConclusion\n\nRefactoring templates into linked templates is good practice - it’s DRY (don’t repeat yourself) and can make maintenance of complicated templates a lot easier. Using VSTS Release Management and a storage container, we can quickly, easily and securely make linked templates available and it all just works ™.\n\nHappy deploying!\n",
      "categories": [],
      "tags": ["cloud","releasemanagement"],
      
      "collection": "posts",
      "url": "/using-linked-arm-templates-with-vsts-release-management/"
    },{
      
      "title": "Using VSTS to Test Python Code (with Code Coverage)",
      "date": "2018-02-25 02:45:11 +0000",
      
      "content": "I recently worked with a customer that had some containers running Python code. The code was written by data scientists and recently a dev had been hired to help the team get some real process in place. As I was helping them with their CI/CD pipeline (which used a Dockerfile to build their image, publish to Azure Container Registry and then spun up the containers in Azure Container Instances), I noted that there were no unit tests. Fortunately the team was receptive to adding tests and I didn’t have to have a long discussion proving that they absolutely need to be unit testing.\n\nPython Testing with PyTest\n\nI’ve done a bit of Python programming, but am by no means an expert. However, after a few minutes of searching I realized there are a ton of good Python frameworks out there. One I came across is PyTest (which is cool since it’s very pythonic). This great post (Python Testing 101: PyTest) from Andy Knight even had a Github repo with some code and tests. So when I got back to the hotel, I forked the repo and was able to quickly spin up a build definition in VSTS that runs the tests - with code coverage!\n\nContinuous Testing with VSTS\n\nAt a high level, here are the steps that your build has to perform:\n\n\n  Clone the repo\n  Install PyTest and other packages required\n  Run PyTest, instructing it to output the results to (JUnit) XML and produce (Cobertura) coverage reports\n  Publish the test results\n  (Optional) Fix the styling of the HTML coverage reports\n  Publish the coverage reports\n\n\nWhy the “fix styles” step? When we create the HTML coverage reports (so that you can see which lines of code are covered and which are not) we publish them to VSTS. However, for security reasons VSTS blocks the css styling when viewing these reports in the Build Summary page. Fortunately if you inline the styles, you get much prettier reports - so we use a node.js package to do this for us.\n\nFortunately I’ve already done this and published the VSTS build definition JSON file in my forked repo. Here’s how you can import the code, import the CI definition and run it so you can see this in action yourself.\n\nImport the Repo from Github\n\nThe source code is in a Github repo - no problem! We’ll import into VSTS and then we can mess with it. We can even fork the Github repo, then import it - that way we can sumbit pull requests on Github for changes we make. In this case, I’ll just import the repo directly without forking.\n\nLog in to VSTS and navigate to the Code hub. Then click on the repo button in the toolbar and click Import repository.\n\n\n\n\nEnter the URL and click Import.\n\n\n\n\nNow we have the code in VSTS! Remember, this is just Git, so this is just another remote (that happens to be in VSTS).\n\nImport the Build Definition\n\nNow we can import the build definition. First, navigate to the Github repo and clone it or download the PythonTesting-CI.json file. Then open VSTS and navigate to a team project and click on Build and Release (in the Blue toolbar at the top) to navigate to the build and release hub. Click on Builds (in the grey secondary toolbar) and click the “+Import” button.\n\n\n\n\nIn the import dialog, browse to the json file you downloaded previously and click Import.\n\nYou’ll then see the build definition - there are a couple things that need to be fixed, but the steps are all there.\n\n\n\n\nNote how the Agent queue is specifying “Hosted Linux Preview” - yep, this is running on a Linux VM. Now you don’t have to do this, since Python will run on Windows, but I like the Linux agent - and it’s fast too! Rename the definition if you want to.\n\nNow we’ll fix the “Get sources” section. Click on “Get sources” to tell the build where to get the sources from. Make sure the “This account” tile is selected and then set the repository to “python-testing-101” or whatever you named your repo when you imported. You can optionally set Tag sources and other settings.\n\n\n\n\nOne more addition: click on the Triggers tab and enable the CI trigger:\n\n\n\n\nNow you can click Save and queue to queue a new build! While it’s running, let’s look at the tasks.\n\n\n  Install Packages: this Bash task uses curl to get the pip install script, then install pip and finally install pytest and pytest-cov packages. If you’re using a private agent you may not have to install pip, but pip doesn’t come out of the box on the Hosted Linux agent so that’s why I install it.\n  Run Tests: invoke python -m pytest (which will run any tests in the current folder or subfolders), passing –junitxml=testresults.xml and the pycov args to create both an XML and HTML report\n  Install fix-styles package: this just runs “npm install” in the root directory to install this node.js package (you can see this if you look at the package.json file)\n  Fix styles: we run the “fix” script from the package.json file, which just invokes the fix-styles.js file to inline the styling into the HTML coverage reports\n  Publish Test Results: we publish the XML file, which is just a JUnit test result XML file. Note how under Control Options, this task is set to run “Even if a previous task has failed, unless the build was cancelled”. This ensures that the publish step works even when tests fail (otherwise we won’t get test results when tests fail).\n  Publish Code Coverage: This task published the XML (Cobertura) coverage report as well as the (now style-inlined) HTML reports\n\n\nReally simple! Let’s navigate to the build run (click on the build number that you just queued) and - oh dear, the tests failed!\n\n\n\n\nSeems there is a bug in the code. Take a moment to see how great the test result section is - even though there are failing tests. Then click on Tests to see the failing tests:\n\n\n\n\nAll 4 failing tests have “subtract” in them - easy to guess that we have a problem in the subtract method! If we click on a test we can also see the stack trace and the failed assertions from the test failure. Click on the “Bug” button above the test to log a bug with tons of detail!\n\n\n\n\nJust look at that bug: with a single button click we have exception details, stack traces and links to the failing build. Sweet!\n\nNow let’s fix the bug: click on the Code hub and navigate to example-py-pytest/com/automationpanda/example and click on the calc_func.py file. Yep, there’s a problem in the subtract method:\n\n\n\n\nClick on the Edit button and change that pesky + to a much better -. Note, this isn’t what you’d usually do - you’d normally create a branch from the Bug, pull the repo, fix the bug and push. Then you’d submit a PR. For the sake of this blog, I’m just fixing the code in the code editor.\n\nClick the Commit button to save the change. In “Work items to link” find the Bug that we created earlier and select it. Then click Commit.\n\n\n\n\nThe commit will trigger a new build! Click on Build and you’ll see a build is already running. Click on the build number to open the build.\n\nThis time it’s a success! Click on the build number to see the report - this time, we see all the tests are passing and we have 100% coverage - nice!\n\n\n\n\nIf you click on “Code Coverage*” just below the header, you’ll see the (prettified) HTML reports. Normally you won’t have 100% coverage and you’ll want to see which methods have coverage and which don’t - you would do so by browsing your files here and noting which lines are covered or not by the color highlighting:\n\n\n\n\nAlso note that we can see that this build is related to the Bug (under Associated work items). It’s almost like we’re professional developers…\n\nConclusion\n\nJust because there is “Visual Studio” in the name, it doesn’t mean that VSTS can’t do Python - and do it really, really well! You get detailed test logging, continuous integration, code coverage reports and details - and for very little effort. If you’re not testing your Python code - just do it ™ with VSTS!\n",
      "categories": [],
      "tags": ["testing"],
      
      "collection": "posts",
      "url": "/using-vsts-to-test-python-code-with-code-coverage/"
    },{
      
      "title": "Using Chrome to Solve Identity Hell",
      "date": "2018-03-08 01:33:59 +0000",
      
      "content": "This week at MVP summit, I showed some of my colleagues a trick that I use to manage identity hell. I have several accounts that I use to access VSTS and the Azure Portal: my own Microsoft Account (MSA), several org accounts and customer org accounts. Sometimes I want to open a release from my 10th Magnitude VSTS account so that I can grab some tasks to put into CustomerX VSTS release. The problem is that if I open the 10M account in a browser, and then open a new browser, I have to sign out of the 10M account and sign in with the CustomerX account and then the windows break… identity hell.\n\nAt first I used to open InPrivate or Incognito windows. That gave me the ability to get to 4 different profiles: IE and IE InPrivate, Chrome and Chrome Incognito. But then my incognito windows don’t have cached identities or history or anything that I like to have in my browser. Hacky - very hacky.\n\nSolution: Chrome People\n\nAbout 2 years ago I stumbled onto Chrome People (or Profiles). This really simple “trick” has been fantastic and I almost never open Incognito anymore. In the upper right of the Chrome chrome (ahem) there is a little text that tells you what your current “person” is:\n\n\n\n\nClick that text to open the People hub:\n\n\n\n\nHere you can see that I have 5 People: ColinMSA, 10M, AdminNWC and NWC and another customer profile. To switch profiles, I just click on the name. To add a person, just click “Manage people”.\n\n\n\n\nI can easily add a new person from this view - and I can assign an icon to the person.\n\nWhen you create a new person, Chrome creates a shortcut to that person’s browser on the desktop. I end up clicking on that and adding it to my taskbar:\n\n\n\n\nIf I want to open up the Azure Portal or VSTS using my MSA, I click the ColinMSA icon and I’m there. If I need to open my customer VSTS or Portal, I just click that icon. Each window is isolated and my identities don’t leak. Very neat, very clean. Under the hood, the shortcuts just add a small arg to the Chrome.exe launcher:\n\n\n--profile-directory=\"Profile 1\"\n\n\n. The first profile is Default, the second is Profile 1, the third Profile 2 and so on.\n\nFinal Thoughts\n\nYou can also do something similar in FireFox, but I like Chrome. This simple trick helps me sort out my identity hell and I can quickly switch to different identity contexts without having to sign in and out all the time. For my MSA I sign into my Google account, but I don’t do that for the other browsers. All in all it’s a great way to manage multiple identities.\n\nHappy browsing!\n",
      "categories": [],
      "tags": ["development"],
      
      "collection": "posts",
      "url": "/using-chrome-to-solve-identity-hell/"
    },{
      
      "title": "Tip: Creating Task Groups with Azure Service Endpoint Parameters",
      "date": "2018-04-26 02:03:43 +0000",
      
      "content": "I’ve been working on some pretty complicated infrastructure deployment pipelines using my release management tool of choice (of course): VSTS Release Management. In this particular scenario, we’re deploying a set of VMs to a region. We then want to deploy exactly the same setup but in a different region. Conceptually, this is like duplicating infrastructure between different datacenters.\n\nHere’s what the DEV environment in a release could look like:\n\n\n\n\nIf we’re duplicating this to 5 regions, we’d need to clone the environment another 4 times. However, that would mean that any updates to any tasks would need to be duplicated over all 5 regions. It’s easy to forget to update or to fat-finger a copy - isn’t there a better way to maintain sets of tasks? I’m glad you asked…\n\nDRY - Don’t Repeat Yourself\n\nDRY (Don’t Repeat Yourself) is a common coding practice - any time you find yourself copying code, you should extract it into a function so that you only have to maintain that logic in a single place. We can do the same thing in a release (or build) using Task Groups. Task Groups are like functions that you can call from releases (or builds) from many places - but maintain in a single place. Just like functions, they have parameters that you can set when you “call” them. Click the selector (checkmark icon to the right of each task) to select all the tasks you want to group, right-click and select “Create task group”:\n\n\n\n\nA popup asks for the name of the Task Group and bubbles up all the parameters that are used in the tasks within the group. You can update the defaults and descriptions and click Create (helpful hint: make variables for all the values so that the variable becomes the default rather than a hard-coded value - this will make it easier to re-use the Task Group when you clone environments later):\n\n\n\n\nSo far, so good:\n\n\n\n\nHowever, there’s a snag: looking at the parameters section, you’ll notice that we don’t have any parameter for the Azure Service Endpoint. Let’s open the tasks and update the value in the dropdown to $(AzureSubscription):\n\n\n\n\nNow you can see that the parameter is bubble up and surfaced as a parameter on the Task Group - it even has the dropdown with the Service Endpoints. Nice!\n\n\n\nConsuming the Task Group\n\nOpen up the release again. You’ll see that you now have a new parameter on the Task Group: the AzureSubscription. We’ll select the DEV sub from the dropdown.\n\n\n\n\nAlso note how the phase is now a single “task” (which is just a call to the Task Group). Under the hood, when the release is created, Release Management deletes the task group and replaces it with the tasks from the Task Group - so any values that are likely to change or be calculated on the fly should be variables.\n\nLet’s now clone the DEV environment to UAT-WESTUS and to UAT-EASTUS.\n\n\n\n\nIf we edit the UAT-WESTUS, we can edit the service endpoint (and any other parameters) that we need to for this environment:\n\n\n\n\nExcellent! Now we can update the Task Group in a single place even if we’re using it in dozens of environments. Of course you’d need to update the other parameter values to have environment-specific values (Scopes) in the Variables section.\n\n\n\nConclusion\n\nTask Groups are a great way to keep your releases (or builds) DRY - even allowing you to parameterize the Azure Service Endpoint so that you can duplicate infrastructure across different subscriptions or regions in Azure.\n\nHappy deploying!\n",
      "categories": [],
      "tags": ["releasemanagement"],
      
      "collection": "posts",
      "url": "/tip-creating-task-groups-with-azure-service-endpoint-parameters/"
    },{
      
      "title": "VSTS, One Team Project and Inverse Conway Maneuver",
      "date": "2018-05-03 20:26:35 +0000",
      
      "content": "There are a lot of ALM MVPs that advocate the “One Team Project to Rule Them All” when it comes to Visual Studio Team Services (VSTS) and Team Foundation Server (TFS). I’ve been recommending it for a long time to any customer I work with. My recommendation was based mostly on experience - I’ve experienced far too much pain when organizations have multiple Team Projects, or even worse, multiple Team Project Collections.\n\nWhile on a flight to New Jersey I watched a fantastic talk by Allan Kelley titled Continuous Delivery and Conway’s Law. I’ve heard about Conway’s Law before and know that it is applied to systems design. A corollary to Conway’s Law, referred to as Inverse Conway Maneuver, is to structure your organization intentionally to promote a desired system architecture. This has a lot of appeal to me with regards to DevOps - since DevOps is not a tool or a product, but a culture: a way of thinking.\n\nWith these thoughts in mind, as I was watching Kelley’s talk I had an epiphany: you can perform an Inverse Conway Maneuver by the way you structure your VSTS account or TFS install!\n\nWhat is Conway’s Law?\n\nIn April 1968, Mel Conway published a paper called “How Do Committees Invent?” The central thesis of this paper is this: “Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization’s communication structure.” In other words, the design of your  organizational and team structures will impose itself on your system designs. At least, if they are out of sync, you will experience friction. The Inverse Conway Maneuver recognizes Conway’s Law and makes it intentional: use organizational and team structure to promote desired systems design. For example, distributed teams tend to develop more modular products, while centralized teams tend to develop monoliths.\n\nHistorical side-note: Conway’s paper was rejected by Harvard Business Review in 1967 since Conway had “failed to prove his thesis”. Ironically, a team of MIT and Harvard Business School researchers published a paper in 2015 which found “strong evidence” to support the hypothesis.\n\nHow does this apply to VSTS and TFS? I’ll explain, but it’s awkward having to type “VSTS and TFS”. For the remainder of this blog I’ll just write VSTS - but the same principles apply to TFS: the VSTS account is like a TFS Team Project Collection. If we equate VSTS account to Team Project Collection, then the rest of the hierarchy (Team Project, Team etc.) is exactly equivalent. In short, when I say VSTS account I also mean TFS Team Project Collection.\n\nOne Objective: Deliver Value to End Users\n\nIn days gone by, IT was a service center to Business. Today, most organizations are IT companies - irrespective of the industry they operate in. Successful businesses are those that embrace the idea that IT is a business enabler and differentiator, not just a cost center. There should be very little (if any) division between “business” and “IT” - there is one team with one goal: deliver value to customers. Interestingly the definition of DevOps, according to Donovan Brown (Principal DevOps Manager at Microsoft), is “the union of people, process and products to enable continuous delivery of value to our end  users” (emphases mine).\n\nOne Objective means everyone is aligned to the overall goal of the business. If you look at two of the Principles behind the Agile Manifesto:\n\n\n  Business people and developers must work together daily throughout the project.\n  The best architectures, requirements, and designs emerge from self-organizing teams.\n\n\nyou’ll see a common theme: aligning everyone to the One Objective. The point I’m making is that there needs to be a “one team” culture that permeates the organization. DevOps is cultural before it’s about tools and products. But putting the thinking into practice is no easy task. Fortunately, having the correct VSTS structures supports an Inverse Conway Maneuver.\n\nOne Team Project\n\nSo how do you use VSTS for an Inverse Conway Maneuver? _ You have a single VSTS account with a single Team Project _.\n\nHaving all the work in a single Team Project allows you to view work at a “portfolio” (or organizational) level - that is, across the entire organization. This is (currently) impossible to do with multiple VSTS accounts and very difficult with multiple Team Project Collections. Even viewing portfolio level information with  multiple Team Projects can be difficult. Work item queries are scoped to Team Projects by default; widgets, dashboards, builds, releases, package feeds, test plans - these all live at Team Project level. If you have multiple Team Projects you’ve probably experienced a fragmented view of work across the organization. Interestingly, that’s probably not only from a VSTS point of view, but this structure (by Conway’s Law) is probably responsible for silos within the organization.\n\nHence the recommendation for a single “Team Project to Rule Them All.” Not only will this allow anyone to see work at a portfolio level, but this allows teams to share source repositories, build definitions, release definitions, reports and package feeds. It’s a technical structure that encourages the One Objective.\n\nTeams\n\nI can hear you already: “But I have 500 developers/analysts/testers/DBAs/Ops managers (let’s say engineers, shall we?) - how can I possibly organize them under a single team project?” That’s where Teams come in. Teams allow organizations to organize work into manageable sets. When you’re an engineer and you want to deliver value, you probably only need a general idea of the One Objective, rather than having to know the minutia of every bit of work across the entire organization. Having your team’s work in a separate ring-fenced area allows you to focus on what you need day-to-day. You can go up to the portfolio level when you need a wider context - but you probably don’t need that every day. Leadership will more likely spend most of their time looking at work at the portfolio level rather than all the way down to the minutia of the team-level work.\n\nSo how should you organize your teams? Again, Conway’s Law is going to have enormous impact here. Do you have a 3-tier application? Then you might be tempted to create a DBA Team, a Service Team and a UI Team. Perhaps create a Mobile team and a Data Analytics Team too. Surely that’s reasonable, right?\n\nThe answer, to quote Consultese (the dialect of the consultant) is: It Depends. Perhaps that is the way to go since that is how your application is architected. But that could be boxing you in: horizontally composed teams violate the Agile principle of cross-functional teams. A better approach is to have your teams composed around functional area or module. Where possible, they should be loosely coupled. Again by Conway’s Law this will start reflecting in your app architecture - and you’ll start seeing your applications become loosely coupled services. Have you ever wondered why micro-services are so popular today? Could it be the Agile movement started to break huge monolithic organizations into small, loosely-coupled, cross-functional and self-organizing teams, and now we’re starting to see that reflected in our architecture? Inverse Conway Maneuvers at work.\n\nIn short, create Teams around functional areas and use Area Paths to denote ownership of that work. If an Epic/Feature/Story belongs to a team, put it in the Area Path for that team and it appears on their backlogs. Another tip is that your Area Paths should be durable (long-lived) while your work items should not: work items should have a definite start and end date. Don’t make an Epic for “Security” since that’s not likely to end at a specific date. Rather, have an Area Path for Security and place work items in that area path.\n\nOrganizational Dimensions in VSTS\n\nThere are four axes that most organizations use to organize work: functional area, iteration, release and team. Unfortunately, VSTS only really gives us two: Area and Iteration. While Release Management in VSTS is brilliant, there isn’t yet a first-class citizen for the concept of a Release. And while you can create a custom Team Field in TFS and slice teams on that field, you can’t do so in VSTS, so you have to munge Team and Area Path together. In my experience it’s best not to fight these limits: use Area Path to denote Team, use iterations to time-box, and if you really need a Release concept, add a custom field.\n\nAreas and Work Item States\n\nOrganizations will still need inter-team communication, but this should be happening far less frequently that intra-team communication. That’s why we optimize for intra-team communication. It’s also why co-locating a team wherever possible is so important. If you do this, then by Conway’s Law you are more likely to end up with modules that are stable, resilient, independent and optimized.\n\nWe’ve already established that vertical Teams are tied to Area Paths. Each Team “owns” a root area path, typically with the same name as the Team. This is the area path for the team’s backlog. The team can then create sub-areas if they need do (leave this up to the team - they’re self-organizing after all). Kanban boards can be customized at the team level, so each team can decide on whatever columns and swim-lanes they want in order to optimize their day-to-day work. Again, leave this up to the team rather than dictating from the organizational level.\n\nWork Item states can’t be customized at Team level - only at the Team Project level. If you only have a single Team Project, that means every team inherits the same work item states. This is actually a good thing: a good design paradigm is to have standard communication protocols, and to have services have good contracts or interfaces, without dictating what the internals of the service should look like. This is reflected by the common “language” of work item state, but let’s teams decide how to manage work internally via customized Kanban boards. Let Conway’s Law work for you!\n\nIterations\n\nWhile teams should have independent backlogs and areas, they should synchronize on cadence. That is, it’s best to share iterations. This means that teams are independent during a sprint, but co-ordinate at the end of the sprint. This enforces the loose coupling: teams are going to have dependencies and you still want teams to communicate - you just want to streamline that communication. Sharing iterations and synchronizing on that heartbeat is good for the Teams as well as the software they’re delivering.\n\nEnterprise Alignment vs Team Autonomy\n\nThe VSTS team have a single Team Project Collection for their work. They speak about Enterprise Alignment vs Team Autonomy. I heard a great illustration the other day: the Enterprise is like a tanker - it takes a while to turn. Agile Teams are like canoes - they can turn easily. However, try to get 400 canoes pointed in the same direction! As you work to self-organizing teams, keep them on the One Objective so that they’re pointed in the same direction. Again, that’s why I like the One Team Project concept: the Team Project is the One Direction, while Teams still get autonomy in their Team Areas for daily work.\n\nOrganizing Source Code, Builds, Releases, Test Plans and Feeds\n\nIf you have a single Team Project, then you’ll have a challenge: all repositories, builds, releases, test plans and package feeds are in a single place. Builds have the concept of Build Folders, so you can organize builds by folders if you need to. However, repos, releases, test plans and feeds don’t have folders. That means you’ll need a good naming strategy and make use of Favorites to manage the noise. In my opinion this is a small price to pay for the benefits of One Team Project.\n\nSecurity\n\nOften I come across organizations that want to set up restrictions on who can see what. In general: don’t do this! Why do you care if Team A can see Team B’s backlog? In fact it should be encouraged! Find out what other teams are working on so that you can better manage dependencies and eliminate double work. Same principle with Source Code: why do you care if Team C and see Team D’s repos?\n\nThere are of course exceptions: if you have external contractors, you may want to restrict visibility for them. In VSTS, deny overrides allow, so in general, leave permissions as “Not Set” and then explicitly Deny groups when you need to. The Deny should be the exception rather than the rule - if not, you’re probably doing something wrong.\n\nOf course you want to make sure you Branch Policies (with Pull Requests) in your source code and approval gates in your Releases. This ensures that the teams are aware of code changes and code deployments. Don’t source control secrets - store them in Release Management or Azure Key Vault. And manage by exception: every action in VSTS is logged in the Activity Log, so you can always work out who did what after the fact. Trust your teams!\n\nConclusion\n\nDon’t fight Conway’s Law: make it work for you! Slim down to a single VSTS Account with a single Team Project and move all your Teams into that single Team Project. Give Teams the ability to customize their sub-areas, backlogs, boards and so on: this gives a good balance of Enterprise Alignment and Team Autonomy.\n\nHere is a brief summary of how you should structure your VSTS account:\n\n\n  A single VSTS Account (or TFS Team Project Collection)\n  A single Team Project\n  Multiple Teams, all owning their root Area Path\n  Shared Iteration Paths\n  Use naming conventions/favorites for Repos, Releases, Test Plans and Feeds\n  Use folders for organizing Build Definitions\n  Enforce Branch Policies in your Repos and use Approval Gates in Release Management\n  Have simple permissions with minimal DENY (prefer NOT SET and ALLOW)\n\n\nHappy delivering!\n",
      "categories": [],
      "tags": ["devops"],
      
      "collection": "posts",
      "url": "/vsts-one-team-project-and-inverse-conway-maneuver/"
    },{
      
      "title": "Auditing VSTS Client Access IPs",
      "date": "2018-05-30 20:27:41 +0000",
      
      "content": "Visual Studio Team Services (VSTS) is a cloud platform. That means it’s publicly accessible from anywhere - at least, by default. However, Enterprises that are moving from TFS to VSTS may want to ensure that VSTS is only accessed from a corporate network or some white-list of IPs.\n\nTo enable conditional access to VSTS, you’ll have to have an Azure Active Directory (AAD) backed VSTS account. The conditional access is configured on AAD, not on VSTS itself, since that’s where the authentication is performed. For instructions on how to do this, read this MSDN article.\n\nAudit Access\n\nHowever, this may be a bit heavy-handed. Perhaps you just want to audit access that isn’t from a white-list of IPs, instead of blocking access totally. If you’re an administrator, you may have come across the Usage page in VSTS. To get there, navigate to the landing page for your VSTS account, click the gear icon and select Usage:\n\n\n\n\nThis page will show you all your access to VSTS. To see the IPs, you have to add the “IP Address” column in the column options:\n\n\n\n\nNice, but what about other users? To get that you have to use the VSTS REST API.\n\nDumping an Exception Report with PowerShell\n\nThere is an (undocumented) endpoint for accessing user access. It’s https://&lt;your account&gt;.visualstudio.com/_apis/Utilization/UsageSummary with a whole string of query parameters. And since it’s a REST call, you’ll need to be authenticated so you’ll have to supply a header with a base-64 encoded Personal Access Token (PAT).\n\nUsing PowerShell, you can make a call to the endpoint and then filter results where the IP is not in the white-list of IPs. Fortunately for you, I’ve made a gist for you, which you can access here. When you call the script, just pass in your account name, your PAT, the start and end date and the white-list of IPs. Any access from an IP not in this list is dumped to a CSV.\n\n\n\nLimitations\n\nThere are some limitations to the API:\n\n\n  You’ll need to be a Project Collection or Account admin to make this call (since there’s no documentation, I’m guessing here).\n  You can only go back 28 days, so if you need this as an official exception report you’ll have to schedule the run.\n\n\nConclusion\n\nVSTS knows the client IP for any access. Using the API, you can dump a list of access events that are not from a white-list of IPs.\n\nHappy auditing!\n",
      "categories": [],
      "tags": ["tfsconfig"],
      
      "collection": "posts",
      "url": "/auditing-vsts-client-access-ips/"
    },{
      
      "title": "Managing Credentials and Secrets in VSTS Release Management",
      "date": "2018-07-18 02:36:55 +0000",
      
      "content": "Releases almost always require some kind of credentials - from service credentials to database usernames and passwords. There are a number of ways to manage credentials in VSTS release management. In this post I’ll look at a couple of common techniques. For brevity, I’m going to refer to secrets as a proxy for secrets and credentials.\n\nDon’t Store Secrets in Source Control\n\nOne bad practice you want to steer away from is storing secrets in source control. A lot of teams I work with have their build process create multiple environment-specific packages, using tools like config transforms. I like to get teams to think of build and release as two separate (but linked) processes:\n\nProcessInputProcessOutput BuildSource CodeCompile, unit test, packageTokenized build packagesReleaseBuild artifacts, config source codeInfrastructure deployment/config, approvals, integration/functional tests, app deploymentsDeployed application\n\nThe point is that the build should be totally environment agnostic. Good unit tests use mocking or fakes, so they shouldn’t need environment-specific information. That means that they need to create packages that are, as I like to call them, swiss cheese - they need to have holes or tokens that can have environment-specific values injected at deployment time. You don’t need tokens if your deployment process is capable of doing variable substitution - like the IIS Deployment on Machine Group task or Azure App Service Deployment task that can both do inline variable replacement (see my earlier post on how to do this - and this also now applies to the IIS Deployment on Machine Group task).\n\nCentralized vs Decentralized Secret Management\n\nI see two broad categories of secret management: centralized and decentralized. Centralized secret management has the advantage of specifying/updating the secret once, even if it’s used in many places - but has the disadvantage of being managed by a small subset of users (admins typically). This can also be an advantage, but can be a bottleneck. Decentralized secret management usually ends up in duplicated secrets (so updating a password leaves you hunting for every occurrence of that password) but removes the bottleneck of centralized management. Choosing a method will depend on your culture, auditing requirements and management overhead.\n\nDecentralized Secret Management\n\nDecentralized secret management is the easiest to consider, and there’s really only one way to do it: in your release definition, define your secrets as variables that are locked and you’re done. If you need to use the same secret in multiple definitions, you just create the same variable. Of course if you change the value, you have to change it in each release that uses it. But you don’t have to log a ticket or wait for anyone to change the value for you - if it changes, you update it in place for each release and you’re done.\n\nCentralized Secret Management\n\nThere are three types of centralized secret management: Azure KeyVault, Variable Groups and Custom Key Vault. Let’s consider each method.\n\nThe KeyVault and Variable Group methods both define a Variable Group - but if you use KeyVault, you manage the values in KeyVault rather than in the Variable Group itself. Otherwise they are exactly the same.\n\nGo to the VSTS release hub and click on Library to see variable groups. Create a new Variable Group and give it a name. If this is a “plain” Variable Group, define all your secrets and their values - don’t forget to padlock the values that you want to hide. If you’re using KeyVault, first define a Service Endpoint in the Services hub for authenticating to the KeyVault. Then come back and link the Variable Group to the KeyVault and specify which Secrets are synchronized.\n\n\n\n\nNow when you run define a release, you link the Variable Group (optionally scoping it) and voila - you have a centralized place to manage secrets, either directly in the Variable Group or via KeyVault.\n\n\n\n\nThe variable group can be linked to many releases, so you only ever have to manage the values in one place, irrespective of how many releases reference them. To use the values, just use $(SecretName) in your tasks.\n\nThe last method is Custom Key Vault. I worked with a customer a few months back that used some sort of third-party on-premises key vault. Fortunately this vault had a REST API and we were able to create a custom task that fetched secrets from this third-party key vault. If you do this, you need to remember to add in a custom task to get the values, but this was an elegant solution for my customer since they already had an internal key vault.\n\nConclusion\n\nThere are a number of ways to manage secrets and credentials in VSTS/TFS. The most robust is to use Azure KeyVault, but if you don’t have or don’t want one you can use Variable Groups in-line. Whatever method you choose, just make sure you don’t store any secrets in source control!\n\nHappy releasing!\n",
      "categories": [],
      "tags": ["tfsconfig"],
      
      "collection": "posts",
      "url": "/managing-credentials-and-secrets-in-vsts-release-management/"
    },{
      
      "title": "Terraform all the Things with VSTS",
      "date": "2018-08-08 01:08:54 +0000",
      
      "content": "I’ve done a fair amount of ARM template authoring. It’s not as bad as XML, but the JSON can get laborious. A number of my colleagues use Terraform templates and I was recently on a project that was using these templates. I quickly did a couple PluralSight Terraform classes to get up to speed and then started hacking away. In this post I’ll jot down a couple thoughts about how we structure Terraform projects and how to deploy them using VSTS. The source code for this post is on Github.\n\nStacks\n\nWhen we create Terraform projects, we divide them into “stacks”. These are somewhat independent, loosely-coupled components of the full infrastructure we’re deploying. Let’s take the example of an Azure App Service with deployment slots that connects to an Azure SQL database and has Application Insights configured. In this scenario, we have three “stacks”: SQL, WebApp and AppInsights. We then have an additional “stack” for the Terraform remote state (an Azure blob) and finally a folder for scripts. Here’s what our final folder structure looks like:\n\n\n\n\nFollow the instructions in the README.md file for initializing the backend using the state folder. Note that backend.tfvars and secrets.tfvars are ignored by the .gitignore file so should not be committed to the repo.\n\nWorkspace = Environment\n\nThinking ahead, we may want to create different environments. This is where Terraform workspaces come in handy - we use them to represent different environments. That way we can have a single template and can re-use it in multiple environments. So if you look at webapp/variables.tf, you’ll see this snippet:\n\nvariable \"stack_config\" {\n  type = \"map\"\n\n  default = {\n    dev = {\n      name = \"webapp\"\n      rg_name_prefix = \"cd-terra\"\n      plan_name_prefix = \"cdterra\"\n      app_name_prefix = \"cdterraweb\"\n    }\n\n    uat = {\n      name = \"webapp\"\n      rg_name_prefix = \"cd-terra\"\n      plan_name_prefix = \"cdterra\"\n      app_name_prefix = \"cdterraweb\"\n    }\n  }\n}\n\n\nYou can see how we have different maps for different workspaces. To consume the environment (or workspace) specific variables, we use the locals resource in our main.tf scripts. The common format is something like this:\n\nlocals {\n  env = \"${var.environment[terraform.workspace]}\"\n  secrets = \"${var.secrets[terraform.workspace]}\"\n  stack = \"${var.stack_config[terraform.workspace]}\"\n  created_by = \"${var.created_by}\"\n  stack_name = \"${local.stack[\"name\"]}\"\n\n  env_name = \"${terraform.workspace}\"\n  release = \"${var.release}\"\n  ...\n  app_name = \"${local.stack[\"app_name_prefix\"]}-${local.env_name}\"\n}\n\n\nWe create local variables for env, secrets and stack by dereferencing the appropriate workspace’s map values. Now we can use “${local.app_name}” and the value will be an environment-specific value. Also note how we have a variable called “release” - we add this as a tag to all the Azure resources being created so that we can tie the resource to the release that created/updated it.\n\nReleases in VSTS\n\nNow we get to the really interesting bit: how we run the templates in a release pipeline in VSTS. I tried a couple of marketplace Terraform extensions, but wasn’t happy with the results. The most promising one was Peter Groenewegen’s extension, but it was not workspace aware and while it did download Terraform so that I could run Terraform using the hosted agents, it didn’t preserve Terraform on the path. I eventually ditched it for a plain ol’ bash script. To perform Terraform operations, we have to:\n\n\n  Replace tokens in the release.tfvars file\n  Download the Terraform executable\n  Run the Terraform apply bash script for each stack in order\n\n\nI ended up using the Hosted Ubuntu 1604 hosted agent for the agent phase - I don’t know for sure, but I suspect this is running in a container - in any case, it’s super fast to start up and execute. Because I’m running the build on Linux, I wrote the scripts I used in bash - but you can easily create equivalent PowerShell scripts if you really want to - though VSTS will run bash scripts on Windows agents just fine - although the paths are different.\n\nArtifact\n\nFor the release to work, it needs access to the terraform templates. I create a new release and use the source repo as the incoming artifact with an alias “infra”. You can add multiple artifacts, so if you’re going to deploy code after deploying infrastructure, then you can add in the build as another artifact.\n\nVariables\n\nIn the release, I define a number of variables:\n\n\n\n\nI tried to use the environment variable format for ARM_ACCESS_KEY, ARM_CLIENT_ID etc. but found I had to supply these explicitly - which I do via the release.tfvars file. The release.tfvars file has tokens that are replaces with the environment values at deploy time. If you add more environment-specific variables, then you need to add their tokens in the tfvars file and add the variable into the variables section of the release. One last note: I use $(Release.EnvironmentName) as the value for the Environment variable - but this needs a different value for the “destroy” environment (each environment I have in the pipeline has a corresponding “destroy” environment for destroying the resources). You can see how I specify “dev” as the Environment name for the “destroy dev” environment.\n\nDownload Terraform\n\nThis is only required if you’re using the hosted agents - if you’re using a private agent, then you’re better off downloading terraform and adding it to the PATH. However, in the scripts folder I have a bash script (download-terraform.sh) that downloads Terraform using curl (from a URL specified in a variable) and untars it to the path specified in the TerraformPath variable. From that point on, you can use $(TerraformPath)\\terraform for any Terraform operations.\n\nApplying a Stack\n\nThe scripts folder contains the script for applying a stack (run-terraform.sh). Let’s dig into the script:\n\n#!/bin/bash -e\n\necho \" *********** Initialize backend\"\necho \"access_key = \\\"${1}\\\"\" &amp;gt; ../backend.tfvars\n$2/terraform init -backend-config=../backend.tfvars -no-color\n\necho \"\"\necho \" *********** Create or select workspace\"\nif [$($2/terraform workspace list | grep $3 | wc -l) -eq 0]; then\n  echo \"Create new workspace $3\"\n  $2/terraform workspace new $3 -no-color\nelse\n  echo \"Switch to workspace $3\"\n  $2/terraform workspace select $3 -no-color\nfi\n\necho \"\"\necho \" *********** Run 'plan'\"\n$2/terraform plan --var-file=../global.tfvars --var-file=../release.tfvars -var=\"release=$4\" --out=./tf.plan -no-color -input=false\n\necho \"\"\necho \" *********** Run 'apply'\"\n$2/terraform apply -no-color -input=false -auto-approve ./tf.plan\n\n\nNotes:\n\n\n  Lines 3 - 5: Initialize the backend. Output the access_key to the root backend.tfvars file (remember this won’t exist in the repo since this file is ignored in .gitignore)\n  Lines 7-15: Create or select the workspace (environment)\n  Lines 17-19: Run terraform plan passing in global variables from global.tfvars, environment-specific variables now encapsulated in release.tfvars and pass in the release number (for tagging)\n  Lines 21-23: Run terraform apply using the plan generated in the previous command\n\n\nDestroying a Stack\n\nDestroying a stack is almost the same as applying one - the script (run-terraform-destroy.sh) just does a “plan -destroy” to preview the operations before calling terraform destroy.\n\nThe Release Steps\n\nNow we can see what these blocks look like in the pipeline. Here’s a pipeline with a dev and a “destroy dev” environment:\n\n\n\n\nThe dev environment triggers immediately after the release is created, while the “destroy dev” environment is a manual-only trigger.\n\nLet’s see what’s in the dev environment:\n\n\n\n\nThere you can see 5 tasks: replace variables, download Terraform and then an apply for each stack (in this case we have 3 stacks). The order here is important only because the WebApp stack reads output variables from the state data of the SQL and AppInsights deployments (to get the AppInsights key and SQL connection strings). Let’s take a closer look at each task:\n\nReplace Variables\n\nFor this I use my trusty ReplaceTokens task from my build and release extension pack. Specify the folder (the root) that contains the release.tfvars file and the file search format, which is just release.tfvars:\n\n\n\n\nNext we use a Shell task to run the download Terraform script, which expects the path to install to as well as the URL for the Terraform binary to download:\n\n\n\n\nFinally we use a Shell task for each stack - the only change is the working folder (under Advanced) needs to be the stack folder - otherwise everything else stays the same:\n\n\n\n\nSuccess! Here you can see a run where nothing was changed except the release tag - the templates are idempotent, so we let Terraform figure out what changes (if any) are necessary.\n\n\n\nConclusion\n\nTerraform feels to me to be a more “enterprise” method of creating infrastructure as code than using pure ARM templates. It’s almost like what TypeScript is to JavaScript - Terraform has better sharing and state awareness and allows for more maintainable and better structured code. Once I had iterated a bit on how to execute the Terraform templates in a release, I got it down to a couple really simple scripts. These could even be wrapped into custom tasks.\n\nHappy deploying!\n",
      "categories": [],
      "tags": ["releasemanagement"],
      
      "collection": "posts",
      "url": "/terraform-all-the-things-with-vsts/"
    },{
      
      "title": "Serverless Parallel Selenium Grid Testing with VSTS and Azure Container Instances",
      "date": "2018-09-07 02:12:08 +0000",
      
      "content": "I’ve written before about Selenium testing (Parallel Testing in a Selenium Grid with VSTS and Running Selenium Tests in Docker using VSTS and Release Management). The problem with these solutions, however, is that you need a VM! However, I was setting up a demo last week and decided to try to solve this challenge using Azure Container Instances (ACI), and I have a neat solution.\n\nSo why ACI? Why not a Kubernetes cluster? This solution would definitely work in a k8s cluster, but I wanted something more light-weight. If you don’t have a k8s cluster, spinning one up just for Selenium testing seemed a bit heavy handed. Also, I discovered that ACI now lets you spin up multiple containers in the same ACI group via a yaml file.\n\nI’ve created a GitHub repo with all the source code for this post so you can follow along - it includes the following:\n\n\n  a sample app (boilerplate File-&gt;New Project-&gt;MVC) with unit tests\n  .NET Core Selenium test project and tokenized runsettings files\n  ARM template for Azure Web App\n  Tokenized yaml file for the ACI infrastructure\n  CI yaml build definition (yay for build-as-code!)\n  json release definition (release-as-code isn’t yet available, so we’ll have to do the release via the designer)\n\n\nArchitecture\n\nOnce we’ve built and package the application code, the release process (which we model in the Release Definition) is as follows:\n\n\n  Provision infrastructure - both for the app as well as the ACI group with the Selenium hub, worker nodes and VSTS agents\n  Deploy the app\n  Install .NET Core and execute “dotnet test”, publishing test results\n  Tear down the ACI\n\n\nThe component architecture for the ACI is pretty straight-forward: we run containers for the following:\n\n\n  A Selenium Hub - this will listen for test requests and match the requested capabilities with a worker node\n  Selenium nodes - Chrome and Firefox worker nodes\n  VSTS Agent(s) - these connects to VSTS and execute tests as part of a release\n\n\nTo run in parallel I’m going to use a multi-configuration phase - so if you want to run Firefox and Chrome tests simultaneously you’ll need at least 2 VSTS agents (and 2 pipelines!). ACI quotas will let you spin up groups with up to 4 processors and 14GB of memory, so for this example we’ll use a Selenium hub, 2 workers and 2 VSTS agents each using .5 processor and .5GB memory.\n\nOne concern is security - how do you secure the test rig since it’s running in the public cloud? Fortunately, this solution doesn’t require any external ports - the networking is all internal (the worker nodes and VSTS agent connect to the hub using “internal” ports) and the VSTS agent itself connects out to VSTS and does not require incoming connections. So we don’t have to worry about securing endpoints - we don’t even expose any!\n\nLet’s take a look at the architecture of this containers in ACI:\n\n\n\n\nNotes:\n\n\n  We are running 5 containers: selenium-hub, selenium-chrome and selenium-firefox and 2 vsts-agents\n  The only port we have to think about is the hub port, 4444, which is available internally - we don’t have any external ports\n  The vsts-agent is connected to our VSTS account, and again no external port is required\n\n\nThe trick to getting multiple Selenium nodes within the group to connect to the hub was to change the ports that they register themselves on - all taken care of in the yaml definition file.\n\nAs for the tests themselves, since I want to run the tests inside a vsts-agent container, they have to be written in .NET Core. Fortunately that’s not really a big deal - except that some methods (like the screenshot method) are not yet implemented in the .NET Core Selenium libraries.\n\nWe’ll get into the nitty-gritty of how to use this ACI in a release pipeline for testing your app, but before we do that let’s quickly consider how to deploy this setup in the first place.\n\nPermanent or Transient?\n\nThere are two ways you can run the ACI - permanently or transiently. The permanent method spins up the ACI and leaves it running full-time, while the transient method spins the ACI up as part of an application pipeline (and then tears it down again after testing). If you are cost-sensitive, you probably want to opt for the transient method, though this will add a few minutes to your releases. That shouldn’t be too much of a problem since this phase of your pipeline is running integration tests which you probably expect to take a bit longer. If you are optimizing for speed, then you probably just want to spin the ACI up in a separate pipeline and just assume it’s up when your application pipeline runs. Fortunately the scripts/templates are exactly the same for both methods! In this post I’ll show you the transient method - just move the tasks for creating/deleting the ACI to a separate pipeline if you prefer the ACI to be up permanently.\n\nPutting it all Together\n\nTo put this all together, you need the following:\n\n\n  A VSTS account\n  An Azure subscription\n  A Service Endpoint to the Azure sub from VSTS\n  Colin’s ALM Corner Build Extension pack installed onto your VSTS account\n  An agent queue in VSTS (the ACI VSTS agents will register onto this queue)\n  A personal access token (PAT) to your VSTS account\n\n\nFor the source code you can link directly to my GitHub repo. However, if you want to change the code, then you’ll either need to fork it to your own GitHub account or import the repo into your VSTS account.\n\nThe Build\n\nThe build is really simple - and since it’s yaml-based, the code is already there. To create a build definition, enable the YAML build preview on your VSTS account, then browse to the Build page. Create a new YAML build and point to the .vsts-ci.yml file. The build compiles code, versions the assemblies, runs unit tests with code coverage and finally publishes the web app as a webdeploy package:\n\n\n\nThe Release\n\nYou’ll need to import the release definition to create it. First download the WebApp.ReleaseDefinition.json file (or clone the repo) so that you have the file on disk. Now, if you’re using the “old” release view, just navigate to the Releases page and click the + button at the top of the left menu, then select “Import release pipeline”. If you’re using the new preview release view, you’ll need to create a dummy release (since the landing page doesn’t show the toolbar). Once you’ve created a dummy release, click the “+ New” button in the toolbar and click “Import a pipeline”. Then browse to the WebApp.ReleaseDefinition.json file and click import. Once it’s imported, you’ll need to fix up a few settings:\n\nVariables\n\nClick on the Variables tab and update the names for:\n\n\n  RGName - the name of the resource group for all the infrastructure\n  WebAppName - the name of the web app (must be globally unique in Azure)\n  ACIName - the name for the Azure Container Instance\n  Location - I’d suggest you leave this on WestUS for the ACI quotas\n  VSTSAccount - the name of your VSTS account (i.e. the bit before .visualstudio.com)\n  VSTSPool - the name of the agent pool that you created earlier\n  VSTSToken - your PAT. Make sure to padlock this value (to make it secret)\n\n    Artifacts\n  \n\n\nClick on the Pipeline tab top open the designer. You’re going to have to delete and recreate the artifacts, since the id’s are specific to my VSTS, so I cleared them in the definition json file. The primary artifact (so add this first) is your web app build - so add a new artifact of type “Build” and point to the WebApp build. Make sure the “Source alias” of this artifact is set to “WebApp” to preserve the paths in the tasks. You can also enable the CD trigger (to queue a release when a new build is available) if you want to. Now add another artifact - this time point to the source code repo (either on GitHub or your VSTS account) and alias this artifact as “infra” to preserve paths.\n\nJob Queues\n\nNow click on the Tasks tab. Click on the “Provision Infrastructure” job header and then select “Hosted Linux Preview” for the agent pool (we’re running some Azure CLI commands via bash, so we need an Ubuntu agent). You can repeat this for the last job “Tear down ACI”.\n\n\n\n\nThe “Deploy using WebDeploy” task requires a Windows agent, so change the queue on this job to “Hosted VS2017”. Finally, change the “Run Tests” queue to the agent queue you created earlier (with the same name as the VSTSPool variable).\n\nAzure Endpoints\n\nYou’ll need to click on each “Azure” task (the Azure CLI tasks, the Azure Resource Group Deployment task and the Azure App Service Deploy task and configure the correct Azure endpoint:\n\n\n\n\nYou should now be able to save the definition - remove “Copy” from the name before you do!\n\nThe Jobs\n\nLet’s have a quick look at the tasks and how they are configured:\n\nProvision Infrastructure\n\nThis job provisions infrastructure for the web app (via ARM template) as well as the ACI (via Azure CLI). The first task executes the ARM template, passing in the WebAppName. The template creates a Free tier App Service Plan and the App service itself. Next we replace some tokens in the ACI yaml definition file - this is somewhat akin to a Kubernetes pod file. In the file we specify that we require 5 containers: the Selenium hub (which opens port 4444), the worker nodes (one firefox and one chrome, running on different ports and connecting to the hub via localhost:4444) and 2 VSTS agents (with environment variables for the VSTS account, pool and PAT) so that the agent can connect to VSTS. I had to specify agent names since both containers get the same hostname, so if you don’t and the agents have the same name, the 2nd agent would override the 1st agent registration.\n\nFinally we invoke the Azure CLI task using an inline script to create the ACI using the yaml file:\n\n\n\n\nThe script itself is really a one-liner to “az container create” and we pass in the resource group name, the ACI name and the path to the yaml file.\n\nDeploy using WebDeploy\n\nThe deploy job is a single task: Deploy Azure App Service. This has to run on a windows agent because it’s invoking webdeploy. We specify the App name from the variable and point to the webdeploy zip file (the artifact from the build).\n\n\n\n\nOf course a real application may require more deployment steps - but this single step is enough for this demo.\n\nRun Tests\n\nThis job should be executing on the agent queue that you’ve configured in variables - this is the queue that the ACI agents are going to join in the first job. The first task installs the correct .NET core framework. We then replace the tokens in the runsettings file (to set the browser and the BaseURL). Then we execute “dotnet test” and finally publish the test results.\n\n\n\n\nYou’ll notice that I have unset “Publish test results” in the dotnet test task. This is because the run is always published as “VSTest Test Run” - there’s no way to distinguish which browser the test run is for. We tweak the test run title in the Publish Test Results step:\n\n\n\n\nYou’ll also notice that we only have a single job - so how does the parallelization work? If you click on the Job name, you’ll see that we’ve configured the parallelization settings for the job:\n\n\n\n\nWe’re “splitting” the values for the variable “Browser” - in this case it’s set to “chrome,firefox”. In other words, this job will spawn twice - once for Browser=chrome and once for Browser=firefox. I’ve set the maximum number of agents to 2 since we only have 2 anyway.\n\nTeardown ACI\n\nFinally we tear down the ACI in a single Azure CLI inline script where we call “az container delete” (passing in the resource group and ACI names):\n\n\n\n\nTo ensure that this job always runs (even if the tests fail) we configure the Advanced options for the job itself, specifying that it should always run:\n\n\n\nRun It!\n\nNow that we have all the pieces in place, we can run it! Once it has completed, we can see 2 “Run Test” jobs were spawned:\n\n\n\n\nIf we navigate to the Test tab, we can see both runs (with the browser name):\n\n\n\nConclusion\n\nUsing ACI we can get essentially serverless parallel Selenium tests running in a release pipeline. We’re only charged for the compute that we actually used in Azure, so this is a great cost optimization. We also gain parallelization or just better test coverage (we are running the same tests in 2 browsers). All in all this proved to be a useful experiment!\n\nHappy testing!\n",
      "categories": [],
      "tags": ["testing","releasemanagement"],
      
      "collection": "posts",
      "url": "/serverless-parallel-selenium-grid-testing-with-vsts-and-azure-container-instances/"
    },{
      
      "title": "Pimp Your Consoles on Windows",
      "date": "2018-09-28 21:45:35 +0000",
      
      "content": "\n  Updating Fonts\n  PowerShell\n  Bash\n  Solarized Theme\n  Conclusion\n\n\nI spend a fair amount of time in consoles - specifically PowerShell and Bash (Windows Subsystem for Linux) on my Windows 10 machine. I also work with Git - a lot. So having a cool console that is Git aware is a must. I always recommend Posh-Git (a PowerShell prompt that shows you which Git branch you’re on as well as the branch status). At Ignite this I saw some zsh consoles in VS Code on Mac machines. So I wondered if I could get my consoles to look as cool. And it’s not just about the looks - seeing your context in the console is a productivity booster!\n\nIt turns out that other than installing some updated fonts for both PowerShell and Bash, you can get pretty sweet consoles fairly easily.\n\nUpdating Fonts\n\nThe fonts that the custom shells use are UTF-8, so you’ll need UTF-8 fonts installed. You’ll also need so-called “powerline” fonts. Fortunately, there’s a simple script you can run to install a whole bunch of cool fonts that will work nicely on your consoles.\n\nHere are the steps for installing the fonts. Open a PowerShell and enter the following commands:\n\ngit clone https://github.com/powerline/fonts.git\n.\\install.ps1\n\n\nThis took about 5 minutes on my machine.\n\nPowerShell\n\nSo to pimp out your PowerShell console, you’ll need to install a couple modules: Posh-Git and Oh-My-Posh. Run\n\n\nInstall-Module Posh-Git\n\n\nand\n\n\nInstall-Module Oh-My-Posh. Once both modules are installed, you need to edit your $PROFILE (you can run code $PROFILE to quickly open your profile file in VSCode). Add the following lines:\n\n\nInstall-Module posh-git\nInstall-Module oh-my-posh\nSet-Theme Paradox\n\n\nYou can of course choose different themes - run\n\n\nGet-Theme\n\n\nto get a list of themes. One last thing to do - set the background color of your PowerShell console to black (I like to make the opacity 90% too).\n\nNow if you cd to a git repo, you’ll get a Powerline status. Sweet!\n\n\n\nBash\n\nYou can do the same thing for your Bash console. I like to use fish shell so you’ll have to install that first. Once you have fish installed, you can install oh-my-fish - a visual package manager for fish (and yes, oh-my-posh is a PowerShell version of oh-my-fish). Once oh-my-fish is installed, use it to install themes. You can install agnoster by running\n\n\nomf install agnoster\n\n\n\n  I like bobthefish, so I just run\n\n\n\nomf install bobthefish\n\n\n. Now my bash console is pimped too!\n\n\n\nSolarized Theme\n\nOne more change you may want to make: update your console colors to the Solarized theme. To do that, follow the instructions from this repo.\n\nConclusion\n\nIf you’re going to work in a console frequently, you may as well work in a pretty one! Oh-My-Fish and Oh-My-Posh let you quickly and easily get great-looking consoles, and Posh-Git adds in Git context awareness. What’s not to love?\n\nHappy console-ing!\n",
      "categories": [],
      "tags": ["development"],
      
      "collection": "posts",
      "url": "/pimp-your-consoles-on-windows/"
    },{
      
      "title": "Implement an Azure DevOps Release Gate to ServiceNow",
      "date": "2018-10-20 03:24:53 +0000",
      
      "content": "\n  Finding the ServiceNow REST API\n  Creating a Custom Release Gate Extension\n  Putting It All Together\n  Conclusion\n\n\nI’m currently doing some work with a customer that is integrating between ServiceNow and Azure DevOps (the artist formerly known as VSTS). I quickly spun up a development ServiceNow instance to play around a bit. One of the use-cases I could foresee was a release gate that only allows a release to continue if a Change Request (CR) is in the Implement state. So I had to do some investigation: I know there are a few out-of-the-box Azure DevOps release gates, including a REST API call - but I knew that you could also create a custom gate. I decided to see if I could create the gate without expecting the release author having to know the REST API call to ServiceNow or how to parse the JSON response!\n\nFollow along to see the whole process - or just grab the code in the Github repo.\n\nFinding the ServiceNow REST API\n\nPart One of my quest was to figure out the REST API call to make to ServiceNow. The ServiceNow documentation is ok - perhaps if you understand ServiceNow concepts (and I don’t have deep experience with them) then they’re fine. But I quickly felt like I was getting lost in the weeds. Add to that many, many versions of the product - which all seem to have different APIs. After a couple hours I did discover that the ServiceNow instance has a REST API explorer - but I’m almost glad I didn’t start there as you do need some knowledge of the product in order to really use the explorer effectively. For example, I was able to query the state of the CR if I had its internal sys_id, but I didn’t expect the user to have that. I wanted to get the state of the CR by its number - and how to do that wasn’t obvious from the REST API explorer.\n\nAnyway, I was able to find the REST API to query the state of a Change Request:\n\n\nhttps://&lt;instance&gt;.servicenow.com/api/now/table/change_request?sysparm_query=number=&lt;number&gt;&amp;sysparm_fields=state&amp;sysparm_display_value=true\n\n\nA couple notes on the query strings:\n\n\n  sysparm_query lets me specify that I want to query the change_request table for the expression “number=&lt;number&gt;”, which lets me get the CR via its number instead of its sys_id\n  sysparm_fields lets me specify which fields I want returned - in this case, just the state field\n  sysparm_value=true expands the enums from ints to strings, so I get the “display value” of the state instead of the state ID\n\n\nThe next problem is authentication - turns out if you have a username and password for your ServiceNow instance, you can include a standard auth header using BasicAuth (this is over HTTPS, so that’s ok). I tested this with curl and was able to get a response that looks something like this:\n\n\n{\"result\":[{\"state\":\"Implement\"}]}\n\nCreating a Custom Release Gate Extension\n\nNow that I know the REST API call to ServiceNow, I turned to how to Part Two of my quest: create a custom Release Gate extension. Fortunately, I had Microsoft DevLabs’ great Azure DevOps Extension extension as a reference (this was originally from Jesse Houwing) - and I use this all the time to package and publish my own Azure DevOps Build and Release extension pack.\n\nIt turns out that the release gate “task” itself is pretty simple, since the entire task is just a JSON file which specifies its UI and the expression to evaluate on the response packet. The full file is here but let’s examine the two most important parts of this task: the “inputs” element and the “execution” element. First the inputs:\n\n\"inputs\": [\n  {\n    \"name\": \"connectedServiceName\",\n    \"type\": \"connectedService:ServiceNow\",\n    \"label\": \"Service Now endpoint\",\n    \"required\": true,\n    \"helpMarkDown\": \"Service Now endpoint connection.\"\n  },\n  {\n    \"name\": \"crNumber\",\n    \"type\": \"string\",\n    \"label\": \"Change Request number\",\n    \"defaultValue\": \"\",\n    \"required\": true,\n    \"helpMarkDown\": \"Change Request number to check.\"\n  },\n  {\n    \"name\": \"validState\",\n    \"type\": \"string\",\n    \"label\": \"State\",\n    \"defaultValue\": \"Implement\",\n    \"helpMarkDown\": \"State that the CR should be in to pass the gate.\",\n    \"required\": true\n  }\n]\n\n\nNotes:\n\n\n  connectedServiceName is of type “connectedService:ServiceNow”. This is the endpoint used to call the REST API and should handle authentication.\n  crNumber is a string and is the CR number we’re going to search on\n  validState is a string and is the state the CR should be in to pass the gate\n\n\nGiven those inputs, we can look at the execute element:\n\n\"execution\": {\n  \"HttpRequest\": {\n    \"Execute\": {\n      \"EndpointId\": \"$(connectedServiceName)\",\n      \"EndpointUrl\": \"$(endpoint.url)/api/now/table/change_request?sysparm_query=number=$(crNumber)&amp;amp;sysparm_fields=state&amp;amp;sysparm_display_value=true\",\n      \"Method\": \"GET\",\n      \"Body\": \"\",\n      \"Headers\": \"{\\\"Content-Type\\\":\\\"application/json\\\"}\",\n      \"WaitForCompletion\": \"false\",\n      \"Expression\": \"eq(jsonpath('$.result[0].state')[0], '$(validState)')\"\n    }\n  }\n}\n\n\nNotes:\n\n\n  The execution is an HttpRequest\n  Endpoint is set to the connectedService input\n  EndpointUrl is the full URL to use to hit the REST API\n  The REST method is a GET\n  The body is empty\n  We’re adding a Content-Type header of “application/json” - notice that we don’t need to specify auth headers since the Endpoint will take care of that for us\n  The expression to evaluate is checking that the state field of the first result is set to the value of the validState variable\n\n\nAnd that’s it! Let’s take a look at the connected service endpoint, which is defined in the extension manifest (not in the task definition):\n\n{\n  \"id\": \"colinsalmcorner-snow-endpoint-type\",\n  \"type\": \"ms.vss-endpoint.service-endpoint-type\",\n  \"targets\": [\n    \"ms.vss-endpoint.endpoint-types\"\n  ],\n  \"properties\": {\n    \"name\": \"ServiceNow\",\n    \"displayName\": \"Service Now\",\n    \"helpMarkDown\": \"Create an authenticated endpoint to a Service Now instance.\",\n    \"url\": {\n      \"displayName\": \"Service Now URL\",\n         \"description\": \"The Service Now instance Url, e.g. `https://instance.service-now.com`.\"\n    },\n    \"authenticationSchemes\": [\n    {\n      \"type\": \"ms.vss-endpoint.endpoint-auth-scheme-basic\",\n      \"inputDescriptors\": [\n        {\n          \"id\": \"username\",\n          \"name\": \"Username\",\n          \"description\": \"Username\",\n          \"inputMode\": \"textbox\",\n          \"isConfidential\": false,\n          \"validation\": {\n            \"isRequired\": true,\n            \"dataType\": \"string\",\n            \"maxLength\": 300\n          }\n        },\n        {\n          \"id\": \"password\",\n          \"name\": \"Password\",\n          \"description\": \"Password for the user account.\",\n          \"inputMode\": \"passwordbox\",\n          \"isConfidential\": true,\n          \"validation\": {\n            \"isRequired\": true,\n            \"dataType\": \"string\",\n            \"maxLength\": 300\n          }\n        }\n      ]\n    }\n  ]\n}\n\n\nNotes:\n\n\n  Lines 2-6: specify that this contribution is of type Service Endpoint\n  Line 8: name of the endpoint type - this is referenced by the gate in the endpoint input\n  Lines 9-10: description and help text\n  Line 11-14: specify a URL input for this endpoint\n  The rest: specify the authentication scheme for the endpoint\n\n\nBy default the\n\n\nms.vss-endpoint.endpoint-auth-scheme-basic\n\n\nauthentication scheme adds an Authorization header to any request made to the URL of the service endpoint. The value of the header is a base64 encoded munge of user:password. It’s great that you don’t have to mess with this yourself!\n\nPutting It All Together\n\nNow we have the service endpoint and the gate, we’re ready to publish and install the extension! The readme.md in the repo has some detail on this if you want to try your own (or make changes to the code from mine), or you can just install the extension that I’ve published if you want to use the gate as-is. If you do publish it yourself, you’ll need to change the publisher and the GUIDs before you publish.\n\nFor the release to work, you’ll need to make the CR a variable. I did this by adding the variable and making it settable at queue time:\n\n\n\n\nNow when I queue the release, I have to add the CR. Of course you could imagine a release being queued off from an automated process, and that can pass the CR as part of the body of the REST API call to queue the release. For now, I’m entering it manually:\n\n\n\n\nSo how do we specify the gate? Edit the release and click on the pre- or post-approval icon for the environment and open the Gates section. Click the + to add a new gate and select the “Change Request Status” gate. We can then configure the endpoint, the CR number and the State we want to pass on:\n\n\n\n\nTo create an endpoint, just click on “+ New” next to the Service Now endpoint drop-down - this will open a new tab to the Service Endpoints page where you can add a new ServiceNow endpoint.\n\nNote how we set the Change Request number to the variable\n\n\n$(ChangeRequestNumber)\n\n\n. That way this field is dynamic.\n\nFinally, set the “Evaluation options” to configure the frequency, timeout and other gate settings:\n\n\n\n\nOnce the release runs, we can see the Gate invocations and results:\n\n\n\n\nNote that the Gate has to pass twice in a row before it’s successful and moves the pipeline on.\n\nConclusion\n\nCreating release gates as extensions is not too hard once you have some of the bits in place. And it’s a far better authoring experience than the out of the box REST API call - which leaves you trying to mess with auth headers and parsing JSON responses. If you want to get release authors to really fully utilize the power of gates, do them a solid and wrap the gate in an extension!\n\nHappy gating!\n",
      "categories": [],
      "tags": ["releasemanagement"],
      
      "collection": "posts",
      "url": "/implement-an-azure-devops-release-gate-to-servicenow/"
    },{
      
      "title": "Modernizing Source Control - Migrating to Git",
      "date": "2018-12-19 08:22:15 +0000",
      
      "content": "\n  Why Git?    \n      Cheap Branches\n      Better Merging\n      Code Reviews\n      Better Offline\n    \n  \n  Common Objections    \n      Overwriting History\n      Large Files\n      Large Repos\n      Git? GitHub?\n      Learning Curve\n    \n  \n  Git and Microservices\n  Migrating to Git    \n      Monorepo or Multirepo?\n      Migrating\n      Tip Migration\n      Single Branch Import\n      Git-tfs\n    \n  \n  Conclusion\n\n\nI remember when I first learned about Git circa 2012. I was skeptical - you can change history? What kind of source control system let you change history? However, it seemed to have huge momentum and so I started learning how to use it. Once you get over the initial learning curve - and there is one when you switch from centralized version control systems like Team Foundation Version Control (TFVC) or Subversion - I started to see the beauty of Git. And now I believe that teams can benefit enormously if they migrate to Git. I believe that so strongly that I spoke about this very topic at VSLive! in Orlando earlier this month.\n\nIn this post I want to detail why I think migrating to Git makes sense, common objections I hear, and some common ways you can migrate to Git. Migrating to Git make business sense as well as technical sense - so I’ll call out business value-adds along the way. I’ll primarily be talking about migrating from TFVC, but similar principles apply if you’re migrating from other centralized source control systems.\n\nWhy Git?\n\nThere are several reasons why I think Git is essential for modern teams:\n\n\n  Branches are cheap\n  Merging is better\n  Code review is baked in via Pull Request\n  Better offline workflow\n\n\nCheap Branches\n\nThe primary reason I love Git is that branches are cheap. We’ll get to the technical reasons why this important next - but the main business benefit of cheap branches lies in the ability to easily isolate (and later merge) development streams. That should be exciting since it means that small changes can be completed, merged and deployed without having to be held hostage by larger, longer-running changes. Delays cost, so anything that eliminates delays is good!\n\nIn centralized version control, a branch is a complete copy of the source - so typically teams keep the number of branches small. With Git, branches are essentially pointers, so creating branches is cheap. This means teams can create a lot of branches. Why does this make a difference anyway? The idea of a branch is to isolate code changes. Typical TFVC branching strategy is “DEV-MAIN-PROD”. This is an attempt to isolate code in development (DEV) from code that’s being tested (MAIN) and code that’s running in production (PROD). That seems at first glance to be exactly what we want branches for - however, there’s a catch: what if we have two or ten or twenty features in development? I coach teams to check in early, check in often - but that means that at times the code that’s checked in will be unstable. Teams expect this at in the DEV branch. In fact, there’s a term for how stable a branch is: hardness. The DEV branch is considered “soft” since it’s not always stable - while PROD is supposed to be “hard” - that is, stable. But this branching strategy is flawed in that it isolates code at too coarse a level. What we really want is to isolate more granularly - especially if we want to deploy smaller features when they’re complete without having to wait for larger features to be ready for deployment.\n\nGit allows teams to create a branch per feature - also commonly referred to as topic branching. You don’t want to do this when each branch is an entire copy of the code-base - but since Git branches are pointers, we can create branches liberally. By using a good naming convention and periodically cleaning branches that are stale (that haven’t been updated for long periods) teams can get very good at isolating changes, and that makes their entire application lifecycle more resilient and more agile and minimize costly delays.\n\nBetter Merging\n\nMerge debt can also be costly - the further away two branches diverge, the more costly and risky merging them becomes. Again, thinking in “business terms”, this means you can move faster, with better quality - and what business doesn’t want that?\n\nLet’s imaging you have 20 features in flight on a single DEV branch, and you somehow manage to coordinate a merge when all the features are ready to go, you’ll probably spend a lot of time working through the merge since there are so many changes. Also, features that are completed quickly are forced to wait until the slowest feature is complete - which is a lot of waste. Or teams decide to merge anyway, knowing that they’re merging incomplete code.\n\nAlso, when a file changes in a Git repo, Git records the entire file, not just the diffs (like TFVC). This means that merging between arbitrary branches works. With TFVC, branches have to be related to merge - or you could try a dreaded “baseless merge”, which is very error-prone. Even though Git stores the entire file for a change, it does so very efficiently, but because of this merging is far easier for the Git.\n\nEmpirically I find that Git teams have fewer merge conflicts and merge issues than TFVC teams. Let’s now imaging that we are using Git and have 20 features in flight - and 3 are ready to be deployed, but we want to test them. If we want to test them individually, no problem - we do a build off the branch which is master (the stable code) plus the branch changes. We can queue 3 builds and test each feature in isolation. We can also merge any branch into any of the others (something you can’t easily do in unrelated branches in TFVC), so we can also test them together and make sure that there are no breaking changes in the merge - even before we merge each branch to master! This let’s teams deploy features much more rapidly and frequently, eliminating waste along the way.\n\nCode Reviews\n\nOne of GitHub’s engineers introduced the concept of Pull Requests (PRs) and it’s since become ubiquitous in the Git world - even though you don’t typically do PRs in your local Git repo. The PR lets developers advertise that their code is ready to be merged. Policies (and reviews) can be built around the PR so that only quality code is merged into master. PRs are dynamic - so if I am the reviewer and comment on some code that a developer has submitted in a PR, the developer can fix the code and I can see the changes “live” in the PR. In contrast, TFVC lets you submit a Code Review work item (only through the Visual Studio IDE) and if the code needs to be changed, a new Code Review needs to be created. The whole code review process is clunky and laborious. However, I find PRs to be unobtrusive - they let us check code quickly, respond and adapt, and finally merge in a really natural manner. The Azure DevOps PR interface is fantastic - and if you add branch policies (available in Azure DevOps) you can enforce links to work items, comment resolution, build verification and even external system checks before a PR is merged. This lets teams “shift left” and build quality into their process in a natural, powerful and unobtrusive manner.\n\nBetter Offline\n\nGit is a distributed version control system - it’s designed to be used locally and synchronized to a central repo for sharing changes. As such, disconnected workflows are natural and powerful - and since cloning a repository gets the entire history of the repo from day 0, and I can branch and merge locally, the disconnected experience is excellent. TFVC used to require connection to the server to do most source control operations - with local workspaces (circa 2013) some source control operations can be performed offline, but you still need to be connected to the server to branch and merge.\n\nCommon Objections\n\nThere are four common objections I often hear to migrating to Git:\n\n\n  I can overwrite history\n  I have large files\n  I have a very large repo\n  I don’t want to use GitHub\n  There’s a steep learning curve\n\n\nOverwriting History\n\nGit technically does allow you to overwrite history - but (as we know from Spiderman) with great power comes great responsibility! If your teams are careful, they should never have to overwrite history. And if you’re synchronizing to Azure DevOps you can also add a security rule that prevents developers from overwriting history (you need the “Force Push” permission enabled to actually sync a repo that’s had rewritten history). The point is that every source control system works best when the developers using it understand how it works and which conventions work. While you can’t overwrite history with TFVC, you can still overwrite code and do other painful things. In my experience, very few teams have managed to actually overwrite history.\n\nLarge Files\n\nGit works best with repos that are small and that do not contain large files (or binaries). Every time you (or your build machines) clone the repo, they get the entire repo with all its history from Day 0. This is great for most situations, but can be frustrating if you have large files. Binary files are even worse since Git just can’t optimize how they are stored. That’s why Git LFS was created - this lets you separate large files out of your repos and still have all the benefits of versioning and comparing. Also, if you’re used to storing compiled binaries in your source repos - stop! Use Azure Artifacts or some other package management tool to store binaries you have source code for. However, teams that have large files (like 3D models or other assets) you can use Git LFS to keep your code repo slim and trim.\n\nLarge Repos\n\nThis used to be a blocker - but fortunately the engineers at Microsoft have been on a multi-year journey to convert all of Microsoft’s source code to Git. The Windows team has a repo that’s over 300GB in size, and they use Git for source control! How? They invented Virtual File System (VFS) for Git. VFS for Git is a client plugin that lets Git think it has the entire repo - but only fetches files from the upstream repo when a file is touched. This means you can clone your giant repo in a few seconds, and only when you touch files does Git fetch them down locally. In this way, the Windows team is able to use Git even for their giant repo.\n\nGit? GitHub?\n\nThere is a lot of confusion about Git vs GitHub. Git is the distributed source control system created by Linus Torvalds in 2005 for the Linux kernel. If you create a repo, you have a fully functioning Git repo on your local machine. However, to share that code, you need to pick a central place that developers can use to synchronize their repos - so if I want your changes, you’d push your changes to the central repo, and I’d pull them from there. We’re still both working totally disconnected - but we’re able to share our code via this push/pull model. GitHub is a cloud service for hosting these sorts of centralized repos - made famous mostly because it’s free for open source projects (so you can host unlimited public repos). You don’t have to use GitHub to use Git - though it’s pretty much the de-facto platform for open source code. They do offer private repos too - but if you’re an enterprise, you may want to consider Azure Repos since you get unlimited private repos on Azure Repos. You can also create Git repos in Team Foundation Server (TFS) from TFS 2015 to TFS 2019 (now renamed to Azure DevOps Server).\n\nLearning Curve\n\nThere is a learning curve - if you’ve never used source control before you’re probably better off when learning Git. I’ve found that users of centralized source control (TFVC or SubVersion) battle initially to make the mental shift especially around branches and synchronizing. Once developers grok how Git branches work and get over the fact that they have to commit and then push, they have all the basics they need to be successful in Git. I’ve never once had a team convert to Git and then decide they want to switch back to centralized source control!\n\nGit and Microservices\n\nMicroservices are all the rage today - I won’t go into details in this post about why - there’s plenty of material available explaining why the industry is trending towards microservices. Conway’s Law tells us that the structure of our architecture is strongly influenced by the structure of our organization. The inverse, Conway’s Inverse Maneuver, postulates that you can influence the structure of an organization by the way you architect your systems! If you’ve been battling to get to microservices within your organization, consider migrating to Git and decomposing your giant central repo into smaller Git repos as a method of influencing your architecture. Perhaps someone has already come up with a “law” for this - if not, I’ll coin “Colin’s Repo Law” which states that the way that you structure your source code will influence everything else in the DevOps lifecycle - builds, releases, testing and so on. So be sure to structure your source code and repos with the end goal in mind!\n\nMigrating to Git\n\nBefore we get to how to migrate, we have to address the issue of history. When teams migrate source control systems, they always ask about history. I push back a bit and inform teams that their old source control system isn’t going away, so you don’t lose history. For a small period of time, you may have two places to check for history - but most teams don’t check history further out than the last month regularly. Some teams may have compliance or regulatory burdens, but these are generally the exception. Don’t let the fear of “losing history” prevent you from modernizing your source control!\n\nMonorepo or Multirepo?\n\nThe other consideration we have to make is monorepo or multirepo? A monorepo is a Git repo that contains all the code for a system (or even organization). Generally, Git repos should be small - my rule of thumb is the repo boundary should be the deployment boundary. If you always deploy three services at the same time (because they’re tightly coupled) you may want to put the code for all three services into a single repo. Then again, you may want to split them and start moving to decouple them - only you can decide what’s going to be correct.\n\nIf you decide to split your repo into multiple Git repos, you’re going to have to consider what to do with shared code. In TFVC, you have shared code in the same repo as the applications, so you generally just have project references. However, if you split out the app code and the common code, you are going to have to have a way to consume the compiled shared code in the application code - that’s a good use case for package management. Depending on your source control structure, the complexity of your system and your team culture, this may not be easy to do - in that case you may decide to just convert to a monorepo instead of a set of smaller repos.\n\nThe Azure DevOps team decided to use a monorepo even though their system is composed of around 40 microservices. They did this because the source code for Azure DevOps (which Microsoft hosts themselves as a SaaS offering) is the same source code that is used for the on-premises out-of-the-box Azure DevOps Server (previously TFS). Their CI builds are triggered off paths in the repo instead of triggering a build for every component every time the repo is changed. If you decide to use a monorepo, make sure your CI system is capable of doing this - and make sure you organize your source code into appropriate folders for managing your builds!\n\nMigrating\n\nSo how can you migrate to Git? There are at least three ways:\n\n\n  Tip migration\n  Azure DevOps single branch import\n  Git-tfs import\n\n\nTip Migration\n\nMost teams I work with wish they could reorganize their source control structure - typically the structure the team is using today was set up by a well-meaning developer a decade ago but it’s not really optimal. Migrating to Git could be a good opportunity to restructure your repo. In this case, it probably doesn’t make sense to migrate history anyway, since you’re going to restructure the code (or break the code into multiple repos). The process is simple: create an empty Git repo (or multiple empty repos), then get-latest from TFS and copy/reorganize the code into the empty Git repos. Then just commit and push and you’re there! Of course if you have shared code you need to create builds of the shared code to publish to a package feed and then consume those packages in downstream applications, but the Git part is really simple.\n\nSingle Branch Import\n\nIf you’re on TFVC and you’re in Azure DevOps (aka VSTS) then you have the option of a simple single-branch import. Just click on “Import repository” from the Azure Repos top level drop-down menu to pop open the dialog. Then enter the path to the branch you’re migrating (yes, you can only choose one branch) and if you want history or not (up to 180 days). Then add in a name for the repo and let ‘er rip!\n\n\n\n\nThere are some limitation here: a single branch and only 180 days of history. However, if you only care about one branch and you’re already in Azure DevOps, then this is a no-brainer migration method.\n\nGit-tfs\n\nWhat if you need to migrate more than a single branch and retain branch relationships? Or you’re going to ignore my advice and insist on dragging all your history with you? In that case, you’re going to have to use Git-tfs. This is an open-source project that is build to synchronize Git and TFVC repos. But you can use it to do a once-off migration using\n\n\ngit tfs clone\n\n\n. Git-tfs has the advantage that it can migrate multiple branches and will preserve the relationships so that you can merge branches in Git after you migrate. Be warned that it can take a while to do this conversion - especially for large repos or repos with long history. You can easily dry-run the migration locally, iron out any issues and then do it for real. There’s lots of flexibility with this tool, so I highly recommend it.\n\nIf you’re on Subversion, then you can use Git svn to import your Subversion repo in a similar manner to using Git-tfs.\n\nConclusion\n\nModernizing source control to Git has high business value - most notably the ability to effectively isolate code changes, minimize merge debt and integrate unobtrusive code reviews which can improve quality. Add to this the broad user-base for Git and you have a tool that is both powerful and pervasive. With Azure DevOps, you can also add “enterprise” features like branch policies, easily manage large binaries and even large repos - so there’s really no reason not to migrate. Migrating to Git will cause some short-term pain in terms of learning curve, but the long term benefits are well worth it.\n\nHappy source controlling!\n",
      "categories": [],
      "tags": ["sourcecontrol"],
      
      "collection": "posts",
      "url": "/modernizing-source-control-migrating-to-git/"
    },{
      
      "title": ".NET Core Multi-Stage Dockerfile with Test and Code Coverage in Azure Pipelines",
      "date": "2019-01-17 07:39:29 +0000",
      
      "content": "\n  tl;dr\n  Adding Code Coverage\n  Removing the Redundancy\n  The Azure Pipelines YML File\n  Final Results\n  Conclusion\n\n\nI read a great blogpost recently by my friend and fellow MVP Jakob Ehn. In this post he outlines how he created a multi-stage Dockerfile to run .NET Core tests. I’ve always been on the fence about running tests during a container build - I usually run the tests outside and then build/publish the container proper only if the tests pass. However, this means I have to have the test frameworks on the build agent - and that’s where doing it inside a container is great, since the container can have all the test dependencies without affecting the host machine. However, if you do this then you’ll have test assets in your final container image, which isn’t ideal. Fortunately, with multi-stage Dockerfiles you can compile (and/or test) and then create a final image that just has the app binaries!\n\nI was impressed by Jakob’s solution, but I wanted to add a couple enhancements:\n\n\n  Jakob builds the container twice and runs the tests twice: one build for the test runs (in a shell task using the –target arg) and one to build the container proper - which would end up execute the tests again. I wanted to improve this if I could.\n  Add code coverage. I think that it’s almost silly to not do code coverage if you have tests, so I wanted to see how easy it was to add coverage to the test runs too!\n\n\ntl;dr\n\nIf you want the final process, have a look at my fork of the PartsUnlimited repo on Github (on the k8sdevops branch). You’ll see the final Dockerfile and the azure-pipelines.yml build definition file there.\n\nAdding Code Coverage\n\nI wanted to take things one step further and add code coverage into the mix. Except that doing code coverage in .NET Core is non-trivial. For that it seems you have to use Coverlet. I ended up adding a coverlet.msbuild package reference to my test project and then I just configured the test args for “dotnet test” to specify coverage options in the “dotnet test” command - we’ll see that in the Dockerfile next.\n\nRemoving the Redundancy\n\nJakob runs a shell script which builds the container only to the point of running the tests - he doesn’t want to build the rest of the container if the tests fail. However, when I was playing with this I realized that if tests fail, then the docker build process fails too - so I didn’t worry about the test and final image being in the same process. If the process completes, I know the tests have passed - if not, then I might have to diagnose to figure out if there is a build issue or a test issue, but logging in Azure pipelines is fantastic so that’s not too much of a concern.\n\nThe next issue was getting the test and coverage files out of the interim image and have a clean final image without test artifacts. That’s where labels come in. Let’s look at the final Dockerfile:\n\nFROM microsoft/dotnet:2.2-sdk AS build-env\nWORKDIR /app\nARG version=1.0.0\n\n# install npm for building\nRUN curl -sL https://deb.nodesource.com/setup_8.x | bash - &amp;amp;&amp;amp; apt-get update &amp;amp;&amp;amp; apt-get install -yq nodejs build-essential make\n\n# Copy csproj and restore as distinct layers\nCOPY PartsUnlimited.sln ./\nCOPY ./src/ ./src\nCOPY ./test/ ./test\nCOPY ./env/ ./env\n\n# restore for all projects\nRUN dotnet restore PartsUnlimited.sln\n\n# test\n# use the label to identity this layer later\nLABEL test=true\n# install the report generator tool\nRUN dotnet tool install dotnet-reportgenerator-globaltool --version 4.0.6 --tool-path /tools\n# run the test and collect code coverage (requires coverlet.msbuild to be added to test project)\n# for exclude, use %2c for ,\nRUN dotnet test --results-directory /testresults --logger \"trx;LogFileName=test_results.xml\" /p:CollectCoverage=true /p:CoverletOutputFormat=cobertura /p:CoverletOutput=/testresults/coverage/ /p:Exclude=\"[xunit.*]*%2c[StackExchange.*]*\" ./test/PartsUnlimited.UnitTests/PartsUnlimited.UnitTests.csproj\n# generate html reports using report generator tool\nRUN /tools/reportgenerator \"-reports:/testresults/coverage/coverage.cobertura.xml\" \"-targetdir:/testresults/coverage/reports\" \"-reporttypes:HTMLInline;HTMLChart\"\nRUN ls -la /testresults/coverage/reports\n\n# build and publish\nRUN dotnet publish src/PartsUnlimitedWebsite/PartsUnlimitedWebsite.csproj --framework netcoreapp2.0 -c Release -o out /p:Version=${version}\n\n# Build runtime image\nFROM microsoft/dotnet:2.2-aspnetcore-runtime\nWORKDIR /app\nEXPOSE 80\nCOPY --from=build-env /app/src/PartsUnlimitedWebsite/out .\nENTRYPOINT [\"dotnet\", \"PartsUnlimitedWebsite.dll\"]\n\n\nNotes:\n\n\n  Line 1: I’m getting the big bloated .NET Core SDK image which is required to compile, test and publish the app\n  Line 6: install npm prerequisites. I could create a custom build image with this on it, but it’s really quick if these dependencies don’t exist. If you’re running on a private agent, this layer is cached so you don’t do it on every run anyway.\n  Lines 9-12: copy app and test files into the container\n  Line 15: restore packages for all the projects\n  Line 19: add a label which we can use later to identify this layer\n  Line 21: install the report generator tool for coverage reports\n  Line 24: run “dotnet test” to invoke the test. I specify the results directory which I’ll copy out later and specify a trx logger to get a VSTest results file. The remainder of the args are for coverage: the format is cobertura, I specify a folder and specify some namespaces to exclude (note how I had to use %2c for commas to get this to work correctly)\n  Line 26: run the report generator tool to produce html coverage reports\n  Line 30: publish the app - this is the only bit I really want in the final image\n  Lines 33-37: copy the final binaries into an image based on the .NET Core runtime - which is far lighter than the SDK image the previous steps started on (about 10% of the size)\n  Line 36: this is where we do the actual copy of any artifacts we want in the final image\n\n\nWhen the build completes, we’ll end up with a number of interim images as well as a final deployable image with just the app - this is the image we’re going to push to container registries and so on. Doing some docker images queries shows how important slimming down the final image is:\n\n$&amp;gt; docker images --filter \"label=test=true\" | head -2\nREPOSITORY TAG IMAGE ID CREATED SIZE\n&amp;lt;none&amp;gt; &amp;lt;none&amp;gt; 13151da78ddb 2 hours ago 2.53 GB\n$&amp;gt; docker images | grep partsunlimited\npartsunlimitedwebsite 1.0.1 957346c64b03 2 hours ago 308 MB\n\n\nYou can see that we get a 2.53GB image for the SDK build process (it’s repo and tag are both &lt;none&gt; since this is an intermediary layer). The final image is only 308MB!\n\nYou’ll also note how we used the label in the filter expression to get only the layers that have a label “test=true”. If we add the “-q” parameter, we’ll get just the id of that layer, which is what we’ll need to get the test and coverage files out to publish in the CI build.\n\nThe Azure Pipelines YML File\n\nThe CI definition turns out to be quite simple:\n\nname: 1.0$(Rev:.r)\n\ntrigger:\n- k8sdevops\n\npool:\n  vmImage: 'Ubuntu-16.04'\n\nvariables:\n  imageName: 'partsunlimitedwebsite:$(build.buildNumber)'\n\nsteps:\n- script: docker build -f Dockerfile -t $(imageName) .\n  displayName: 'docker build'\n  continueOnError: true\n\n- script: |\n    export id=$(docker images --filter \"label=test=true\" -q | head -1)\n    docker create --name testcontainer $id\n    docker cp testcontainer:/testresults ./testresults\n    docker rm testcontainer\n  displayName: 'get test results'\n\n- task: PublishTestResults@2\n  inputs:\n    testResultsFormat: 'VSTest'\n    testResultsFiles: '**/test*.xml' \n    searchFolder: '$(System.DefaultWorkingDirectory)/testresults'\n    publishRunAttachments: true\n  displayName: 'Publish test results'\n\n- task: PublishCodeCoverageResults@1\n  inputs:\n    codeCoverageTool: 'cobertura'\n    summaryFileLocation: '$(System.DefaultWorkingDirectory)/testresults/coverage/coverage.cobertura.xml'\n    reportDirectory: '$(System.DefaultWorkingDirectory)/testresults/coverage/reports'\n  displayName: 'Publish coverage reports'\n\n\nNotes:\n\n\n  Lines 13-15: build and tag the docker image using the Dockerfile\n  Lines 17-22: get the id of the interim image and create a container. Then copy out the test results files and then delete the container.\n  Lines 24-30: publish the test file\n  Lines 32-37: publish the coverage results and reports\n\n\nFinal Results\n\nThe final results are fantastic. Below are screenshots of the summary page, the test results page, the coverage report and a drill-down to see coverage for a specific class:\n\n\n\nConclusion\n\nRunning tests (with code coverage) inside a container is actually not that bad - you need to do some fancy footwork after the build to get the test/coverage results, but all in all the process is pleasant. We’re able to run tests inside a container (not that this mandates real unit tests - tests that have no external dependencies!), get the results out and publish a super-slim final image.\n\nHappy testing!\n",
      "categories": [],
      "tags": ["docker","build"],
      
      "collection": "posts",
      "url": "/net-core-multi-stage-dockerfile-with-test-and-code-coverage-in-azure-pipelines/"
    },{
      
      "title": "Lessons about DevOps from 3D Printing",
      "date": "2019-02-07 01:11:20 +0000",
      
      "content": "\n  You Have To Just Jump In At Some Stage\n  Understanding Fundamentals Improves Your Success\n  Small Changes Can Have Radical Impact\n  Just When You Think Everything Is Perfect, Something Fails\n  Fast Feedback Is Critical\n  You Can Use a Printer To Improve a Printer\n  Sometimes You Have To Cut Your Losses And Try Again\n  Your Printer Is Unique\n  Conclusion\n\n\nIt’s no surprise that I’m passionate about DevOps. I think that has to do with my personality - my top five StrengthsFinder strengths are Strategic, Ideation, Learner, Activator, Achiever. I love the combination of people and tools that DevOps brings together. Being deeply technical and also fascinated by how people interact means I’m built for DevOps consulting. Add to that my love of learning, and I’m in my perfect job, since if there’s one thing I’ve learned about DevOps - it’s that I’ll never learn everything there is to learn about it!\n\n\n\n\nI recently got a 3D printer (an Ender3) for my birthday - I was actually looking for some Raspberry Pi projects to do with my kids when I saw a post about a guy who created some programmable LED Christmas lights. He’d printed himself a case for his Raspberry Pi and I wondered, “How much does a 3D printer go for nowadays?” I was pleasantly surprised to learn that you can get a pretty decent printer for around $200. So I got one for my birthday - and it’s been a ton of fun learning how to model, then slice models, then tweak the printer to make it bend to my will. But watching an abstract model turn into physical reality before your eyes is fantastic!\n\nWhile I was learning how to 3D print, I realized there are a lot of parallels between 3D printing and DevOps. Some may accuse me of “when you have a hammer, everything looks like a nail” and I suspect they’re partly right. But that doesn’t mean you can’t learn about one discipline from studying another! Remember, Lean and Agile have roots in auto manufacturing!\n\n\n\n\nSo here are some thoughts about DevOps that I got from 3D Printing.\n\nYou Have To Just Jump In At Some Stage\n\nI ordered my printer just before Christmas (my birthday is in January) but I had about 3 weeks between ordering and receiving my printer. I watched a ton of videos, read as much as I could, learned a couple of modeling and slicing programs - but no matter how much I read, I had to just start printing! There’s something about learning while you go that’s fun (and sometimes frightening) about both 3D printing and DevOps. Don’t get into “analysis paralysis” - start somewhere and you’ll be surprised how quickly you can move. Having a partner who can help you decide on some “low-lying fruit” will help you start faster - but don’t be afraid to start somewhere. Also, no matter how much theoretical knowledge you have, you’re going to have to start implementing and then adjust along the way.\n\nUnderstanding Fundamentals Improves Your Success\n\nWhile it was frustrating to wait so long for my printer to arrive, I am ultimately grateful because I learned a lot of fundamentals. Before ordering the printer I had no idea what PLA filament was or what slicers were. But learning the fundamentals of how printers actually work has helped me troubleshoot and improved my success.\n\nLearning about DevOps can help you on your journey - some teams implement automation and call it DevOps. This shows that they don’t fully understand what DevOps is about - and understanding the higher-level goals and history of DevOps can help you on your journey. Don’t just jump onto the buzz words - understand why they make a difference. Understanding fundamentals will help you improve your success.\n\nSmall Changes Can Have Radical Impact\n\nBecause Fused Deposition Modeling (FDM) printing is additive, the 1st layer is critical. This layer needs to bond correctly to the build bed, otherwise the print is doomed. Adjusting the bed to make it level and the correct height from the nozzle is a fiddly task - and small changes can make a huge difference.\n\nIn DevOps, sometimes small changes make a big difference. Be mindful of how you make changes in your teams, processes or tools. Making too many changes at once will prevent you from determining which changes are working and which are not. Also small changes let your team get some wins and build momentum.\n\nJust When You Think Everything Is Perfect, Something Fails\n\nOne of the challenges with printing is extrusion - the amount of plastic that is fed into the hot-end as the printer works. Too little and you get holes and missing layers, too much and you get blobs and stringing. The printer firmware has a multiplier for the extruder - if you program it to extrude 100mm of filament, it should extrude 100mm of filament! However, the stepper motor isn’t perfectly calibrated, so you have to tweak the multiplier to get the correct extrusion. I had gone through the process of setting the extruder multiplier and was happy with the prints I was getting. I wanted to install some upgrades and wanted to print a baseline print for comparison - but the baseline print was terrible! There was clear under-extrusion - which I wasn’t expecting since I hadn’t touched the extruder settings. Eventually I had to recalibrate the extruder multiplier again.\n\nIn DevOps, you never “arrive”. DevOps is a journey, and things can sometimes just blow up when you least expect. Remember that DevOps is more than just tooling and automation - people are a critical component of DevOps. And people change, new people come in or leave - and these changes can affect your culture - and therefore your DevOps. Keeping your eyes open, ensuring that you’re following the vision and making sure everyone is still with you is key to success.\n\nFast Feedback Is Critical\n\n\n\nSome prints can take a while - the Yoda print I made for my son took just over 7 hours. I watched closely (especially in the beginning) to make sure the first couple layers worked correctly - fortunately I got a good couple layers early on and the print turned out great. I have also done some other prints where the first couple layers didn’t bond to the build surface correctly, and I aborted before wasting time (and filament). Getting feedback quickly was critical - fortunately I got to see the layers as they ran, so I got immediate feedback.\n\nGetting feedback quickly is one of the primary goals of DevOps - reducing cycle times ensures that you can iterate rapidly and adjust rapidly. You may have heard the expression “Fail fast”. Rather get feedback after 2 weeks and adjust than go off for 3 months building the wrong thing. Whatever you do and however far you are on the DevOps journey, make sure that you get rapid feedback - both for the software you’re building and for your DevOps processes (how you’re building) so that you can adjust quickly and often.\n\nYou Can Use a Printer To Improve a Printer\n\nIt’s almost a rite of passage - when you get your printer, your first dozen prints are upgrades for your printer! Every print you make for your printer improves your printer so that you keep getting better prints.\n\nIn DevOps, you can apply principles for good software delivery to the process itself. How about “evidence gathered in production”? There’s no place like production, so that’s where you want to get usage and performance metrics from. Similarly, you want to measure your team in their native habitat to see how they’re doing. Reducing cycle times and getting feedback fast improves your software, but are you applying the same principles to your processes? Try something, evaluate, and abort quickly if it’s not working.\n\nAt some stage, however, you need to print something other than parts for your printer. If all I did was print parts, then the parts become pointless since I bought the printer to enable me to print prototypes or art or whatever, not just printer parts. Don’t navel gaze too much into your DevOps processes and remember that the ultimate goal is to deliver value to your customers!\n\nSometimes You Have To Cut Your Losses And Try Again\n\nI love board games - and we have a good collection of them. I wanted to print some inserts for sorting the cards and components to Dead of Winter - and the prints kept failing. I got a bit frustrated and stopped printing (for now) until I’ve got some confidence back.\n\nIn DevOps, you can try things and if they fail, you may have to cut your losses and start again. Perhaps you need to try again - perhaps you need to try something different. Don’t give up because something didn’t work like you expected it to. Regroup, recoup and try again!\n\nYour Printer Is Unique\n\nI’d seen a few experts recommend TL smoothers to improve prints. The stepper motors on a printer are driven by voltage differentials, and the TL smoother is a set of diodes that prevent a voltage “flutter” when the voltage dips towards 0 - which smooths the movement of the stepper motor. So I decided I want to get some. I got a baseline print, installed the smoothers and repeated the print. Absolutely no difference. To be fair, the smoothers only cost $12 so it’s not a big deal. It turns out that my Ender3 probably has better components than some earlier models, so the smoothers weren’t necessary, even though experts had recommended them.\n\nWith DevOps, you’re far better off being pragmatic about how and when to apply changes in your processes, tools and people. Don’t just implement blindly - understand how DevOps practices will affect your team and how best to implement them for your team. Just because another organization or team was successful with some practice, doesn’t mean you have to do it in the same way (or at all). Each team, environment and organization is different, so your DevOps could look different from that in other orgs. As Dori says, “Just keep swimming!”\n\nConclusion\n\nThere’s a ton to learn about DevOps from 3D printing. As with anything, we’re all on a journey. But sometimes we have to remember to step back and remember how far we’ve come - and why we’re on the journey in the first place! Hopefully this reflection is positive for you.\n\nHappy printing!\n",
      "categories": [],
      "tags": ["devops"],
      
      "collection": "posts",
      "url": "/lessons-about-devops-from-3d-printing/"
    },{
      
      "title": "Container DevOps: Beyond Build (Part 5) - Prometheus Operator",
      "date": "2019-05-08 09:35:23 +0000",
      
      "content": "\n  Container DevOps Recap: The Importance of Monitoring\n  Prometheus Operator    \n      Configuring Prometheus Operator\n      Configuring ServiceMonitors\n    \n  \n  PromQL\n  Conclusion\n\n\nSeries:\n\n\n  Part 1: Intro\n  Part 2: Traefik Basics\n  Part 3: Canary Testing\n  Part 4: Telemetry with Prometheus\n  Part 5: Prometheus Operator (this post)\n\n\nIn part 4 of this series I showed how I created a metrics endpoint using Prometheus in my .NET Core application. While not perfect for business telemetry, Prometheus is a standard for performance metrics. But that only exposes an endpoint for metrics - it doesn’t do any visualization. In this post I’ll go over how I used Prometheus Operator in a k8s cluster to easily scrape metrics from services and then in the next post I’ll cover how I configured Grafana  to visualize those metrics - first by hand and then using infrastructure-as-code so that I can audit and/or recreate my entire monitoring environment from source code.\n\nContainer DevOps Recap: The Importance of Monitoring\n\nMonitoring is often the black sheep of DevOps - it’s not stressed very much. I think that’s partly because monitoring is hard - and often, contextual. Boards for work management and pipelines for build and release are generally more generic in concept and most teams starting with DevOps seem to start with these tools. However, Continuous Integration and Continuous Deployment should be complimented by Continuous Monitoring.\n\nOne of my DevOps heroes (and by luck of life, friend) Donovan Brown coined the quintessential definition of DevOps a few years ago: DevOps is the union of people, products and process to enable continuous delivery of value to end users. I’ve heard some folks criticize this definition for its lack of mention of monitoring among other things - but I think that a lot of Donovan’s definition is implied (at least should be implied) in the phrase “value”.\n\nMost teams think of value in terms of features: I’d like to propose that monitoring as a mechanism of keeping systems stable, resilient and responsive is just as important as delivering features. So in a very real sense, his definition implies monitoring. I’ve also heard Donovan state that it doesn’t matter how good your code is, if it’s not in the hands of your users, it doesn’t deliver value. In the same vein, it doesn’t matter how good your features are: if you can’t monitor for errors, scale or usage then you’re missing delivering value for your users.\n\nIn a world of microservices and Kubernetes, the need for solid monitoring is paramount, and more difficult. Monoliths may be hard to change, but they are by and large easy to monitor. Microservices increase the complexity of monitoring, but there are some techniques that teams can use to manage the complexity.\n\nPrometheus Operator\n\nIn the last post I showed how I exposed Prometheus metrics from my services. Imagine you have 10 or 15 services - how do you monitor each one? Exposing metrics via Prometheus is all well and good, but how do you aggregate and visualize the metrics that are being produced? The first step is Prometheus itself - or the Prometheus instance (to be disambiguated by the Prometheus metrics endpoint that containers or services expose).\n\nIf you were manually setting up an instance of Prometheus, you would have to install the pods and services in a k8s cluster as well as configure Prometheus to tell it where to scrape metrics from. This manual process is complex, error prone and time-consuming: enter the Prometheus Operator.\n\nInstalling the Prometheus operator (and instance) itself is simple thanks to the official helm chart.  This also (optionally) includes endpoints for monitoring the health of your k8s cluster components via kube-prometheus. It also installs AlertManager for automating alerts - I haven’t played with this though.\n\nK8s Operators are a mechanism for deploying applications to a k8s cluster - but these applications tend to be “smarter” than regular k8s applications in that they can hook into the k8s lifecycle. Point-in-case: scraping telemetry for a newly deployed service. The Prometheus Operator will automagically update the Prometheus configuration via the k8s API when you declare that a new service has Prometheus endpoints. This is done via a custom resource definition (CRD) that is created by the Prometheus helm chart: ServiceMonitors. When you create a service that exposes a Prometheus metrics endpoint, you simply declare a ServiceMonitor alongside your service to dynamically update Prometheus and let it know that you have a new service that can be scraped: including which port and how frequently to scrape.\n\nConfiguring Prometheus Operator\n\nThe helm chart for Prometheus Operator is a beautiful thing: it means you can install and configure a Prometheus instance, the Prometheus Operator and kube-prometheus (for monitoring cluster components) with a couple lines of script. Here’s how I do this in my release pipeline:\n\nhelm repo add coreos https://s3-eu-west-1.amazonaws.com/coreos-charts/stable/\nhelm upgrade --install prometheus-operator coreos/prometheus-operator --namespace monitoring\nhelm upgrade --install kube-prometheus coreos/kube-prometheus --namespace monitoring\n\n\nNotes:\n\n\n  Line 1: Add the CoreOS repo for the stable Prometheus operator charts\n  Line 2: Install (or upgrade) the Prometheus operator into a namespace called “monitoring”\n  Line 3: Install the kube-prometheus components - this gives me cluster monitoring\n\n\nThese commands are also idempotent so I can run them every time without worrying about current state - I always end up with the correct config. Querying the services in the monitoring namespace we see the following:\n\nalertmanager-operated ClusterIP None &lt;none&gt; 9093/TCP,6783/TCP 60d\nkube-prometheus ClusterIP 10.0.240.33 &lt;none&gt; 9090/TCP 60d\nkube-prometheus-alertmanager ClusterIP 10.0.168.1 &lt;none&gt; 9093/TCP 60d\nkube-prometheus-exporter-kube-state ClusterIP 10.0.176.16 &lt;none&gt; 80/TCP 60d\nkube-prometheus-exporter-node ClusterIP 10.0.251.145 &lt;none&gt; 9100/TCP 60d\nprometheus-operated ClusterIP None &lt;none&gt; 9090/TCP 60d\n&lt;/none&gt;&lt;/none&gt;&lt;/none&gt;&lt;/none&gt;&lt;/none&gt;&lt;/none&gt;\n\n\nConfiguring ServiceMonitors\n\nNow that we have a Prometheus instance (and the Operator) configured we can examine how to tell the Operator that there’s a new service to monitor. Fortunately, now that we have the ServiceMonitor CRD, it’s pretty straight-forward: we just declare a ServiceMonitor resource alongside our service! Let’s take a look at one:\n\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: website-monitor\n  namespace: monitoring\n  labels:\n    prometheus: kube-prometheus\n    tier: website\nspec:\n  jobLabel: app\n  selector:\n    matchLabels:\n      release: pu-dev-website\n      system: PartsUnlimited\n      app: partsunlimited-website\n  namespaceSelector:\n    any: true\n  endpoints:\n  - port: http\n    path: /site/metrics\n    interval: 15s\n\n\nNotes:\n\n\n  Lines 1-2: We’re using the custom resource ServiceMonitor\n  Line 7: We’re using a label to ringfence this Service - we’ve configured the Prometheus service to look for ServiceMonitors with this label\n  Lines 11-15: This ServiceMonitor applies to all services with these matching labels\n  Lines 18-21: We configure the port (a named port in this case) and path for the Prometheus endpoint, as well as what frequency to scrape the metrics\n\n\nWhen we create this resource, the Operator picks up the creation of the ServiceMonitor resource via the k8s API and configures the Prometheus server to now scrape metrics from our service(s).\n\nPromQL\n\nNow that we have some metrics going into Prometheus, we have the ability to query the metrics. We start by port-forwarding the Prometheus instance so that we can securely access it (you can also expose the instance publicly if you want to):\n\n\nkubectl port-forward svc/kube-prometheus -n monitoring 9090:9090\n\n\nThen we can browse to http://localhost:9090/graph and see the Prometheus instance. We can then query metrics using PromQL - a language to query, aggregate and visualize metrics. For example, to see the rate of increase in sales by category for the last 2 minutes, we can write\n\n\nsum(increase(pu_product_add[2m])) by (category)\n\n\nAnd then we can visualize it by clicking on the graph button:\n\n\n\n\nThis is a proxy for how well sales are doing on our site by category. And while this is certainly great, it’s far better to visualize Prometheus metrics in Grafana dashboards - which we can do using the PromQL queries. But that’s a topic for a future post in this series!\n\nConclusion\n\nPrometheus and the Prometheus Operator make configuring metric aggregation really easy. It’s declarative, dynamic and intuitive. This makes it a really good framework for monitoring services within a k8s cluster. In the next post I’ll show how you can hook Grafana up to the Prometheus server in order to make visualizations of the telemetry. Until then:\n\nHappy monitoring!\n",
      "categories": [],
      "tags": ["docker","devops"],
      
      "collection": "posts",
      "url": "/container-devops-beyond-build-part-5-prometheus-operator/"
    },{
      
      "title": "Container DevOps: Beyond Build (Part 1)",
      "date": "2019-05-08 09:36:17 +0000",
      
      "content": "\n  PartsUnlimited 1.0\n  Components of Container DevOps    \n      Building Quality\n      Environment Isolation\n      Canary Testing\n      Monitoring\n      Security\n      Resiliency\n      Infrastructure as Code\n    \n  \n  Conclusion\n\n\nSeries:\n\n\n  Part 1: Intro (this post)\n  Part 2: Traefik Basics\n  Part 3: Canary Testing\n  Part 4: Telemetry with Prometheus\n  Part 5: Prometheus Operator\n\n\nI’ve written before that I think that containers - and Kubernetes (k8s) - are the way of the future. I was fortunate enough to attend my first KubeCon last year in Seattle, and I was happy to see the uptake of k8s and the supporting cloud native technologies around k8s are robust and healthy. But navigating the myriad of services, libraries and techniques is a challenge! This is going to be the first in a series of posts about Container DevOps - and I don’t just mean building images and deploying them. What about monitoring? And A/B testing? And all the other stuff that successful DevOps teams are supposed to be doing? We’ll look at how you can implement some of these tools and techniques in this series.\n\nPartsUnlimited 1.0\n\nFor the last couple of years I’ve had the opportunity to demo DevOps using Azure DevOps probably a few hundred times. I built a demo in Azure DevOps using a fork of Microsoft’s PartsUnlimited repo. When I originally built the demo, the .NET Core tooling was a bit of a mess, so I just stuck with the full framework version. The demo targets Azure App Services and shows how you can make a change in code, then submit a Pull Request, which triggers a build that compiles, runs static code analysis and unit tests and triggers a release, which deploys the new version of the app to a staging slot in the Azure web app, routing some small percentage of all traffic to the slot for canary testing. All the while metrics are being collected in Application Insights, and after doing the canary deployment, you can analyze the metrics and decide if the canary is successful or not - and then either promote it to the rest of the site or just shift all traffic to the existing prod version in the case of a failure.\n\nBut how do you do the same sort of thing with k8s? That’s what I set out to discover. But before we get there, let’s take a step back and consider what we should be investigating in the world of Container DevOps in the first place!\n\nComponents of Container DevOps\n\nHere are some of the components of Container DevOps that I think need to be considered:\n\n\n  Building Quality - including multi-stage image builds, reportable unit testing and publishing to secure image repositories\n  Environment isolation - isolating Dev, Test and Prod environments\n  Canary Testing - testing changes on a small set or percentage of users before promoting to everyone (also called Testing in Production or Blue/Green testing)\n  Monitoring - producing, consuming and analyzing metrics about your services\n  Security - securing your services using TLS\n  Resiliency - making services resilient through throttling or circuit-breakers\n  Infrastructure and Configuration as Code\n\n\nThere are some more that I think should be on the list that I haven’t yet gotten to exploring in detail - such as vulnerability scanning. Hopefully I get around to adding to the above list, but we’ll start here for now.\n\nBuilding Quality\n\nI’ve previously blogged about how to run unit tests - and publish the test and code coverage results - in Azure DevOps pipelines. It was a good exercise, but as I look at it now I realize why I prefer to build code outside the container and copy the binaries in: it’s hard to do ancillary work (like unit test, code scans etc.) in a Dockerfile. One advantage to the multi-stage Dockerfile that you’ll lose is the dependency management - you have to manage that on the build machine (or container) if you’re building the code outside the Dockerfile. But I think the dependency management ends up being simpler than trying to run (and publish) tests and test coverage and static analysis inside the Dockerfile. My post covered how to do unit testing/code coverage, but when I thought about adding SonarQube analysis or vulnerability scanning with WhiteSource, I realized the Dockerfile starts becoming clumsy. I think it’s easier to just drop in the SonarQube and WhiteSource tasks into a pipeline and build on the build machine - and then just have a Dockerfile copy the compiled binaries in to create the final light-weight container image.\n\nEnvironment Isolation\n\nThere are a couple of ways to do this: the most isolated (and expensive) is to spin up a k8s cluster per environment - but you can achieve good isolation using k8s namespaces. You could also use a combination: have a Prod cluster and a Dev/Test cluster that uses namespaces to separate environments within that cluster.\n\nCanary Testing\n\nThis one took a while for me to wrap my head around. Initially I struggled with this concept because I was too married to the Azure Web App version, which works as follows:\n\n\n  Traffic Manager routes 0% traffic to the “blue” slot\n  Deploy the new build to the blue slot - it doesn’t matter if this slot is down, since no traffic is incoming anyway\n  Update Traffic Manager to divert some percentage of traffic to the blue slot\n  Monitor metrics\n  If successful, swap the “blue” slot with the production slot (an instantaneous action) and update Traffic Manager to route 100% of traffic to the new production slot\n\n\nI started trying to achieve the same thing in k8s, but k8s doesn’t have the notion of slots. An easy enough solution is to have a separate Deployment (and Service) for “blue” and “green” versions. But then there’s no Traffic Manager - so I stated investigating various Ingresses and Ingress Controllers to see if I could get the same sort of functionality. I initially got a POC running in Istio - but was still “swapping slots” in my mental model. Unfortunately, Istio, while very capable, is large and complicated - it felt like a sledge-hammer when all I needed was a screwdriver. I then tried linkerd - which was fine until I hit some limitations. Finally, I tried Traefik - and I found I could do everything I wanted to (and more) using Traefik. There’s definitely a follow on post here detailing this part of my Container DevOps journey - so stay tuned!\n\nThe other mental breakthrough came when I realized that Deployments (unlike slots in Azure Web Apps) are inherently highly available: that is, I can deploy a new version of a Deployment and k8s automatically does rolling updates to ensure the Deployment is never “down”. So I didn’t have to worry about diverting traffic away from the “blue” Deployment while I was busy updating it - and I can even keep the traffic split permanent. What that means is that I have two versions of the Deployment/Service: a “blue” one and a “green” one. I set up traffic splitting using Traefik and route say 20% of traffic to the “blue” service. When rolling out a new version, I simply update the tag of the image in the Deployment and k8s automatically does a rolling update for me - the blue slot never goes down and it’s “suddenly” on the new version. Then I can monitor metrics, and if successful, I can update the “green” Deployment. Now both Blue and Green Deployments are on the new version, so while the Blue Deployment is getting 20% of the traffic, the versions are the same, so 100% of the traffic is now on the new version - and I had zero downtime! Of course I can simply revert the Blue Deployment back to the old version to get the same effect if the experiment is NOT a success.\n\nOne more snippet to whet your appetite for future posts on this topic: Traefik also handles TLS certs via LetsEncrypt, circuit-breaking and more.\n\nMonitoring\n\nI am a huge fan of Application Insights - and there’s no reason not to continue logging to AppInsights from within your containers - assuming your containers can reach out to Azure. However, I wanted to see how I could do monitoring completely “in-cluster” and so I turned to Prometheus and Grafana. This is also a subject for another blog post, but I managed to (without too much hassle) get the following to work:\n\n\n  Export Prometheus metrics from .NET Core containers\n  Create a Prometheus Operator in my k8s cluster to easily (and declaratively) add Prometheus scraping to new deployments\n  Create custom dashboards in Grafana\n  Export the dashboards to code so that I can deploy them from scratch for new clusters\n\n\nI didn’t explore AlertManager - but this would be essential for a complete monitoring solution. However, the building blocks are in place. I also found that “business telemetry” is difficult in Prometheus. By business telemetry I mean telemetry that has nothing to do with performance - things like how many products from category A were sold in a particular date range? AppInsights made “business telemetry” a breeze - the closest I could get in Prometheus was some proxy telemetry that gave me some idea of what was happening from a “business” perspective. Admittedly, Prometheus is a performance metric framework, so I wasn’t too surprised.\n\nSecurity\n\nThere’s a whole lot to security that I didn’t fully explore - especially in-cluster Role Based Access Control (RBAC). What I did explore was how to secure your services using certificates - and how to do that declaratively and easily as you roll out a publicly accessible service. Again Traefik made this simple - I’ll detail how I did it in my Traefik blogpost. As a corollary, I did also isolate “internal” from “external” services - the internal services are not accessible from outside the cluster at all - the simplest way to secure a service!\n\nResiliency\n\nI’ve already mentioned how using Deployments with rolling updates gives me zero-downtime deployments “for free”. But what about throttling services that are being overwhelmed with connections? Or circuit-breakers? Again Traefik came to the rescue - and again, details are coming up in a post dedicated to how I configured Traefik.\n\nInfrastructure as Code\n\nIt (almost) goes without saying that all of these capabilities should be doable from scripts and templates - and that there shouldn’t be any manual steps. I did that from the first - using Azure CLI scripts to spin up my Azure Kubernetes Service (AKS) clusters, and configure Public IPs and Load Balancers. I used some bash scripts for doing kubectl commands and finally used Helm for packaging my applications so that deployment is a breeze - including creating Ingresses, ServiceMonitors, Deployments, Secrets and all the pieces you need to run your services.\n\nConclusion\n\nI know I haven’t showed very much - but I’ve gotten all of the pieces in place and will be blogging about how I did it - and sharing the code too! The point is that Container DevOps is more than just building images - there is far involved to do mature DevOps, and it’s possible to achieve all of these mature practices using k8s. Traefik is definitely the star of the show! For now, I’ve hopefully prodded you into thinking about how to do some of these practices yourself.\n\nHappy deploying!\n",
      "categories": [],
      "tags": ["docker"],
      
      "collection": "posts",
      "url": "/container-devops-beyond-build-part-1/"
    },{
      
      "title": "Container DevOps Beyond Build: Part 2 - Traefik",
      "date": "2019-05-08 09:36:39 +0000",
      
      "content": "\n  Works In My Orchestrator ™\n  Ops Criteria    \n      Internal and External Routing\n      Secure Communication\n      Traffic Shifting\n      Resiliency\n      Tracing\n    \n  \n  Configuring Traefik Controllers\n  Configuring Ingresses\n  Conclusion\n\n\nSeries:\n\n\n  Part 1: Intro\n  Part 2: Traefik (this post)\n  Part 3: Canary Testing\n  Part 4: Telemetry with Prometheus\n  Part 5: Prometheus Operator\n\n\nIn Part 1 of this series, I outlined some of my goals and some of the thinking around what I think Container DevOps is - it’s far more than just being able to build and run a container or two. Beyond just automating builds, you have to think about how you’re going to release. Zero downtime and canary testing, resiliency and monitoring are all table stakes - but while I understand how to do that using Azure Web Apps, I hadn’t done a lot of these for containerized applications. After working for a couple of months on a Kubernetes (k8s) version of .NET Core PartsUnlimited, I have some thoughts to share on how I managed to put these practices into place.\n\nWhen thinking about running containers in production, you have to think about the end to end journey, starting at building images right through deployment and into monitoring and tracing. I’m a firm believer in building quality into the pipeline early, so automated builds should unit test (with code coverage), do static analysis and finally package applications. In “traditional web” builds, the packaging usually means a zip or WebDeploy package or NPM package or even just a drop of files. When building container images, you’re inevitably using a Dockerfile - which makes compiling and packaging simple, but leaves a lot to be desired when you want to test code or do static analysis, package scanning and other quality controls. I’ve already blogged about how I was able to add unit testing and code coverage to a multi-stage Dockerfile - I just got SonarQube working too, so that’s another post in the works.\n\nWorks In My Orchestrator ™\n\nHowever, assume that we have an image in our container registry that we want to deploy. You’ve probably run that image on your local machine to make sure it at least starts up and exposes the right ports, so it works on your machine. Bonus points if you ran it in a development Kubernetes cluster! But now how do you deploy this new container to a production cluster? If you just use k8s Deployment rolling update strategy. you’ll get zero-downtime for free, since k8s brings up the new container and replaces the existing ones only when the new ones are ready (assuming you have good liveness and readiness probes defined). But how do you test the new version for only a small percentage of users? Or secure traffic to that service? Or if you’re deploying multiple services (microservices anyone?) how do you monitor traffic flow in the service mesh? Or cut out “bad” services so that they don’t crash your entire system?\n\nWith these questions in mind, I started to investigate how one does these sorts of things with deployments to k8s. The rest of this post is about my experiences.\n\nOps Criteria\n\nHere’s the list of criteria I had in mind to cover - and I’ll evaluate three tools using these criteria:\n\n\n  Internal and External Routing - I want to be able to define how traffic “external” traffic (traffic originating outside the cluster) and “internal” traffic (traffic originating and terminating within the cluster) is routed between services.\n  Secure Communication - I want communication to endpoints to be secure - especially external traffic.\n  Traffic Shifting - I want to be able to shift traffic between services - especially for canary testing.\n  Resiliency - I want to be able to throttle connections or implement circuit breaking to keep my app as a whole resilient.\n  Tracing - I want to be able to see what’s going on across my entire application.\n\n\nI explored three tools: Istio, Linkerd and Traefik. I’ll evaluate each tool against the five criteria above.\n\nSpoiler: Traefik won the race!\n\nDisclaimer: some of these tools do more than these five things, so this isn’t a wholistic showdown between these tools - it’s a showdown over these five criteria only. Also, Traefik is essentially a reverse proxy on steroids, while Istio and Linkerd are service meshes - so you may need some functionality of a service mesh that Traefik can’t provide.\n\nInternal and External Routing\n\nAll three tools are capable of routing traffic. Istio and Linkerd both inject sidecar proxies to your containers. I like this approach since you can abstract away the communication/traffic/monitoring from your application code. This seemed to be promising, and while I was able to get some of what I wanted using both Istio and Linkerd, both had some challenges. Firstly, Istio is huge, rich and complicated. It has a lot of Custom Resource Definitions (CRDs) - more than k8s itself in fact! So while it worked for routing like I wanted, it seemed very heavy. Linkerd worked for external routing, but due to limitations in the current implementation, I couldn’t get it working to route internal traffic.\n\nLet’s say you have a website and make a code change - you want to test that in production - but only to a small percentage of users. With Azure App Services, you can use Traffic Manager and deployment slots for this kind of canary testing. Let’s say you get the “external” routing working - most clients connecting to your website get the original version, while a small percentage get the new version. This is what I mean by “external” traffic. But what if you have a microservice architecture and your website code is calling internal services which call other internal services? Surely you want to be able to do the same sort of traffic shifting - that’s “internal” routing - routing traffic internally within the cluster. Linkerd couldn’t do that for me - mostly due to incompatibility between the control plane and the sidecars, I think.\n\nTraefik did this easily via Ingress Controllers (abstractions native to k8s). I set up two controllers - one to handle “external” traffic and one to handle “internal” traffic - and it worked beautifully. More on this later.\n\nSecure Communication\n\nI didn’t explore this topic too deeply with either Istio or Linkerd, but Traefik made securing external endpoints with certificates via LetsEncrypt really easy. I tried to get secure communication for my internal services, but I was trying with a self-signed cert and I think that’s what prevented it from working. I’m sure that you could just as easily add this capability into internal traffic using Traefik if you really needed to. We’ll see this later too - but using a static IP and DNS on an Azure Load Balancer, I was able to get secure external endpoints with very little fuss!\n\nTraffic Shifting\n\nIf you’ve got routing, then it follows that you should be able to shift traffic to different services (or more likely, different versions of the same service). I got this working in Istio (see my Github repo and mardown on how I did this here) and Linkerd only worked for external traffic. With Istio you shift by defining a VirtualService - it’s an Istio CRD that’s a love-child between a Service and an Ingress. With Linkerd, traffic rules are specified using dtabs - it’s a cool idea (abstracting routes) but the implementation was horrible to work with - trying to learn the obscure format and debug it was not great.\n\nBy far the biggest problem with both Istio and Linkerd is that their network routing doesn’t understand readiness or liveness probes since the work via their sidecar containers. This becomes a problem when you’re deploying a new version of a service using a rolling upgrade - as soon as the service is created, Istio or Linkerd start sending traffic to the endpoint, irrespective of the readiness of that deployment. You can probably work around this, but I found that I didn’t have to if I used Traefik.\n\nTraefik lets you declaratively specify weight rules to shift traffic using simple annotations on a standard Ingress resource. It’s clean and intuitive when you see it. The traffic shifting also obeys readiness/liveness probes, so you don’t start getting traffic routed to services/deployments that are not yet ready. Very cool!\n\nResiliency\n\nFirst, there’s a lot of things to discuss in terms of resiliency - for this post I’m just looking at features like zero-downtime deployment, circuit breaking and request limiting. Istio and Linkerd both have control planes for defining circuit breakers and request limiting - Traefik let’s you define these as annotations. Again, this comparison is a little apples-for-oranges since Traefik is “just” a reverse proxy, while Istio and Linkerd are service meshes. However, the ease of declaration of these features is so simple in Traefik, it’s compelling. Also, since Traefik builds “over” rolling updates in Deployments, you get zero-downtime deployment for free. If you’re using Istio, you have to be careful about your deployments since you can get traffic to services that are not yet ready.\n\nTracing\n\nTraefik offloads monitoring to Prometheus and the helm chart has hooks into DataDog, Zipkin or Jaeger for tracing. For my experiments, I deployed Prometheus and Grafana for tracing and monitoring. Both Istio and Linkerd have control planes that include tracing - including mesh visualization - which can be really useful for tracing microservices since you can see traffic flow within the mesh. With Traefik, you need additional tools.\n\nConfiguring Traefik Controllers\n\nSo now you know some of the reasons that I like Traefik - but how do you actually deploy it? There are a couple components to Traefik: the Ingress Controller (think of this as a proxy) and then ingresses themselves - these can be defined at the application level and specify how the controller should direct traffic to the services within the cluster. There’s another component (conceptually) and that is the ingress class: you can have multiple Traefik ingress controllers, and if you do, you need to specify a class for each controller. When you create an ingress, you also annotate that ingress to specify which controller should handle its traffic - you’re essentially carving the ingress space into multiple partitions, each handled by a different controller.\n\nFor the controller, there are some other “under the hood” components such as secrets, config maps, deployments and services - but all of that can be easily deployed and managed via the Traefik Helm chart. You can quite easily deploy Traefik with a lot of default settings using\n\n\n--set\n\n\nfrom the command line, but I found it started getting unwieldy. I therefore downloaded the default values.yml file and customized some of the values. When deploying Traefik, I simply pass in my customized values.yml file to specify my settings.\n\nFor my experiments I wanted two types of controller: an “external” controller that was accessible from the world and included SSL. I also wanted an “internal” controller that was not accessible outside of the cluster that I could use to do internal routing. I use Azure Kubernetes Service (AKS), so the code for this series assumes that.\n\nLet’s take a look at the values file of the “internal” controller:\n\nimage: traefik\nimageTag: 1.7.7\nserviceType: NodePort\n\nkubernetes:\n  ingressClass: \"dev-traefik-internal\"\nssl:\n  enabled: false\n\nacme:\n  enabled: false\n\ndashboard:\n  enabled: true\n  domain: traefik-internal-ui.aks\nrbac:\n  enabled: true\nmetrics:\n  prometheus:\n    enabled: true\n    restrictAccess: false\n  datadog:\n    enabled: false\n  statsd:\n    enabled: false\n\n\nNotes:\n\n\n  Lines 1-3: The image it “traefik” and we want the 1.7.7 version. Since this is just internal, we only need a NodePort service (I tried ClusterIP, but that didn’t work).\n  Line 6: we want this ingress controller to watch and manage traffic for Ingresses that have this class as their annotation. This is how we have multiple Traefik controllers within a cluster. I prepend the class with the namespace (dev) too!\n  Lines 7,8: Since this is an internal ingress, we don’t need SSL. I tried to get this working, but suspect I had issues with the certs. If you need internal SSL, this is where you’d set it.\n  Lines 10,11: This is for generating a cert via LetsEncrypt. Not needed for our internal traffic.\n  Lines 14,15: Enable the dashboard. I accessed via port-forwarding, so the domain isn’t critical.\n  Lines 16,17: RBAC is enabled.\n  Lines 18-25: tracing options - I just enabled Prometheus.\n\n\nLet’s now compare the values file for the “external” controller:\n\nimage: traefik\nimageTag: 1.7.7\nserviceType: LoadBalancer\nloadBalancerIP: \"101.22.98.189\"\n\nkubernetes:\n  ingressClass: \"dev-traefik-external\"\nssl:\n  enabled: true\n  enforced: true\n  permanentRedirect: true\n  upstream: false\n  insecureSkipVerify: false\n  generateTLS: false\n\nacme:\n  enabled: true\n  email: myemail@somewhere.com\n  onHostRule: true\n  staging: false\n  logging: false\n  domains:\n    enabled: true\n    domainsList:\n      - main: \"mycoolaks.westus.cloudapp.azure.com\"\n  challengeType: tls-alpn-01\n  persistence:\n    enabled: true\n    \ndashboard:\n  enabled: true\n  domain: traefik-external-ui.aks\n  \nrbac:\n  enabled: true\nmetrics:\n  prometheus:\n    enabled: true\n    restrictAccess: false\n  datadog:\n    enabled: false\n  statsd:\n    enabled: false\n\ntracing:\n  enabled: false\n\n\nMost of the file is the same, but here are the differences:\n\n\n  Line 4: We specify the static IP we want the LoadBalancer to use - I have code that pre-creates this static IP (with DNS name) in Azure before I execute this script.\n  Line 7: We specify a different class to divide the “ingress space”.\n  Lines 8-14: These are the LetsEncrypt settings, including the domain name, challenge type and persistence to store the cert settings.\n\n\nNow that we have the controllers (internal and external) deployed, we can deploy “native” k8s services and ingresses (with the correct annotations) and everything Will Just Work ™.\n\nConfiguring Ingresses\n\nAssuming you have the following service:\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: dev-partsunlimitedwebsite\n  namespace: dev\nspec:\n  type: NodePort\n  selector:\n    app: partsunlimited-website\n    function: web\n  ports:\n  - name: http\n    port: 80\n\n\nThen you can define the following ingress:\n\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n    kubernetes.io/ingress.class: dev-traefik-external\n  labels:\n    app: partsunlimited-website\n\n    function: web\n  name: dev-website-ingress\n  namespace: dev\nspec:\n  rules:\n  - host: mycoolaks.westus.cloudapp.azure.com\n    http:\n      paths:\n      - backend:\n          serviceName: dev-partsunlimitedwebsite\n          servicePort: http\n        path: /site\n\n\nNotes:\n\n\n  Line 2: This resource is of type “Ingress”\n  Lines 4,5: We define the class - this ties this Ingress to the controller with this class - for our case, this is the “external” Traefik controller\n  Lines 12-18: We’re specifying how the Ingress Controller (Traefik in this case) should route traffic. This is the simplest configuration - take requests coming to the host “mycoolaks.westus.cloudapp.azure.com” and route them to the “dev-partsunlimitedwebsite” service onto the “http” port (port 80 if you look at the service definition above).\n  Line 19: We can use the Traefik controller to front multiple services - using the path helps to route effectively.\n\n\nWhen you access the service, you’ll see the secure padlock in the browser window and be able to see details for the valid cert:\n\n\n\n\nThe best thing is I didn’t have to generate the cert myself - Traefik did it all for me.\n\nThere’s more that we can configure on the Ingress - including the traffic shifting for canary or A/B testing. We can also annotate the service to include circuit-breaking - but I’ll save that for another post now that I’ve laid out the foundation for traffic management using Traefik.\n\nConclusion\n\nContainer DevOps requires thinking about how traffic is going to flow in your cluster - and while there are many tools for doing this, I like the combination of simplicity and power you get with Traefik. There’s still a lot more to explore in Container DevOps - hopefully this post gives you some insight into my thoughts.\n\nHappy container-ing!\n",
      "categories": [],
      "tags": ["docker"],
      
      "collection": "posts",
      "url": "/container-devops-beyond-build-part-2-traefik/"
    },{
      
      "title": "Container DevOps: Beyond Build (Part 3) - Canary Testing",
      "date": "2019-05-08 09:36:56 +0000",
      
      "content": "\n  Canary Testing\n  Traffic Shifting Using Label Selectors\n  Traffic Shifting Using Traefik    \n      The Services\n    \n  \n  Helm    \n      Challenges with Helm\n    \n  \n  Conclusion\n\n\nSeries:\n\n\n  Part 1: Intro\n  Part 2: Traefik Basics\n  Part 3: Canary Testing (this post)\n  Part 4: Telemetry with Prometheus\n  Part 5: Prometheus Operator\n\n\nIn my previous post I compared Istio, Linkerd and Traefik and motivated why I preferred Traefik for Container DevOps. I showed how I was able to spin up Traefik controllers - one for internal cluster traffic routing, one for external cluster in-bound traffic routing. With that foundation in place, I can easily implement canary testing - both for external endpoints as well as internal services.\n\nCanary Testing\n\nWhat is canary testing (sometimes referred to as A/B testing)? This is a technique of “testing in production” where you shift a small portion of traffic to a new version of a service to ensure it is stable, or that it is meeting some sort of business requirement, in the case of hypothesis-driven development. This is an important technique because no matter how good your test and staging environments are, there’s no place like production. Sure, you can test performance in a test/stage environment, but you can only ever test user behavior in production! Being able to trickle a small amount of traffic to a new service limits exposure.\n\nHowever, a lot of teams that do use canary testing tend to use it just for proving that a service is stable. I think that they’re missing a trick - namely, telemetry and “proving hypotheses”. Without good telemetry, you’re never going to unlock the true potential of canary testing. Think of your canary as an experiment - and make sure you have a means to measure the success (or failure) of that experiment - otherwise you’re just pointlessly mixing chemicals. I’ll cover monitoring and telemetry in another post.\n\nTraffic Shifting Using Label Selectors\n\nYou can do canary testing “natively” in Kubernetes (k8s) by using good label selectors. Imagine you have service Foo and it has label selectors “app=foo”. Any pods that you deploy (typically via Deployments, DaemonSets or StatefulSets) that have the label “app=foo” get traffic routed to them when the service endpoint is targeted. Imagine you had a Deployment that spins up two replicas of a pod with labels “app=foo,version=1.0”. Hitting the Service endpoint will cause k8s to route traffic between the two pods. Now you have a new version of the container image and you create a Deployment that spins up one pod with labels “app=foo,version=1.1”. Now because all three pods match the Service label selector “app=foo” traffic is distributed between all three pods - you’ve effectively routed 33% of traffic to the new pod.\n\nSo far so good. But here’s where things get tricky: say you’re monitoring the pods and decide that version 1.1 is good to go - how do you “promote” it to production fully? You could update the labels on the original pods and remove “app=foo” - they’ll no longer match and so now all traffic is going to the third version 1.1 pod. But now you only have one pod, where originally you had two. So you’d have to also scale the Deployment of version 1.1 to ensure it gets as many replicas as the original service. And now you have a Deployment that’s missing some labels - so you’d have to dig to find out what those pods are.\n\nAlternatively, you could just add “version=1.1” to the Service label selectors. Again you’d have to scale the version 1.1 Deployment, but at least you don’t get “dangling pods”. But what about deploying version 1.2? Now you have to remove the “version=1.1” label from the Service since just adding “app=foo” won’t be good enough to get traffic onto pods with labels “app=foo,version=1.2”.\n\nAnd how would you go about testing traffic shifting of just 2%? You’d need to deploy 49 replicas of version 1.1 and a single version 1.2 just to get that percentage.\n\nWhat it boils down to is that using label selectors proves to be too much cognitive load since you spend too much time juggling labels, and the dial is “too course” - you can’t easily test traffic percentages lower that say 20% very easily. In contrast, if you use Traefik to do the traffic shifting, you get the added bonus of circuit breakers, SSL and other features too.\n\nTraffic Shifting Using Traefik\n\nLet’s see how we’d do traffic shifting using Traefik. Let’s suppose that I’ve already deployed a Traefik controller with “ingressClass=traefik.external”. To route traffic between two identical services (where the only difference between the services is the image version) I can create this ingress:\n\nkind: Ingress\nmetadata:\nannotations:\n  kubernetes.io/ingress.class: traefik-external\n  traefik.ingress.kubernetes.io/service-weights: |\n    partsunlimited-website-blue: 5%\n    partsunlimited-website-green: 95%\nlabels:\n  app: partsunlimited-website\n  name: partsunlimited-website\nspec:\n  rules:\n  - host: cdk8spu-dev.westus.cloudapp.azure.com\n    http:\n      paths:\n      - backend:\n          serviceName: partsunlimited-website-blue\n          servicePort: http\n        path: /site\n      - backend:\n          serviceName: partsunlimited-website-green\n          servicePort: http\n        path: /site\n\n\nNotes:\n\n\n  Line 1: the kind of resource in “Ingress” - nothing special about this, it’s a native k8s Ingress resource\n  Line 4: this is where we specify which IngressController should do the heavy lifting for this particular Ingress\n  Lines 5-7: simple, intuitive and declarative - we want 5% of traffic to be routed to the “blue” Service\n  Line 13: when inbound traffic has host “cdk8spu-dev.westus.cloudapp.azure.com” (the DNS for the LoadBalancer), then we want the ingress to use the following rules to direct the traffic\n  Lines 16-23: we specify the backend Services and Ports that the Ingress should route to and can even specify custom paths to map different backends to different URL paths\n\n\nThe Services\n\nThis assumes that we have two services: partsunlimited-website-blue and partsunlimted-website-green. In my case these are exactly the same service - they will sometimes just have pods on different versions of the images I’m building. Let’s look at the services:\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: partsunlimited-website-blue\n  labels:\n    app: partsunlimited\n    canary: blue\nannotations:\n  traefik.backend.circuitbreaker: \"NetworkErrorRatio() &amp;gt; 0.2\"\nspec:\n  ...\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: partsunlimited-website-green\n  labels:\n    app: partsunlimited\n    canary: green\nspec:\n  ...\n\n\nNotes:\n\n\n  Lines 5-7, 17-19: these are out-of-box label selectors for services. There’s the common “app” label and then a label for each canary “slot” that I have\n  Lines 8-9: since I am using Traefik, I can easily create a circuit-breaker using the annotation. In this case, we instruct the controller to cease to send traffic to the blue service if its network failure rate rises above 20%\n  The other lines are exactly what you would use for defining any k8s service\n\n\nHelm\n\nNow that I’ve shown you how to define the ingress and the services, I can discuss how I actually deployed my services. If you use “native” k8s yml manifests, it can become difficult to manage all your resources. Imagine you have several services, configmaps, secrets, ingresses, ingress controllers, persistence volumes - you’d need to manage each type of resource. Helm simplifies that task by “bundling” the related resources. That way “helm upgrade” gives you a single command to install or upgrade all the resources - and similarly, “helm status” and “helm delete” let you inspect or destroy the app and all its resources quickly. So I built a helm package for my application that included the Traefik plumbing.\n\nChallenges with Helm\n\nIt’s not all roses and unicorns though - helm has some disadvantages. Firstly, there’s Tiller - the “server side” component of helm. To use helm, you need to install Tiller on your k8s cluster, and give it some pretty beefy permissions. Helm 3 is abandoning Tiller, so this should improve in the near future.\n\nThe other (more pertinent) challenge is the way helm performs upgrades. Let’s have a look at a snippet of the values file that I have for my service - this file is used to override (or supply) values to an actual deployment:\n\ncanaries:\n  - name: blue\n    replicaCount: 1\n    weight: 20\n    tag: 1.0.0.0\n    annotations:\n      traefik.backend.circuitbreaker: \"NetworkErrorRatio() &amp;gt; 0.2\"\n  - name: green\n    replicaCount: 2\n    weight: 80\n    tag: 1.0.0.0\n    annotations: {}\n\n\nNotes:\n\n\n  Line 4,10 - I define the weights for each canary. Helm injects this value into the Ingress resource.\n  Lines 5,6 - I define annotations to apply to the service - in this case the Traefik circuit-breaker, but I could add others too\n\n\nInitially, I wanted to do a deployment with “version=1.0.0.0” for both canaries, and then just run “helm upgrade –set-values canaries[0].imageTag=1.0.0.1” to update the version of the blue canary. However, helm doesn’t work this way and so I have to supply all the values for the chart, rather than just the ones I want to update. In a pipeline, the version to deploy to the blue canary is the latest build number - but I have to calculate the green canary version number or it will be overwritten with “1.0.0.0” every time. It’s not a big deal since I can work it out, but it would be nice if helm had a way to only update a single value and leave all other current values “as-is”.\n\nIn the end, the ease of managing the entire application (encompassing all the resources) using helm outweighed the minor paper-cuts. I still highly recommend helm to manage app deployment - even if they’re simple apps!\n\nConclusion\n\nTraffic shifting using Traefik is pretty easy - it’s also intuitive since it’s based on annotations and is specified over “native” k8s resources instead of having to rely on custom constructs or sidecars or other rule-language formats. This makes it an ideal tool for performing canary testing in k8s deployments.\n\nHappy canary testing!\n",
      "categories": [],
      "tags": ["docker"],
      
      "collection": "posts",
      "url": "/container-devops-beyond-build-part-3-canary-testing/"
    },{
      
      "title": "Container DevOps: Beyond Build (Part 4) - Telemetry with Prometheus",
      "date": "2019-05-08 09:37:11 +0000",
      
      "content": "\n  Business vs Performance Telemetry\n  Prometheus\n  Performance Telemetry\n  Total Sales by Product Category\n  Viewing Telemetry\n  Conclusion\n\n\nSeries:\n\n\n  Part 1: Intro\n  Part 2: Traefik Basics\n  Part 3: Canary Testing\n  Part 4: Telemetry with Prometheus (this post)\n  Part 5: Prometheus Operator\n\n\nIn my previous post in this series I wrote about how I used Traefik to do traffic shifting and canary testing. I asserted that without proper telemetry, canary testing is (almost) pointless. Without some way to determine the efficacy of a canary deployment, you may as well just deploy straight out and not pretend.\n\nI’ve also written about how I love and use Application Insights to monitor .NET applications. Application Insights (or AppInsights for short) is still my go-to telemetry tool. And it’s not only a .NET tool - there are SDKs for Java, Javascript and Python among others. But since we’re delving into container-land, I wanted to at least explore one of the popular k8s tools: Prometheus. There are other monitoring tools (like Datadog) and I think it’ll be worth doing a compare/contrast of various monitoring tools at some stage. But for this post, I’ll stick to Prometheus.\n\nBusiness vs Performance Telemetry\n\nMost developers that are using any kind of telemetry understand “performance” telemetry - requests per second, read/writes per second, errors per second, memory and CPU usage - usual, bread-and-butter telemetry. However, I often encourage teams not to stop at performance telemetry and to also start looking at how to instrument their applications with “business” telemetry. Business telemetry is telemetry that has nothing to do with the running application - and everything to do with how the site or application is doing in business terms. For example, how many products of a certain category were sold today? What products are popular in which geos? And so on.\n\nAppInsights is one of my go-to tools because you get performance telemetry “for free” - just add it to your project and you get all of the perf telemetry you need to have a good view of your application performance - and that’s without changing a single line of code! However, if you do want business telemetry, you can add a few lines of code and it’s simple to get business telemetry. Add to that the ability to connect PowerBI to your telemetry (something I’ve written about before) and you’re able to produce the telemetry and have business users consume it using PowerBI - that’s a recipe for success!\n\nOn the down-side, making sense of AppInsights telemetry definitely isn’t simple, and the learning curve for analyzing your data is steep. The AppInsights query language is a delight though, and even has some built-in machine learning capabilities).\n\nPrometheus\n\nPrometheus has long been a popular telemetry solution - however, as I was exploring it I came across some challenges. Firstly, integrating into .NET isn’t simple - and you don’t get anything “for free” - you have to code in the telemetry. Secondly, there are only four types of metrics you can utilize: Counter, Gauge, Histogram and Summary. These are great for performance telemetry, but are very difficult to use for business telemetry. However, creating graphs from Prometheus data is really simple (at least using Grafana, as I’ll discuss in a later post) and there’s a whole query language called PromQL for querying Prometheus metrics.\n\nIn the remainder of this post I’ll show how I used Prometheus in a .NET Core application.\n\nPerformance Telemetry\n\nTo add performance telemetry to a .NET Core application, you have to add some middleware. You also need to expose the Prometheus endpoint. Here’s a snippet from my Startup.cs file:\n\nusing Prometheus;\n\npublic class Startup\n{\n    public void Configure(IApplicationBuilder app)\n    {\n        var basePath = Configuration[\"PathBase\"] ?? \"/\";\n        ...\n\n        // prometheus\n        var version = Assembly.GetEntryAssembly().GetCustomAttribute&lt;assemblyfileversionattribute&gt;()\n                .Version.ToString();\n        app.UseMethodTracking(version, Configuration[\"ASPNETCORE_ENVIRONMENT\"], Configuration[\"CANARY\"]);\n        app.UseMetricServer($\"{basePath}/metrics\");\n&lt;/assemblyfileversionattribute&gt;\n\n\nNotes:\n\n\n  Line 1: Import the Prometheus namespace - this is from the Prometheus NuGet package\n  Line 7: We need to set a base path - this is for sharing Traefik frontends for multiple backend services\n  Lines 10-11: Get the version of the application\n  Line 12: Call the UseMethodTracking method (shown below) to configure middleware, passing in the version, environment and canary name\n  Line 13: Tell Prometheus to expose an endpoint for the Prometheus server to scrape\n\n\nI want my metrics to be dimensioned by version, environment and canary. This is critical for successful canary testing! We also need a pathbase other than “/” since when we deploy services behind the Traefik router, we want to use path-based rules to route traffic to different backend services, even though there’s only a single front-end base URL.\n\nHere’s the code for the UseMethodTracking method:\n\npublic static class PrometheusAppExtensions\n{\n    static readonly string[] labelNames = new[] { \"version\", \"environment\", \"canary\", \"method\", \"statuscode\", \"controller\", \"action\" };\n\n    static readonly Counter counter = Metrics.CreateCounter(\"http_requests_received_total\", \"Counts requests to endpoints\", new CounterConfiguration\n    {\n        LabelNames = labelNames\n    });\n\n    static readonly Gauge inProgressGauge = Metrics.CreateGauge(\"http_requests_in_progress\", \"Counts requests currently in progress\", new GaugeConfiguration\n    {\n        LabelNames = labelNames\n    });\n\n    static readonly Histogram requestHisto = Metrics.CreateHistogram(\"http_request_duration_seconds\", \"Duration of requests to endpoints\", new HistogramConfiguration\n    {\n        LabelNames = labelNames\n    });\n\n    public static void UseMethodTracking(this IApplicationBuilder app, string version, string environment, string canary)\n    {\n        app.Use(async (context, next) =&amp;gt;\n        {\n            // extract values for this event\n            var routeData = context.GetRouteData();\n            var action = routeData?.Values[\"Action\"] as string ?? \"\";\n            var controller = routeData?.Values[\"Controller\"] as string ?? \"\";\n            var labels = new string[] { version, environment, canary,\n                context.Request.Method, context.Response.StatusCode.ToString(), controller, action };\n\n            // start a timer for the histogram\n            var stopWatch = Stopwatch.StartNew();\n            using (inProgressGauge.WithLabels(labels).TrackInProgress()) // increments the inProgress, decrementing when disposed\n            {\n                try\n                {\n                    await next.Invoke();\n                }\n                finally\n                {\n                    // record the duration\n                    stopWatch.Stop();\n                    requestHisto.WithLabels(labels).Observe(stopWatch.Elapsed.TotalSeconds);\n\n                    // increment the counter\n                    counter.WithLabels(labels).Inc();\n                }\n            }\n        });\n    }\n}\n\n\nNotes:\n\n\n  Line 2: set up the names of the dimensions I want to configure - note version, environment and canary\n  Lines 5-8: set up a counter to count method hits\n  Lines 10-13: set up a gauge to report how many requests are in progress\n  Lines 15-18: set up a histogram to record duration of each method call\n  Line 20: create a static extension method to inject Prometheus tracking into the middleware\n  Line 22: add a new handler into the pipeline\n  Lines 25-28: extract action, controller, method and response code from the current request if available\n  Line 32: start a stopwatch\n  Line 33: tell Prometheus that a method call is in progress - the “end of operation” is automatic at the end of the using (line 47)\n  Line 35-38: invoke the actual request\n  Line 39-43: stop the stopwatch and log the time recorded\n  Line 46: increment the counter for this controller/action/method/version/environment/canary combination\n\n\nThis code gives us performance metrics - we inject a step into the pipeline that starts a stopwatch, calls the operation, tells Prometheus an operation is in progress, and then when the operation completes, records the time taken and increments the call counter. Each “log” includes the “withLabels()” call that creates the context (dimensions) for the event.\n\nTotal Sales by Product Category\n\nLet’s examine what telemetry looks like if we want to track a business metric: say, sales of products by category. For this to work, I’d need to track the product category and price of each item sold. I could add other dimensions too (such as user) so that I can extend my analytics. If I know which users are purchasing products, I can start slicing and dicing by geo or language or other user attributes. If I know when sales occur, I can slice and dice by day of week or hour or any other time-based dimensions. The more dimensions I have, the more insights I can drive.\n\nLet’s see how we would track this business metric using Prometheus. Firstly, which metric type do I need? If we use a Counter, we can count how many items are sold, but not track the price - because counters can only increment by 1, not anything else. I could try Gauge since Gauge lets me set an arbitrary number - but unfortunately that doesn’t give me a running total - it’s just a number at a point in time. Both Histogram and Summary are snapshots of observations in a time period (my wording) so they don’t work either. In the end I decided to settle for number of products sold as a proxy for revenue - each time an item is sold, I want to log a counter for the product category and other dimensions so I get some idea of business telemetry.\n\nGenerally I like a logging framework with an interface that abstracts the logging mechanism or storage away from the application. I found that this was relatively easy to do using AppInsights - however, Prometheus doesn’t really work that way because the metric types are very specific to a particular event or method.\n\nHere’s how I ended up logging some telemetry in Prometheus in my .NET Core application:\n\npublic class ShoppingCartController : Controller\n{\n    static readonly string[] labelNames = new[] { \"category\", \"product\", \"version\", \"environment\", \"canary\" };\n\n    readonly Counter productCounter = Metrics.CreateCounter(\n        \"pu_product_add\", \"Increments when product is added to basket\", \n        new CounterConfiguration\n        {\n            LabelNames = labelNames\n        });\n    \n    public async Task&lt;iactionresult&gt; AddToCart(int id)\n    {\n        // Retrieve the product from the database\n        var addedProduct = _db.Products\n            .Include(product =&amp;gt; product.Category)\n            .Single(product =&amp;gt; product.ProductId == id);\n\n        var labels = new string[] { properties[\"ProductCategory\"], properties[\"Product\"], version, environment, canary };\n        productCounter.WithLabels(labels).Inc();\n        ...\n    }\n    ...\n}\n&lt;/iactionresult&gt;\n\n\nNotes:\n\n\n  Line 3: Set up a list of label names - again, these are dimensions for the telemetry\n  Lines 5-10: Set up a Counter for counting when a product is added to a basket, using labelNames for the dimensions\n  Line 19: Create an array of values that correspond to the labelNames array\n  Line 20: Increment the counter, again using WithLabels()\n\n\nViewing Telemetry\n\nNow that we have telemetry integrated, we can view the telemetry by browsing to the endpoint we configured Prometheus to expose. We’ll get some of the metrics live:\n\n# HELP process_open_handles Number of open handles\n# TYPE process_open_handles gauge\nprocess_open_handles 346\n# HELP process_start_time_seconds Start time of the process since unix epoch in seconds.\n# TYPE process_start_time_seconds gauge\nprocess_start_time_seconds 1556149570.76\n# HELP dotnet_total_memory_bytes Total known allocated memory\n# TYPE dotnet_total_memory_bytes gauge\ndotnet_total_memory_bytes 8133304\n# HELP process_virtual_memory_bytes Virtual memory size in bytes.\n# TYPE process_virtual_memory_bytes gauge\nprocess_virtual_memory_bytes 12298391552\n# HELP http_request_duration_seconds Duration of requests to endpoints\n# TYPE http_request_duration_seconds histogram\nhttp_request_duration_seconds_sum{version=\"1.0.0.45\",environment=\"Production\",canary=\"blue\",method=\"GET\",statuscode=\"200\",controller=\"\",action=\"\"} 6.6218794\nhttp_request_duration_seconds_count{version=\"1.0.0.45\",environment=\"Production\",canary=\"blue\",method=\"GET\",statuscode=\"200\",controller=\"\",action=\"\"} 926\nhttp_request_duration_seconds_bucket{version=\"1.0.0.45\",environment=\"Production\",canary=\"blue\",method=\"GET\",statuscode=\"200\",controller=\"\",action=\"\",le=\"0.005\"} 872\nhttp_request_duration_seconds_bucket{version=\"1.0.0.45\",environment=\"Production\",canary=\"blue\",method=\"GET\",statuscode=\"200\",controller=\"\",action=\"\",le=\"0.01\"} 909\nhttp_request_duration_seconds_bucket{version=\"1.0.0.45\",environment=\"Production\",canary=\"blue\",method=\"GET\",statuscode=\"200\",controller=\"\",action=\"\",le=\"0.025\"} 916\nhttp_request_duration_seconds_bucket{version=\"1.0.0.45\",environment=\"Production\",canary=\"blue\",method=\"GET\",statuscode=\"200\",controller=\"\",action=\"\",le=\"0.05\"} 919\nhttp_request_duration_seconds_bucket{version=\"1.0.0.45\",environment=\"Production\",canary=\"blue\",method=\"GET\",statuscode=\"200\",controller=\"\",action=\"\",le=\"0.075\"} 919\nhttp_request_duration_seconds_bucket{version=\"1.0.0.45\",environment=\"Production\",canary=\"blue\",method=\"GET\",statuscode=\"200\",controller=\"\",action=\"\",le=\"0.1\"} 922\nhttp_request_duration_seconds_bucket{version=\"1.0.0.45\",environment=\"Production\",canary=\"blue\",method=\"GET\",statuscode=\"200\",controller=\"\",action=\"\",le=\"0.25\"} 923\nhttp_request_duration_seconds_bucket{version=\"1.0.0.45\",environment=\"Production\",canary=\"blue\",method=\"GET\",statuscode=\"200\",controller=\"\",action=\"\",le=\"0.5\"} 925\nhttp_request_duration_seconds_bucket{version=\"1.0.0.45\",environment=\"Production\",canary=\"blue\",method=\"GET\",statuscode=\"200\",controller=\"\",action=\"\",le=\"0.75\"} 925\nhttp_request_duration_seconds_bucket{version=\"1.0.0.45\",environment=\"Production\",canary=\"blue\",method=\"GET\",statuscode=\"200\",controller=\"\",action=\"\",le=\"1\"} 925\nhttp_request_duration_seconds_bucket{version=\"1.0.0.45\",environment=\"Production\",canary=\"blue\",method=\"GET\",statuscode=\"200\",controller=\"\",action=\"\",le=\"2.5\"} 925\nhttp_request_duration_seconds_bucket{version=\"1.0.0.45\",environment=\"Production\",canary=\"blue\",method=\"GET\",statuscode=\"200\",controller=\"\",action=\"\",le=\"5\"} 926\nhttp_request_duration_seconds_bucket{version=\"1.0.0.45\",environment=\"Production\",canary=\"blue\",method=\"GET\",statuscode=\"200\",controller=\"\",action=\"\",le=\"7.5\"} 926\nhttp_request_duration_seconds_bucket{version=\"1.0.0.45\",environment=\"Production\",canary=\"blue\",method=\"GET\",statuscode=\"200\",controller=\"\",action=\"\",le=\"10\"} 926\nhttp_request_duration_seconds_bucket{version=\"1.0.0.45\",environment=\"Production\",canary=\"blue\",method=\"GET\",statuscode=\"200\",controller=\"\",action=\"\",le=\"+Inf\"} 926\n# HELP process_num_threads Total number of threads\n# TYPE process_num_threads gauge\nprocess_num_threads 24\n# HELP dotnet_collection_count_total GC collection count\n# TYPE dotnet_collection_count_total counter\ndotnet_collection_count_total{generation=\"0\"} 3\ndotnet_collection_count_total{generation=\"2\"} 0\ndotnet_collection_count_total{generation=\"1\"} 1\n# HELP process_working_set_bytes Process working set\n# TYPE process_working_set_bytes gauge\nprocess_working_set_bytes 159961088\n# HELP http_requests_received_total Counts requests to endpoints\n# TYPE http_requests_received_total counter\nhttp_requests_received_total{version=\"1.0.0.45\",environment=\"Production\",canary=\"blue\",method=\"GET\",statuscode=\"200\",controller=\"\",action=\"\"} 926\n# HELP process_private_memory_bytes Process private memory size\n# TYPE process_private_memory_bytes gauge\nprocess_private_memory_bytes 0\n# HELP pu_product_add Increments when product is added to basket\n# TYPE pu_product_add counter\npu_product_add{category=\"Wheels &amp;amp; Tires\",product=\"Disk and Pad Combo\",version=\"1.0.0.45\",environment=\"Production\",canary=\"blue\"} 1\npu_product_add{category=\"Oil\",product=\"Oil and Filter Combo\",version=\"1.0.0.45\",environment=\"Production\",canary=\"blue\"} 1\n# HELP http_requests_in_progress Counts requests currently in progress\n# TYPE http_requests_in_progress gauge\nhttp_requests_in_progress{version=\"1.0.0.45\",environment=\"Production\",canary=\"blue\",method=\"GET\",statuscode=\"200\",controller=\"\",action=\"\"} 1\n# HELP process_cpu_seconds_total Total user and system CPU time spent in seconds.\n# TYPE process_cpu_seconds_total counter\nprocess_cpu_seconds_total 15.26\n\n\nNotes:\n\n\n  Lines 14-32: shows http request duration in buckets - notice how each bucket also has the version of the app, the environment, canary, statuscode, controller and action\n  Lines 50-53: shows the number of products added to the basket by category, product, version, environment and canary\n\n\nConclusion\n\nI didn’t get round to showing how Prometheus scrapes the metrics from various services so that you can start to dashboard and analyze - that’s the subject for the next post. While I find Prometheus is fairly painful to implement on the tracking side (certainly compared to AppInsights), the graphing and querying can be worth the pain. I’ll show you how to do that in a k8s cluster in the next post.\n\nUnfortunately, though I experimented with using Prometheus for “business” telemetry, I can’t say I recommend it. It’s really meant for performance telemetry. So use AppInsights - which you can totally do even from within containers - if you need to do any business telemetry. And you do!\n\nHappy monitoring!\n",
      "categories": [],
      "tags": ["docker"],
      
      "collection": "posts",
      "url": "/container-devops-beyond-build-part-4-telemetry-with-prometheus/"
    },{
      
      "title": "Enterprise GitHub",
      "date": "2019-08-14 06:54:11 +0000",
      
      "content": "\n  Blurring the Line\n  This Shouldn’t Be A Surprise\n  Azure DevOps and GitHub Head to Head    \n      Source Control\n      Project Management\n      CI/CD\n      Package Management\n    \n  \n  Just Tell Me Which One To Use Already!    \n      Unit Of Management: Code or Work Items?\n      Single Pane of Glass or Bring Your Own Tools?\n      Manual Test Management or Centralized Source Control?\n    \n  \n  Conclusion\n\n\nSince Microsoft acquired GitHub, and the anti-Microsoft folks had calmed down, there have been a number of interesting developments in the GitHub ecosystem. If you’ve ever read one of my blogs or attended any events that I’ve spoken at, you’ll know that I am a raving Azure DevOps fan. I do, however, also have several repos on GitHub. As a DevOpsologist (someone who is in a constant state of learning about DevOps) I haven’t ever recommended GitHub for Enterprises – but the lines of functionality are starting to blur between GitHub and Azure DevOps. So which should you use, and when?\n\nOne note before we launch in: Azure DevOps is the name of the suite of functionality for the Microsoft DevOps platform. There are a number of “verticals” within the suite, which you can mix and match according to your needs. Azure Repos is the source control feature, Azure Boards the project management feature, Azure Test Plans the manual test management feature, Azure Pipelines the Continuous Integration/Continuous Deployment (CI/CD) feature and Azure Artifacts, the package management feature.\n\nBlurring the Line\n\nUp until a couple years ago, the line between GitHub and Azure DevOps was fairly simple to me: if you’re doing open-source development, use GitHub. If you’re doing Enterprise development, use Azure DevOps. The primary reasons were that GitHub was mainly just source control with some basic issue tracking, and you got unlimited public repos (at least, private repos were a lot more expensive). Azure DevOps (or Visual Studio Team Services, VSTS, as it used to be called), on the other hand, offered Project Management, CI/CD, Test Management and Package Management in addition to unlimited private repos. However, now you can now create private repos inexpensively in GitHub and you can create public repos in Azure DevOps. And last week, GitHub announced an update to GitHub Actions that enables first-class CI, which conceivably will expand to CD fairly soon. While you can’t do manual Test Management using GitHub, you get a far better “InnerSource” experience in GitHub than you do in Azure DevOps.\n\nThis Shouldn’t Be A Surprise\n\nThis isn’t entirely apples-to-apples, because (to oversimplify a bit) Azure DevOps is a DevOps platform, while GitHub is primarily a source control platform. At least, that’s been my experience up until recently. GitHub has some good, basic project management capabilities that work fantastically for open source development, but is a little simplistic for Enterprise development. As the industry shifts more and more to automated testing over manual testing, fewer teams have a need to manage manual testing. While you can publish releases on GitHub repos, Azure Artifacts arguably offers a more feature-rich service for package management. Also, while Azure DevOps, when it was still Team Foundation Server (TFS), used to be a “better together” suite, where you were probably better off doing everything in TFS, Azure DevOps is embracing the fact that developers have diverse toolsets, languages, platforms and target platforms. You can now very easily have source code in GitHub, Project Management in Jira, CI/CD in Azure Pipelines and have good visibility and traceability end-to-end.\n\nWe shouldn’t be surprised by the blur between GitHub and Azure DevOps. After all, Microsoft owns both now. And I think acquiring GitHub was an astute move by the tech giant – because, if you win the developer, you’re likely to win the organization. The perception of a “big bad Microsoft” is rapidly changing. Even before the GitHub acquisition, Microsoft employees were the top contributors to open source projects in GitHub. So not only is Microsoft embracing open source more and more, but they purchased the premier open source platform in the world!\n\nThe question then becomes: Where is Microsoft focusing? Are you better off in GitHub or in Azure DevOps, or some Frankenstein mix of the two? Will Azure Repos continue to evolve, or will Microsoft “Silverlight” Azure Repos? If I could gaze into a crystal ball, I’d predict that in 3 – 5 years, most organizations will have source code in GitHub and utilize Azure DevOps for Project Management, Pipelines and Package Management. Disclaimer: this is pure conjecture on my part!\n\nAzure DevOps and GitHub Head to Head\n\nThe blurred lines between GitHub and Azure DevOps should be cause for celebration, not consternation. It just means that we have more options! And options are good, if you consider them carefully and don’t make hype-based decisions. So let’s compare Azure DevOps and GitHub head to head in the realms of Source Control, Project Management, CI/CD and Package Management.\n\nSource Control\n\nThere’s not much difference as far as source control management goes between Azure Repos and GitHub, if you’re talking about Git. Fortunately, when the Azure DevOps team decided to add distributed version control to Azure DevOps, they just added Git. Not some funky Microsoft version of Git (though they contribute actively to Git and specifically to libgit2, the core Git libraries). So if you have a Git repo, you can add a GitHub remote, or an Azure DevOps remote, or both. Both are just Git remote repositories. However, if you want centralized source control (don’t do this any more) then you have to go with Azure DevOps. I would argue that the Pull Request experience is slightly better in Azure DevOps, but not by much. Both platforms allow you to protect branches using policies, so not much difference there either. Both platforms have WebHooks that you can use to trigger custom actions off events. Both have APIs for interaction. GitHub Enterprise has pre-receive hooks that can validate code before it is actually imported into the repo. Azure DevOps has a similar mechanism for centralized version control with Check-In policies, but these do not work for Git repos. We’ll call this one a tie.\n\nProject Management\n\nAzure Boards has a better Enterprise Project Management story. With GitHub you get Issues and Pull Requests (PRs) as the base “work item” types. You can add Task Lists to Issues, but the overall forms and flows of Issues and PRs is basic. Azure Boards work items can be a lot more complex, but offer much more customization opportunities. You can also do portfolio management more effectively in Azure Boards, since you can create work item hierarchies. GitHub does have the notion of Milestones and Projects, but again the functionality is fairly basic and probably too simplistic for Enterprises. While you can create basic filters for work items in GitHub, Azure DevOps has an advanced Work Item Query Language and elastic search. Both platforms allow you to tag (or label) work items. Azure Boards also lets you create widgets and dashboards and even has an OData feed and an Analytics Service so that you can create reports (say from PowerBI) over your work items. Of course you could use neither system for Project Management, you could use Jira, integrating Jira tickets easily to both Azure Repos (and Pipelines) or GitHub.\n\nIn terms of Enterprise project management, I’d have to give this one to Azure Boards.\n\nCI/CD\n\nGitHub introduced GitHub actions about a year ago. Hundreds of Actions were created by the community, validating the demand for actions triggered off events on a repo. But it seemed that doing any sort of Enterprise-scale CI with Actions was a challenge. Last week, a new and improved version of GitHub Actions was announced, and now CI is baked into GitHub through GitHub actions. I expect that we’ll see a huge surge in adoption of this CI tool and platforms like CircleCI and other cloud-CI systems may battle to compete. The feature isn’t GA yet, so we’ll see. It’s also suspiciously close to the YML format used by Azure Pipelines and supports Windows, Mac and Linux, just like Azure Pipelines…\n\nThe story doesn’t quite end there – if you want CI for GitHub repos, you now have a choice of GitHub Actions or Azure Pipelines, since Azure Pipelines has native support for GitHub repos. If you have repos outside of GitHub, you can’t use Actions.\n\nI’d have to give this one to Azure Pipelines, at least for now. Azure Pipelines does include Release Management (for CD) or multi-stage YML files. I am sure we’ll see similar support soon for Actions, but for now while you can do CI pretty easily in GitHub Actions, CD is going to be a challenge. At this point in time, I’ll give this one to Azure Pipelines.\n\nPackage Management\n\nGitHub releases allow you to tag a repo and publish that version of the repo as a release. You can also upload binaries that are the versioned packages for that release. Azure Artifacts allows you to create feeds that can be consumed – you can access a feed using NuGet, npm, Maven, Python packages or Universal Packages (which is a feed of arbitrary files – think NuGet for anything). Feeds are usually better than releases since tools like NuGet or npm know how to connect to feeds. Again, in terms of Enterprise package management, this one goes to Azure Artifacts.\n\nJust Tell Me Which One To Use Already!\n\nSo the final score is 3.5 to 0.5 for Azure DevOps. But that’s not a full reflection of the situation, so don’t start porting to Azure DevOps just yet. Remember, options are great if you consider them carefully. And they’re not mutually exclusive either. So here is what I think are the key considerations:\n\nUnit Of Management: Code or Work Items?\n\nDo you track your work by looking at work item tracking, reporting and rolling up across your portfolio? Then you probably need to look at Azure Boards, since GitHub Issues are not going to handle complex Enterprise-level portfolio management. However, if your teams operate a bit more independently and you track work by looking at changes to repos, GitHub may be a better fit. Don’t forget that you can still keep source code in GitHub and use Azure Boards for project management!\n\nSingle Pane of Glass or Bring Your Own Tools?\n\nI’ve seen Enterprises that are trying to standardize tooling and processes across teams. In this case, since Azure DevOps is a complete end to end platform, you’re probably better off using Azure DevOps. If you prefer a smorgasbord of tools, you could go either way. Even if you are managing work with Jira, building with TeamCity and deploying with Octopus Deploy, Azure DevOps could still tie these tools together to serve as a “backbone” giving you a single pane of glass.\n\nManual Test Management or Centralized Source Control?\n\nIf you need a tool for Manual Test Management, then Azure DevOps is for you. However, you could easily keep source code in GitHub and still use Azure Test Management to manage manual tests. And if you need centralized source control for some reason, then your only option is Azure Repos using Team Foundation Version Control (TFVC).\n\nConclusion\n\nAs you can see, there’s a lot of overlap between GitHub and Azure DevOps. Here’s my final prediction – we’ll see more innovation in the source control space in GitHub than we will in Azure Repos. Once again, the disclaimer is that this is my observation, and is in no way based on any official communication from either GitHub or Azure DevOps. I do think that a very viable option for Enterprises in the next few years will be to manage source code in GitHub and use Azure DevOps for CI/CD, Project Management and Package Management. This gives you the best of both worlds – you’re on (arguably) the best source control system – GitHub – and you get Enterprise-grade features for project, build and package management. As GitHub Actions evolves, perhaps CI/CD can be moved over to GitHub too. Yes, it’s still fuzzy even after thinking through all these options!\n\nIn short, think clearly about which platform (or combination of platforms) is going to best suit your Enterprise’s culture. Remember, DevOps is as much about people (culture) as is is about tooling.\n\nHappy DevOps!\n",
      "categories": [],
      "tags": ["sourcecontrol"],
      
      "collection": "posts",
      "url": "/enterprise-github/"
    },{
      
      "title": "Azure DevOps Build and Test Reports using OData and REST in PowerBI",
      "date": "2019-09-17 05:06:26 +0000",
      
      "content": "\n  TL;DR\n  Exploring Metadata\n  Connecting with PowerBI\n  Performance – Be Mindful\n  Limitations\n  API Calls From PowerBI    \n      Create a REST API Function\n    \n  \n  Relating Entities\n  Charts\n  Conclusion\n\n\nI have been playing with the Azure DevOps OData service recently to start creating some reports. Most of my fiddling has been with the Work Item and Work Item Board Snaphot entities, but I recently read a great post focused more on Build metrics by my friend and fellow ALM MVP, Wouter de Kort. I just happened to be working with a customer that is migrating from Azure DevOps Server to Azure DevOps Services and they had some SSRS reports that I knew could fairly easily be created using OData and PowerBI. In this post I’ll go over some of my experiences with the OData service and share a PowerBI template so that you can start creating some simple build reports yourself.\n\nTL;DR\n\nIf you just want the PowerBI template, then head over to this Github repo and have at it!\n\nExploring Metadata\n\nIf you want to see what data you can grab from the OData endpoint for your Azure DevOps account, then navigate to this URL: https://analytics.dev.azure.com/{organization}/_odata/v3.0-preview/$metadata (you’ll need to replace {organization} with your organization name). This gives an XML document that details the entities and relationships. Here’s a screenshot of what it looks like in Chrome:\n\n\n\n\nTo get the data for an entity, you need to use OData queries. Some of these are pretty obscure, but powerful. First tip: pluralize the entity to get the entries. For example, the entity “Build” is queried by navigating to https://analytics.dev.azure.com/{organization}/_odata/v3.0-preview/Builds. You definitely want to learn how to apply $filter (for filtering data), $select (for specifying which columns you want to select), $apply (for grouping and aggregating) and $expand (for expanding fields from related entities). Once you have some of these basics down, you’ll be able to get some pretty good data out of your Azure DevOps account.\n\nHere’s an example. Let’s imagine you want a list of all builds (build runs) from Sep 1st to today. The Build entity has the ProjectSK (an identifier to the project), but you’ll probably want to expand to get the Project name. Similarly, the Build entity includes a reference to the Build Definition ID, but you’ll have to expand to get the Build Definition Name. Here’s what the request would look like:\n\n    https://analytics.dev.azure.com/{organization}/_odata/v3.0-preview/Builds?\n       $apply=filter(CompletedDate ge 2019-09-01Z \"\n       &amp;amp;$select=* \n       &amp;amp;$expand=Project($select=ProjectName),BuildPipeline($select=BuildPipelineName),Branch($select=RepositoryId,BranchName)\n\n\nIf you look at the metadata for the Build entity, you’ll see that there are navigation properties for Project, BuildPipeline, Branch and a couple others. These are the names I use in the $expand directive, using an internal $select to specify which fields of the related entities I want to select.\n\nConnecting with PowerBI\n\nTo connect with PowerBI, you just connect to an OData field. You then have to expand some of the columns and do some other cleanup. Here’s what the M query looks like (view it by navigating to the “Advanced editor” for a query) for getting all the Builds since September 1st:\n\n    let\n        Source = OData.Feed (\"https://analytics.dev.azure.com/\" &amp;amp; #\"AzureDevOpsOrg\" &amp;amp; \"/_odata/v3.0-preview/Builds?\"\n            &amp;amp; \"$apply=filter(CompletedDate ge \" &amp;amp; Date.ToText(Date.From(Date.AddDays(DateTime.LocalNow(), -14)), \"yyyy-MM-dd\") &amp;amp; \"Z )\"\n            &amp;amp; \"&amp;amp;$select=* \"\n            &amp;amp; \"&amp;amp;$expand=Project($select=ProjectName),BuildPipeline($select=BuildPipelineName),Branch($select=RepositoryId,BranchName)\"\n         ,null, [Implementation=\"2.0\",OmitValues = ODataOmitValues.Nulls,ODataVersion = 4]),\n        #\"Changed Type\" = Table.TransformColumnTypes(Source,{ {\"BuildSK\", type text}, {\"BuildId\", type text}, {\"BuildDefinitionId\", type text}, {\"BuildPipelineId\", type text}, {\"BuildPipelineSK\", type text}, {\"BranchSK\", type text}, {\"BuildNumberRevision\", type text}}),\n        #\"Expanded BuildPipeline\" = Table.ExpandRecordColumn(#\"Changed Type\", \"BuildPipeline\", {\"BuildPipelineName\"}, {\"BuildPipelineName\"}),\n        #\"Expanded Branch\" = Table.ExpandRecordColumn(#\"Expanded BuildPipeline\", \"Branch\", {\"RepositoryId\", \"BranchName\"}, {\"RepositoryId\", \"BranchName\"}),\n        #\"Renamed Columns\" = Table.RenameColumns(#\"Expanded Branch\",{ {\"PartiallySucceededCount\", \"PartiallySucceeded\"}, {\"SucceededCount\", \"Succeeded\"}, {\"FailedCount\", \"Failed\"}, {\"CanceledCount\", \"Canceled\"}}),\n        #\"Expanded Project\" = Table.ExpandRecordColumn(#\"Renamed Columns\", \"Project\", {\"ProjectName\"}, {\"ProjectName\"})\n    in\n        #\"Expanded Project\"\n\n\nNotes:\n\n\n  Line 2: Connect to the OData feed Build entities (the #”AzureDevOpsOrg” is a parameter so that the account can be changed in a single place)\n  Line 3: Use Date.ToText and other M functions to get dates going back 2 weeks\n  Line 6: Standard OData feed arguments\n  Lines 7-11: Update some column types, rename some columns and expand some record columns to make the data easier to work with\n\n\nYou can see how we can use PowerBI functions (like DateTime.LocalNow) and so on. This allows us to create dynamic reports.\n\nPerformance – Be Mindful\n\nBe careful with your queries – try to aggregate where you can. For detail reports, make sure you limit the result sets using filters like date, team or team project and so on. You don’t want to be bringing millions of records back each time you refresh a report! For my particular report, I limit the date range to the builds completed in the last 2 weeks. In my case, that’s not a lot of data – but if you run hundreds of builds every day, even that date range might be too broad.\n\nLimitations\n\nThere are still some gaps when using the OData feeds. For example, you can get TestRun and TestResult entities – both for automated as well as manual tests. This data is sufficient for doing some reporting on automated tests – but it’s impossible to tie the TestResults back to test plans and suites. The TestResult actually has a TestCaseReferenceId so you can get back to the Test Case, but there’s no way to aggregate these to Suites and Plans since these entities are entirely absent from the OData model. Or the Build entity has a relationship to the Branch entity, which contains a RepositoryId, but no repository name – and there isn’t an entity for Repo in the OData model either.\n\nAPI Calls From PowerBI\n\nTwo other limitations that I found was that there’s no queue information in the OData fields (so you can’t see which queue a build was routed to) and there’s no code coverage information either. So doing any analysis on code coverage statistics or queues isn’t possible using pure OData. Wouter makes the same discovery in his blog post, where he calls out using PowerShell to call the Azure DevOps REST APIs to get some additional queue data.\n\nHowever, you can call REST APIs from PowerBI. I wanted a report where users could filter by repo, so I wanted a list of repositories in my organization. I also wanted to include queue and code coverage information on the Build entities.\n\nBefore we look at how to do this in PowerBI, there is a caveat to doing API calls, especially if you’re looping over records: don’t do this for large datasets! When I was trying to aggregate test runs to test suites and plans, I was actually able to get a list of test plans and test suites in an organization using REST APIs. But then I wanted a list of test IDs in each Test Suite – and that’s when my dream died. The organization I was doing this for had over 20,000 Test Suites – that means that PowerBI would have to make over 20,000 REST API calls to get all the Tests in Test Suites in an organization. I was forced to abandon that plan. In short, be mindful of where you use your REST API calls, and try to limit the number of rows you’re making the calls for!\n\nAnother caveat is that while you can authenticate to the OData feed using org credentials, you need a PAT for the REST API call! So there are now two authentication mechanisms for the report – org account and PAT.\n\nEnough caveats – let’s get to it!\n\nCreate a REST API Function\n\nThe first step is to create a function that can call the Azure DevOps API. Here’s the function to get a list of repositories for a give Team Project:\n\n    (project as text) =&amp;gt;\n    let\n        Source = Json.Document(Web.Contents(\"https://dev.azure.com/\" &amp;amp; #\"AzureDevOpsOrg\" &amp;amp; \"/\" &amp;amp; project &amp;amp; \"/_apis/git/repositories?api-version=5.1\"))\n    in\n        Source\n\n\nThis function takes a single arg called “project” of type text.\n\nNow that we have the function defined, we can use it to expand a table with a list of Team Projects to end up with a list of all the repos in an org. Add a new Data Source, open the advanced editor and paste in this query:\n\n    let\n        Source = OData.Feed (\"https://analytics.dev.azure.com/\" &amp;amp; #\"AzureDevOpsOrg\" &amp;amp; \"/_odata/v3.0-preview/Projects?\"\n            &amp;amp;\"&amp;amp;$select=ProjectSK, ProjectName \"\n        ,null, [Implementation=\"2.0\",OmitValues = ODataOmitValues.Nulls,ODataVersion = 4])\n    in\n        Source\n\n\nIf it runs, you’ll get a table of projects in your Azure DevOps organization:\n\n\n\n\nNow comes the magic:\n\n\n  Click on Add Column in the ribbon\n  Click on “Invoke Custom Function”\n  Enter “Repos” as the new column name\n  Select “GetGitRepos” (the function we created earlier) from the list of functions\n  Make sure the type is set to column so that PowerBI will loop through each row in the table, calling the function\n  Change the column to ProjectName – this is the value for the project arg for the function\n\n\n\nOnce you click OK, PowerBI will call the function for each row in the table – this is why you don’t want to do this on a table with more than a few hundred rows! Here’s what the result will look like:\n\n\n\n\nNow we want to expand the Record in each row, so click on the expand glyph to the right of the column name. We don’t really care about count, we just want value expanded and we don’t need the prefix:\n\n\n\n\nThis expands, but we’ll need to expand “value” once more, since it too is a complex object. Click the expand glyph again and select “Expand to Rows”. You can now filter out nulls – I could only do this by adding a line in the Advanced Editor:\n\n\n#\"Filter nulls\" = Table.SelectRows(#\"Expanded value\", each [value] &lt;&gt; null)\n\n\nDon’t forget to change the “in” to #“Filter nulls”. You will then need to expand value again:\n\n\n\n\nNow we can finally see the fields for the repo itself – I just selected name, size, defaultBranch and webUrl. Now you can update any types and rename columns as you need. We now have a list of repos! Here’s the final M query:\n\n    let\n       Source = OData.Feed (\"https://analytics.dev.azure.com/\" &amp;amp; #\"AzureDevOpsOrg\" &amp;amp; \"/_odata/v3.0-preview/Projects?\"\n            &amp;amp;\"&amp;amp;$select=ProjectSK, ProjectName \"\n        ,null, [Implementation=\"2.0\",OmitValues = ODataOmitValues.Nulls,ODataVersion = 4]),\n        #\"Invoked Custom Function\" = Table.AddColumn(Source, \"Repos\", each GetGitRepos([ProjectName])),\n        #\"Expanded Repos\" = Table.ExpandRecordColumn(#\"Invoked Custom Function\", \"Repos\", {\"value\"}, {\"Repos.value\"}),\n        #\"Expanded Repos.value\" = Table.ExpandListColumn(#\"Expanded Repos\", \"Repos.value\"),\n        #\"Filter nulls\" = Table.SelectRows(#\"Expanded Repos.value\", each [Repos.value] &amp;lt;&amp;gt; null),\n        #\"Expanded Repos.value2\" = Table.ExpandRecordColumn(#\"Filter nulls\", \"Repos.value\", {\"id\", \"name\", \"defaultBranch\", \"size\", \"webUrl\"}, {\"Repos.value.id\", \"Repos.value.name\", \"Repos.value.defaultBranch\", \"Repos.value.size\", \"Repos.value.webUrl\"}),\n        #\"Renamed Columns1\" = Table.RenameColumns(#\"Expanded Repos.value2\",{ {\"Repos.value.id\", \"RepositoryId\"}, {\"Repos.value.name\", \"Name\"}, {\"Repos.value.defaultBranch\", \"DefaultBranch\"}, {\"Repos.value.size\", \"Size\"}, {\"Repos.value.webUrl\", \"WebURL\"}})\n    in\n        #\"Renamed Columns1\"\n\n\nFor adding queue information to builds, I created a function to get build detail for a build number (so that I could extract the queue). For code coverage, I created a function to call the coverage API for a build – again expanding the records that came back. You can see the final queries in the template.\n\nRelating Entities\n\nNow that I have a few entities, PowerBI detects most of the relationships. I added a CalendarDate table so that I could filter all builds/tests on a particular date (the CompletedDate column is a DateTime field, so this is necessary to group on a day). The final ERD looks like this:\n\n\n\n\nI had some trouble relating branch to repo, so I eventually just added a LOOKUP function to lookup the repo name for the branch via the repositoryId. That’s why Repo isn’t related to other entities in the ERD. Similarly, I originally had a Project entity, but found that creating slicers on the Project column in the build worked just fine and kept the ERD simple.\n\nCharts\n\nI created two simple reports in the template – one showing a Build Summary and another showing a test summary. Feel free to start from these and go make some pretty reports!\n\n\n\n\nTo open the template, you can get it from this Github repo. There’s also instructions on how to update the auth.\n\nConclusion\n\nThe OData feed for Azure DevOps is getting better – mixing in some REST API calls allows you to fill in some gaps. If you’re careful about your filtering and what data you’re querying, you’ll be able to make some compelling reports. Go forth and measure…\n\nHappy reporting!\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/azure-devops-build-and-test-reports-using-odata-and-rest-in-powerbi/"
    },{
      
      "title": "Azure Pipeline Variables",
      "date": "2020-02-12 03:35:28 +0000",
      "description": "In this post I take a deep dive into Azure Pipeline variables.\n",
      "content": "\n  Inline Variables\n  Predefined Variables\n  Pipeline Variables\n  Secrets\n  Dynamic Variables and Logging Commands\n  Variable Groups    \n      KeyVault Integration\n      Consuming Variable Groups\n    \n  \n  Variable Templates\n  Precedence and Expansion\n  Conclusion\n\n\nI am a big fan of Azure Pipelines. Yes it’s YAML, but once you get over that it’s a fantastic way to represent pipelines as code. It would be tough to achieve any sort of sophistication in your pipelines without variables. There are several types of variables, though this classification is partly mine and pipelines don’t distinguish between these types. However, I’ve found it useful to categorize pipeline variables to help teams understand some of the nuances that occur when dealing with them.\n\nEvery variable is really a key:value pair. The key is the name of the variable, and it has a string value. To dereference a variable, simply wrap the key in $(). Let’s consider this simple example:\n\n\n    variables:\n      name: colin\n    \n    steps:\n    - script: echo \"Hello, $(name)!\"\n\n\n\nThis will write “Hello, colin!” to the log.\n\nInline Variables\n\nInline variables are variables that are hard coded into the pipeline YML file itself. Use these for specifying values that are not sensitive and that are unlikely to change. A good example is an image name: let’s imagine you have a pipeline that is building a Docker container and pushing that container to a registry. You are probably going to end up referencing the image name in several steps (such as tagging the image and then pushing the image). Instead of using a value in-line in each step, you can create a variable and use it multiple times. This keeps to the DRY (Do not Repeat Yourself) principle and ensures that you don’t inadvertently misspell the image name in one of the steps. In the following example, we create a variable called “imageName” so that we only have to maintain the value once rather than in multiple places:\n\n\n    trigger:\n    - master\n    \n    pool:\n      vmImage: ubuntu-latest\n    \n    variables:\n      imageName: myregistry/api-image\n    \n    steps:\n    - task: Docker@2\n      displayName: Build an image\n      inputs:\n        repository: $(imageName)\n        command: build\n        Dockerfile: api/Dockerfile\n    \n    - task: Docker@2\n      displayName: Push image\n      inputs:\n        containerRegistry: $(ACRRegistry)\n        repository: $(imageName)\n        command: push\n        tags: $(Build.BuildNumber)\n\n\n\nNote that you obviously you cannot create “secret” inline variables. If you need a variable to be secret, you’ll have to use pipeline variables, variable groups or dynamic variables.\n\nPredefined Variables\n\nThere are several predefined variables that you can reference in your pipeline. Examples are:\n\n\n  Source branch: “Build.SourceBranch”\n  Build reason: “Build.Reason”\n  Artifact staging directory: “Build.ArtifactStagingDirectory”\n\n\nYou can find a full list of predefined variables here.\n\nPipeline Variables\n\nPipeline variables are specified in Azure DevOps in the pipeline UI when you create a pipeline from the YML file. These allow you to abstract the variables out of the file. You can specify defaults and/or mark the variables as “secrets” (we’ll cover secrets a bit later). This is useful if you plan on triggering the pipeline manually and want to set the value of a variable at queue time.\n\nOne thing to note: if you specify a variable in the YML variables section, you cannot create a pipeline variable with the same name. If you plan on using pipeline variables, you must not specify them in the “variables” section!\n\nWhen should you use pipeline variables? These are useful if you plan on triggering the pipeline manually and want to set the value of a variable at queue time. Imagine you sometimes want to build in “DEBUG” and other times in “RELEASE”: you could specify “buildConfiguration” as a pipeline variable when you create the pipeline, giving it a default value of “debug”:\n\n\n\n\nIf you specify “Let users override this value when running this pipeline” then users can change the value of the pipeline when they manually queue it. Specifying “Keep this value secret” will make this value a secret (Azure DevOps will mask the value).\n\nLet’s look at a simple pipeline that consumes the pipeline variable:\n\n\n    name: 1.0$(Rev:.r)\n    \n    trigger:\n    - master\n    \n    pool:\n      vmImage: ubuntu-latest\n      \n    jobs:\n    - job: echo\n      steps:\n      - script: echo \"BuildConfiguration is $(buildConfiguration)\"\n\n\n\nRunning the pipeline without editing the variable produces the following log:\n\n\n\n\nIf the pipeline is not manually queued, but triggered, any pipeline variables default to the value that you specify in the parameter when you create it.\n\nOf course if we update the value when we queue the pipeline to “release”, of course the log reflects the new value:\n\n\n\n\nReferencing a pipeline variable is exactly the same as referencing an inline variable – once again, the distinction is purely for discussion.\n\nSecrets\n\nAt some point you’re going to want a variable that isn’t visible in the build log: a password, an API Key etc. As I mentioned earlier, inline variables are never secret. You must mark a pipeline variable as secret in order to make it a secret, or you can create a dynamic variable that is secret.\n\n“Secret” in this case just means that the value is masked in the logs. It is still possible to expose the value of a secret if you really want to. A malicious pipeline author could “echo” a secret to a file and then open the file to get the value of the secret.\n\nAll is not lost though: you can put controls in place to ensure that nefarious developers cannot simply run updated pipelines – you should be using Pull Requests and Branch Policies to review changes to the pipeline itself (an advantage to having pipelines as code). The point is, you still need to be careful with your secrets!\n\nDynamic Variables and Logging Commands\n\nDynamic variables are variables that are created and/or calculated at run time. A good example is using the “az cli” to retrieve the connection string to a storage account so that you can inject the value into a web.config. Another example is dynamically calculating a build number in a script.\n\nTo create or set a variable dynamically, you can use logging commands. Imagine you need to get the username of the current user for use in subsequent steps. Here’s how you can create a variable called “currentUser” with the value:\n\n\n    - script: |\n        curUser=$(whoami)\n        echo \"##vso[task.setvariable variable=currentUser;]$curUser\"\n\n\n\nWhen writing bash or PowerShell commands, don’t confuse “$(var)” with “$var”. “$(var)” is interpolated by Azure DevOps when the step is executed, while “$var” is a bash or PowerShell variable. I often use “env” to create environment variables rather than dereferencing variables inline. For example, I could write:\n\n\n    - script: echo $(Build.BuildNumber)\n\n\n\nbut I can also use environment variables:\n\n\n    - script: echo $buildNum\n      env:\n        buildNum: $(Build.BuildNumber)\n\n\n\nThis may come down to personal preference, but I’ve avoided confusion by consistently using env for my scripts!\n\nTo make the variable a secret, simple add “issecret=true” into the logging command:\n\n\n    echo \"##vso[task.setvariable variable=currentUser;issecret=true]$curUser\"\n\n\n\nYou could do the same thing using PowerShell:\n\n\n    - powershell: |\n        Write-Host \"##vso[task.setvariable variable=currentUser;]$env:UserName\"\n\n\n\nNote that there are two flavors of PowerShell: “powershell” is for Windows and “pwsh” is for PowerShell Core which is cross-platform (so it can run on Linux and Mac!).\n\nOne special case of a dynamic variable is a calculated build number. For that, calculate the build number however you need to and then use the “build.updatebuildnumber” logging command:\n\n\n    - script: |\n        buildNum=$(...) # calculate the build number somehow\n        echo \"##vso[build.updatebuildnumber]$buildNum\"\n\n\n\nOther logging commands are documented here.\n\nVariable Groups\n\nCreating inline variables is fine for values that are not sensitive and that are not likely to change very often. Pipeline variables are useful for pipelines that you want to trigger manually. But there is another option that is particularly useful for multi-stage pipelines (we’ll cover these in more detail later).\n\nImagine you have a web application that connects to a database that you want to build and then push to DEV, QA and Prod environments. Let’s consider just one config setting - the database connection string. Where should you store the value for the connection string? Perhaps you could store the DEV connection string in source control, but what about QA and Prod? You probably don’t want those passwords stored in source control.\n\nYou could create them as pipeline variables - but then you’d have to prefix the value with an environment or something to distinguish the QA value from the Prod value. What happens if you add in a STAGING environment? What if you have other settings like API Keys? This can quickly become a mess.\n\nThis is what Variable Groups are designed for. You can find variable groups in the “Library” hub in Azure DevOps:\n\n\n\n\nThe image above shows two variable groups: one for DEV and one for QA. Let’s create a new one for Prod, specifying the same variable name (“ConStr”) but this time entering in the value for Prod:\n\n\n\n\nSecurity is beyond the scope of this post- but you can specify who has permission to view/edit variable groups, as well as which pipelines are allowed to consume them. You can of course mark any value in the variable group as secret by clicking the padlock icon next to the value.\n\nThe trick to making variable groups work for environment values is to keep the names the same in each variable group. That way the only setting you need to update between environments is the variable group name. I suggest getting the pipeline to work completely for one environment, and then “Clone” the variable group - that way you’re assured you’re using the same variable names.\n\nKeyVault Integration\n\nYou can also integrate variable groups to Azure KeyVaults. When you create the variable group, instead of specifying values in the variable group itself, you connect to a KeyVault and specify which keys from the vault should be synchronized when the variable group is instantiated in a pipeline run:\n\n\n\nConsuming Variable Groups\n\nNow that we have some variable groups, we can consume them in a pipeline. Let’s consider this pipeline:\n\n\n    trigger:\n    - master\n    \n    pool:\n      vmImage: ubuntu-latest\n      \n    jobs:\n    - job: DEV\n      variables:\n      - group: WebApp-DEV\n      - name: environment\n        value: DEV\n      steps:\n      - script: echo \"ConStr is $(ConStr) in enviroment $(environment)\"\n    \n    - job: QA\n      variables:\n      - group: WebApp-QA\n      - name: environment\n        value: QA\n      steps:\n      - script: echo \"ConStr is $(ConStr) in enviroment $(environment)\"\n    \n    - job: Prod\n      variables:\n      - group: WebApp-Prod\n      - name: environment\n        value: Prod\n      steps:\n      - script: echo \"ConStr is $(ConStr) in enviroment $(environment)\"\n\n\n\nWhen this pipeline runs, we’ll see the DEV, QA and Prod values from the variable groups in the corresponding jobs.\n\nNotice that the format for inline variables alters slightly when you have variable groups: you have to use the “- name/value” format.\n\nVariable Templates\n\nThere is another type of template that can be useful - if you have a set of inline variables that you want to share across multiple pipelines, you can create a template. The template can then be referenced in multiple pipelines:\n\n\n    # templates/variables.yml\n    variables:\n    - name: buildConfiguration\n      value: debug\n    - name: buildArchitecture\n      value: x64\n    \n    # pipelineA.yml\n    variables:\n    - template: templates/variables.yml\n    steps:\n    - script: build x ${{ variables.buildArchitecture }} ${{ variables.buildConfiguration }}\n    \n    # pipelineB.yml\n    variables:\n    - template: templates/variables.yml\n    steps:\n    - script: echo 'Arch: ${{ variables.buildArchitecture }}, config ${{ variables.buildConfiguration }}'\n\n\n\nPrecedence and Expansion\n\nVariables can be defined at various scopes in a pipeline. When you define a variable with the same name at more than one scope, you need to be aware of the precedence. You can read the documentation on precedence here.\n\nYou should also be aware of when variables are expanded. They are expanded at the beginning of the run, as well as before each step. This example shows how this works:\n\n\n    jobs:\n    - job: A\n      variables:\n        a: 10\n      steps:\n        - bash: |\n            echo $(a) # This will be 10\n            echo '##vso[task.setvariable variable=a]20'\n            echo $(a) # This will also be 10, since the expansion of $(a) happens before the step\n        - bash: echo $(a) # This will be 20, since the variables are expanded just before the step\n\n\n\nConclusion\n\nAzure Pipelines variables are powerful – and with great power comes great responsibility! Hopefully you understand variables and some of their gotchas a little better now. There’s another topic that needs to be covered to complete the discussion on variables – parameters. I’ll cover parameters in a follow up post.\n\nFor now – happy building!\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/azure-pipeline-variables/"
    },{
      
      "title": "Executing JMeter Tests in an Azure Pipeline",
      "date": "2020-02-14 13:20:31 +0000",
      "description": "Visual Studio Load Testing tools have been deprecated, along with Cloud Load Testing. In this post I investigate how to use JMeter as a load testing alternative.\n",
      "content": "\n  The Solution\n  Test Plan Considerations    \n      run.sh\n      test.sh\n      WSL Gotcha\n    \n  \n  Executing from a Pipeline    \n      Executable Attributes\n    \n  \n  Conclusion\n\n\nMicrosoft have deprecated Load Testing in Visual Studio. Along with this, they have also deprecated the cloud load testing capability in Azure/Azure DevOps. On the official alternatives document, several alternative load testing tools and platforms are mentioned, including JMeter. What is not clear from this page is how exactly you’re supposed to integrate JMeter into your pipelines.\n\nI have a demo that shows how you can use Application Insights to provide business telemetry. In the demo, I update a website (PartsUnlimited) and then use traffic routing to route 20% of traffic to a canary slot. To simulate traffic, I run a cloud load test. Unfortunately, I won’t be able to use that for much longer since the cloud load test functionality will end of life soon! I set about figuring out how to run this test using JMeter.\n\nJMeter tests can be run on a platform called BlazeMeter. BlazeMeter has integration with Azure DevOps. However, I wanted to see if I could get a solution that didn’t require a subscription service.\n\nThe Solution\n\nJMeter is a Java-based application. I’m not a big fan of Java – even though I authored a lot of the Java Hands on Labs for Azure DevOps! I had to install the JRE on my machine in order to open the JMeter GUI so that I could author my test. However, I didn’t want to have to install Java or JMeter on the build agent – so of course I looked to Docker. And it turns out that you can run JMeter tests in a Docker container pretty easily!\n\nTo summarize the solution:\n\n\n  Create your JMeter test plans (and supporting files like CSV files) and put them into a folder in your repo\n  Create a “run.sh” file that launches the Docker image and runs the tests\n  Create a “test.sh” file for each JMeter test plan – this just calls “run.sh” passing in the test plan and any parameters\n  Publish the “reports” directory for post-run analysis\n\n\nI’ve created a GitHub repo that has the source code for this example.\n\nTest Plan Considerations\n\nI won’t go over recording and creating a JMeter test in this post – I assume that you have a JMeter test ready to go. I do, however, want to discuss parameters and data files.\n\nIt’s common to have some parameters for your test plans. For example, you may want to run the same test plan against multiple sites – DEV or STAGING for example. In this case you can specify a parameter called “host” that you can specify when you run the test. To access parameters in JMeter, you have to use the parameter function, “__P”. JMeter distinguishes between parameters and variables, so you can have both a variable and a parameter of the same name.\n\nIn the figure below, I have a test plan called CartTest.jmx where I specify a User Defined Variable (UDV) called “host”. I use the parameter function to read the parameter value if it exists, or default to “cdpartsun2-prod.azurewebsites.net” if the parameter does not exist:\n\n\n\n\nThe value of the host UDV is “${__P(host,cdpartsun2-prod.azurewebsites.net)}”. Of course you can use the __P function wherever you need it – not just for UDVs.\n\nIn my test plan, I also have a CSV for test data. I set the path of this file as a relative path to the JMX file:\n\n\n\n\nNow that I have the test plan and supporting data files, I am ready to script test execution. Before we get to running the test in a container, let’s see how I can run the test from the command line. I simply execute this command from within the folder containing the JMX file:\n\n\njmeter -n -t CartTest.jmx -l results.jtl -Jhost=cdpartsun2-dev.azurewebsites.net –j jmeter.log –e –o reports\n\n\nNotes:\n\n\n  -n tells JMeter to run in non-GUI mode\n  -t specifies the path to the test plan\n  -l specifies the path to output results to\n  -J&lt;name&gt;=&lt;value&gt; is how I pass in parameters; there may be multiple of these\n  -j specifies the path to the log file\n  -e specifies that JMeter should produce a report\n  -o specifies the report folder location\n\n\nWe now have all the pieces to script this into a pipeline! Let’s encapsulate some of this logic into two scripts: “run.sh” which will launch a Docker container and execute a test plan, and “test.sh” that is a wrapper for executing the CartTest.jmx file.\n\nrun.sh\n\nI based this script off this GitHub repo by Just van den Broecke.\n\n\n    #!/bin/bash\n    #\n    # Run JMeter Docker image with options\n    \n    NAME=\"jmetertest\"\n    IMAGE=\"justb4/jmeter:latest\"\n    ROOTPATH=$1\n    \n    echo \"$ROOTPATH\"\n    # Finally run\n    docker stop $NAME &amp;gt; /dev/null 2&amp;gt;&amp;amp;1\n    docker rm $NAME &amp;gt; /dev/null 2&amp;gt;&amp;amp;1\n    docker run --name $NAME -i -v $ROOTPATH:/test -w /test $IMAGE ${@:2}\n\n\n\nNotes:\n\n\n  The NAME variable is the name of the container instance\n  The IMAGE is the container image to launch – in this case “justb4/jmeter:latest” – this container includes Java and JMeter, as well as an entrypoint that launches a JMeter test\n  ROOTPATH is the first arg to the script and is the path that contains the JMeter test plan and data files\n  The script stops any running instance of the container, and then deletes it\n  The final line of the script runs a new instance of the container, mapping a volume from “ROOTPATH” on the host machine to a folder in the container called “/test” and then passes in remaining parameters (skipping ROOTPATH) as arguments to the entrypoint of the script. These are the JMeter test arguments.\n\n\ntest.sh\n\nNow we have a generic way to launch the container, map the files and run the tests. Let’s wrap this call into a script for executing the CartTest.jmx test plan:\n\n\n    #!/bin/bash\n    #\n    rootPath=$1\n    testFile=$2\n    host=$3\n    \n    echo \"Root path: $rootPath\"\n    echo \"Test file: $testFile\"\n    echo \"Host: $host\"\n    \n    T_DIR=.\n    \n    # Reporting dir: start fresh\n    R_DIR=$T_DIR/report\n    rm -rf $R_DIR &amp;gt; /dev/null 2&amp;gt;&amp;amp;1\n    mkdir -p $R_DIR\n    \n    rm -f $T_DIR/test-plan.jtl $T_DIR/jmeter.log &amp;gt; /dev/null 2&amp;gt;&amp;amp;1\n    \n    ./run.sh $rootPath -Dlog_level.jmeter=DEBUG \\\n    \t-Jhost=$host \\\n    \t-n -t /test/$testFile -l $T_DIR/test-plan.jtl -j $T_DIR/jmeter.log \\\n    \t-e -o $R_DIR\n    \n    echo \"==== jmeter.log ====\"\n    cat $T_DIR/jmeter.log\n    \n    echo \"==== Raw Test Report ====\"\n    cat $T_DIR/test-plan.jtl\n    \n    echo \"==== HTML Test Report ====\"\n    echo \"See HTML test report in $R_DIR/index.html\"\n\n\n\nNotes:\n\n\n  Lines 3-5: We need 3 args: the rootPath on the host containing the test plan, the name of the test plan (the jmx file) and a host parameter, which is specific to this test plan\n  Line 9: set the T_DIR to the current directory\n  Lines 14-16: Create a report directory, cleaning it if it exists already\n  Line 18: Clear previous result files\n  Lines 20-23: Call run.sh, passing in the rootPath and all the other JMeter args and parameters we need to invoke the test\n  Lines 22-29: Echo the location of the log, raw report and HTML reports\n\n\nAs long as we have Docker, we can run the script and we don’t need to install Java or JMeter!\n\nWe can execute the test from bash like so:\n\n./test.sh $PWD CartTest.jmx cdpartsun2-dev.azurewebsites.net\n\n\nWSL Gotcha\n\nOne caveat for Windows Subsystem for Linux (WSL): $PWD will not work for the volume mapping. This is because Docker for Windows is running on Windows, while the WSL paths are mounted in the Linux subsystem. In my case, the folder in WSL is “/mnt/c/repos/10m/partsunlimited/jmeter”, while the folder in Windows is “c:\\repos\\10m\\partsunlimited\\jmeter”. It took me a while to figure this out – the volume mapping works, but the volume is always empty. To work around this, just pass in the Windows path instead:\n\n./test.sh 'C:\\repos\\10m\\partsunlimited\\jmeter' CartTest.jmx cdpartsun2-dev.azurewebsites.net\n\n\nExecuting from a Pipeline\n\nWe’ve done most of the hard work – now we can put the script into a pipeline. We need to execute the test script with the correct arguments and upload the test results and we’re done! Here’s the pipeline:\n\n\n    variables:\n      host: cdpartsun2-dev.azurewebsites.net\n    \n    jobs:\n    - job: jmeter\n      pool:\n        vmImage: ubuntu-latest\n      displayName: Run JMeter tests\n      steps:\n      - task: Bash@3\n        displayName: Execute JMeter tests\n        inputs:\n          targetType: filePath\n          filePath: 'jmeter/test.sh'\n          arguments: '$PWD CartTest.jmx $(host)'\n          workingDirectory: jmeter\n          failOnStderr: true\n      - task: PublishPipelineArtifact@1\n        displayName: Publish JMeter Report\n        inputs:\n          targetPath: jmeter/report\n          artifact: jmeter\n\n\n\nThis is very simple – and we don’t even have to worry about installing Java or JMeter – the only prerequisite we have is that the agent is able to run Docker containers! The first step executes the test.sh script, passing in the arguments just like we did from in the console. The second task publishes the report folder so that we can analyze the run.\n\nHere’s a snippet of the log while the test is executing: we can see the download of the Docker image and the boot up – now we just wait for the test to complete.\n\n\n\nExecutable Attributes\n\nOne quick note: initially when I committed the scripts to the repo, they didn’t have the executable attribute set – this caused the build to fail because the scripts were not executable. To set the executable attribute, I ran the following command in the folder containing the sh files:\n\n\ngit update-index --chmod=+x test.sh\n\n\ngit update-index --chmod=+x run.sh\n\n\nOnce the build completes, we can download the report file and analyze the test run:\n\n\n\nConclusion\n\nOnce you have a JMeter test, it’s fairly simple to run the it in a Docker container as part of your build (or release) process. Of course this doesn’t test load from multiple locations and is limited to the amount of threads the agent can spin up, but for quick performance metrics it’s a clean and easy way to execute load tests. Add to that the powerful GUI authoring capabilities of JMeter and you have a good performance testing platform.\n\nHappy load testing!\n",
      "categories": [],
      "tags": ["testing","build"],
      
      "collection": "posts",
      "url": "/executing-jmeter-tests-in-an-azure-pipeline/"
    },{
      
      "title": "Azure Pipeline Parameters",
      "date": "2020-02-27 07:35:44 +0000",
      "description": "In this post I dive into parameters for Azure Pipelines.\n",
      "content": "\n  Type: Any\n  Variable Dereferencing\n  Parameters and Expressions\n  Extends Templates\n  Conclusion\n\n\nIn a previous post, I did a deep dive into Azure Pipeline variables. That post turned out to be longer than I anticipated, so I left off the topic of parameters until this post.\n\nType: Any\n\nIf we look at the YML schema for variables and parameters, we’ll see this definition:\n\n\n    variables: { string: string }\n    \n    parameters: { string: any }\n\n\n\nParameters are essentially the same as variables, with the following important differences:\n\n\n  Parameters are dereferenced using “${{}}” notation\n  Parameters can be complex objects\n  Parameters are expanded at queue time, not at run time\n  Parameters can only be used in templates (you cannot pass parameters to a pipeline, only variables)\n\n\nParameters allow us to do interesting things that we cannot do with variables, like if statements and loops. Before we dive in to some examples, let’s consider variable dereferencing.\n\nVariable Dereferencing\n\nThe official documentation specifies three methods of dereferencing variables: macros, template expressions and runtime expressions:\n\n\n  Macros: this is the “$(var)” style of dereferencing\n  Template parameters use the syntax “${{ parameter.name }}”\n  Runtime expressions, which have the format “$[variables.var]”\n\n\nIn practice, the main thing to bear in mind is when the value is injected. “$()” variables are expanded at runtime, while “${{}}” parameters are expanded at compile time. Knowing this rule can save you some headaches.\n\nThe other notable difference is left vs right side: variables can only expand on the right side, while parameters can expand on left or right side. For example:\n\n\n    # valid syntax\n    key: $(value)\n    key: $[variables.value]\n    ${{ parameters.key }} : ${{ parameters.value }}\n    \n    # invalid syntax\n    $(key): value\n    $[variables.key]: value\n\n\n\nHere’s a real-life example from a TailWind Traders I created. In this case, the repo contains several microservices that are deployed as Kubernetes services using Helm charts. Even though the code for each microservice is different, the deployment for each is identical, except for the path to the Helm chart and the image repository.\n\nThinking about this scenario, I wanted a template for deployment steps that I could parameterize. Rather than copy the entire template, I used a “for” expression to iterate over a map of complex properties. For each service deployment, I wanted:\n\n\n  serviceName: The path to the service Helm chart\n  serviceShortName: Required because the deployment requires two steps: “bake” the manifest, and then “deploy” the baked manifest. The “deploy” task references the output of the “bake” step, so I needed a name that wouldn’t collide as I expanded it multiple times in the “for” loop\n\n\nHere’s a snippet of the template steps:\n\n\n    # templates/step-deploy-container-service.yml\n    parameters:\n      serviceName: '' # product-api\n      serviceShortName: '' # productapi\n      environment: dev\n      imageRepo: '' # product.api\n      ...\n      services: []\n    \n    steps:\n    - ${{ each s in parameters.services }}:\n      - ${{ if eq(s.skip, 'false') }}:\n        - task: KubernetesManifest@0\n          displayName: Bake ${{ s.serviceName }} manifest\n          name: bake_${{ s.serviceShortName }}\n          inputs:\n            action: bake\n            renderType: helm2\n            releaseName: ${{ s.serviceName }}-${{ parameters.environment }}\n            ...\n        - task: KubernetesManifest@0\n          displayName: Deploy ${{ s.serviceName }} to k8s\n          inputs:\n            manifests: $(bake_${{ s.serviceShortName }}.manifestsBundle)\n            imagePullSecrets: $(imagePullSecret)\n\n\n\nHere’s a snippet of the pipeline that references the template:\n\n\n    ...\n      - template: templates/step-deploy-container-service.yml\n        parameters:\n          acrName: $(acrName)\n          environment: dev\n          ingressHost: $(IngressHost)\n          tag: $(tag)\n          autoscale: $(autoscale)\n          services:\n          - serviceName: 'products-api'\n            serviceShortName: productsapi\n            imageRepo: 'product.api'\n            skip: false\n          - serviceName: 'coupons-api'\n            serviceShortName: couponsapi\n            imageRepo: 'coupon.api'\n            skip: false\n          ...\n          - serviceName: 'rewards-registration-api'\n            serviceShortName: rewardsregistrationapi\n            imageRepo: 'rewards.registration.api'\n            skip: true\n\n\n\nIn this case, “services” could not have been a variable since variables can only have “string” values. Hence I had to make it a parameter.\n\nParameters and Expressions\n\nThere are a number of expressions that allow us to create more complex scenarios, especially in conjunction with parameters. The example above uses both the “each” and the “if” expressions, along with the boolean function “eq”. Expressions can be used to loop over steps or ignore steps (as an equivalent of setting the “condition” property to “false”). Let’s look at an example in a bit more detail. Imagine you have this template:\n\n\n    # templates/steps.yml\n    parameters:\n      services: []\n    \n    steps:\n    - ${{ each s in parameters.services }}:\n      - ${{ if eq(s.skip, 'false') }}:\n        - script: echo 'Deploying ${{ s.name }}'\n\n\n\nThen if you specify the following pipeline:\n\n    jobs:\n    - job: deploy\n      - steps: templates/steps.yml\n        parameters:\n          services:\n          - name: foo\n            skip: false\n          - name: bar\n            skip: true\n          - name: baz\n            skip: false\n\n\nyou should get the following output from the steps:\n\n\nDeploying foo\nDeploying baz\n\n\nParameters can also be used to inject steps. Imagine you have a set of steps that you want to repeat with different parameters - except that in some cases, a slightly different middle step needs to be executed. You can create a template that has a parameter called “middleSteps” where you can pass in the middle step(s) as a parameter!\n\n\n    # templates/steps.yml\n    parameters:\n      environment: ''\n      middleSteps: []\n    \n    steps:\n    - script: echo 'Prestep'\n    - ${{ parameters.middleSteps }}\n    - script: echo 'Post-step'\n    \n    # pipelineA\n    jobs:\n    - job: A\n      - steps: templates/steps.yml\n        parameters:\n          middleSteps:\n          - script: echo 'middle A step 1'\n          - script: echo 'middle A step 2'\n    \n    # pipelineB\n    jobs:\n    - job: B\n      - steps: templates/steps.yml\n        parameters:\n          middleSteps:\n          - script: echo 'This is job B middle step 1'\n          - task: ... # some other task\n          - task: ... # some other task\n\n\n\nFor a real world example of this, see this template file. This is a demo where I have two scenarios for machine learning: a manual training process and an AutoML training process. The pre-training and post-training steps are the same, but the training steps are different: the template reflects this scenario by allowing me to pass in different “TrainingSteps” for each scenario.\n\nExtends Templates\n\nPassing steps as parameters allows us to create what Azure DevOps calls “extends templates”. These provide rails around what portions of a pipeline can be customized, allowing template authors to inject (or remove) steps. The following example from the documentation demonstrates this:\n\n\n    # template.yml\n    parameters:\n    - name: usersteps\n      type: stepList\n      default: []\n    steps:\n    - ${{ each step in parameters.usersteps }}:\n      - ${{ each pair in step }}:\n        ${{ if ne(pair.key, 'script') }}:\n          ${{ pair.key }}: ${{ pair.value }}\n    \n    # azure-pipelines.yml\n    extends:\n      template: template.yml\n      parameters:\n        usersteps:\n        - task: MyTask@1\n        - script: echo This step will be stripped out and not run!\n        - task: MyOtherTask@2\n\n\n\nConclusion\n\nParameters allow us to pass and manipulate complex objects, which we are unable to do using variables. They can be combined with expressions to create complex control flow. Finally, parameters allow us to control how a template is customized using extends templates.\n\nHappy parameterizing!\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/azure-pipeline-parameters/"
    },{
      
      "title": "ChatOps with GitHub Actions and Azure Web Apps",
      "date": "2020-03-31 06:15:38 +0000",
      "description": "In this video, I show you how to use GitHub Actions to implement ChatOps with Azure Web Apps.\n",
      "content": "\nOver this weekend, I ported a task from Azure Pipelines to GitHub Actions. It was a fun project, and while I was busy I realized that I could now do some ChatOps. I decided to create a quick video which is below.\n\n\n\n\n\nCode is in GitHub here.\n\nHappy chat-opsing!\n",
      "categories": [],
      "tags": ["github"],
      
      "collection": "posts",
      "url": "/chatops-with-github-actions-and-azure-web-apps/"
    },{
      
      "title": "LetsEncrypt Auto-Renewal For Azure Web Apps for Linux",
      "date": "2020-04-24 22:34:56 +0000",
      "description": "In this post I show how I achieved automated LetsEncrypt cert registration and renewal for Azure Web Apps for Linux using nginx and CertBot.\n",
      "content": "\n  tl;dr\n  Ghost\n  CertBot\n  The Solution\n  CertBot Customization\n  Wrapping Up\n\n\nThis is my first post after converting my blog to Ghost. There are dozens of posts from all sorts of people about how they adopted/migrated to Ghost. I had some interesting challenges to get my site going, which I will post about. One of them was SSL security.\n\n\n\nMy previous blog engine was a fork of MiniBlog by Mads Kristensen. I customized it because I was previously on Blogger (remember that?) and had to import from Blogger. I also added Azure Storage rather than using file system and a search function. I was running the .NET framework version (which is fairly old) and was using Windows Live Writer (or now Open Live Writer) to author. Being able to author in markdown was the primary driver for me getting to Ghost!\n\n\ntl;dr\n\nIf you just want to jump straight to the code, head to the repo here. There’s a detailed readme with instructions.\n\n\nGhost\n\n\nThere is a Ghost docker image that is stupid simple to use to get Ghost up and running. I won’t bore you with how I converted my posts from my old blog, but extracting an import of all my old content was manageable. I was now ready to run this sucker live!\n\nThat’s when I hit my first big snag - I wanted to enforce SSL (of course). No problem - I can just install the LetsEncrypt Azure Web App extension and I’d be good to go, right? Wrong - Web Apps for Linux can’t have extensions!!\n\nNo problem - I’ll just run an nginx side-car container reverse proxy using multi-containers and let nginx handle the SSL termination. Except I could not get that to work.\n\nI found this great post by Jessica Deen on how to use SSL on Azure Linux Web Apps (coincidentally she was doing this for her Ghost blog!). While this looked promising, I didn’t want to have to manually renew the cert every 90 days!\n\n\nCertBot\n\n\nI scrathed around and found a Docker image for registering (and renewing) certs called CertBot. I tried running this with nginx like this post. It was exactly what I was trying to do - except that the cert magic happend outside the images and docker-compose!\n\nEventually it dawned on me - I could combine both approaches. To automate certificate registration using CertBot, CertBot issues a request to LetsEncrypt and listens for a HTTP request from LetsEncrypt (to the CDN you’re registering). If it receives the call, it knows you’re making the request from a domain you own and the cert is issued. So I’d need to route the challenge request to the certbot container. Of course all other calls needed to be routed to my app container.\n\nAfter registration (or renewal) there’s a hook for executing a script. So I could use some of Jessica’s az cli code to register the cert to the web app! I could then just loop CertBot, checking for renewals. When a renewal is performed, the same hook could register the new cert for me - voila, automated cert renewal with LetsEncrypt!\n\nThe Solution\n\nLet’s start with the yml file that describes the containers I spin up in my multi-container app:\n\n\n    version: '3.3'\n    \n    services:\n      app: # this name should be the value for APP_CONTAINER_NAME in the nginx config\n        image: myregistry/myapp:1.0.0 # registry for your application image\n        ports: \n        - \"2368:2368\" # port your app listens on (the EXPOSE port); the value for APP_EXPOSE_PORT in the nginx config\n        restart: always\n    \n      nginx:\n        depends_on:\n        - app\n        image: myregistry/my-nginx:latest # registry for your custom nginx with the nginx config\n        ports:\n        - \"0:80\" # must be this mapping to route all traffic to the web app to nginx\n        restart: always\n    \n      certbot:\n        depends_on:\n        - nginx\n        image: myregistry/my-certbot:latest # registry for your custom certbot image\n        ports:\n        - \"80:80\" # must be this mapping to respond to LetsEncrypt challenge\n        restart: always\n        volumes:\n        - ${WEBAPP_STORAGE_HOME}/certbot/letsencrypt:/etc/letsencrypt # maps to persistent storage\n\n\nNotes:\n\n- There are 3 containers: `app`, `nginx` and `certbot` (the names are important for the nginx config file)\n- The port mapping is important - nginx _must_ be on `0:80` so that it gets all traffic inbound from the web app. Certbot must be on `80:80` to correctly respond to the LetsEncrypt challenge. Finally, the app port should _not_ be 80 or 8080 - I could not get this to work if the app was using either of these ports.\n- The `certbot` image is mapping a volume for the `/etc/letsencrypt` folder - this is required to retain the cert if the container restarts; otherwise certbot will request a cert every time it starts, which isn't what we want.\n\nLet's now look at the `nginx.conf` file:\n\n~~~nginx\n\n    user nginx;\n    worker_processes 1;\n    \n    error_log /var/log/nginx/error.log warn;\n    pid /var/run/nginx.pid;\n    \n    \n    events {\n        worker_connections 1024;\n    }\n    \n    http {\n        include /etc/nginx/mime.types;\n        default_type application/octet-stream;\n    \n        log_format main '$remote_addr - $remote_user [$time_local] \"$request\" '\n                          '$status $body_bytes_sent \"$http_referer\" '\n                          '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n    \n        access_log /var/log/nginx/access.log main;\n        client_max_body_size 10M;\n    \n        sendfile on;\n        #tcp_nopush on;\n    \n        keepalive_timeout 65;\n    \n        #gzip on;\n     \n        server {\n            listen 80 default_server;\n            listen [::]:80 default_server;\n    \n            # certbot challenge\n            location ~ /.well-known {\n                proxy_pass http://certbot;\n                proxy_redirect off;\n            }\n    \n            location / {\n                # APP_EXPOSE_PORT is the port the app container exposes    \n                proxy_pass http://app:APP_EXPOSE_PORT;  \n                proxy_set_header Host $host;\n            }\n        }\n    }\n\n\n\nNotes:\n\n\n  The server listens on port 80 (the SSL termination occurs at the Web App layer, so traffic coming in at this point is http)\n  The location ~ /.well-known routes any route with /.well-known in the URL to certbot\n  Location / forwards all other requests to the app container - make sure you update this to match your app port. In my example compose file above, this would be 2368.\n\n\nNote: I spent many hours debugging an infinte loop of redirects - I found that I had to ensure that none of the directives below were specified in the location rules. This is something to do with how Azure Web Apps handles incoming traffic.\n\n\n    proxy_set_header X-Forwarded-Proto $scheme;\n    proxy_set_header X-Real-IP $remote_addr;\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n\n\n\nCertBot Customization\n\nTo customize CertBot to handle certificate registration and renewal, I customized the CMD for the container to invoke this script:\n\n\n    #!/bin/sh\n    \n    rsa_key_size=4096\n    if [-z $STAGING] || [$STAGING != \"0\"]; then staging_arg=\"--staging\"; fi\n    \n    if [-z $EMAIL] || [-z $CDN]; then\n      echo \"Please set email and CDN environment variables!\"\n    else\n        wwwArg=\"\"\n        if [-z $WWW] || [$WWW != \"0\"]; then\n          echo \"Adding www.$CDN to registration\"\n          wwwArg=\"-d www.$CDN\" \n        fi\n    \n        if [! -f \"$WORKING_PATH/live/$CDN/fullchain.pem\"]; then\n          echo \"Creating cert\"\n          echo \"Staging arg: $STAGING\"\n    \n          certbot certonly --standalone \\\n            --preferred-challenges=http \\\n            --email $EMAIL \\\n            $staging_arg \\\n            --agree-tos \\\n            --no-eff-email \\\n            --manual-public-ip-logging-ok \\\n            --domain $CDN $wwwArg\n          \n          # run the script to register the cert with web apps\n          deploy-cert-az-webapp.sh\n        fi\n    \n        timeout=\"12h\"\n        if [! -z $DEBUG] &amp;&amp; [$DEBUG == \"TRUE\"]; then\n          timeout=\"30s\"\n        fi\n    \n        # loop infinitely and check for cert renewal every 12 hours\n        # if the cert does not need renewing, certbot does nothing\n        # after renewal, the deploy-cert-az-webapp.sh should fire to\n        # register the renewed cert\n        trap exit TERM; while :; do certbot renew --post-hook \"deploy-cert-az-webapp.sh\"; sleep $timeout &amp; wait $!; done;\n    fi\n\n\n\nNotes:\n\n\n  The script runs of environment variables like $CDN etc.\n  Line 4: If staging (for test certificates) is set, --staging is added to the registration call\n  Line 10: If you want to register $CDN and www.$CDN then you set WWW to 1. In my case, my CDN is colinsalmcorner.com and I wanted www.colinsalmcorner.com to be registered too, so I set WWW to 1. Subdomains like blog.colinsalmcorner.com should obviously set WWW to 0\n  Line 15: Check if the cert exists, and make a registration request if it does not. This is why the persistent storage (the ${WEBAPP_STORAGE_HOME} volume mapping) on the certbot image is so important.\n  Lines 19-26: Register a request for a cert from LetsEncrypt. At this point, certbot will listen for the challenge from LetsEncrypt to http://$CDN/.well-known/challenge/{some_random_goop}which means that the DNS should be pointing to the Azure Web App and the custom domain registered on the Web App.\n  Line 29: invoke the script to register the cert with the Web App\n  Line 41: Loop forever, calling cerbot renew every 12 hours. If the cert is not due for renewal, this ends as a no-op. If the cert(s) are renewed, the register script is invoked right after the renewal completes.\n\n\nHere’s the script to register the cert with Azure Web Apps:\n\n\n    #!/bin/sh\n    \n    certPath=\"$WORKING_PATH/live/$CDN\"\n    \n    if [! -f \"$WORKING_PATH/live/$CDN/fullchain.pem\"]; then\n      echo \"ERROR: $WORKING_PATH/live/$CDN/fullchain.pem does not exist\"\n      exit 1\n    fi\n    \n    # convert pem to pfx for azure web app\n    echo \"Converting pem to pfx\"\n    openssl pkcs12 \\\n        -password pass:$PFX_PASSWORD \\\n        -inkey $certPath/privkey.pem \\\n        -in $certPath/cert.pem \\\n        -export -out $certPath/cert.pfx\n    \n    # upload and get the thumbprint\n    if [! -z $DEBUG] &amp;&amp; [$DEBUG != \"TRUE\"]; then\n        echo \"DEBUG:: Running pfx upload and bind cert commands here\"\n        echo \"DEBUG:: WebApp: $WEB_APP_NAME\"\n        echo \"DEBUG:: Resource $RESOURCE_GROUP\"\n        echo \"Contents of $certPath\"\n        ls -la $certPath\n        \n    else\n        echo \"Running az login\"\n        az login --service-principal -u $AZ_CLIENT_ID -p $AZ_CLIENT_KEY --tenant $AZ_TENANT_ID\n    \n        echo \"Upload $certPath/cert.pfx to $WEB_APP_NAME in $RESOURCE_GROUP and get thumbprint\"\n        thumbprint=$(az webapp config ssl upload --certificate-file $certPath/cert.pfx \\\n                    --certificate-password $PFX_PASSWORD \\\n                    --name $WEB_APP_NAME --resource-group $RESOURCE_GROUP \\\n                    --query thumbprint --output tsv)\n        \n        # bind using the thumbprint\n        echo \"Bind cert\"\n        az webapp config ssl bind \\\n            --certificate-thumbprint $thumbprint \\\n            --ssl-type SNI \\\n            --name $WEB_APP_NAME --resource-group $RESOURCE_GROUP\n    fi\n    \n    echo \"Done!\"\n\n\n\nNotes:\n\n\n  The script first converts the pem to a pfx using a password\n  After that it uses az cli to login, upload the cert and bind the custom domain to the newly uploaded cert\n\n\nWrapping Up\n\nYou can find all the steps and configuration settings you need to configure for this to work in the readme. There’s also this script that shows how I spin up the site, configure the custom domain, upload the compose configuration, update the registry settings and then app settings and finally hit the site to start it. You should be able to get it going pretty quickly from here on!\n\nHappy securing!\n",
      "categories": [],
      "tags": ["azure"],
      
      "collection": "posts",
      "url": "/letsencrypt-auto-renewal-for-azure-web-apps-for-linux/"
    },{
      
      "title": "Azure DevOps Work Item Hierarchy Reports in PowerBI",
      "date": "2020-05-11 23:13:44 +0000",
      "description": "In this post I show how you can query Work Item data and build hierarchical reports using PowerBI.\n",
      "content": "\n  tl;dr Just give me the pbix file!\n  Raw vs Aggregated Data\n  Relating Tables\n  State Category Order Tables\n  Calculated Columns\n  Slicers\n  Conclusion\n\n\nMy colleague Josh Johanning and I were working with a team that is using Azure DevOps to track work. Their team lead was opening Excel and pulling in Work Item data and creating a feature timeline by hand. This process was manual, painstaking and error-prone. Josh and I set out to see if we could create the same report using OData and PowerBI.\n\ntl;dr Just give me the pbix file!\n\nIf you just want the pbit file to hook up to a Team Project, you can grab it here. When you open it, you’ll be prompted to enter the URL of your Team Project OData endpoint and the name of the work item types for Epic, Feature and Story level respectively. The little i glyphs will give you help if you need it.\n\nRaw vs Aggregated Data\n\nTo create the report we wanted, we need to aggregate work items along the hierarchy - typically Epic -&gt; Feature -&gt; Story. With OData feeds, you can apply aggregations like count of Features in Iteration X or sum of story points for Active Stories in Area Y. However, while this makes the query efficient, you’re hard-coding any filters, so creating reports with slicers won’t work.\n\nIn the end we decided to use a raw query - we’d get all the work items and then let PowerBI do the aggregation. Of course, if you’re doing a raw OData query like this, you don’t really want to get all work items: you want to apply some filtering, like AreaPath or IterationPath or State. You can read more about recommended query guidelines here.\n\nThe next challenge we had was how to relate the work items so that we could aggregate. After thinking about it, we decided to create three work item tables (and queries) to segregate work items by WorkItemType: so we ended up with an Epics table, a Features table and a Stories table. Initially we thought we’d have to create another query to the OData source to get WorkItemLinks and then relate parents to children using that data, but the default OData work item query conveniently adds a ParentWorkItemID column in so you don’t have to!\n\nLet’s take a look at the Epics query from within PowerBI:\n\n\n\nlet\n    Source = OData.Feed(ODataBaseUrl &amp; \"/WorkItems?$filter=WorkItemType eq '\" &amp; EpicWorkItemType &amp; \"'\", null, [Implementation=\"2.0\"]),\n    #\"Removed Other Columns\" = Table.SelectColumns(Source,{\"WorkItemId\", \"LeadTimeDays\", \"CycleTimeDays\", \"ProjectSK\", \"WorkItemRevisionSK\", \"AreaSK\", \"IterationSK\", \"AssignedToUserSK\", \"CreatedByUserSK\", \"Revision\", \"Watermark\", \"Title\", \"ParentWorkItemId\", \"WorkItemType\", \"ChangedDate\", \"CreatedDate\", \"State\", \"Effort\", \"StoryPoints\", \"TagNames\", \"StateCategory\", \"BoardLocations\", \"Teams\", \"Parent\", \"Iteration\", \"AssignedTo\", \"Tags\"})\nin\n    #\"Removed Other Columns\"\n\n\n\n\nNotes:\n\n\n  Line 2 is using the OData.Feed function to get data from an OData endpoint. I’ve created a parameter to the OData endpoint of the Azure DevOps instance called ODataBaseUrl - in fact, I’ve added the Team Project into the URL so that I’m only getting data from a single team project. After that, we’re getting /WorkItems which is the WorkItem entity set - but we’re filtering further by getting work items where WorkItemType = EpicWorkItemType which is another parameter that has the default value Epic.\n  Line 3 is using Table.SelectColumns to select only certain columns, rather than bringing back columns that we don’t need. Under the hood, PowerBI is smart enough to apply “query folding” so the Table.SelectColumns is translated into a $select= OData clause\n\n\nWe repeat this for the Features and Stories, but just update the work item type value that we’re filtering on.\n\n\n\n\n  Note: You should probably add a date filter (CreatedDate &gt; Today - 14 days or something silimar) too, since this query could return a lot of data. Behind the scenes, PowerBI is automatically adding a TOP 1000 but you still want to be optimal about what work items you’re getting via the OData query. In our case since we only had a single team project, we decided we didn’t need a date filter.\n\n\n\n\nWe then add in queries to some ancillary tables that we are going to use for filtering, like Areas, Iterations and Users. Here’s the query for Areas:\n\n\n\nlet\n    Source = OData.Feed(#\"ODataBaseUrl\"),\n    Areas_table = Source{[Name=\"Areas\",Signature=\"table\"]}[Data],\n    #\"Removed Columns\" = Table.RemoveColumns(Areas_table,{\"ProjectSK\", \"AreaId\", \"Number\", \"AreaLevel5\", \"AreaLevel6\", \"AreaLevel7\", \"AreaLevel8\", \"AreaLevel9\", \"AreaLevel10\", \"AreaLevel11\", \"AreaLevel12\", \"AreaLevel13\", \"AreaLevel14\", \"Depth\", \"AnalyticsUpdatedDate\", \"Project\", \"Teams\"})\nin\n    #\"Removed Columns\"\n\n\n\n\nAgain we’re pulling data from the ODataBaseUrl, and then scoping the call to the table Areas (the query folding just ends up appending /Areas to the URL). We then remove columns we don’t need to clean up the table.\n\nRelating Tables\n\nAt this point, PowerBI will automatically detect relationships - so it’ll create a link from AreaSK in the work item tables to the Areas table. However, it will create some of the relationships as Active (solid line) and that can mess up the next step. We need to update all the relationships (to Areas, Iterations and Users as non-active (they’ll now appear dotted in the Model view).\n\nOnce you’ve done that, you can add the following active relationships:\n\n\n  Story:ParentWorkItemID -&gt; Feature:WorkItemID\n  Feature:ParentWorkItemID -&gt; Epic:WorkItemID\n\n\nState Category Order Tables\n\nFor our visuals, we wanted to show by state - or more accurately, state category. By default these are Proposed, In Progress and Complete. Unfortunately, category names on visuals are sorted alphabetically be default, so the charts always showed Completed first.\n\nTo work around this, we entered data to create a new StateCategories table with two columns: StateCategory and Order where the Order value was 1 for Proposed, 2 for In Progress and 3 for Complete. We created a new relationship from Stories:StateCategory -&gt; StateCategories:StateCategory. We then tried to do the same from Features to StateCategories, but hit the “ambiguous relationship” issue (since Stories is already related to Features). We then just renamed StateCategories to StoryStateCategories. We then duplicated this table and renamed it to FeatureStateCategories and created the Features:StateCategory -&gt; FeatureStateCategories:StateCategory link.\n\nThe final model looks like this:\n\nThe final data Model in PowerBI\nCalculated Columns\n\nWe then added the following calculated columns on the Stories table:\n\n\n    Iteration End Date = LOOKUPVALUE(Iterations[EndDate], Iterations[IterationSK], Stories[IterationSK])\n    \n    Iteration Start Date = LOOKUPVALUE(Iterations[StartDate], Iterations[IterationSK], Stories[IterationSK])\n    \n    StateCateroryOrder = RELATED(StoryStateCategories[Order])\n\n\n\nThese are just lookups to bring in the related values so that they are all “inline”.\n\nTo utilize the StateCategoryOrder column, we click on the StateCategory column, and then click “Order By” and select StateCategoryOrder.\n\nFor Features the calculations were a little more complex since we were aggregating the Stories value. For each Features row, we can perform calculations on RELATEDTABLE(Stories): this is the subset of Stories rows that are related to the current Features row via the Stories:ParentWorkItemID -&gt; Features:WorkItemID relationship. With that in mind, we create the following calculated columns:\n\n\n    Stories Completed Count = \n    VAR num = CALCULATE(\n        COUNTROWS(RELATEDTABLE(Stories)),\n        FILTER(\n            RELATEDTABLE(Stories),\n            Stories[StateCategory] = \"Completed\"\n        )\n    )\n    RETURN IF(ISBLANK(num), 0, num)\n    \n    Stories InProgress Count = \n    var num = CALCULATE(\n        COUNTROWS(RELATEDTABLE(Stories)),\n        FILTER(\n            RELATEDTABLE(Stories),\n            Stories[StateCategory] = \"InProgress\"\n        )\n    )\n    RETURN IF(ISBLANK(num), 0, num)\n    \n    Stories Proposed Count = \n    var num = CALCULATE(\n        COUNTROWS(RELATEDTABLE(Stories)),\n        FILTER(\n            RELATEDTABLE(Stories),\n            Stories[StateCategory] = \"Proposed\"\n        )\n    )\n    RETURN IF(ISBLANK(num), 0, num)\n    \n    Stories Total Count = \n    VAR num = CALCULATE(\n        COUNTROWS(RELATEDTABLE(Stories))\n    )\n    RETURN IF(ISBLANK(num), 0, num)\n    \n    Stories Completed Percentage = \n    var perc = [Stories Completed Count]/[Stories Total Count]\n    RETURN IF([Stories Total Count] = 0, 0, IFERROR(perc, 0))\n    \n    Stories Start Date = \n    CALCULATE(MIN(Stories[Iteration Start Date]))\n    \n    Stories End Date = \n    CALCULATE(MAX(Stories[Iteration End Date]))\n    \n    AssignedTo = LOOKUPVALUE(Users[UserName], Users[UserSK], Features[AssignedToUserSK])\n    \n    StateCategoryOrder = RELATED(FeatureStateCategory[Order])\n\n\n\nWe’re calculating related Stories using a filter on StateCategory and just summing the count. We don’t use Story Points, but you could easily sum the Story Points column to do the same operation on Story Points instead of work item count. We then calculate a Completed Percentage. We then add in calculations for the earliest start date and latest end date for related Stories (via the Iteration). This gives us a start and end date for each Feature. Finally, we added in the StateCategoryOrder and the AssignedTo lookup: the AssignedTo relationship to the user table didn’t work as we expected, probably because we made the relationship inactive. Fortunately the lookup is simple.\n\nWe now have all the data we need for some visualizations!\n\nSlicers\n\nWe first added some slicers: Area, Iteration, Epic State and Feature State and map these to Areas:AreaPath, Iterations:IterationPath, Epics:StateCategory and Features:StateCategory respectively.\n\nSlicers\n\nWe then added a Donut chart to show the total count of Stories: we set the Legend to State Category and the Values to Count of WorkItemId.\n\nConfiguring the Stories Count Donut\n\nWe also added a Stacked Bar Chart of Stories, using AssignedTo as the Axis, State Category as the Legend and Count of WorkItemId as the Value.\n\nConfiguring the Stories by Assigned to and State Category Bar Chart\n\nWe also configured the colors to be Blue, Yellow, Green for Proposed, In Progress and Completed respectively on both charts.\n\nThe final chart we configured is a timeline - we’re actually using the Gannt Chart custom visual from the PowerBI gallery to render this:\n\nConfiguring the Timeline Visual\n\nHere’s how we mapped the fields:\n\n\n  Legend: Features:StateCategory\n  Task: Features:Title\n  Parent: Epic:Title\n  Start Date: Features:Stories Start Date\n  End Date: Latest Features:Stories End Date\n  % Completion: Features:Stories Completed Percentage\n  Resource: Features:AssignedTo\n\n\nThis rolls up the Stories into a single bar per Feature. The color of the bar is determined by the state of the Feature. The completed percentage is the percentage of Stories for that Feature in the completed state. The start and end date are the earliest Iteration start and latest Iteration end date for all the Stories in that Feature. The resource is the AssignedTo (owner) of that Feature. And all these Features are grouped into their corresponding Epic (Parent).\n\nNow you can click on a Feature in the Timeline and it will filter the remaining visuals! And you can of course use the slicers to get more specific on the work items you’re interested in viewing.\n\nConclusion\n\nIf you’re circumspect about how much data you’re grabbing from the OData endpoint, you can fairly easily create useful PowerBI reports for your work items. These can end up being better than widgets in Azure Boards Dashboards because you can slice and dice them more easily.\n\nHappy reporting!\n",
      "categories": [],
      "tags": ["reporting","analytics"],
      
      "collection": "posts",
      "url": "/work-item-hierarchy-reports-in/"
    },{
      
      "title": "Hosting Code On Premises: GitHub Enterprise with Azure DevOps",
      "date": "2020-06-25 19:12:33 +0000",
      "description": "Do you want to be on the latest DevOps platforms, but are required to keep source code on premises? In this post I talk about considerations for hosting GitHub Enterprise and Azure DevOps Server on premises.\n",
      "content": "\n  Why Host Source Code On Premises?\n  CI/CD\n  Work Item Tracking\n  Cloud Hosting vs Hosting On Premises\n  Conclusion\n\n\nI’ve been working recently with several customers that are migrating source code to GitHub Enterprise. Most of these customers are also on Team Foundation Server (TFS) or Azure DevOps Server (the newer version of TFS).\n\nIn this post I want to point out some considerations when migrating to GitHub Enterprise:\n\n\n  CI/CD : GitHub Actions are not currently available on GitHub Enterprise, so how do you perform automated build and release?\n  Work Item Tracking : GitHub Projects and Issues may not be adequate for work item tracking, especially when the organization requires Portfolio Management\n  Cloud Hosting vs Hosting On Premises : Hosting GitHub Enterprise (and Azure DevOps Server) requires infrastructure and introduces operational overhead. Are there other options?\n\n\nWhy Host Source Code On Premises?\n\nIdeally, customers should host source code in the cloud - on GitHub.com or on Azure DevOps Server. Even if you do not (yet) deploy to the cloud, you can still perform builds in the cloud and have on premises agents on your network to deploy to on premises environments.\n\nHowever, there are organizations that have regulatory requirements or their InfoSec teams do not (yet) have enough trust in cloud providers. I recommend that teams have an open discussion with InfoSec to find out how hosted cloud platforms like GitHub and Azure DevOps are very secure and meet stringent compliance requirements. Use the Azure Compliance page as well as the Azure DevOps Data Protection overview page in your discussions.\n\nStill, many organizations will mandate that source code be kept on premises - so how can you achieve this goal and still make use of cutting edge platforms?\n\nCI/CD\n\nGitHub Actions are promising, but are not yet available on GitHub Enterprise Server. Additionally, while Actions are great for CI, there are still a number of features that are not yet implemented in order to support sophisticated CD (including environments and approvals, among others). GitHub advanced security is also not yet available - though I expect that GitHub Enterprise Server will eventually catch up to GitHub.com.\n\nAzure Pipelines is available in Azure DevOps Server 2019. However, the same limitations apply in that environments (and therefore approvals) are not yet available (though they will be in Azure DevOps Server 2020 which is imminent).\n\nAssuming that you can wait for Azure DevOps Server 2020, the ideal scenario is that you have GitHub Enterprise for source control and integrate to Azure DevOps Server 2020 for Boards (work item tracking) and Pipelines (using YML files in the repos).\n\nWork Item Tracking\n\nAzure Boards let you track work according to iterations (or sprints) or just visualize and flow work using Kanban. It has rich customization capabilities and ties neatly into Azure Pipelines and other Azure DevOps capabilities.\n\nIf you have any kind of Portfolio (cross-team) management requirements, then using Azure Boards is arguably going to be better than using GitHub projects and issues.\n\nAgain the ideal scenario is using GitHub Enterprise Server for source control and Azure Boards for work item tracking.\n\nCloud Hosting vs Hosting On Premises\n\nThere are several drawbacks to hosting on premises:\n\n\n  You need to maintain infrastructure - VMs, SQL Servers, backups, patching - you’ll need to have someone perform all of these operations\n  Upgrade cycles - you’ll need to upgrade GitHub Enterprise and Azure DevOps Server periodically to get new features, and these upgrades can be disruptive\n  No extranet access - you will have to VPN into your network in order to access your source control, which makes working from home more challenging and makes it harder to utilize contractors\n\n\nIs there an alternative? There are two. The first is to use GitHub Private Instances (which are not publicly available yet). These are instanced of GitHub Enterprise that are fully managed in Azure, but can be placed on to a private network. This may be good enough for your InfoSec teams.\n\nThe second alternative, which is possible right now, requires opening a firewall port to your source control system, which InfoSec may not allow. But if you can convince them to open port 443 to a whitelist of Azure DevOps IPs, then you can use this architecture:\n\n\n  Have source code in a local GitHub Enterprise Server\n  Open a port (443) in the corporate firewall to the whitelist describe here\n  Create an Azure DevOps Services (cloud) account\n  Create a Service Endpoint from Azure DevOps Services to your on premises GitHub Enterprise server (this is why the firewall opening is required)\n  Run builds using hosted agents OR\n  Run builds using private (self-hosted) agents\n  Deploy on premises using private agents\n\n\nIn this scenario, you only have to maintain infrastructure for GitHub Enterprise, but your Pipelines and Boards are on the cloud.\n\nConclusion\n\nIdeally, you want to host your source code in the cloud. If that is not an option, then I recommend using GitHub Enterprise Server on premises and Azure DevOps Services (cloud) for Boards and Pipelines, since this has the least amount of operational overhead and infrastructure requirements and removes the Azure DevOps upgrade cycle. However, this requires opening a firewall port to your GitHub Enterprise Server. If this is not an option, then you’re left with having to host both GitHub Enteprise Server and Azure DevOps Server locally. Not ideal, but al least you’ll be “near current” for your DevOps platform.\n",
      "categories": [],
      "tags": ["github","devops"],
      
      "collection": "posts",
      "url": "/github-enterprise-with-azure-devops/"
    },{
      
      "title": "az devops cli like a boss",
      "date": "2020-07-20 19:59:14 +0000",
      "description": "One of the best features of Azure DevOps is the extensive API. However, while having a REST API is great, interacting with a service at HTTP level can be frustrating. In this post, I examine the az devops cli using 10 practical examples.\n",
      "content": "\n  Installing az and az devops\n  -h is your friend\n  Organization\n  JMESPath\n  Authentication\n  Examples    \n      Example 1: Creating a Team Project\n      Example 2: Managing Security Groups\n      Example 3: Determining if a Git Repo Exists\n      Example 4: Creating a Git Repo and Importing an External Repo\n      (Bonus) Example 5: Automating Git Commands After Cloning\n      Example 6: Deleting a Repo\n      Example 7: Creating an ARM Service Endpoint\n      Example 8: Creating and Deleting (YML) Environments using invoke\n      Example 9: Creating Variable Groups\n      Example 10: Creating a YML Pipeline\n    \n  \n  Conclusion\n\n\nOne of the best features of Azure DevOps is the extensive API. However, while having a REST API is great, interacting with a service at HTTP level can be frustrating.\n\nAzure itself has an extensive API, and the API has been wrapped into an easy to use cross-platform command line interface (CLI) called az. Fortunately, there is an extension to az for interacting with Azure DevOps called az devops.\n\nIn this post I’ll walk through installing the cli, some basics for using the cli effectively and then 10 practical examples of how to use it:\n\n\n  Creating a Team Project\n  Managing Security Groups\n  Determining if a Git Repo Exists\n  Creating a Git Repo and Importing an External Repo\n  (Bonus) Automating Git Commands After Cloning\n  Deleting a Git Repo\n  Creating an ARM Service Endpoint\n  Creating and Deleting (YML) Environments using invoke\n  Creating Variable Groups\n  Creating YML Pipelines\n\n\nInstalling az and az devops\n\nTo install az you can follow these instructions. Once you’ve installed az you can install the devops extension by following these instructions (all you really have to do is type az extension add --name azure-devops).\n\n-h is your friend\n\nThe -h switch (help) is your friend. I have often discovered new subcommands by using the switch, and of course for each command you need the help to figure out all of the args you need to pass. For example, az devops project create -h prints out all the args needed to create a new Azure DevOps Team Project.\n\nOrganization\n\nWhen you run az devops commands, you’ll need to specify the organization (and often Team Project) that you want to run commands against. Supposedly, you can set a default using az devops configure defaults organization=https://dev.azure.com/myOrg but I could not get that to work. Instead, I put the org URL into a variable and use the --org $orgUrl argument for every command.\n\nJMESPath\n\nThe other argument that you’ll need to want to know is the --query argument. This allows you to specify JSON queries against the results of a command to extract certain information. JMESPath is at once powerful and frustrating. You can find documentation on this query language here.\n\nYou’ll also want to use the -o parameter to specify the output. For scripts, using -o tsv (table separated values) will give you plain text results that you can assign to variables for use further down in your scripts.\n\nFor example, if you want to determine if a Team Project already exists, you can use the az devops project list command: but it will return an array of objects which you’ll have to try to parse:\n\n\n    /home/colin/repos/foo [master ≡]&gt; az devops project list --org $orgUrl\n    {\n      \"continuationToken\": null,\n      \"value\": [\n        {\n          \"abbreviation\": null,\n          \"defaultTeamImageUrl\": null,\n          \"description\": null,\n          \"id\": \"&lt;redacted&gt;\",\n          \"lastUpdateTime\": \"2020-07-17T20:54:47.090000+00:00\",\n          \"name\": \"Project1\",\n          \"revision\": 622,\n          \"state\": \"wellFormed\",\n          \"url\": \"https://dev.azure.com/&lt;redacted&gt;/_apis/projects/&lt;redacted&gt;\",\n          \"visibility\": \"private\"\n        },\n        {\n          \"abbreviation\": null,\n          \"defaultTeamImageUrl\": null,\n          \"description\": null,\n          \"id\": \"&lt;redacted&gt;\",\n          \"lastUpdateTime\": \"2020-07-15T04:33:19.377000+00:00\",\n          \"name\": \"Project2\",\n          \"revision\": 306,\n          \"state\": \"wellFormed\",\n          \"url\": \"https://dev.azure.com/&lt;redacted&gt;/_apis/projects/&lt;redacted&gt;\",\n          \"visibility\": \"private\"\n        },\n        ...\n        {\n          \"abbreviation\": null,\n          \"defaultTeamImageUrl\": null,\n          \"description\": null,\n          \"id\": \"&lt;redacted&gt;\",\n          \"lastUpdateTime\": \"2020-07-20T15:33:57.950000+00:00\",\n          \"name\": \"Projectn\",\n          \"revision\": 670,\n          \"state\": \"wellFormed\",\n          \"url\": \"https://dev.azure.com/&lt;redacted&gt;/_apis/projects/&lt;redacted&gt;\",\n          \"visibility\": \"private\"\n        }\n      ]\n    }\n\n\n\nUsing the --query parameter, we can return a simple array of strings (each entry is a Team Project name) making parsing much simpler: az devops project list --org $orgUrl --query \"value[].name\" -o tsv:\n\n\n    /home/colin/repos/foo [master ≡]&gt; az devops project list --org $orgUrl --query \"value[].name\" -o tsv\n    Project1\n    Project2\n    ...\n    Projectn\n\n\n\nAuthentication\n\nTo authenticate, you need to run az devops login which will ask for a Personal Access Token (PAT) to connect to your Azure DevOps organization. Being prompted is fine when you’re working in a console, but if you want to automate az devops commands, you’re going to want to log in without the prompt. To do this, you can set an environment variable called AZURE_DEVOPS_EXT_PAT to the value of your PAT. In my pwsh (cross-platform PowerShell) scripts, this didn’t seem to work totally, so I ended up piping the PAT to the login command too: $pat | az devops login.\n\nExamples\n\nNow that we’ve got some basics out the way, let’s take a look at a few examples.\n\nExample 1: Creating a Team Project\n\nThis one is pretty straightforward: az devops project create --org $orgUrl --name MyNewProject. However, in the script I was creating, I wanted to add an organizational group to the Project Admins group. That leads to…\n\nExample 2: Managing Security Groups\n\nLet’s get the descriptor (id) of the newly created Team Project’s Project Administrator Group:\n\naz devops security group list -p $projectName --org $orgUrl --query \"graphGroups[?contains(principalName,'Project Administrators')].descriptor\" -o tsv\n\nWe’re using az devops security group to list out the groups of a Team Project. We’re then using the contains JMESPath function to query for the node that has the attribute principalName like Project Administrators and returning the descriptor attribute.\n\nNext we query an org-level group by adding --scope organization to the command:\n\naz devops security group list --org $orgUrl --scope organization --query \"graphGroups[?contains(principalName,'Specialists')].descriptor\" -o tsv\n\nHere we query the org-level groups looking for a group containing the word Specialists and again return the descriptor.\n\nFinally, we use the group membership command to add the Specialists group to the Project Admins group:\n\naz devops security group membership add --org $orgUrl --group-id $projAdminGroupDescriptor --member-id $specialistGroupDescriptor\n\nExample 3: Determining if a Git Repo Exists\n\nGetting a list of Repo names in a Team Project is easy:\n\n$repoList = az repos list --org $orgUrl -p $ProjectName --query \"[].name\" -o tsv\n\nNext I wanted to determine if a repo with a given name existed. Since I’m in pwsh on linux, I initially tried -contains but this is a case sensitive search. To make the search case insensitive, I convert the list to an ObjectCollection and use FindIndex:\n\n\n    $repoCollection = [Collections.Generic.List[Object]]$repoList\n    if ($repoCollection -and $repoCollection.FindIndex({ $args[0] -eq $RepoName }) -ge 0) {\n        Write-Host \"Repo already exists\"\n    } else {\n        Write-Host \"Cloning repo...\"\n    }\n\n\n\nExample 4: Creating a Git Repo and Importing an External Repo\n\nBefore importing a repo, we have to have a repo to import into. To create an empty Git repo, we can use this command:\n\naz repos create --name $RepoName -p $ProjectName --org $orgUrl\n\nNext we want to create an import request. In my case, the source repo was another Azure DevOps organization repo and so authentication was required. The same would be true of any private repo. If you require authentication, then generate an authentication token on the source repo and set the environment variable AZURE_DEVOPS_EXT_GIT_SOURCE_PASSWORD_OR_PAT to the value of the token (or password). Then the import request will include authentication:\n\naz repos import create --git-url $sourceRepoURL -p $ProjectName --org $orgUrl --repository $RepoName --requires-authorization\n\nThis creates the request and performs the import from the external Git repo.\n\n(Bonus) Example 5: Automating Git Commands After Cloning\n\nIn our case, we needed to manipulate some files after the import. Assuming we already have a PAT for the new target Team Project (same one we used to authenticate using az devops login), we can configure Git to allow git operations using -c http.extraHeader.\n\nLet’s first get the URL of the new repo using:\n\n$repoUrl = az repos show -p $ProjectName --org $orgUrl -r $RepoName --query \"webUrl\" -o tsv\n\nNext, we encode the header to authenticate to the new repo:\n\n$b64Pat = [Convert]::ToBase64String([System.Text.Encoding]::UTF8.GetBytes(\":$PAT\"))\n\nFinally, we use the encoded PAT when performing remote git operations (like clone, pull and push):\n\ngit -c http.extraHeader=\"Authorization: Basic $b64Pat\" clone $repoUrl\n\nExample 6: Deleting a Repo\n\nWhen you create a new Team Project (using Git) you get a repo with the same name as the Team Project. We wanted to delete that repo. First, get the repo id and then delete it:\n\n\n    $repoId = az repos show -r $ProjectName -p $ProjectName --org $orgUrl --query \"id\" -o tsv\n    az repos delete --id $repoId --org $orgUrl -p $ProjectName -y\n\n\n\nExample 7: Creating an ARM Service Endpoint\n\nWe needed to create a Service Endpoint to an Azure Subscription in our script. Once again, there is authentication involved because you need an SPN key (we’re connecting via SPN and not certificate). Once again, setting an environment variable AZURE_DEVOPS_EXT_AZURE_RM_SERVICE_PRINCIPAL_KEY saved the day. Assuming we have the SPNClientID, AzureSubscriptionID and Name and TenantID, we can create a service endpoint using:\n\naz devops service-endpoint azurerm create --azure-rm-service-principal-id $SPNClientID --azure-rm-subscription-id $AzureSubscriptionID --azure-rm-subscription-name $AzureSubscriptionName --azure-rm-tenant-id $TenantID --name $ServiceEndpointName -p $ProjectName --org $orgUrl\n\nOne more thing: once we created the endpoint, we wanted it to be authorized for all pipelines. Fortunately we can do this using az devops too! First we retrieve the ID of the newly created endpoint, and then update it:\n\n\n    $epId = az devops service-endpoint list --org $orgUrl -p $ProjectName --query \"[?name=='$ServiceEndpointName'].id\" -o tsv\n    az devops service-endpoint update --id $epId --enable-for-all true --org $orgUrl -p $ProjectName\n\n\n\nExample 8: Creating and Deleting (YML) Environments using invoke\n\nUp until this point, I have been able to do everything I needed using “native” az devops commands. However, when it comes to manipulating YML environments, there are not yet commands for this in az devops. Fortunately, az devops provides a “catch all” command called invoke that lets you easily invoke any REST API method against Azure DevOps.\n\nHere is the REST API call to list YML environments from this help doc:\n\nGET https://dev.azure.com/{organization}/{project}/_apis/distributedtask/environments?api-version=6.0-preview.1\n\nThis URL has a “routing parameter” {project} and then the portion after the _apis specifies the area (distributedtask) and resource (environments). Finally, we have to note the api-version.\n\nArmed with this, we can formulate the request for the az devops invoke command:\n\n$envs = az devops invoke --area distributedtask --resource environments --route-parameters project=$ProjectName --org $orgUrl --api-version \"6.0-preview\" -o json | ConvertFrom-Json\n\n\n  Note: the api-version value is “6-0-preview” without the .1 from the API documentation\n\n\nWe can then query the environments. If we decide that we want to create an environment, we’ll need to do a POST. az devops invoke requires a JSON file as the body of the POST, so we can create the body and dump it to a json file and then pass that in as the --in-file argument:\n\n\n    $envBody = @{\n      name = $env\n      description = \"My $env environment\"\n    }\n    $infile = \"envbody.json\"\n    Set-Content -Path $infile -Value ($envBody | ConvertTo-Json)\n    az devops invoke `\n       --area distributedtask --resource environments `\n       --route-parameters project=$ProjectName --org $orgUrl `\n       --http-method POST --in-file $infile `\n       --api-version \"6.0-preview\"\n    rm $infile -f\n\n\n\nTo delete an environment, find its ID:\n\n$id = az devops invoke --area distributedtask --resource environments --route-parameters project=$ProjectName --org $orgUrl --api-version \"6.0-preview\" --query \"value[?name=='myEnvToDelete'].id\" -o tsv\n\nOnce you have its ID, you can use the DELETE http-method to invoke the REST API DELETE method:\n\naz devops invoke --area distributedtask --resource environments --route-parameters project=$ProjectName environmentId=$id --org $orgUrl --http-method DELETE --api-version \"6.0-preview\"\n\nExample 9: Creating Variable Groups\n\nWe can use az pipelines variable-group commands to manipulate variable groups. Here’s how we list all the variable groups in a Team Project:\n\naz pipelines variable-group list -p $ProjectName --org $orgUrl --query \"[].name\" -o tsv\n\nTo create a variable group, we use the create subcommand. We can also specify key/value pairs using the --variables arg:\n\naz pipelines variable-group create --name $varGroupName -p $ProjectName --org $orgUrl --authorize --variables var1=\"val1\" var2=\"val2\"\n\nExample 10: Creating a YML Pipeline\n\nTo create a YML pipeline, you’ll need a name as well as the repository, branch and path to the YML file within the repo (the path relative to the root path of the repo):\n\naz pipelines create -p $ProjectName --org $orgUrl --name $PipelineName --description $PipelineDescription --repository $RepoName --repository-type tfsgit --branch master --skip-first-run --yml-path $YmlPath\n\n\n  Note: The –skip-first-run tells Azure DevOps not to immediately run the pipeline. If you do not specify this, the pipeline is automatically queued.\n\n\nIf you need UI variables for the pipeline, you can add these using az pipelines variables create like this:\n\naz pipelines variable create -p $ProjectName --org $orgUrl --pipeline-name $PipelineName --name myVar --value \"some value\" --allow-override $true\n\n\n  Note: The –allow-override is the checkbox on the UI that allows users to override the variable value at queue time.\n\n\nConclusion\n\nAzure DevOps has a rich REST API and az devops certainly makes interacting with the API much easier. This cli is under constant development, so even if there are APIs that are not yet wrapped (like the YML Environment APIs) you can use invoke to interact with those APIs. This makes az devops a natural choice for scripting and automating Azure DevOps tasks!\n\nHappy commanding!\n",
      "categories": [],
      "tags": ["automation"],
      
      "collection": "posts",
      "url": "/az-devops-like-a-boss/"
    },{
      
      "title": "Little's Law Doesn't Work",
      "date": "2020-09-21 21:09:53 +0000",
      "description": "Little’s Law is well known, but not well understood. Daniel Vacanti has some deep insights into the assumptions that need to be made to make Little’s law “work” for you.\n",
      "content": "\n  Little’s Law\n  Assumptions That Make Little’s Law Work    \n      1. Average Input Rate Should Roughly Equal Average Output Rate\n      2. Work Started Must Eventually Exit\n      3. WIP Should Be Roughly The Same at the Start and End of the Measured Period\n      4. Average Age of WIP is Relatively Constant\n    \n  \n  Then What’s the Use of Little’s Law?    \n      Forecasting with Little’s Law\n      Variability\n    \n  \n  Conclusion\n\n\nI remember being mesmerized watching one of my favorite humans, Steven Borg, talk about flow in knowledge work. Watching him train teams on queues and lean practices was amazing. Of course, Steve pointed me to The Principles of Product Development Flow by Donald G. Reinertsen and I highly recommend that you read it if you’re in any kind of knowledge work.\n\nIt’s been a while since I read anything fresh on these topics - mostly due to a change in role for me at work. I’m doing more technical sales than delivery, so I don’t get as much time code-slinging and working with teams as I used to. However, I finally decided to read some work on Agile metrics by Daniel S. Vacanti.\n\nDaniel makes application of some of the math and statistics of agile metrics easy to understand and absorb. I’ve been thinking a lot about his writing, and I wanted to jot down a few of my thoughts.\n\nLittle’s Law\n\nIf you’ve ever seen any Lean talks or materials at all, you’ll undoubtedly have come across Little’s Law. Little’s Law originally is written as follows:\n\n\n\nThis is written in terms of arrival rate.\n\nIn knowledge work, you will more commonly see it written as:\n\n\n\nThis is written in terms of throughput or completion rate. This was something I had never considered before and Vacanti goes into detail about why this is an important distinction. In short, there are a couple of assumptions that need to be made to make the law work - if you don’t adhere to these assumptions, then the math breaks.\n\nThe other epiphany I had was that Little’s Law should never be used for forecasting - I’ve always heard Little’s Law bandied as a method to apply some easy math to predict forward what cycle time or throughput is going to be. Vacanti says that at best it can be used to do a gut-check, but should not be used to provide estimates.\n\nThe question is then: is Little’s Law a lie?\n\nAssumptions That Make Little’s Law Work\n\nVacanti lists five assumptions to make Little’s Law work. The fifth one is really simple - make sure you’re consistent with your units for Cycle Time, Work in Progress (WIP) and Throughput, so no need to go into too much detail on this one.\n\nLet’s take a look at the other four, which really revolve around the stability of the system under observation. The more stable a system is, the more useful Little’s Law will be. (Statisticians speak of how stationary the underlying stochastic processes are - but what that really means is how stable the system is).\n\n1. Average Input Rate Should Roughly Equal Average Output Rate\n\nThe original form of Little’s Law is written in terms of arrival rates, while the knowledge work form is written in terms of departure (throughput) rates. So it makes sense that these should be roughly the same for the law to hold. Most teams will notice that work does not exit their processes at the same rate that work enters their processes. If you plotted arrival rates and departure rates over time, you’d see that the arrival rate gradient would be greater than the departure rate gradient - you’ll see this on Cumulative Flow Diagrams (CFDs).\n\nThe reason that Little’s Law doesn’t work in this case is that work gets stuck in the system and skews the averages. Remember, the law is written in terms of averages, so large deviations from the averages are going to break the math. More on variability later.\n\n2. Work Started Must Eventually Exit\n\nThis should follow naturally if arrival and departure rates are roughly the same. Teams should be relentless in ensuring that work started is completed. If work is hanging around for long (indefinite) periods, it will break the math.\n\n3. WIP Should Be Roughly The Same at the Start and End of the Measured Period\n\nFor any time period that you measure, you should have roughly the same amount of WIP at the start and end of the process. If you’re on a (pure) Scrum team, you should always end your sprint with 0 WIP. Technically you start with 0 WIP too and pull in as the Sprint proceeds - but teams that carry WIP from Sprint to Sprint will probably notice that Little’s Law doesn’t work so well for them because they’re breaking this assumption.\n\nIf you’ve recently added a lot of work into your process, then you won’t yet have time to measure how that work is flowing. Similarly, if you suddenly remove a lot of work in the middle somewhere, the math is going to break.\n\n4. Average Age of WIP is Relatively Constant\n\nYou can measure WIP age if you measure how “old” your WIP is (the difference between today and the day the work entered the system or state). It’s a good idea to produce an aging histogram to analyze your WIP average age. If this is too variable, the math for Little’s Law breaks. Again this makes sense since items that age too long are not flowing well and items that age too quickly probably indicate an issue with measurements.\n\nThen What’s the Use of Little’s Law?\n\nAs Vacanti points out, the assumptions of Little’s Law are actually more important than the math itself. What you should be doing with Little’s Law is constantly evaluating how well you’re tracking to the assumptions. If any of the metrics start skewing (like average age of WIP is all over the place) then you have a good leading indicator that is pointing to process issues.\n\nContext is important - even if you’re presented with a CFD that looks textbook with nice, even bands - that doesn’t mean that your process is perfect. You still need to interpret the data and charts in context of what is happening with the team that is being measured.\n\nForecasting with Little’s Law\n\nVacanti points out that it’s a mistake to forecast solely based on the math of Little’s Law. The main reason is that you cannot assume that nothing will change in the future. Little’s Law works on historical data, but since it depends on these assumptions, you cannot use it to forecast. At best you can use it to “gut check” when you’re doing any sort of forecasting. That’s a topic for a different post.\n\nVariability\n\nI made mention earlier that too much variability in WIP age can be bad. So does that mean that you have to estimate all your work items to be a constant effort or number of story points?\n\nNo! One beauty of Little’s Law is that it works without having to know the size or complexity of the items in the system. However, since the law works on averages, too much variation in the averages is bad. Remember the assumptions revolve around the overall stability of the system - if the underlying system itself is fairly stable, then it can absorb variation and is predictable. However, if the underlying system is constantly changing, then predictability is a pipe dream.\n\nConclusion\n\nLittle’s Law isn’t a lie - as long as you’re adhering to the assumptions that need to be met in order for it to work. Teams that pay attention to the assumptions and address behaviors, patterns or practices that break the assumptions are far more likely to gain process efficiency. As teams improve the stability of their systems, they become more predictable and that’s good for the team as well as for the end users the team is trying to serve.\n",
      "categories": [],
      "tags": ["devops"],
      
      "collection": "posts",
      "url": "/littles-law-doesnt-work/"
    },{
      
      "title": "Azure Pipelines for Private AKS Clusters",
      "date": "2020-10-27 19:25:25 +0000",
      "description": "Creating private AKS clusters is a good step in hardening your Azure Kubernetes clusters. In this post I walk through the steps you’ll need to follow to enable deployment to private AKS clusters.\n",
      "content": "\n  Private Agents\n  Deploying to Private AKS    \n      Create a Generic k8s Endpoint\n    \n  \n  The Pipeline\n  Conclusion\n\n\nAzure Kubernetes Service (AKS) Clusters are amazing - all the power of Kubernetes (K8s) without the hassle of a full tin-based installation. However, by default the management plane, or k8s API, is public. If you want to harden your cluster, one sensible step would be to prevent public access to the management API by making your cluster private.\n\n\n  Note: There are many other steps you should take to truly harden your cluster, so making your cluster private alone does not guarantee a secure cluster!\n\n\nHowever, this comes at a price. If you do this, then you can only access the API from a VNet that has a private link to the AKS cluster. That means that you cannot deploy to the cluster from an Azure Pipelines hosted agent (since the agent is coming in to the cluster from the internet).\n\nFurthermore, you probably want to make your Azure Container Registry (ACR) private too. Once again this step comes with additional overhead: you can only push/pull to/from this ACR from the VNet, so hosted agents won’t work.\n\nPrivate Agents\n\nFortunately, we can provision private agents for deployments. The procedure is as follows:\n\n\n  Create a VM on a VNet that has the private link to the AKS cluster so that the VM can reach the private endpoint for the cluster API.\n  Create an agent pool inside of Azure DevOps (AzDO).\n  Download the AzDO pipelines agent and register it to the agent pool.\n  Target the agent pool in your pipelines.\n\n\nYou can (of course) do this manually, but if you’re using Terraform, then you can use the following code to spin up a VM and execute a custom script that will register the agent:\n\n\n    resource \"azurerm_linux_virtual_machine\" \"devopsvm\" {\n      name = var.devops_vm_name\n      resource_group_name = azurerm_resource_group.azdorg.name\n      location = azurerm_resource_group.azdorg.location\n      network_interface_ids = [azurerm_network_interface.azdovmnic.id]\n      size = \"Standard_DS1_v2\"\n    \n      os_disk {\n        name = \"${var.devops_vm_name}_osdisk\"\n        caching = \"ReadWrite\"\n        storage_account_type = \"Standard_LRS\"\n      }\n    \n      source_image_reference {\n        publisher = \"Canonical\"\n        offer = \"UbuntuServer\"\n        sku = \"18.04-LTS\"\n        version = \"latest\"\n      }\n    \n      computer_name = var.devops_vm_name\n      admin_username = var.agent_user\n      disable_password_authentication = true\n    \n      admin_ssh_key {\n        username = var.agent_user\n        public_key = tls_private_key.azdossh.public_key_openssh\n      }\n    \n      tags = var.tags\n    }\n\n\n\n\n  Note: I’m not showing all the resources here, just the main VM resource. This is spinning up an Ubuntu 18.04 VM and attaching it to the VNet that has my AKS cluster private link.\n\n\nOnce you have the VM defined, you need to add a custom extension:\n\n\n    resource \"azurerm_virtual_machine_extension\" \"installtools\" {\n        name = \"customconfig\"\n        virtual_machine_id = azurerm_linux_virtual_machine.devopsvm.id\n        publisher = \"Microsoft.Azure.Extensions\"\n        type = \"CustomScript\"\n        type_handler_version = \"2.0\"\n        protected_settings = &lt;&lt;SETTINGS\n        {\n          \"script\" : \"${base64encode(templatefile(\"${path.module}/install_tools.sh\", {\n            AGENT_USER = var.agent_user\n            AGENT_POOL = \"${var.agent_pool_prefix}-${var.environment}\"\n            AGENT_TOKEN = var.agent_token\n            AZDO_URL = var.azdo_url\n          }))}\"\n        }\n        SETTINGS\n        depends_on = [azurerm_linux_virtual_machine.devopsvm]\n    }\n\n\n\nThe trick here is to invoke a script (install_tools.sh) with the args that it requires, namely the agent user name and token, the agent pool and the AzDO account URL.\n\n\n  Note: The token is a Personal Access Token (PAT) that has permissions to administer the agent pool.\n\n\nLet’s have a look at the install_tools.sh script:\n\n\n    #!/bin/bash\n    agentuser=${AGENT_USER}\n    pool=${AGENT_POOL}\n    pat=${AGENT_TOKEN}\n    azdourl=${AZDO_URL}\n    \n    # install az cli\n    curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n    \n    # install docker\n    sudo apt install -y apt-transport-https ca-certificates curl software-properties-common\n    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n    sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\"\n    sudo apt update\n    sudo apt install -y docker-ce\n    sudo usermod -aG docker $agentuser\n    \n    # install kubectl\n    curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add\n    sudo apt-add-repository \"deb http://apt.kubernetes.io/ kubernetes-xenial main\"\n    sudo apt update\n    sudo apt-get install -y kubectl\n    \n    # install helm\n    curl -o helm.tar.gz https://get.helm.sh/helm-v3.3.4-linux-amd64.tar.gz\n    tar zxvf helm.tar.gz\n    sudo mv linux-amd64/helm /usr/local/bin/helm\n    rm -rf linux-amd64\n    rm -f helm.tar.gz\n    \n    # download azdo agent\n    mkdir -p /opt/azdo &amp;&amp; cd /opt/azdo\n    cd /opt/azdo\n    curl -o azdoagent.tar.gz https://vstsagentpackage.azureedge.net/agent/2.175.2/vsts-agent-linux-x64-2.175.2.tar.gz\n    tar xzvf azdoagent.tar.gz\n    rm -f azdoagent.tar.gz\n    \n    # configure as azdouser\n    chown -R $agentuser /opt/azdo\n    chmod -R 755 /opt/azdo\n    runuser -l $agentuser -c \"/opt/azdo/config.sh --unattended --url $azdourl --auth pat --token $pat --pool $pool --acceptTeeEula\"\n    \n    # install and start the service\n    ./svc.sh install\n    ./svc.sh start\n\n\n\nThis script will:\n\n\n  Set the variables - the tokens like ${AGENT_USER} are injected when the Terraform templatefile function is invoked in the custom extension above\n  Install tools such as az cli, docker, kubectl and helm\n  Download and untar the AzDO private agent\n  Run a silent config command to register the agent to the agent pool - I do this using runuser since this command cannot be run as root (which is the context when Terraform/Azure executes the custom script extension)\n  Install and start the agent service\n\n\n\n  Note: if you’re using ARM templates to deploy your VM, then you can still use the install_tools.sh script using the ARM custom script extension. You’ll have to change how the parameters are handled though.\n\n\nTo use this agent in a YML pipeline, just set the pool property to the name of the pool you registered the agent to:\n\n\n    jobs:  \n    - job: Build\n      displayName: Build\n      pool: my-private-pool\n      steps:\n      - script: echo Hello from private agent!\n\n\n\nNow you can push/pull to/from your private ACR - as long as the build runs on the private agent, just use the docker tasksas per normal.\n\nDeploying to Private AKS\n\nNow that we have a private agent, we can deploy to the AKS cluster. However, there are a couple further steps required, especially if we want approvals(which you do!).\n\nFor public AKS cluster, you can create a Pipeline Environment in AzDO and add a Kubernetes resource. Under the hood AzDO will query the namespaces of the target cluster so that you can select a namespace for the environment, as well as for querying workloads (services, deployments and pods) within the cluster. However, if you try this for a private cluster, you’ll get an error:\n\nError when creating an AzDO environment to a private AKS cluster\n\nYou can see the URL to the cluster API has .privatelink in it - so this makes sense. AzDO is unable to connect to the AKS API.\n\nTo work around this, we need to:\n\n\n  Create a generic environment for approvals\n  Create a Kubernetes service connection for connecting to the cluster using kubeconfig\n\n\nTo create the environment, navigate to Pipelines-&gt;Environments and add a new environment. Simply select None for the resource type:\n\nCreating a generic AzDO environment\n\nNow you can define approvals and/or gates (checks) on the environment as you would for any other environment.\n\nCreate a Generic k8s Endpoint\n\nNext we have to create a k8s service endpoint. The service endpoint allows you to abstract authentication from your pipelines - as long as the person that initiates the pipeline has access to use the endpoint (you can configure endpoint security) then they just specify the endpoint name - the authentication is handled under the hood.\n\nIf you try to create a service connection to a public AKS cluster, you can use the dialog and it will confirm connectivity and show namespaces etc. - but we can’t in this case for the same reason as the environment above - the API is private!\n\nThat means that we have to fall back on creating a generic k8s endpoint using a kubeconfig. When we use this connection, AzDO will download the kubeconfig and issue kubectl config use-context for the specified context, so that we’re ready to issue subsequent commands.\n\nTo extract your cluster’s kubeconfig you’ll need to connect to a VM in the VNet. You can then use az cli to login and then get the credentials for the AKS cluster:\n\n\n    # login to azure\n    az login\n    \n    # login to the AKS cluster\n    az aks get-credentials -n &lt;cluster_name&gt; -g &lt;cluster_resource_group_name&gt;\n\n\n\nAt this point, you can just copy the ~/.kube/config file.\n\nNow navigate to your Azure DevOps account and Team Project. Click on the gear icon to go to project (not account) settings. Click on Service connections and then click the New Connection button.\n\nSelect Kubernetes from the connection type and click Next:\n\nCreating a KubeConfig Service Connection\n\nFill out the form as follows:\n\n\n  Change the type to Kubeconfig\n  Paste your kubeconfig file into the KubeConfig field\n  Select the correct context from the dropdown (values will be filled in from the parsed kubeconfig file\n  Enter a name and (optional) description\n  Clicking Verify will fail (because the API is private!) so you have to select Save without verification from the button menu to save the connection.\n\n\n\n  Note: I initially did a cat of the kubeconfig and pasted it - this didn’t work since the copy command inserted newlines in the certs inside the file. I had to clean the value so that it was valid kubeconfig yml.\n\n\nWe now have a generic environment for approvals and a kubeconfig service connection for authentication to the cluster!\n\nThe Pipeline\n\nWe can now make use of the environment and service connection in our pipelines. Here is a snippet showing that:\n\n\n    stages:\n    - stage: dev\n      displayName: DEV\n      jobs:  \n      - deployment: deploy\n        displayName: Deploy\n        pool: my-private-pool\n        environment: 'Private AKS Dev'\n        strategy:\n          runOnce:\n            deploy:\n              steps:\n              - checkout: self\n              - task: HelmDeploy@0\n                displayName: Helm Deploy app\n                inputs:\n                  connectionType: 'Kubernetes Service Connection'\n                  kubernetesServiceConnection: 'Private AKS Connection Dev'\n                  namespace: 'dev'\n                  command: upgrade\n                  chartType: FilePath\n                  chartPath: '/path/to/chart'\n                  releaseName: 'release-name'\n                  valueFile: '/path/to/value/file'\n                  arguments: '--create-namespace'\n\n\n\nOn line 7, we’re specifying the private agent pool so that this pipeline stage runs on the private agent. On line 8 we specify the environment we created earlier - if there are any approvals and/or gates configured for this environment, the stage would only proceed once the approvals and gates have passed. On line 17 we specify that the helm task should authenticate via a Kubernetes Service Connection and we specify the connection name in line 18.\n\nOf course if you were using the Kubernetes manifest task, the same principles would apply.\n\nConclusion\n\nCreating a private AKS cluster makes sense when hardening your cluster. However, it does make deployment challenging. However, by creating a private agent, a generic environment and a kubeconfig service connection, you can still deploy to the cluster securely. Be aware that you are not going to be able to see the workloads (services, deployments, pods etc.) in AzDO like you can with a public AKS Environment. Hopefully you have Container Insights configured anyway, so this shouldn’t be a huge concern.\n\nHappy private kube’ing!\n",
      "categories": [],
      "tags": ["build"],
      
      "collection": "posts",
      "url": "/azure-pipelines-for-private-aks-clusters/"
    },{
      
      "title": "DevOps Benefits of Limiting WIP",
      "date": "2020-11-19 01:57:40 +0000",
      "description": "Generally limiting WIP is discussed in the context of work item tracking - but too much WIP has detrimental effects on branching, testing, architecture and technical debt too!\n",
      "content": "\n  Branching\n  Testing\n  Architecture\n  Technical Debt\n  Conclusion\n\n\nWe’ve recently completed several DevOps Assessments for various customers. These assessments go through work item tracking and planning, source control practices, automated build and test, release and test management, database DevOps, security, architecture, ops and monitoring and team structures. It’s a lot to cover - and I’ve noticed a common theme that comes up over and over: almost every team has too much work in progress (WIP).\n\nThis generally comes up early on when we talk about work item tracking, which I won’t cover here since there are plenty of posts (including some of mine) that cover why you should limit WIP for work items. But there are numerous impacts of having too much WIP in other areas of the delivery lifecycle. In this post I’ll go through some of them.\n\nBranching\n\nI’m a fan of GitHub Flow (also known as trunk development or master development).\n\n\n  Note: GitHub Flow is not the same as GitFlow. I don’t like GitFlow for several reasons, but the most glaring is that it is at heart a manner of code-promotion: I think that binary promotion is far better.\n\n\nAt a high level, the idea behind GitHub Flow is that you have a single stable branch (master) and every User Story is developed on a topic branch (sometimes called a feature branch too, but the word feature can get overloaded). Once you’re code complete, you create a Pull Request (PR) to merge the isolated topic-branch code into the master branch.\n\nThis lends itself well to Continuous Integration and Continuous Delivery (CI/CD) since the code change is usually small. Several topic branches can be in flight at the same time.\n\nHowever, if you have too many topic branches because you have too much WIP, then you quickly get into situations where topic branches cannot be merged because the large number of branches is overwhelming the test environments. Also, if you release in large batches, you’ll probably want to merge topic branches together before merging to master to do integration testing.\n\nI’ve seen teams introduce a second “semi-stable” branch between master and the topic branches (often called DEV or INT). The idea is that topic branches merge to the DEV branch for integration testing and the DEV branch is then merged to master for release.\n\nHowever, this isn’t solving the issue. Now the DEV branch becomes congested. To make matters worse, teams that use this approach often have product owners that want certain features to go to PROD while other untested features should not. Now developers are not merging early, they’re deferring merging to some later date when the rest of the process is “ready” for the code. This leads to stale branches and an accumulation of merge debt and general mess.\n\nIn short, reducing the number of topic branches by limiting WIP reduces the number of isolated branches (and corresponding test environments) required to release completed code. Teams can then use GitHub Flow without needing interim branches and they won’t accumulate merge debt.\n\nUltimately, limiting WIP is going to simplify your branching strategy!\n\nTesting\n\nTeams that have high or uncontrolled WIP generally have huge resource contention on test environments. Ideally, each story in flight has its own branch in source control and its own environment. That way each story can be developed and tested in isolation. However, if you have do not have WIP limits, then you fast end up running out of test environments.\n\nSpinning up environments dynamically can certainly help, but often this is not cost effective. Again teams are left trying to come up with workarounds. For example, sharing environments: which makes test results harder to decipher since if there are multiple builds in an environment at the same time and there are failures, which build is responsible? How do you handle conflicting changes and dependencies?\n\nI hear the astute reader ask, “What about integration testing? How do you test the combined effects of multiple stories that are in flight together?” Well, if there are a low number of these, you can easily merge one story into another (or create a combined branch) for this sort of testing. Of course, if you have lower WIP, then your cycle times will reduce proportionately (via Little’s Law, since lowering WIP also lowers cycle times) so you can release one story at a time, meaning you have less time where there are multiple incomplete stories at any one time.\n\nSimply put, reducing WIP will increase your ability to test and reduce test environment management overhead.\n\nArchitecture\n\nRather than stating how reducing WIP can affect architecture, this one works that other way around. Monolithic architectures, by nature of being large, tend to require batching changes and releases, which drives up the WIP. However, decomposing systems into loosely coupled services that can be released independently (ahem, microservices, anyone?) means that changes to components can be released far quicker. And, again via Little’s Law, if we reduce cycle time, we’ll be reducing WIP.\n\nIn other words, modular, testable, loosely-coupled components allow teams to reduce WIP.\n\nTechnical Debt\n\nEvery application has some form of technical debt. And most product owners don’t care about fixing technical debt, particularly because it is often abstract: it’s not a new feature or a bug-fix. However, technical debt, like unpaid mortgage debt, tends to get worse the longer you don’t pay it down.\n\nDealing with technical debt will ultimately make systems (and teams) more agile, since there can be more focus on new features and updates, rather than bug-fixing. However, even if your team recognizes the value of paying down technical debt, if there is no elasticity in the backlog or project plan because there is too much WIP, then your team will never have time to pay down the technical debt!\n\nLowering WIP allows teams time to actually address technical debt. And as teams pay down the technical debt, they become faster at new features and at bug-fixes, which ultimately is going to make users happier.\n\nConclusion\n\nAs you can see, lowering WIP not only has an impact on the project plan and work item tracking, but can simplify branching strategies, lower test overhead and resource contention, aid modernizing architecture and reduce technical debt. Whatever you’re doing to improve your DevOps, make sure you’re relentless about lowering your WIP!\n\nHappy delivering!\n",
      "categories": [],
      "tags": ["devops","process"],
      
      "collection": "posts",
      "url": "/benefits-of-limiting-wip/"
    },{
      
      "title": "Deployment with GitHub Actions: The Bad and the Ugly",
      "date": "2021-01-11 22:42:40 +0000",
      "description": "GitHub Actions can be used for Continuous Deployment (CD) - but there are some rough edges. In this post I take you through a deep dive and lift the kimono on Actions.\n",
      "content": "\n  Environments, Stages, Jobs and Templates\n  Infrastructure and Code\n  TailWind Web App\n  Interlude: Terraform Wrapper\n  The Workaround\n  Secrets and Approvals on Environments\n  Manual Triggers\n  Disappearing History - Sort of\n  Pesky Environment URL\n  Fireside Chat with Chris Patterson\n  Conclusion\n\n\nLet me start this post with three disclaimers:\n\n\n  GitHub Actions Environments is in beta\n  I’m a huge fan of Azure Pipelines, which is a fairly mature product\n  The GitHub engineers (and PM Chris Patterson) have been amazing with answering my questions and helping me deep dive into Actions for CD\n\n\nWhy start with disclaimers? Because (SPOILER ALERT) deployment with GitHub Actions is rough. Unfortunately, not even enough “good” to add “The Good” to the title of this post.\n\n\n  Note: for ease of typing and reading, I’ll refer to GitHub Actions as simply Actions, and Azure Pipelines as simply Pipelines for the remainder of this post.\n\n\nEver since Actions was born, the vision has been “code to cloud”. This vision is exactly correct and certainly demos nicely if you have a simple web app and a single environment. But how do you handle a more complex real-world scenario? How do you deploy and configure infrastructure? How do you model multiple environments? Where do you store secrets and environment settings? How do you handle approvals?\n\nAt GitHub Universe in December 2020, GitHub released Environments to Beta. This was the first baby step in moving towards a more “enterprise” deployment capability.\n\nEnvironments, Stages, Jobs and Templates\n\nI’ve made a few Actions workflows and have always approached it as a slightly differently formatted Azure Pipeline that can only have jobs (no stages or environments). So when Actions Environments got released, I thought I’d approach Actions with the same paradigm I use when I create Pipelines.\n\nI was wrong.\n\nFirstly, I was annoyed that there is no such thing as a “stage” in Actions. When writing Pipelines, I usually equate the environment with a stage and then run multiple jobs in each stage. This lets me create job templates that I can reuse. Deployments should be as similar as possible between environments which I can easily model with parameterized templates.\n\nBut Actions doesn’t have templates - though it’s on the roadmap. For now, I’ll just have to live with some copy-and-paste.\n\nWhile there are no stages, I could conceptually map jobs to stages in an Action workflow, and use the environment attribute to tell GitHub that a job is targeting an environment. If I need multiple jobs for a stage, I can use a naming convention to loosely group them, even if the UI won’t.\n\nExcept that Actions behaves strangely when you have multiple jobs targeting a single environment (more about this later).\n\nInfrastructure and Code\n\nI bounce back and forth between having infrastructure and app code in the same pipeline or having separate pipelines for these concerns. Using Pipelines, I typically have an infrastructure job to get the environment resources spun up and/or configured, and then have a deployment job to deploy app code, and then have one or more test jobs for running tests. I always use a single build job to produce a single binary that is promoted between environments.\n\nHowever, this can complicate approvals. Typically, ops folks will approve infrastructure changes, while dev folks will approve code deployments. As much as we should be working on cross-functional teams, it’s all too common to still see silos in the enterprise. And even on cross-functional teams, we’re usually all T-shaped - that is, we’re usually more “ops-ey” or more “dev-ey” and so approvals tend to fall along that divide. This scenario works fine with Pipelines since each job in the stage is targeting an environment and the approvals are defined on the environment. There’s no way of specifying who exactly should do the review, so the team will have to work out who’s approval is currently pending (is it for infra or for code). GitHub Environments have the same limitation.\n\nAfter chatting through some of the challenges with Chris Patterson (PM for Actions) he suggested splitting infra and code into separate pipelines. I can at least trigger the infra pipeline only if infra code changes, and trigger the app pipeline for everything else this way.\n\nTailWind Web App\n\nLet’s make this a bit more concrete. For ease of use, I’ll be using a DotNetCore app called TailWind originally from Microsoft, just so that I have some code I can deploy to Azure. I forked the Microsoft repo and added in Terraform templates for the infra into my version. Now I have infra-as-code as well as the app code.\n\nUsing Azure Pipelines, I would have a workflow that looks something like this:\n\nAzure Pipeline for deploying TailWind Web App\n\nThere are multiple stages/jobs here:\n\n\n\n  Stage 1: Build and test sources\n    \n      this builds and packages the code and runs unit tests\n    \n  \n  Stage 2: Deploy to DEV\n    \n      Run terraform to create/configure infrastructure in the DEV environment\n      Deploy the web app using the package from Stage 1\n      Run Selenium (coded UI) tests\n    \n  \n  Stage 3: Canary deployment to PROD\n    \n      Run terraform to create/configure infrastructure in the PROD environment\n      Deploy the web app using the package from Stage 1\n      Route traffic to the staging slot\n      Run JMeter (load) tests\n    \n  \n  Stage 4: Promote or Rollback\n    \n      Depending on the outcome of the tests against the Canary slot, we can either promote to PROD (slot swap) or simply route all traffic back to the production slot (this is effectively a rollback)\n\n    \n  \n\n\nAll the jobs in the Pipeline are templatized - you’ll note that I use the same Run terraform and Deploy the web app jobs in stage 2 and 3, just passing in environment specific values.\n\nSince I don’t have stages in Actions, my initial Action workflow looked like this:\n\nEnd to end Action\n\nHere I have the following jobs:\n\n\n  build - builds and packages code (and would run unit tests here if I had any)\n  dev_infra_plan - runs terraform plan against the DEV environment\n  dev_infra_apply - downloads the output plan from the previous job and runs terraform apply\n  dev_deploy - deploys web app package from job 1 to the DEV environment\n  prod_infra_plan - runs terraform plan against the PROD environment\n  prod_infra_apply - downloads the output plan from the previous job and runs terraform apply\n  prod_deploy - deploys web app package from job 1 to the PROD environment\n\n\nYes, I had to use copy and paste for jobs 2 and 5, 3 and 6 and 4 and 7. But I know templates are coming, so I could live with that for a while. However, the job got stuck in this state and I could never get to job 5 (the start of the PROD jobs). That’s when Chris Patterson helped me understand that because I have multiple jobs targeting the same environment(s) strange things are happening…\n\nSo it seems you shouldn’t have multiple jobs targeting the same environment. And you can’t have templates. Now I was faced with giant blocks of Actions that were reduced into BUILD-&gt;DEV-&gt;PROD with a single job for each. And a single approval for each.\n\nHorrible. Just horrible.\n\nInterlude: Terraform Wrapper\n\nBefore I get to some workarounds for some of the challenges, I want to mention how I lost several hours of my life. Take a look at this snippet:\n\n\n    - name: Install TF\n      uses: hashicorp/setup-terraform@v1\n      with:\n        terraform_version: ${{ env.tf_version }}\n    \n    - name: Init TF\n      run: terraform init --backend-config=\"key=${{ env.env_name }}.terraform.tfstate\"\n    \n    - name: TF Output web app URL\n      run: |\n        url=$(terraform output -raw webAppURL)\n        echo \"The URL is $url\"\n\n\n\nThis workflow snippet is really modelling the following commands:\n\n\n    terraform init --backend-config=\"key=DEV.terraform.tfstate\"\n    url=$(terraform output -raw webAppURL)\n    echo \"The URL is $url\"\n\n\n\nWhen running these locally, I get The URL is http://&lt;url&gt; which is what I expect.\n\nWhen running the workflow, I could not get this to work. I wasted a lot of time trying to figure out what I was doing wrong. The message looked something like this:\n\nThe url is [command]/home/runner/work/_temp/056bc345-efd0-4b6d-ae6c-94094f124a7f/terraform-bin output -raw\n\n\nTurns out that all I had done wrong was make an assumption about how the hashicorp/setup-terraform@v1 task works. I assumed it just installed terraform. However, in this configuration, it installs terraform and creates a wrapper around the executable, so you cannot treat terraform commands as you would locally - you have to access the output parameters.\n\nTo be fair, the Hashi folks make this clear on their documentation, but I didn’t think I’d need to read the docs for running an install! I logged this issue for them to update the logging so that others who make the same assumption don’t waste hours before realizing that the Action installs a wrapper by default.\n\nSo at this point, while I was trying hard to love Actions, it just wasn’t looking good.\n\nThe Workaround\n\n\n  Note: Workflow code is here for my infra end to end and here for my app end to end.\n\n\nThe first thing to do was to split the infrastructure Actions from the code deployment Actions. This made the workflows more aligned to a single responsibility. Since approvals are still on the environments, I still don’t have a good way to separate infra approvals from code deployment approvals. I could create two environment objects in Actions that are conceptually the same environment, but that can get hard to manage if I have secrets (which are defined on the environment). Having two environments with the same settings means I have to duplicate all the settings.\n\nSecrets and Approvals on Environments\n\nI need to reference DEV for both the plan and approve jobs so that I can authenticate to the Azure subscription. Ideally, plan would run without requiring an approval and then I can review the output of the plan before I run apply. However, since I need to reference the environment in both jobs, I have to approve both jobs. Azure Pipelines suffers a similar limitation.\n\nI ended up with three environments: DEV, PROD-PLAN and PROD. I have defined the AZURE_CREDENTIALS secret for PROD at the repository level. That way, when any job refers to $ by default it will get the credentials for PROD. For jobs running in DEV, I explicitly reference the DEV environment using the environment attribute. That way I can get the credentials for DEV.\n\nWhy did I default to PROD and not to DEV? The only reason was to avoid having to approve the plan job for PROD. What happens now is that I have an approval on the environment prod but I don’t reference that for the prod-plan job. Instead I reference PROD-PLAN just so that I know this is a deployment job - but there is no approval on this environment. The value for $ is the PROD value, so the terraform plan goes ahead and performs a plan against PROD. The next job, prod-apply explicitly references PROD and so waits for the approval.\n\nFor DEV I don’t have approvals, so plan and apply, though both referencing DEV so that they get the DEV credentials, do not wait for approvals.\n\nOf course, if I had a UAT environment, I’d have to choose to split it or just live with having to approve plan and apply jobs.\n\nIn fact, I don’t really need the PROD-PLAN environment at all in this case - there are no secrets for this faux environment and no approvals. But if infra folks did require an approval before running a plan I can just update the environment.\n\nManual Triggers\n\nYou can create a manual trigger for a workflow - but there are some gotchas. Firstly, you have to specify a workflow_dispatch on event - even if it’s empty like so:\n\n\n    on:\n      workflow_dispatch: # manual trigger\n      push:\n        branches:\n        - main\n        - actions  \n\n\n\nThe empty workflow_dispatch: looks a bit odd, but that is the correct syntax.\n\nAnd the “Run Workflow” button won’t appear for that workflow , even if it has the workflow_dispatch trigger, unless that workflow is in the default (main or master or whatever you’ve set your default branch to) branch.\n\nDisappearing History - Sort of\n\nAnother issue with the UI is the tree of workflows to the left of the run history UI:\n\nTree listing all workflows in the main branch of the repo\n\nI initially had a workflow called End to End. When I split it into two files for App Deploy End to End and Infra Deploy End to End, the node in the tree for End to End disappeared. The runs are still in the list (the 210 results you can see in the screenshot) but I can’t filter by that workflow. I’m guessing that the tree on the left is built from current workflows in the .github folder in the main branch - so if you delete or rename a workflow, you’ll lose the node on the left.\n\nPesky Environment URL\n\nLet’s have a look at the url attribute of the environment in an app deployment workflow job:\n\n\n    prod_deploy:\n      needs: prod_deploy_canary\n      runs-on: ubuntu-latest\n      environment:\n        name: PROD\n        url: ${{ steps.slot_swap.outputs.url }}\n\n\n\nThis will set the URL of the job in the UI accordingly so that users can easily link to the environment - as long as it’s a valid URL. The URL has to start with http:// or https:// otherwise it won’t show anything. This foxed me for a while - since some az cli commands just output the URL without the protocol, like somesite.azurewebsites.net. Remember to prepend http:// or they won’t show.\n\nAnother quirk is that the URL updates for all jobs referencing the environment. In my scenario, I have two jobs for the PROD environment: a canary deployment that deploys to a staging slot and a “full” deployment that performs a slot swap. For the canary job, the URL is http://my-site-staging.azurewebsites.net while for the “full” deployment the URL is http://my-site.azurewebsites.net. But both are targeting the PROD environment. Here is what the workflow looks like after the canary job:\n\nWorkflow after the Canary job\n\nHowever, after the prod_deploy job, the URL is updated for PROD which both jobs reference, and the UI ends up dropping the canary URL:\n\nCanary URL is gone\n\nI’m not sure why Actions does this - perhaps both environments should show the final value? It’s strange that the value is wiped.\n\nFireside Chat with Chris Patterson\n\nChris Patterson (PM at Actions) kindly connected with me to walk through some of the challenges I was experiencing and questions I had. We also got to chat about some of the upcoming improvements that he and the team are planning. Rest assured that the GitHub team know that Actions Environments are in beta and have a lot of ideas for future features and improvements. Since Chris was PM of Azure Pipelines, he has a lot of experience with CI/CD tools and has a lot of experience of what worked well and what made life complicated. I don’t envy his job: make Actions simple to approach, but powerful. Make it deterministic, but dynamic - it’s a very fine balancing act.\n\nConclusion\n\nActions is still relatively young and already it is the most popular CI tool on GitHub. It has some maturing to do to compete in the CD space. For now, I’d still recommend using GitHub for source control and security scanning, and use Azure DevOps for CI/CD Pipelines and Boards.\n\nGitHub Actions still have a ways to go to become a mature CD tool. For now, you’re probably better off trying to craft your CD workflows as a single job per environment, splitting infrastructure and code deployments to reduce complexity in the overall workflow. Hopefully we’ll get templates too, as well as some way to better manage jobs that need environment values but are not actually deployment jobs (like terraform plan which needs values, but does not change the environment). And hopefully being able to have multiple jobs (like canary and full deploy) targeting the same environments will receive a better experience too. Lots of work for the engineering team to do still!\n\nFinally, I’ll remind the reader that Actions are in beta - so some rough edges are to be expected. But there is a seed of promise there that we can fully expect to blossom soon!\n\nHappy deploying!\n",
      "categories": [],
      "tags": ["actions"],
      
      "collection": "posts",
      "url": "/deployment-with-github-actions/"
    },{
      
      "title": "Custom CodeQL",
      "date": "2021-02-09 17:17:58 +0000",
      "description": "CodeQL is a powerful code scanning tool that can be integrated into your pipelines. In this post I show you some basics, as well as how to develop and integrate custom queries into your pipelines.\n",
      "content": "\n  CodeQL to the Rescue\n  CodeQL Scanning Example\n  Customizing Scans\n  Custom Queries    \n      Local Development\n      Writing CodeQL for Code Scanning\n      Example CodeQL Query\n    \n  \n  Custom Queries in Your Action    \n      Repo\n    \n  \n  Conclusion\n\n\nSecurity is a big deal. So big that the marketing folks have created the moniker “DevSecOps” to highlight a focus on security. I’ve never liked that term, since DevOps is supposed to include security by definition. However, integrating security into your culture and into your pipelines can be a challenge.\n\nTypically, security is left to the end of the delivery life cycle. Perhaps you’re fortunate enough to work in a team where you are at least somewhat security conscious - but you probably don’t have InfoSec involved in your daily routine. This challenge has given rise to the term “shift left” where teams work to intentionally embed security earlier in the life cycle.\n\nBut how do you do that? Security professionals often work with arcane tools that don’t integrate into pipelines and are difficult to automate. If the tooling doesn’t support the culture shift, then it can be doomed from the start.\n\nCodeQL to the Rescue\n\nEnter CodeQL. CodeQL (or Code Query Language) is a code scanning tool. It was called Semmle (pronounced “sem-il”) before being acquired by GitHub. GitHub now offers CodeQL as part of the GitHub Advanced Security Suite.\n\nCodeQL can be used for a variety of popular languages: C/C++, C#, JavaScript/TypeScript, Java, Python and Go. To integrate CodeQL into your workflow, you create an Action. The Action initializes the CodeQL scanner which intercepts compilation calls in order to build a database of your code. After compilation, you can run queries against the code database using CodeQL syntax.\n\nThe CodeQL syntax is very powerful - but, just like many other security tools, it too is arcane. However, the beauty of CodeQL is that you can tap into the community. There are some very smart security professionals in the community who have already written suites of queries that check for common security vulnerabilities and have open-sourced them! You don’t need to know how to write CodeQL to integrate it into your pipelines.\n\nCodeQL Scanning Example\n\nLet’s look at an example GitHub Action that performs a CodeQL scan on a Python repo:\n\n\n    jobs:\n      analyze:\n        name: Analyze\n        runs-on: ubuntu-latest\n    \n        strategy:\n          fail-fast: false\n          matrix:\n            language: ['python']\n    \n        steps:\n        - name: Checkout repository\n          uses: actions/checkout@v2\n    \n        # Initializes the CodeQL tools for scanning.\n        - name: Initialize CodeQL\n          uses: github/codeql-action/init@v1\n          with:\n            languages: ${{ matrix.language }}\n    \n        # - name: Autobuild\n        # uses: github/codeql-action/autobuild@v1\n    \n        - name: Perform CodeQL Analysis\n          uses: github/codeql-action/analyze@v1\n\n\n\nNotes:\n\n\n  Lines 6 - 9: We define an array (matrix) of languages - in this case, just python. fail-fast is set to false for multiple languages: if one fails, we want the others to run to completion rather than aborting all the jobs.\n  Lines 12/13: We checkout the repo - nothing special here\n  Lines 16 - 19: We initialize the CodeQL scanner - this sets up the interception calls so that CodeQL can build a database of our code as we compile. This step would cause the entire job to “fan out” to multiple jobs if we had more than 1 language specified.\n  Lines 21/22: For Python we don’t run a compilation, so I’ve commented out the autobuild step. If you’re using a compiled language like C++ or C#, the autobuild will attempt to build your code. If this fails, you can swap it out for your own set of steps to build your code.\n  Lines 24/25: This is where CodeQL will run a scan using a default set of queries\n\n\nThis already gets us a good way into integrating security into our pipelines. Without having to understand CodeQL or write our own custom queries, we can start scanning our code on whatever trigger makes sense. Typically, you want to scan on merges into your main branch. You may also want to add a scheduled scan so that the latest suites are run against your code even if it hasn’t changed.\n\nCustomizing Scans\n\nThere are a couple of levels of customizations for scanning. The easiest are customizing the suites and customizing the paths to include in your scans.\n\nTo do this, add a yml file to your repo - you can place it anywhere, but convention is to place this file in .github/codeql inside your repo. Let’s look at this simple config:\n\n\n    name: \"Custom CodeQL Config\"\n    \n    queries:\n    - uses: security-and-quality\n    \n    paths:\n    - src\n    \n    paths-ignore:\n    - src/node_modules\n    - '**/*.test.js'\n\n\n\nNotes:\n\n\n  Line 1: we specifying a name for this configuration\n  Line 4: we customize the scans to use the security-and-quality suite (the default is just security-extended which does not include code quality scans)\n  Lines 6 - 11: we use paths and paths-ignore to specify which folders should be included or ignored in the scans\n\n\nIgnoring paths is useful when you want to exclude test code or 3rd party libraries: though you’ll want to integrate Dependabotto ensure you’re scanning 3rd party libraries for vulnerabilities! Fortunately Dependabot is automatically enabled for public repos on GitHub.\n\nNext, we update the codeql-action/init task to tell it to use our custom config file:\n\n\n    - name: Initialize CodeQL\n      uses: github/codeql-action/init@v1\n      with:\n        languages: ${{ matrix.language }}\n        config-file: ./.github/codeql/codeql-config.yml\n\n\n\nNow when scans are performed, CodeQL will read in the config file and configure itself according to our settings.\n\nCustom Queries\n\nWe can, however, also create custom queries. As I mentioned previously, CodeQL is a strange and arcane language, so this is only recommended for advanced scenarios and users.\n\nThere are two considerations with creating custom queries: firstly, the development environment. You need to create and run the queries locally. Once you’ve created a query, you can integrate it into your pipeline with some more config customization.\n\nLocal Development\n\nTo develop queries locally, you should install VSCode. After that, install the CodeQL extension. If you don’t have any code to work on, you can install the starter workspace. However, I want to show you how you can get CodeQL working on your code.\n\nThe trick is to clone the CodeQL core libraries to your VSCode workspace. Clone your repo locally and open it in VSCode. Then clone the CodeQL repo to a location on your machine (not inside your repo). Finally, use File-&gt;Add Folder to Workspace to add the codeQL folder to your VSCode workspace. Your Workspace explorer should look something like this:\n\nWorkspace Explorer after importing CodeQL libraries\n\nNow you need to create a code database. In a console, cd to your repo directory and run the following command:\n\ncodeql database create codeqldb --language=python\n\nOf course you’ll have to update the --language setting to the appropriate language. This will create a code database inside a folder called codeqldb (you can customize that name too if needed). Don’t forget to add this folder to your .gitignore file! In your CI/CD workflows, CodeQL will create a new database from the latest code, so you don’t want this database in source control.\n\nNow you can open your code database in the CodeQL extension. Click on the CodeQL icon in the extension pane. In the DATABASES section, click Add from folder and browse to the folder above - in my case, codeqldb. It should import the database and show a check-mark:\n\nImporting the code database\n\nNow you can run queries that you write against the code database.\n\nWriting CodeQL for Code Scanning\n\nA full CodeQL tutorial is beyond the scope of this post, but you can follow these fun detective tutorials here. They’re good for introducing CodeQL concepts, but mapping these to actual code is a challenge.\n\nIf you just want to analyze your code, you can output whatever you want from a custom query. However, for custom queries to work in a pipeline, they must output specific values.\n\nThere are two kinds of query: problem and path-problem. problem is used for detecting issues in a specific location in the code, while path-problem is used to analyze flow between sources and sinks.\n\nFor problem queries, we must only output two values: a CodeQL object and a description string. You’ll also want to add meaningful metadata to describe what the query is doing - the metadata is used to mark up results in the repo itself. There is also the ability to write CodeQL help files that can guide developers on what the query is detecting and include examples of how to fix issues and links to CVEs and other useful information. Unfortunately, help files only render for standard queries, and not for custom queries.\n\nExample CodeQL Query\n\nI was working with a customer that wanted to ensure that files are deleted in a certain way. As a naïve check, we wanted to find all calls to shutil.rmtree() in the code base and surface them as warnings for review. Here’s the problem CodeQL query we wrote:\n\n\n    /**\n    * @id python/call-to-shutil-rmtree\n    * @name Use of shutil.rmtree\n    * @description We have specific ways to delete files - this query\n    * notifies when there are calls to `shutil.rmtree` so\n    * that we can revue how deletion is done.\n    * @kind problem\n    * @problem.severity warning\n    * @precision high\n    * @tags correctness\n    * rmtree\n    * \n    */\n    \n    import python\n    \n    from ControlFlowNode call, Value eval\n    where eval = Value::named(\"shutil.rmtree\") and\n          call = eval.getACall()\n    select call, \"Call to 'shutil.rmtree' detected.\"\n\n\n\nLet’s first examine the metadata:\n\n\n  Line 2 - 4: we specify an id, name and description for this query\n  Line 7: we specify that this is a problem query (as opposed to a path-problem)\n  Line 8: we specify the severity of this query - in this case we just wanted to surface this as a warning\n  Line 9: we specify the precision is high since we will not get many false positives for this query\n  Line 10/11: we add tags that can be used to filter/analyze results\n\n\nWe can now look at the query itself:\n\n\n  Line 15: we import the python core libraries that contain definitions about python programs and constructs\n  Line 17: we are looking for Value objects as well as ControlFlowNode nodes\n  Line 18/19: we are looking for any call that is targeting shutil.rmtree either directly or indirectly\n  Line 20: we select the call object and specify a string message for matches\n\n\nIn VSCode, we write this query in a .ql file. This file can reside anywhere, but conventionally we should put it into the .github/codeql/custom-queries/&lt;language&gt; folder (where language is one of the supported CodeQL languages).\n\nWe can test this against our code by right-clicking the file and selecting CodeQL: Run Query:\n\nRunning a query\n\nAssuming everything works, we should see results:\n\nLocal VSCode CodeQL results\n\nIn the results pane on the right, we can see the message text as well as a link (this is generates from the object we selected). Clicking on the link will navigate us to the location of that object in the code.\n\n\n  Note: This does not simply do a regex match - CodeQL understands syntax, so it will find references to shutil.rmtree even if you try to obfuscate it by creating a var for shutil and calling var.rmtree. It will also exclude functions called rmtree that are not defined in shutil.\n\n\nWhen this is run against a codebase that contains calls to shutil.rmtree, we’ll see alerts like this in the CodeQL scanning alerts section of the Security tab in the repo:\n\nResults of matches to CodeQL queries\n\nCustom Queries in Your Action\n\nNow that we have a query, we need to customize the CodeQL config to tell the engine to include our query when performing a scan.\n\nThe first thing we need to do is create a qlpack file - this tells the CodeQL engine where to find any dependencies. In our case, we have a dependency on the python core libs, so we create a qlpack file like so:\n\n\n    name: Custom Python Queries\n    version: 0.0.0\n    libraryPathDependencies:\n    - codeql-python\n\n\n\nThis specifies a name for the pack (which is any query in this folder, so we put the qlpack file in the .github/codeql/custom-queries/python folder). We also specify a version and list the libraryPathDependencies.\n\nNext, we update the codeql-config.yml file to look as follows:\n\n\n    name: \"Custom CodeQL Config\"\n    \n    disable-default-queries: true\n    \n    queries:\n    - uses: security-and-quality\n    - uses: ./.github/codeql/custom-queries/python\n\n\n\nWe disable the default queries (so that we don’t get duplicates) and specify that we want to run the security-and-quality suite as well as the custom queries in the .github/codeql/custom-queries/python folder.\n\nThat’s it! Now we’ll get alerts as shown above.\n\nRepo\n\nThe code I used can be found in this repo.\n\n\n  You cannot see the security alerts unless you are a repo owner, so if you want to follow along with this and see the results, you’ll have to fork the repo and then enable Actions (by default Actions are disabled on forks).\n\n\nConclusion\n\nCodeQL is a powerful tool that can be incorporated fairly easily into your daily workflow. By using the standard queries, you get a strong foundation for securing your code.\n\nIf you have security professionals who can author CodeQL queries, you can integrate those queries into your pipelines to customize the scanning.\n\nUnfortunately, the documentation for CodeQL, while extensive, proved to be a bit hard to apply to real-world examples. I also spent several hours trying to figure out why my qhelp file was not rendering - only to be informed by a GitHub engineer that custom query qhelp rendering is not currently supported - something the documentation does not mention anywhere.\n\nHowever, assuming you can get a grasp on CodeQL, it is easy to integrate into scanning Actions. CodeQL is a great tool for shifting security left, so use it!\n\nHappy securing!\n\n\n  Post image from ShutterStock\n\n",
      "categories": [],
      "tags": ["security","github"],
      
      "collection": "posts",
      "url": "/custom-codeql/"
    },{
      "image": "/assets/images/posts/actions.png",
      "title": "GitHub Composite Actions",
      "date": "2021-09-01 17:22:01 +0000",
      "description": "Composite Actions now allow you to run other Actions, not just script steps. This is great for composability and maintainability, but there are some limitations that you should be aware of.\n",
      "content": "\n  Sidetrail: Azure Pipeline Templates\n  Composite Actions\n  Case Study: eShopOnContainers\n  Conclusion\n\n\n\n  Resistance is futile.\nThe Borg. And GitHub.\n\n\n\n\n\n  Edit: Thanks to Tiago for pointing out that you can have more than one Action in a repo.\n\n\nGitHub Actions has rapidly become one of the most widely used CI/CD system on the planet. However, despite massive adoption, it is still fairly immature as a product, certainly when compared to Azure Pipelines. That will inevitably change as the Product Team iterates - but at the moment, there are some limitations that make Actions hard to use in many enterprise scenarios.\n\nThe biggest drawback to date has been the fact that there is very limited support for composability in Actions. That is, there are not templates that can be reused.\n\n\n  To those who are paying attention, Actions does have “templates” but these are more like starter pipelines - they are not composable or callable - they simply give a suggested starting point based on the language found within a repo.\n\n\nComposite Actions have been around for a while - however, they were limited to running scripts. While this may allow some reusability, not being able to run other Actions was a severe limitation. Fortunately, you can now run other Actions from within a Composite Action.\n\nWhile this is certainly a step in the right direction, there are still some limitations and gotchas that I want to explore in this post.\n\nSidetrail: Azure Pipeline Templates\n\nBefore we continue analyzing Composite Actions, I think it’s worthwhile considering templating in Azure Pipelines. Pipelines has a very strong templating system. There are several types of templates:\n\n\n  Variable templates - for templatizing common variables\n  Step templates - for templatizing common steps\n  Job and Stage templates - for templatizing entire jobs and stages\n\n\nThese templates are fully featured - that is, steps in a template have exactly the same operation and limitations as steps inline in a Pipeline. As we will see, this is not true for Composite Actions, which is why I mention it here.\n\nComposite Actions\n\nIf I compare Azure Pipeline templates and Composite Actions, I would liken Composite Actions to Step templates in Azure Pipelines: that is, they really serve to group and parameterize sets of steps.\n\nBelow we’ll look at an example. Before we do, let’s consider the limitations that Composite Actions have:\n\n\n  You cannot pass in complex objects (like arrays of steps)\n  You cannot use if conditions for steps Update 10/11/2021 if is now supported in Composite Actions\n  Composite Actions cannot read secrets - you have to pass secrets in as parameters\n  The Actions log does not show a separate log per step as you would see in a “normal” Action - all the steps of the Composite are executed as if they were a single step, making debugging Composite Action logs harder to analyze:\n\n\n\n\n  Note how the entire Composite Action only shows as a single step in the log, even though there are multiple steps in the Composite itself.\n\n\nEven though there are some limitations, they are certainly a vast improvement to Actions. Hopefully we’ll see more features evolve in this space to allow better composition and sharing of Actions.\n\nCase Study: eShopOnContainers\n\nA few months ago I got to do some work on the documentation for DevOps for ASP.NET Core Developers. The repo with example is on GitHub. The sample application runs as a set of microservices on Kubernetes. Let’s take a look at the Action to build the basketAPI:\n\n\n    name: basket-api\n    \n    on:\n      workflow_dispatch:\n      push:\n        branches:\n        - dev\n    \n        paths:\n        - src/BuildingBlocks/**\n        - src/Services/Basket/**\n        - .github/workflows/basket-api.yml\n      \n      pull_request:\n        branches:\n        - dev\n    \n        paths:\n        - src/BuildingBlocks/**\n        - src/Services/Basket/**\n        - .github/workflows/basket-api.yml\n    env:\n      SERVICE: basket-api\n      IMAGE: basket.api\n      DOTNET_VERSION: 5.0.x\n    \n    jobs:\n    \n      BuildContainersForPR_Linux:\n        runs-on: ubuntu-latest\n        if: ${{ github.event_name == 'pull_request' }}\n        steps:\n        - name: 'Checkout Github Action'\n          uses: actions/checkout@master\n        \n        - name: Setup dotnet\n          uses: actions/setup-dotnet@v1\n          with:\n            dotnet-version: ${{ env.DOTNET_VERSION }}\n    \n        - name: Build and run unit tests\n          run: |\n            cd src\n            dotnet restore \"eShopOnContainers-ServicesAndWebApps.sln\"\n            cd Services/Basket/Basket.API\n            dotnet build --no-restore\n            cd -\n            cd Services/Basket/Basket.UnitTests\n            dotnet build --no-restore\n            dotnet test --no-build -v=normal\n    \n        - name: Compose build ${{ env.SERVICE }}\n          run: sudo -E docker-compose build ${{ env.SERVICE }}\n          working-directory: ./src\n          shell: bash\n          env:\n            TAG: ${{ env.BRANCH }}\n            REGISTRY: ${{ secrets.REGISTRY_ENDPOINT }}\n    \n      BuildLinux:\n        runs-on: ubuntu-latest\n        if: ${{ github.event_name != 'pull_request' }}\n        steps:\n        - name: 'Checkout Github Action'\n          uses: actions/checkout@master\n    \n        - name: Setup dotnet\n          uses: actions/setup-dotnet@v1\n          with:\n            dotnet-version: ${{ env.DOTNET_VERSION }}\n    \n        - name: Build and run unit tests\n          run: |\n            cd src\n            dotnet restore \"eShopOnContainers-ServicesAndWebApps.sln\"\n            cd Services/Basket/Basket.API\n            dotnet build --no-restore\n            cd -\n            cd Services/Basket/Basket.UnitTests\n            dotnet build --no-restore\n            dotnet test --no-build -v=normal\n    \n        - name: Enable experimental features for the Docker daemon and CLI\n          run: |\n              echo $'{\\n \"experimental\": true\\n}' | sudo tee /etc/docker/daemon.json\n              mkdir -p ~/.docker\n              echo $'{\\n \"experimental\": \"enabled\"\\n}' | sudo tee ~/.docker/config.json\n              sudo service docker restart\n              docker version -f '{{.Client.Experimental}}'\n              docker version -f '{{.Server.Experimental}}'\n    \n        - name: Login to Container Registry\n          uses: docker/login-action@v1\n          with:\n            registry: ${{ secrets.REGISTRY_HOST }}\n            username: ${{ secrets.USERNAME }}\n            password: ${{ secrets.PASSWORD }}\n    \n        - name: Set branch name as env variable\n          run: |\n            currentbranch=$(echo ${GITHUB_REF##*/})\n            echo \"running on $currentbranch\"\n            echo \"BRANCH=$currentbranch\" &gt;&gt; $GITHUB_ENV\n          shell: bash\n    \n        - name: Compose build ${{ env.SERVICE }}\n          run: sudo -E docker-compose build ${{ env.SERVICE }}\n          working-directory: ./src\n          shell: bash\n          env:\n            TAG: ${{ env.BRANCH }}\n            REGISTRY: ${{ secrets.REGISTRY_ENDPOINT }}\n    \n        - name: Compose push ${{ env.SERVICE }}\n          run: sudo -E docker-compose push ${{ env.SERVICE }}\n          working-directory: ./src\n          shell: bash\n          env:\n            TAG: ${{ env.BRANCH }}\n            REGISTRY: ${{ secrets.REGISTRY_ENDPOINT }}\n    \n        - name: Create multiarch manifest\n          run: |\n            docker --config ~/.docker manifest create ${{ secrets.REGISTRY_ENDPOINT }}/${{ env.IMAGE }}:${{ env.BRANCH }} ${{ secrets.REGISTRY_ENDPOINT }}/${{ env.IMAGE }}:linux-${{ env.BRANCH }}\n            docker --config ~/.docker manifest push ${{ secrets.REGISTRY_ENDPOINT }}/${{ env.IMAGE }}:${{ env.BRANCH }}\n          shell: bash\n\n\n\nThis file is 126 lines long - which is not too bad for a single service. But there are 14 services! Before Composite Actions, you had no option but to copy/paste the code for each microservice. And that’s bad - since copy/paste inevitably leads to errors. And even if you don’t fat-finger it, what if you need to change something in the build process? Now you have to update 14 files. There are also deployment Actions for all the services - so now we have 28 files to maintain!\n\nWhen we analyze the common steps, there are 4 logical actions:\n\n\n  Build an image\n  Run tests, then build an image\n  Build and push an image\n  Deploy a helm chart\n\n\nThe steps for these logical actions are the same if we can parameterize them appropriately.\n\nHave a look at the BuildLinux job above - this is the job that executes steps to “Build and push an image”. Here is what this job looks like if we refactor the steps into a Composite Action (which we’ll see next):\n\n\n      BuildLinux:\n        runs-on: ubuntu-latest\n        if: ${{ github.event_name != 'pull_request' }}\n        steps:\n        - name: Checkout code\n          uses: actions/checkout@v2\n        - uses: ./.github/workflows/composite/build-push\n          with:\n            service: ${{ env.SERVICE }}\n            registry_host: ${{ secrets.REGISTRY_HOST }}\n            registry_endpoint: ${{ secrets.REGISTRY_ENDPOINT }}\n            image_name: ${{ env.IMAGE }}\n            registry_username: ${{ secrets.USERNAME }}\n            registry_password: ${{ secrets.PASSWORD }}\n\n\n\nThat’s much better! We’re checking out the repo using actions/checkout@v2 (we need to do this to get access to the code for the Composite Actions as well as the application code) and then we’re executing a Composite Action that is going to run all the steps we had inline previously.\n\n\n  Note how we reference an Action in the local repo, using the full path to the folder (not the yaml file).\n\n\nIf Composite Actions could read secrets, we’d save another 4 lines. However, since secrets are not readable within Composite Actions, we have to pass them in as parameters.\n\nLet’s have a look at the action.yml for this Composite Action:\n\n\n    name: \"Build and push image\"\n    description: \"Builds and pushes an image to a registry\"\n    \n    inputs:\n      service:\n        description: \"Service to build\"\n        required: true\n      registry_host:\n        description: \"Image registry host e.g. myacr.azureacr.io\"\n        required: true\n      registry_endpoint:\n        description: \"Image registry repo e.g. myacr.azureacr.io/eshop\"\n        required: true\n      image_name:\n        description: \"Name of image\"\n        required: true\n      registry_username:\n        description: \"Registry username\"\n        required: true\n      registry_password:\n        description: \"Registry password\"\n        required: true\n      \n    runs:\n      using: \"composite\"\n      steps:\n      - name: Enable experimental features for the Docker daemon and CLI\n        shell: bash\n        run: |\n            echo $'{\\n \"experimental\": true\\n}' | sudo tee /etc/docker/daemon.json\n            mkdir -p ~/.docker\n            echo $'{\\n \"experimental\": \"enabled\"\\n}' | sudo tee ~/.docker/config.json\n            sudo service docker restart\n            docker version -f '{{.Client.Experimental}}'\n            docker version -f '{{.Server.Experimental}}'\n    \n      - name: Login to Container Registry\n        uses: docker/login-action@v1\n        with:\n          registry: ${{ inputs.registry_host }}\n          username: ${{ inputs.registry_username }}\n          password: ${{ inputs.registry_password }}\n    \n      - name: Set branch name as env variable\n        run: |\n          currentbranch=$(echo ${GITHUB_REF##*/})\n          echo \"running on $currentbranch\"\n          echo \"BRANCH=$currentbranch\" &gt;&gt; $GITHUB_ENV\n        shell: bash\n    \n      - name: Compose build ${{ inputs.service }}\n        shell: bash\n        run: sudo -E docker-compose build ${{ inputs.service }}\n        working-directory: ./src\n        env:\n          TAG: ${{ env.BRANCH }}\n          REGISTRY: ${{ inputs.registry_endpoint }}\n    \n      - name: Compose push ${{ inputs.service }}\n        shell: bash\n        run: sudo -E docker-compose push ${{ inputs.service }}\n        working-directory: ./src\n        env:\n          TAG: ${{ env.BRANCH }}\n          REGISTRY: ${{ inputs.registry_endpoint }}\n    \n      - name: Create multiarch manifest\n        shell: bash\n        run: |\n          docker --config ~/.docker manifest create ${{ inputs.registry_endpoint }}/${{ inputs.image_name }}:${{ env.BRANCH }} ${{ inputs.registry_endpoint }}/${{ inputs.image_name }}:linux-${{ env.BRANCH }}\n          docker --config ~/.docker manifest push ${{ inputs.registry_endpoint }}/${{ inputs.image_name }}:${{ env.BRANCH }}\n\n\n\nNothing too complex here - we have name and description attributes, followed by a map of input parameters. Each parameter has a description and required attribute, and can optionally have a default attribute too. Then we have a runs keyword with the using set to composite. Thereafter, we have steps as we would in any other Action - including the ability to use other Actions (not only run scripts)!\n\n\n  Note that in Composite Actions, each run step requires that you explicitly define the shell.\n\n\nConclusion\n\nComposite Actions are a welcome addition to the Actions ecosystem, despite their limitations. I highly recommend that you start using them to reduce copy/paste and keep you and your Actions DRY!\n\nHappy building!\n",
      "categories": [],
      "tags": ["actions","build"],
      
      "collection": "posts",
      "url": "/github-composite-actions/"
    },{
      "image": "/assets/images/posts/azdo.png",
      "title": "Create Azure DevOps Work Item Action",
      "date": "2021-09-01 17:22:01 +0000",
      "description": "If you’re managing backlogs in Azure Boards but using GitHub Actions for CI/CD, you may have scenarios where you want to create Work Items from an Action.\n",
      "content": "\n  Azure DevOps Work Item Creation Action    \n      Setting up a Skeleton\n      Coding the Action\n      Authentication\n      Creating a Work Item\n      Tying It All Together\n      Publishing the Action\n    \n  \n  Creating a PR Boards Work Item\n  Conclusion\n\n\nAzure Boards is a mature product for managing backlogs. Many teams are using Boards even while hosting code in GitHub, and using GitHub Actions for CI/CD.\n\nThere may be scenarios where you want to create a work item on Azure Boards from within an Action:\n\n\n  when a Pull Request is created\n  when particular tests fail\n  when a deployment is commencing or completing\n\n\nBefore we look at one of these scenarios, let’s have a look at how I created an Action that can create an Azure DevOps Work Item.\n\nAzure DevOps Work Item Creation Action\n\n\n  tldr; just go to the Action in the markeplace to start using it in your workflows!\n\n\nSetting up a Skeleton\n\nI really prefer developing Actions with TypeScript, so I found the TypeScript Action template repo and created a repo from the template. This set up a skeleton Action that I could work from. I then opened the Action in a CodeSpace and I was developing! I switched from npm to yarn but that’s not something you have to do.\n\nCoding the Action\n\nNext I added the azure-devops-node-api so that I could easily interact with the Azure DevOps REST API. In the main.ts file, I use core.getInput() to parse the inputs (which I specify in the action.yml file). I then just call the createWorkItem() method in a separate work-item-functions.ts file.\n\nAuthentication\n\nThe client library makes authenticating fairly easy, assuming you have an org name and a Personal Access Token (PAT). Here’s the method I created to authenticate and get a WorkItemTrackingClient, which has methods for interacting with work items:\n\n// file: 'work-item-functions.ts'\nasync function getWiClient(\n  token: string,\n  orgName: string\n): Promise&lt;IWorkItemTrackingApi&gt; {\n  const orgUrl = `https://dev.azure.com/${orgName}`;\n  core.debug(`Connecting to ${orgUrl}`);\n  const authHandler = azdev.getPersonalAccessTokenHandler(token);\n  const connection = new azdev.WebApi(orgUrl, authHandler);\n\n  core.info(`Connected successfully to ${orgUrl}!`);\n  return connection.getWorkItemTrackingApi();\n}\n\n\nAuthenticating with Azure DevOps\n\nNotes:\n\n  We first instantiate an AuthenticationHandler, in this case, a PAT handler, passing in the token\n  Then we instanticate the connection, passing in the org URL and the handler\n  Finally, we get and return a WorkItemTrackingClient\n\n\nCreating a Work Item\n\nOnce we have a WorkItemTrackingClient, we can manipulate work items in Azure DevOps. Let’s look at the method that creates a work item:\n\n\n// file: 'work-item-functions.ts'\nexport async function createWorkItem(\n  token: string,\n  orgName: string,\n  project: string,\n  workItemInfo: IWorkItemInfo\n): Promise&lt;number&gt; {\n  core.debug('Try get work item client');\n  const wiClient = await getWiClient(token, orgName);\n  core.debug('Got work item client');\n\n  const patchDoc = [\n    {op: 'add', path: '/fields/System.Title', value: workItemInfo.title},\n    {\n      op: 'add',\n      path: '/fields/System.Description',\n      value: workItemInfo.description\n    }\n  ] as JsonPatchDocument;\n\n  if (workItemInfo.areaPath !== '') {\n    (patchDoc as any[]).push({\n      op: 'add',\n      path: '/fields/System.AreaPath',\n      value: workItemInfo.areaPath\n    });\n  }\n  if (workItemInfo.iterationPath !== '') {\n    (patchDoc as any[]).push({\n      op: 'add',\n      path: '/fields/System.IterationPath',\n      value: workItemInfo.iterationPath\n    });\n  }\n\n  core.debug('Calling create work item...');\n  const workItem = await wiClient.createWorkItem(\n    null,\n    patchDoc,\n    project,\n    workItemInfo.type\n  );\n\n  if (workItem?.id === undefined) {\n    throw new Error('Work item was not created');\n  }\n\n  return workItem.id;\n}\n\n\n\nCreating an Azure DevOps Work Item\n\nNotes:\n\n  First we get the WorkItemTrackingClient from the method we created earlier\n  Next, we construct a JsonPatchDocument which contains operations - in this case, all adds. This is how we create the field values for the new work item.\n  Finally, we call createWorkItem on the WorkItemTrackingClient to create the work item. We then return the id of the new work item.\n\n\nTying It All Together\nWhen invoked, the Action will execute the main.ts file, in which we just extract the input args and invoke the methods we created earlier:\n\n\n// file: 'main.ts'\nimport * as core from '@actions/core';\nimport {createWorkItem} from './work-item-functions';\n\nasync function run(): Promise&lt;void&gt; {\n  try {\n    const token: string = core.getInput('token');\n    const orgName: string = core.getInput('orgName');\n    const project: string = core.getInput('project');\n    const type: string = core.getInput('type');\n    const title: string = core.getInput('title');\n    const description: string = core.getInput('description');\n    const areaPath: string = core.getInput('areaPath');\n    const iterationPath: string = core.getInput('iterationPath');\n\n    core.debug(`orgName: ${orgName}`);\n    core.debug(`project: ${project}`);\n    core.debug(`type: ${type}`);\n    core.debug(`title: ${title}`);\n    core.debug(`description: ${description}`);\n    core.debug(`areaPath: ${areaPath}`);\n    core.debug(`iterationPath: ${iterationPath}`);\n\n    core.debug('Creating new work item...');\n    const newId = await createWorkItem(token, orgName, project, {\n      type,\n      title,\n      description,\n      areaPath,\n      iterationPath\n    });\n    core.info(`Created work item [${title}] with id ${newId}`);\n\n    core.setOutput('workItemId', newId);\n  } catch (error) {\n    core.setFailed((error as any)?.message);\n  }\n}\n\nrun();\n\n\n\nThe main code for the Action.\n\nPublishing the Action\nI said that the Action invokes main.ts - this isn’t entirely correct. If you look at the action.yml file, you’ll see the following metadata:\n\n\n# file: 'action.yml'\nruns:\n  using: 'node12'\n  main: 'dist/index.js'\n\n\n\nMetadata snippet for specifying Action exection.\n\nYou’ll see that the main property is set to dist/index.js. This file is generated by packaging the Action. This is set up for you already if you create the Action from the template repo like I did. Looking in pacakge.json in the scripts section, we see the following:\n\n\"scripts\": {\n  \"build\": \"tsc\",\n  \"format\": \"prettier --write **/*.ts\",\n  \"format-check\": \"prettier --check **/*.ts\",\n  \"lint\": \"eslint src/**/*.ts\",\n  \"package\": \"ncc build --source-map --license licenses.txt\",\n  \"all\": \"yarn run build &amp;&amp; yarn run format &amp;&amp; yarn run lint &amp;&amp; yarn run package\"\n},\n\n\nScripts specified in the package.json file\n\nNote the all script: it runs the build script for transpiling, then runs format and lint for formatting and linting and finally runs the package command. The package command invokes ncc to package the Action and make it ready for publication. After these commands have run, the TypeScript has been transpiled, formatted and linted into the dist folder.\n\n\n  Note: For now, I’m running this manually before committing, but ideally I should have an Action that will run this command on push and commit the generated js files in the dist folder automatically - that way I’ll never be out of sync!\n\n\nCreating a PR Boards Work Item\n\nLet’s imagine that you want to create a work item whenever a PR is created. This is actually quite easy now that we have the az-create-work-item Action:\n\n\n# file: 'pr.yml'\non:\n  pull_request:\n    types: [opened]\n\njobs:\n  create-work-item:\n    runs-on: ubuntu-latest\n\n    steps:\n    - run: |\n        prNum=$(jq --raw-output .pull_request.number \"$GITHUB_EVENT_PATH\")\n        echo \"::set-env name=prNum::$prNum\"\n      displayName: Exract PR number\n    - uses: colindembovsky/az-create-work-item@v1.0.0\n      with:\n        token: ${{ secrets.AZDO_TOKEN }}\n        orgName: myorg\n        project: myproject\n        type: User Story\n        title: PR ${{ env.prNum }} in repository ${{ github.repository }}\n        description: '&lt;div&gt;A Pull Request was created &lt;a href=\"https://github.com/${{ env.repository }}/pulls/${{ env.prNum }}\"&gt;here&lt;/a&gt;.&lt;/div&gt;\n\n\n\n\nAn example of creating a Work Item in AzDO when a PR is created.\n\nNotes:\n\n  You can specify “sub-events” for the pull_request trigger. In this case, we filter down to the opened type.\n  We execute two steps: the first extracts the PR number from the GITHUB_EVENT_PATH metadata.\n  The second step invokes the az-create-work-item Action to create the work item.\n  For this to work, we need to provide an Azure DevOps PAT and we store it as a repository secret called AZDO_TOKEN.\n\n\nConclusion\nCreating Azure DevOps work items by using the az-create-work-item Action is fairly simple. Many teams are using both GitHub and Azure DevOps, and commonly this involves planning and tracking in Azure Boards. This Action allows teams to easily create work items in Azure Boards from Actions.\n\nHappy building!\n",
      "categories": [],
      "tags": ["actions","devops"],
      
      "collection": "posts",
      "url": "/azdo-create-work-item-action/"
    },{
      "image": "/assets/images/2021/10/reuse.jpeg",
      "title": "Musings on GitHub Actions Reusable Workflows",
      "date": "2021-10-05 01:22:01 +0000",
      "description": "Newly released Reusable Workflows allows you to reuse workflows in your GitHub workflows. While this still has some limitations, it’s still better than copy/paste!\n",
      "content": "\n  Motivation for Reusable Workflows\n  Limitations\n  Making a Workflow Reusable    \n      Add the Call Trigger\n      Enable Actions Access on the Repo\n    \n  \n  Creating a Reusable DotNet Build, Test and Publish Workflow    \n      Parameterize the Platform\n      Parameterize the Source Directory\n      Parameterize Running Tests\n      Publish the App\n      Putting it all together\n      Invoking the Workflow\n    \n  \n  Array Hack\n  Secrets\n  Conclusion\n\n\nReusable workflows have just been released to beta. Some level of reuse has been possible previously using Composite Actions.\n\nI wrote about Composite Actions and some of their limitations here. In that post I also compared Composite Actions to Azure Pipeline step templates. Reusable workflows are akin to job templates in Azure Pipelines.\n\nMotivation for Reusable Workflows\n\nReusable workflows allow you to centralize a set of common jobs. For example, you may have a common job for “Build a .NET application”. You may have a common job for “Deploy a Web App to Azure Web Apps”. Instead of each repo having the same set of steps for these common jobs, you can now create a centralized repo (or set of repos) that can be reused.\n\nThis keeps your workflows DRY (Don’t Repeat Yourself) and allows you to change a workflow in a single place. Since you can pin to a tag or branch when referencing a reusable workflow, you can ensure that you maintain backward compatibility.\n\nMany organizations have a “DevOps Team”. I really don’t like this terminology, since I think DevOps should be everyone’s resposibility, not just fobbed off to some other team. However, in practice, many organizations have teams that write application code and have other teams that are focused on automating building, testing, securing and deploying that code. Reusable workflows is great for these teams.\n\nFor teams that are building their own automation, they can still benefit from a centralized location for reusable workflows. In this case, anyone could contribute to the workflows, where organizations with “DevOps teams” may want only the DevOps teams to be able to contribute to the workflows. In the end, this becomes a process question rather than a tooling question - you can set permissions (or lack of permissions) on the centralized repos however you like.\n\nLimitations\n\nThere are still some limitations to reusable wofklows:\n\n\n  Reusable workflows only run on hosted runners, not yet on private runners. Update: Reusable workflows are supported on self-hosted runners now (10/14/2021)!\n  Reusable workflows cannot call other reusable workflows.\n  You cannot access job outputs from reusable workflows. Update: You can now declare outputs to reusable workflows. These work just like job outputs and are available via needs.&lt;reusable&gt;.outputs.&lt;output&gt; format once you declare the output.\n  env variables set in the calling workflow are not accessible to the called workflow.\n  The only parameter types that are supported are string, number and boolean. Arrays are not supported.\n  You cannot pass in steps to inject (like you can with Azure Pipelines templates).\n  Repos that contain reusable workflows must be either internal or public.\n  You cannot enforce use of a reusable workflow. In other words, there is no equivalent for extends templates.\n\n\nThese limitations mean that reusable workflows are nowhere near as powerful as Azure Pipeline templates, but it’s a step in the right direction for Actions. At the very least, reusable workflows minimized copy/paste for simple scenarios.\n\nMaking a Workflow Reusable\n\nAdd the Call Trigger\nMaking a workflow reusable is not too hard - you just add a workflow_call trigger, with some optional inputs and secrets. The workflow_call trigger allows your workflow to be called by other workflows, and obviously you can pass values for the inputs and secrets using the with keyword, just like any other Action.\n\nEnable Actions Access on the Repo\nThe workflow_call trigger makes the workflow callable, but you still need to allow access. In the Settings-&gt;Actions tab of your repo, scroll to the bottom and enable the desired level of access:\n\n\n\nSetting Actions Access level on a repo.\n\nCreating a Reusable DotNet Build, Test and Publish Workflow\n\nImagine you have a standard way of building and testing dotnet applications. Before reusable workflows, you would have had to copy/paste a workflow to every dotnet repo. Now you can set up a single workflow that can be reused. Let’s take a look at an example:\n\n\n# file: 'simple-build.yml'\nname: Build dotnet application\n\non:\n  workflow_call:\n    inputs:\n      dotnet-version:\n        description: Version of dotnet to use\n        type: string\n        default: 5.0.x\n\njobs:\n  build:\n    name: Build dotnet app\n    \n    runs-on: ubuntu-latest\n    \n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup .NET\n        uses: actions/setup-dotnet@v1\n        with:\n          dotnet-version: ${{ inputs.dotnet-version }}\n      - name: Restore dependencies\n        run: dotnet restore\n      - name: Build\n        run: dotnet build --no-restore\n      - name: Test\n        run: dotnet test --no-build --verbosity normal\n\n\n\nA simple workflow to build a dotnet application.\n\nNotes:\n\n  We specify the workflow_call trigger in the on section to indicate that this is a reusable workflow.\n  We include an input called  dotnet-version with some metadata as well as a default value of 5.0.x\n  The steps are really easy: checkout the code, setup the specified version of dotnet and then run restore, build and test\n\n\nSo far so good. Let’s enhance this workflow to add in some more functionality.\n\nParameterize the Platform\n\nLet’s parameterized the runs-on so that we can pass in the hosted or private pool we want to run the workflow on:\n\n\non:\n  workflow_call:\n    inputs:\n      ...\n      runs-on:\n        description: Platform to execute on\n        type: string\n        default: ubuntu-latest\n\njobs:\n  build:\n    name: Build dotnet app\n    \n    runs-on: ${{ inputs.runs-on }}\n    ...\n\n\n\nAdding a runs-on input.\n\nParameterize the Source Directory\n\nWhat if the project file isn’t in the root directory of the repo? Many teams put the app code into a folder called src or have a mono-repo with several apps in different folders. Not a problem - we can add a project-folder input and set that as the default working directory:\n\n\non:\n  workflow_call:\n    inputs:\n      ...\n      project-folder:\n        description: The folder containing the project to build\n        type: string\n        default: .\n\njobs:\n  build:\n    name: Build dotnet app\n    \n    runs-on: ${{ inputs.runs-on }}\n    defaults:\n      run:\n        working-directory: ${{ inputs.project-folder }}\n    ...\n\n\n\nAdding a project-folder input.\n\nParameterize Running Tests\n\nWhat if there are no tests to run or we don’t want to run them because they take too long? We can add a boolean parameter and skip the test step if this value is false:\n\n\non:\n  workflow_call:\n    inputs:\n      ...\n      run-tests:\n        description: Run tests\n        type: boolean\n        default: true\n\njobs:\n  build:\n    name: Build dotnet app\n    \n    ...\n    steps:\n    ...\n    - name: Test\n      if: ${{ inputs.test }}\n      run: dotnet test --no-build --verbosity normal\n\n\n\nAdding a run-tests boolean input.\n\nPublish the App\n\nWhat if we want to publish the compiled app, and specify the configuration and name of the published artifact? No problem.\n\n\non:\n  workflow_call:\n    inputs:\n      ...\n      publish-configuration:\n        description: Configuration to publish\n        type: string\n        default: Release\n        \n      artifact-name:\n        description: Name of the artifact to upload\n        type: string\n        default: drop\n\njobs:\n  build:\n    name: Build dotnet app\n    \n    ...\n    steps:\n    ...\n    - name: Publish\n      run: dotnet publish -c ${{ inputs.publish-configuration }} -o wdrop\n    - name: Upload a Build Artifact\n      uses: actions/upload-artifact@v2.2.2\n      with:\n        name: ${{ inputs.artifact-name }}\n        path: ${{ inputs.project-folder }}/wdrop/**\n        if-no-files-found: error\n\n\n\nAdding steps and inputs for publishing.\n\nPutting it all together\n\nThe final workflow looks like this:\n\n\n# file: 'completed-dotnet-build.yml'\nname: Build dotnet application\n\non:\n  workflow_call:\n    inputs:\n      runs-on:\n        description: Platform to execute on\n        type: string\n        default: ubuntu-latest\n        \n      dotnet-version:\n        description: Version of dotnet to use\n        type: string\n        default: 5.0.x\n      \n      project-folder:\n        description: The folder containing the project to build\n        type: string\n        default: .\n        \n      run-tests:\n        description: Run tests\n        type: boolean\n        default: true\n      \n      publish-configuration:\n        description: Configuration to publish\n        type: string\n        default: Release\n        \n      artifact-name:\n        description: Name of the artifact to upload\n        type: string\n        default: drop\n    \njobs:\n  build:\n    name: Build dotnet app\n    \n    runs-on: ${{ inputs.runs-on }}\n    defaults:\n      run:\n        working-directory: ${{ inputs.project-folder }}\n    \n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup .NET\n        uses: actions/setup-dotnet@v1\n        with:\n          dotnet-version: ${{ inputs.dotnet-version }}\n      - name: Restore dependencies\n        run: dotnet restore\n      - name: Build\n        run: dotnet build --no-restore\n      - name: Test\n        if: ${{ inputs.test }}\n        run: dotnet test --no-build --verbosity normal\n      - name: Publish\n        run: dotnet publish -c ${{ inputs.publish-configuration }} -o wdrop\n      - name: Upload a Build Artifact\n        uses: actions/upload-artifact@v2.2.2\n        with:\n          name: ${{ inputs.artifact-name }}\n          path: ${{ inputs.project-folder }}/wdrop/**\n          if-no-files-found: error\n\n\n\nThe final workflow.\n\nInvoking the Workflow\nNow that we have a reusable workflow, how do we invoke it? It’s pretty easy - almost like invoking an Action. In the code repo, we can add a new workflow that just invokes the centralized workflow we created:\n\n\n# file: 'build.yml'\nname: Build application\n\non:\n  push:\n    branches:\n    - main\n\njobs:\n  build-ubuntu:\n    name: Build app on Ubuntu\n    uses: octodemo/colind-reusable-workflows/.github/workflows/dotnetbuild.yml@main\n    with:\n      runs-on: ubuntu-latest\n      run-tests: false\n      project-folder: src\n      artifact-name: drop-ubuntu\n\n\n\nInvoking the reusable workflow.\n\nWe declare a job (build-ubuntu) and then use uses to specify the job template. We use the format owner/repo/path@label to specify the exact location and version of the workflow. Then we use the with keyword to specify values for the inputs.\n\nArray Hack\nI love being able to pass stepLists (or jobLists) in Azure Pipelines (see this doc). This allows you to create generic templates that allow some customization of pre- or post-actions. You could have a template that build an app, allowing some post-build steps to copy files to a location or something along those lines. Unfortunately there is no equivalent in Actions. Even passing arrays of primitives requires a hack.\n\nLet’s say you wanted to build the dotnet app on different platforms using a matrix, and you wanted to pass in a list of the platforms. There is no way to pass a list of values into a reusable workflow. But we can use fromJSON and pass in a JSON string:\n\n\n# file: 'build-template.yml'\nnname: Build dotnet application\n\non:\n  workflow_call:\n    inputs:\n      runs-on:\n        description: Platforms to execute on, in format of a string JSON array\n        type: string\n        default: '[\"ubuntu-latest\"]'\n      ...\n\njobs:\n  build:\n    name: Build dotnet app\n    strategy:\n      matrix:\n        runs-on: ${{ fromJSON(inputs.runs-on) }}\n    \n    runs-on: ${{ matrix.runs-on }}\n    ...\n\n\n\nCreating a JSON string input and parsing it for a matrix.\n\nThen we pass in the JSON string array when we invoke the workflow:\n\n\n# file: 'build-app.yml'\nname: Build Matrix\n\non:\n  workflow_dispatch:\n    \njobs:\n  build-ubuntu:\n    name: Build app on matrix\n    uses: octodemo/colind-reusable-workflows/.github/workflows/build-matrix.yml@main\n    with:\n      runs-on: '[\"ubuntu-latest\", \"windows-latest\"]'\n      run-tests: false\n      project-folder: src\n\n\n\nInvoking a workflow with a JSON array parameter.\n\nYou can see the value that we pass to runs-on: it’s a JSON array that has been stringified.\n\nThis is definitely hacky, but it’s probably the only way to pass a list to a reusable workflow. And it won’t work for injecting steps.\n\nSecrets\n\nIf you want to pass secrets to a reusable workflow, you should use the secrets keyword. These are really the same as inputs, but instead of being plaintext, they are treated as secrets. Here’s an example:\n\n\n# file: 'reusable.yml'\nname: Deploy\n\non:\n  workflow_call:\n    inputs:\n      username:\n        required: true\n        type: string\n    secrets:\n      token:\n        required: true\n    \njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: ./.github/actions/some-authenticated-action@v1\n      with:\n        username: ${{ inputs.username }}\n        token: ${{ secrets.token }}      \n\n\n\nDefining a secret input to a reusable workflow.\n\n\n# file: 'caller.yml'\nname: Deploy\n\non:\n  push:\n    \njobs:\n  deploy:\n    - uses: my-org/my-workflow-repo./.github/workflows/reusable.yml@v1\n      with:\n        username: ${{ github.actor }}\n      secrets:\n        token: ${{ github.token }}      \n\n\n\nInvoking a reusable workflow with a secret.\n\nYou can see how the secret token is passed using secrets under the uses keyword.\n\nConclusion\n\nReusable Workflows are a great evolution for GitHub Actions. They can drastically reduce redundancy in your workflows and start paving the way for some centralized templates that can be used to standardize jobs in an org. While they have limitations, they are still powerful tools to add to your toolbelt.\n\nHappy building!\n",
      "categories": [],
      "tags": ["actions","build"],
      
      "collection": "posts",
      "url": "/musings-on-reusable-workflows/"
    },{
      "image": "/assets/images/2021/10/self-hosted-runners/boom.jpeg",
      "title": "On Demand Ephemeral Self-Hosted Runners",
      "date": "2021-10-26 01:22:01 +0000",
      "description": "Do you need to deploy to private VNets using GitHub Actions, but don’t want to have to keep self-hosted runners running all the time? In this post I show you how you can use Ephemeral Runners to create on-demand self-hosted runners.\n",
      "content": "\n  Self-Hosted Runners\n  Using WebHooks\n  On-Demand Self-Hosted Runners    \n      Prerequisites\n      Reusable Workflows        \n          Deploy Ephemeral Runner\n          Delete ACI\n        \n      \n      The Main Workflow        \n          Secrets\n        \n      \n      Analyzing the Runs\n    \n  \n  Conclusion\n\n\nSelf-Hosted Runners\n\nGitHub hosted runners are great if you need to run workflows that don’t require special tools or access to protected resources. If you have specific tool requirements, you can install tools as steps within a workflow. However, if custom tool installs take too long, or you have other build/deploy requirements (like license files for 3rd party libraries), you may want to create custom images (containers or VMs) that you can run self-hosted runners on. Obviously you’ll need self-hosted runners to deploy to private networks.\n\nBut self-hosted runners mean overhead - you have to maintain them as well as pay for the compute. One technique you could use is to autoscale your self-hosted runners. However, this requires that you have AKS or use the Terraform/AWS method. What if you want something a little more light-weight?\n\nUsing WebHooks\n\nWhen a workflow is queued, GitHub publishes an event workflow_job (docs here). Initially when thinking about this scenario, I explored creating a workflow that would trigger off this event and spin up a self-hosted runner.\n\nUnfortunately, most, but not all GitHub events trigger workflows. You can create a webhook to listen for workflow_job and then call a REST API - but then you’d have to host something capable of processing the POST from the webhook, which could then turn around and POST to the repository_webhook event of a workflow. Sounds like more overhead to manage!\n\nInstead of doing this, I realized that I could create a reusable workflow to spin up (and another to tear down) a self-hosted runner. In this manner, I’d be spinning up a self-hosted runner on demand and tearing it down afterwards to save overhead and compute.\n\nOn-Demand Self-Hosted Runners\n\nIn this post, I’ll walk you through this scenario. The idea is as follows:\n\n\n  When a workflow runs, execute a job on a hosted runner that spins up a self-hosted runner using Azure Container Instances (ACI) connected to a specific VNet, registering the runner as ephemeral\n  Execute the “real” work, targeting the self-hosted runner that was just created\n  When the job completes, the runner unregisters from the repo (since it was created as an ephemeral runner)\n  Execute a clean-up job that deletes the ACI\n\n\nIn this manner, you get the experience of a hosted runner, but the “real” work is performed on a self-hosted runner that is spun up on-demand in your private VNet. You sacrifice a little bit of time (it takes some time to spin up the ACI) but in return you decrease overhead, since the ACI only lives just long enough to execute the work - there’s nothing to manage after the job completes.\n\n\n  Note: You also have to maintain the container image, but if you’re not frequently messing with the tools, this is usually not a lot of overhead.\n\n\n\n  Note: The GitHub PAT required by the self-hosted runner is VISIBLE on the config of the ACI in the Azure Portal or az cli. Ensure that you set up appropriate RBAC to prevent leaking this credential!\n\n\nCode for this post can be found in this repo.\n\nPrerequisites\n\nIn order for this to work, you’ll need a container image that, when started, will register an ephemeral runner. That image should then be customized to install any further custom build or deploy tools that you may need. You can see this example repo that builds a container image that registers a self-hosted runner when the container is started. That repo includes a workflow that will build the Dockerfile and publish the image to GitHub Container Registry (GHCR).\n\nSecondly, you need to create a resource group for housing the ACI instances and to create an SPN that has permissions to create resources in the resource group.\n\nFinally, you need to get the ID of the subnet that you want to connect your runner to. If you’re deploying to private VNets, this is an essential step.\n\n\n  Note: You could customize the code so that the subnet is not required - this will just spin an ACI that is not connected to any specific VNet. You’d then have a self-hosted runner running on your custom image, but it won’t be connected to any private VNets.\n\n\nReusable Workflows\n\nThere are two reusable workflows in this scenario: one to spin up an ACI that connects a self-hosted runner to the invoking workflow’s repository. The second deletes the ACI. We don’t have to worry about unregistering the runner, since we’ll spin the runner using the --ephemeral switch, which automatically unregisters the runner after it has completed a single job.\n\nBoth workflows execute using hosted runners. The hosted runners simply spin up (and tear down) the ephemeral self-hosted runner: but it is the self-hosted runner that is doing the “real” work.\n\nDeploy Ephemeral Runner\n\nThe code for the deployment workflow is as follows:\n\n\n# file: 'deploy-ephemeral-runner.yml'\nname: Deploy Ephemeral Runner\n\non:\n  workflow_call:\n    inputs:\n      rg_name:\n        type: string\n        description: Name of RG to deploy to\n        default: cd-ephemeral\n      location:\n        type: string\n        description: Location for ACI\n        default: southcentralus\n      subnet_id:\n        type: string\n        description: Subnet to create ACI on\n        default: /subscriptions/f12d732d-4a47-4edc-a11b-d6dc6909ddbe/resourceGroups/cd-ephemeral/providers/Microsoft.Network/virtualNetworks/private-vnet/subnets/default\n      aci_prefix:\n        type: string\n        description: Prefix for ACI name\n        default: cd-shrunner-aci\n      runner_image:\n        type: string\n        description: Image of runner container\n        default: ghcr.io/colindembovsky/ubuntu-actions-runner:6d7a59dfa95ec094a5fa8292bad01158c374e3ad\n      labels:\n        type: string\n        description: Comma-separated list of labels to apply to the runner\n        required: true\n        \n    secrets:\n      azure_creds:\n        description: Azure credentials\n        required: true\n      repo_pat:\n        description: PAT with `repo` permissions\n        required: true\n\njobs:\n  deploy_runner:\n    name: Deploy Ephemeral Runner\n    runs-on: ubuntu-latest\n    steps:\n    - name: Login to Azure\n      uses: azure/login@v1\n      with:\n        creds: ${{ secrets.azure_creds }}\n    \n    - name: Create runner\n      run: |\n        az container create \\\n          -g ${{ inputs.rg_name }} -n ${{ inputs.aci_prefix }}-${{ github.run_id }} \\\n          --image ${{ inputs.runner_image }} --restart-policy Never \\\n          --subnet ${{ inputs.subnet_id }} \\\n          --environment-variables \\\n            RUNNER_REPOSITORY_URL=https://github.com/${{ github.repository }} \\\n            GITHUB_TOKEN=${{ secrets.repo_pat }} \\\n            RUNNER_OPTIONS=\"--ephemeral\" \\\n            RUNNER_LABELS=${{ inputs.labels }} \\\n            RUNNER_NAME=${{ inputs.aci_prefix }}-${{ github.run_id }}\n\n\n\nWorkflow to deploy an ephemeral self-hosted runner to ACI.\n\nNotes:\n\n  We specify the workflow_call trigger in the on section to indicate that this is a reusable workflow.\n  We include an inputs for specifying the rg_name (resource group name), location, subnet_id, aci_prefix and runner_image. Most inputs are self-explanatory - we’ll append the run_id to the aci_prefix to ensure a unique ACI per workflow run.\n  We need to pass in two secrets: azure_creds (to authenticate to Azure) as well as repo_pat which is a GitHub Personal Access Token (PAT) that has repo priveledges. We need this to register the runner to the repo.\n  The first step authenticates to Azure.\n  The second step uses az container create to create the ACI using the inputs provided. A few callouts: we set the --restart-policy to Never since we don’t want the container to restart when the runner unregisters after completing a job. We attache the ACI to the specified subnet via the subnet argument. Finally, we pass in --ephemeral to mark the runner as ephemeral.\n\n\nDelete ACI\n\nTo delete the ACI after the runner has unregistered, we can use this reusable workflow:\n\n\n# file: 'delete-aci.yml'\nname: Delete ACI\n\non:\n  workflow_call:\n    inputs:\n      rg_name:\n        type: string\n        description: Name of RG to deploy to\n        default: cd-ephemeral\n      aci_prefix:\n        type: string\n        description: Prefix for ACI name\n        default: cd-shrunner-aci\n        \n    secrets:\n      azure_creds:\n        description: Azure credentials\n        required: true\n\njobs:\n  delete_aci:\n    name: Delete ACI\n    runs-on: ubuntu-latest\n    steps:\n    - uses: azure/login@v1\n      with:\n        creds: ${{ secrets.azure_creds }}\n    \n    - name: Delete ACI\n      run: |\n        az container delete -g ${{ inputs.rg_name }} -n ${{ inputs.aci_prefix }}-${{ github.run_id }} --yes\n\n\n\nWorkflow to delete the ACI.\n\nNotes:\n\n  We pass in the rg_name and aci_prefix just like we did for the Deploy ACI workflow.\n  We pass in the azure_creds to authenticate to Azure.\n  After logging in to Azure, we invoke az container delete with --yes to delete the ACI completely.\n\n\n\n  Note: Just a reminder: at this stage, the runner has unregistered itself and the container is no longer running. We’re just cleaning up the ACI to ensure we are not billed for compute.\n\n\nThe Main Workflow\n\nNow that we have automation to spin up the ACI and tear it down, we can incorporate these jobs into our “main” workflow:\n\n\n# file: 'work.yml'\nname: Do Some Work\n\non:\n  workflow_dispatch:\n\njobs:\n  # deploy a runner for the job\n  deploy_runner:\n    uses: colindembovsky/scaling-self-hosted-aci/.github/workflows/deploy-ephemeral-runner.yml@main\n    with:\n      labels: uniqueString\n    secrets:\n      azure_creds: ${{ secrets.AZURE_CREDENTIALS }}\n      repo_pat: ${{ secrets.REPO_PAT }}\n  \n  # do the real work\n  build:\n    needs: deploy_runner\n    runs-on: [ self-hosted, uniqueString ]\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Simulate some work\n        run: |\n          sleep 10\n\n  delete_aci:\n    needs: build\n    if: ${{ always() }}\n    uses: colindembovsky/scaling-self-hosted-aci/.github/workflows/delete-aci.yml@main\n    secrets:\n      azure_creds: ${{ secrets.AZURE_CREDENTIALS }}\n\n\n\nThe main workflow.\n\nNotes:\n\n  We invoke the deploy runner workflow which spins up a self-hosted runner in an ACI connected to our private VNet. To make sure that our job targets the correct runner, we add a labels value of uniqueString.\n  In the build job, we target any runner that has labels self-hosted and uniqueString (there should only be the one).\n  Steps executed here run on the self-hosted runner, that is connected to our private VNet, so it should be able to access resource on this VNet and any peered VNets.\n  The delete_aci job invokes the reusable workflow to clean up the ACI after the job complets. We add if: $ to ensure that the job executes even if the build job fails.\n  We use needs to specify the ordering: first create the ACI hosting the runner, then execute the build and then only delete the ACI.\n\n\nSecrets\n\nIn order for this to work, you’ll need to configure Azure Credentials to the SPN that can create resources in the resource group and that has permissions to add interfaces to the subnet. You’ll also need to generate a GitHub PAT that has repo priveledges.\n\nAnalyzing the Runs\n\nIf you run the above workflow, you’ll have a small window of time where you can see the self-hosted runner (navigate to Settings-&gt;Actions-&gt;Runners on the repo):\n\n\n\nSelf-hosted runner registered after the ACI spins up.\n\nWhen the job completes, the runner unregisters itself and is no longer listed. Similarly, if you look at your Azure subscription, you’ll see the ACI spin up. If you click Containers and check the logs for the container, you’ll see the runner register and wait for jobs:\n\n\n\nLogs in ACI after spinning up.\n\nOnce the job completes, the runner automatically unregisters. Again, looking at the logs you’ll see this operation:\n\n\n\nLogs after completing the job.\n\nThe ACI is then removed to complete the workflow.\n\n\n\nThe completed workflow. The “work” was to sleep 120.\n\nLooking at the jobs, I recorded spinning the ACI took about 90 - 180 seconds. Tear down took approximately 30 seconds. All in all, we’re adding about 2 minutes total time to the job, but we don’t have any infrastructure to manage once the jobs complete. Not too bad to get an “on-demand” self-hosted agent!\n\nConclusion\n\nHosted runners provide a very low overhead mechanism for running Actions. However, there are times when you need custom images or when you’re targeting private VNets, and in those situations you need to run a self-hosted agent. But this requires that you manage a container runtime (like Kubernetes) or manage VMs. However, if you are looking to deploy to private VNets or run jobs on custom images without having to manage long-lived runners, then using ACI to host ephemeral runners is a viable “on-demand” solution.\n\nHappy building!\n",
      "categories": [],
      "tags": ["actions","build"],
      
      "collection": "posts",
      "url": "/on-demand-ephemeral-self-hosted-runners/"
    },{
      "image": "/assets/images/2021/11/code-quality/matrix.jpeg",
      "title": "Comparing Code Quality Metrics with Code Security",
      "date": "2021-11-01 01:22:01 +0000",
      "description": "Code security is becoming more important for modern software development. What about code quality metrics? How do code quality metrics and code security compare and contrast? I’ll discuss some thoughts in this post.\n",
      "content": "\n  Code Security\n  Security for Developers\n  Code Quality Metrics    \n      Can You Trust Code Quality Metrics?\n    \n  \n  When Do You Care - A Thought Exercise\n  Quality vs Code Quality Metrics    \n      Quality Gates\n      Code Quality Metrics Over Time\n      Code Security Over Time\n      Consequences\n    \n  \n  Conclusion\n\n\nCode Security\n\nCode security has traditionally been an “after the fact” activity. Developers would develop, build, test applications, and then when they’re ready to ship to production, attempt to get a security sign-off. This not only isolates developers from security professionals, but this usually ends up either blocking deployments completely or causing teams to deploy vulnerable code with the promise to come back and fix later.\n\nThe irony is that we’ve had security awareness, training and tooling for decades. OWASP was founded 20 years ago! Tools like Black Duck (2002), Fortify (2003), Veracode (2006) and Checkmarx (2006) are in a rich landscape of security tools. So why are we still seeing so many breaches?\n\n\n  Note: Even Semmle (which was acquired by GitHub and turned into CodeQL) has been around for many years.\n\n\n“Not in my code! Vulnerabilities are in infra,” I hear you state confidently. But the Verizon Data Breach Investigation reports between 2016 and 2020 show that the primary attach vector in breaches is application flaws. Furthermore, GitHub’s Data Science team analyzed 70 million lines of open source code and showed a linear relationship between lines of code and security threats introduced. In other words, the more code you have, the more potential threats you have.\n\nMany of these companies have been banging the “shift-left” drum: that is, integrate security earlier into the development life cylce. Still we don’t see drastically more secure code. Why?\n\nI believe the primary security failure in the industry at the moment is due to the fact that most security tools are build by security professionals for security professionals.\n\nSecurity for Developers\n\nGitHub Advanced Security (or GHAS) is a game-changer. GHAS is security focused on developers. By integrating security tooling into the very platform and into the daily workflows developers use, GHAS finally empowers developers to be responsible for security in a natural way.\n\nI won’t go into all the features of GHAS in this post, but I want to focus on a key component of GHAS and that is integrating in SAST (Static Application Security Testing) into CI/CD using CodeQL within GitHub Actions, the native GitHub automation engine.\n\nThe beauty of CodeQL is that you can integrate it into your CI/CD workflow without having to write CodeQL queries. Of course, if you have security professionals on your team or in your organization, they can write custom queries. But even if you don’t, you can tap into the growing suite of standard queries that is constantly being updated by the security community.\n\n\n  Note: CodeQL can also be configured to run maintainability and reliability queries by using the security-and-quality suite instead of the default security-extended suite.\n\n\nCode Quality Metrics\n\nMany teams try to measure quality through code quality metrics, and there are tools that are good at collecting these metrics - like SonarQube. Using SonarQube you can get a grade on how maintainable your code is for example.\n\nSounds great - we should all be deploying high quality code, right?\n\nCan You Trust Code Quality Metrics?\n\nThere are some problems with code quality metrics. A common code quality metric is cyclomatic complexity - a measure of how many paths there are through a portion of code. Perhaps we want to ensure that no single file has a cyclomatic complexity higher than 10. Now if a file has a cyclomatic complexity of 11, we might have a “bad file” - or maybe the logic is just complicated.\n\nIn the “negative” direction, we may or may not agree with the metric result. What about the “positive” direction? If a file has a cyclomatic complexity of 7, does that tell us if the code is good or not?\n\nYou begin to see the problem - if we can’t trust the metrics, then what value do they really have? If we have code that “scores high” in code quality metrics, can we conclude definitively that we have good code?\n\nIn contrast, assuming we have a good security tool like CodeQL that is known to have very low false positive rates, we can most definitely trust the code security alerts. If we run through the CodeQL suite and there are no alerts, we have high confidence in the security of our code!\n\nAnother problem is where (or when) in the lifecycle you really care about code quality metrics. Let me explain it using a hypothetical scenario.\n\nWhen Do You Care - A Thought Exercise\nImagine your team is maintaining an application that’s in production and has a solid user base. Let’s imagine it’s an e-commerce site, something like Amazon. Now imagine that you are implementing improvements to the checkout experience to ensure that customers can more easily pay using PayPal. The team has been working on the “PayPal Improvement Feature” for several weeks and are getting ready to deploy. Black Friday is coming, and you know that it’s a huge day for your company and site because of all the specials that you run. Your team has been unit testing and they’ve been running continuous deployment to staging environments and they’ve demonstrated performance is acceptable through integration testing. All systems are a go!\n\nBut suddenly you get a B for some code quality metric. Perhaps the team have been getting A’s so far - but the latest merged code has some code that could be written in an academically better way. Mind you, no tests are failing, and integration and performance testing are all green. What do you do? Are you going to deploy? Or block the deployment until the team has improved the code quality metric from a B to an A?\n\nLet’s now imagine that your security scans reveal that there is a vulnerability in the latest merged code. Do you still deploy, or get the team to fix the vulnerability?\n\nQuality vs Code Quality Metrics\n\nI’m not for a second insinuating that quality is unimportant. What I’m saying is that, in general, there are diminishing returns on measuring code quality metrics. We really conflate code quality and code quality metrics, but they are different things.\n\nCustomers that use your code don’t care about your code quality metrics. Performance, reliability, how fast you release new features - these are the things that your customers really care about (product quality if you will). The question is how many of these are predicted by code quality metrics? In other words, can we definitively say that code with high quality metrics is always performant? Or scalable? Or secure? On the other hand, if our site has good (or even good enough) performance, how much should we care about code quality metrics?\n\nQuality Gates\n\nI’ll repeat: I’m not saying that quality is unimportant. I think that there are other far more effective ways to measure and ensure quality than quality code metrics.\n\nI have coached many teams that are looking to implement testing that start by attempting to implementing UI testing. After all, they reason, if the test is at the UI layer, then we can ensure quality through the service layer to the data layer - no need to test those separately, right?\n\nIt turns out that this is a trap: different types of testing have different challenges, and differing rates of Return on Investment (ROI). I wrote about this here. Unit tests have a high ROI, since they are usually easy to write and don’t require data or environment management. Integration and Functional tests are more expensive to write and maintain, since you need consistent, stable environments and have to manage test data. UI tests are notoriously fragile. ROI diminishes quickly beyond unit testing.\n\nIn the same manner, the ROI for code quality metrics diminishes over time. Teams can (and should) implement quality gates to ensure that deployed code meets quality criteria. Assuming you peer-review Pull Requests, implement unit testing, have some Integration and Functional tests, and monitor performance of your code running in production, what real value do code quality metrics add? It definitely has some usefulness, but I would argue over time its usefulness diminishes over time, especially if you have other quality gates in place.\n\nCode Quality Metrics Over Time\n\nLet me suggest a rough graph showing criticality of Code Quality Metrics over time:\n\n\n\nCriticality of Code Quality Metrics over time.\n\nAny good team is going to enforce quality through mechanisms like protected branches, peer code-review, unit tests, automated build and deploy workflows and integration and performance testing. Code quality metrics beyond these will arguably have value before initial deployment, and then taper in criticality over time.\n\nCode Security Over Time\n\nIn contrast, here’s how I think a rough graph of criticality of Code Security looks over time:\n\n\n\nCriticality of Code Security over time.\n\nIf we assume that your code base is going to grow, and that attackers are going to uncover more vulnerabilities in your dependencies and come up with new attack vectors, we can assume that threats are going to increase over time! So the criticality of code security is going to keep increasing over time. You (or the community) may uncover vulnerabilities in dependencies tomorrow that are thought to be safe today. Constant vigilance is required.\n\nConsequences\n\nWe can also contrast the consequences of code quality metrics and code security being ignored. Going back to our Thought Exercise, you may well decide to deploy code that has a B rating for some Code Quality Metric. Let’s imagine that this causes the PayPal checkout experience to demonstrate some allowable performance impact (like taking .75 seconds instead of .5 seconds to complete). If your quality gate for performance is .8 seconds, you’re still within your performance quality gate, so while you probably do want to fix this at some stage, but the consequences of ignoring this metric are minimal.\n\nLet’s assume no-one is going to ignore security vulnerabilities that are surfaced through tooling. More likely, teams are not going to be performing code security scanning regularly. But what are the consequences of not ensuring that the PayPal checkout experience is secure? What would the impact be if a customer account is hacked because of a flaw in your code?\n\nClearly, the risk presented to not measuring code quality metrics and the risk of not securing code orders of magnitude apart.\n\nConclusion\n\nCode Quality Metrics are useful, but their criticality typically decreases over time, especially when teams implement good quality gates in their software development life cycle. The criticality of Code Security, on the other hand, steadily increases over time as code bases and attack vectors grow.\n\nWhile there is a lot of tooling in both the Code Quality Metrics and Code Security spaces, GitHub Advanced Security offers a unique platform that enables developer-first security, integrating security into developer workflows naturally and seamlessly, making it an indispensable tool for modern software development.\n\nHappy securing!\n",
      "categories": [],
      "tags": ["security"],
      
      "collection": "posts",
      "url": "/comparing-code-quality-metrics-with-code-security/"
    },{
      "image": "/assets/images/2021/11/compliance-reusable/hacker.jpg",
      "title": "Enforcing Reusable Workflows for Standardization",
      "date": "2021-11-03 01:22:01 +0000",
      "description": "Reusable workflows are great, but how do you ensure that teams are using your reusable workflows? In this post I show how you can structure repos, teams and environments to ensure standardization for your workflows.\n",
      "content": "\n  Problem Statement\n  Configuration    \n      Team and Repo Configuration\n      Workflows        \n          App Team Code Scan Workflow\n          App Team Code Scan Workflow\n          The Build Workflow\n          Reusable Code Scan Workflow\n          Reusable Deploy Workflow\n        \n      \n      CODEOWNERS File\n      Branch Protection Policy\n      Environments\n      Secrets\n    \n  \n  Working Like a Charm\n  Can It Be Bypassed?    \n      Attempt to Inject Bad Code\n      Attempt to Deploy a Bad Branch to Prod\n      Attempt to Change Deployment Steps\n      Attempt to Change CODEOWNERS\n    \n  \n  Caring About Culture\n  Conclusion\n\n\n\n  Image from www.freepik.com\n\n\nProblem Statement\n\nThere is a delicate balance between team autonomy and enterprise alignment. Too much autonomy can result in chaos, rework and runaway spending. Too much red tape can result in logn cycle times, frustration and lack of innovation. But it is possible to implement some level of compliance and leave teams some autonomy too.\n\nImagine you want to ensure that code that gets deployed is scanned using CodeQL. Furthermore, you want to enforce a specific set of steps for deploying your apps. You would prefer your app teams to be able to build, test and package applications themselves. In this post I’ll show how you can achieve do this using GitHub.\n\nWe’ll be working with two imaginary Teams: an App Team and a Compliance Team. Let’s take a look at the roles of these two teams:\n\n\n  \n    \n      Item\n      App Team\n      Compliance Team\n    \n  \n  \n    \n      main branch\n      Ensure that all code changes to main are peer-reviewed and pass quality gates before merge\n      Same as App Team\n    \n    \n      Build, test and package the application\n      Responsible for this workflow\n      Reviews, but is really interested in the final deployable package\n    \n    \n      Code Scan\n      -\n      Must ensure that all deployable code has been scanned\n    \n    \n      Deployment\n      Provide package for deployment. Can also collaborate on deployment steps.\n      Final accountability and maintenance of deployment process\n    \n    \n      Dev Environment\n      May want to gate with manual approval\n      Just supply the deployment process\n    \n    \n      Prod Environment\n      -\n      Ensure only code from protected branches is deployed and configure manual approval gate\n    \n  \n\n\nOne way to achieve this may be for all PRs to require approvals from the Compliance Team - but this is not practical and does not scale.\n\nA better solution is to use the following mechanism:\n\n  Compliance Team configures the repo to allow admin access for the Compliance Team and write access for the App team.\n  Compliance Team creates reusable workflows for code scanning and deployment in a Workflows repo.\n  Compliance Team creates one or more compliant- workflows in the app repo’s .github/workflows directory. These workflows call the reusable workflows in the Worfklows repo.\n  Compliance Team creates a CODEOWNERS file, enforcing that changes to .gitub/workflows/compliant-* require approvals from the Compliance Team\n  Compliance Team configures main branch to be protected. Require Code Review by CODEOWNERS is enabled. Required Checks is enabled, requiring the compliant-code-scan workflow to pass before allowing merges. Other settings can be set in collaboration with the App Team.\n  Compliance Team creates dev and prod environments, requiring the appropriate approvers. On prod, require appropriate approvers and also require that only protected branches can be deployed.\n\n\nWith this set of configuration, the Compliance Team can ensure that teams are following approved processes, not beome a bottleneck, and leave the App Team to get on with their work. Let’s walk through this configuration step by step.\n\nConfiguration\n\n\n  Note: All the code for this demo is available in this app repo and in this workflow repo. I’ve made these public repos to share content, but typically these would be internal or private repos in your GitHub org.\n\n\nTeam and Repo Configuration\n\nLet’s start by configuring the Teams. While you could configure these setting using individuals, setting the CODEOWNERS to be owned bu the Compliance Team is more scalable - and as folks leave/join the team, you don’t have to update configuration in app repos. So you’ll need to create at least the Compliance Team in your organization, adding appropriate members. You can even make the team Secret if you choose to:\n\n\n\nCreate a Compliance Team.\n\nNow you can create a repo for your approved workflows. Obviously the Compliance Team should be the owners/contributors to this repo. Other teams can create PRs if they wish to, but should not be able to directly write to this repo, or at leat to the main branch.\n\nFor this example, I’m putting my reusable workflows into a repo called super-approved-workflows.\n\nYou can now create an App Team, though this is not necessary.\n\nNow you can create the application repo. An administrator should ensure that the Compliance Team is set with admin priveledges on this repo, since they’ll have to do some initial configuration and will be adding files that the App Team cannot change without Compliance Team approvals. In the settings tab of my compliant-app repo, I’ve configured the Teams like this:\n\n\n\nCConfigure Team access on app repo.\n\n\n  Note: Ensure that the App Team are not given admin permissions, or they will be able to work around the compliance settings! I think that the team should only require write permissions, but there may be cases where maintain is required. Default to lowest priviledges first (i.e. write) and create an App Maintainer team for a subset of the app team if you really need maintain permissions for some operations. You can see the different between maintain and write here.\n\n\nWorkflows\n\nTo help visualize how the workflow files are organized, I drew this awesome PowerPoint art:\n\n\n\nOverview of how the workflows are structured.\n\nLet’s now dig into the workflows.\n\nApp Team Code Scan Workflow\n\nThe Compliance Team should add a workflow in the App Repo to scan code. We’ll have a look at the called workflow later, but for now, here is the workflow for the App Team:\n\n\n# file: 'APP_REPO/.github/workflows/compliant-scan.yml'\nname: Scan app\n\non:\n  workflow_dispatch:\n  pull_request:\n    branches: [ main ]\n  push:\n    branches: [ main ]\n\njobs:\n  scan:\n    name: Code scan\n    uses: colinsalmcorner/super-approved-workflows/.github/workflows/codeql-scan.yml@main\n    with:\n      languages: '[\"csharp\"]'\n\n\n\ncompliant-scan.yml workflow that invokes the centrally managed Code Scanning workflow.\n\nNotes:\n\n  The file name has the special compliant- prefix. Any changes to this file will require Compliance Team approvals (configured via the CODEOWNERS file).\n  We can add whatever triggers make sense, but should at least have pull_request and push to main trigger this workflow.\n  The job just calls the centrally managed, reusable code scan workflow, passing in the language(s) we want scanned.\n\n\nApp Team Code Scan Workflow\n\nThe Compliance Team should now add a workflow in the App Repo to scan code. We’ll have a look at the called workflow later, but for now, here is the workflow for the App Team:\n\n\n# file: 'APP_REPO/.github/workflows/compliant-deploy.yml'\nname: Deploy Pipeline\n\non:\n  workflow_dispatch:\n\njobs:\n  build:\n    name: Build\n    uses: colinsalmcorner/compliant-app/.github/workflows/build.yml@main\n    with:\n      artifact-name: drop\n\n  dev:\n    name: Deploy to DEV\n    needs: build\n    uses: colinsalmcorner/super-approved-workflows/.github/workflows/deploy-app.yml@main\n    with:\n      artifact-name: drop\n      environment-name: dev\n      environment-url: https://dev.my-super-app.net\n    secrets:\n      PASSWORD: ${{ secrets.DEV_PASSWORD }}\n  \n  prod:\n    name: Deploy to PROD\n    needs: dev\n    uses: colinsalmcorner/super-approved-workflows/.github/workflows/deploy-app.yml@main\n    with:\n      artifact-name: drop\n      environment-name: prod\n      environment-url: https://my-super-app.net\n    secrets:\n      PASSWORD: ${{ secrets.PROD_PASSWORD }}\n\n\n\ncompliant-deploy.yml workflow that invokes a “local” reusable workflow and then invokes the centrally managed Deploy workflow for each environment.\n\nNotes:\n\n  Again, the file name has the special compliant- prefix. Any changes to this file will require Compliance Team approvals (configured via the CODEOWNERS file).\n  We can add whatever triggers make sense - in this case, I’ve just configured manual trigger (via workflow_dispatch).\n  There are three jobs: build, dev and prod.\n  The build job is calling a “local” (in the same repo) reusable workflow that the App Team has full control over that builds, tests and packages the app. To make this work with the deployment workflows, we need to publish an artifact (called drop in this case). The deploy workflow will download the artifact and then deploy.\n  We can pass whatever parameters we need to here - in this case I’ve configured parameters for the artifact-name, environment-name and environment-url.\n  Secrets are tricky since reusable workflows can’t (yet?) read environment secrets. So you have to specify secrets on the repo (or org) level that are prefixed with the environment name in some way.\n\n\nThe Build Workflow\n\nBefore we switch over to the reusable workflows from the Compliance Team, let’s have a look at the build.yml workflow from the App Team:\n\n\n# file: 'APP_REPO/.github/workflows/compliant-deploy.yml'\nname: Build app\n\non:\n  workflow_call:\n    inputs:\n      artifact-name:\n        description: Name of the artifact to upload\n        type: string\n        default: drop\n\njobs:\n  build:\n    name: Build\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n    - name: Setup .NET\n      uses: actions/setup-dotnet@v1\n      with:\n        dotnet-version: 5.0.x\n    - name: Restore dependencies\n      run: dotnet restore\n    - name: Build\n      run: dotnet build --no-restore\n    - name: Test\n      if: ${{ inputs.test }}\n      run: dotnet test --no-build --verbosity normal\n    - name: Publish\n      run: dotnet publish -c Release -o tmpdrop\n    - name: Upload a Build Artifact\n      uses: actions/upload-artifact@v2.2.2\n      with:\n        name: ${{ inputs.artifact-name }}\n        path: tmpdrop/**\n        if-no-files-found: error\n\n\n\ncompliant-deploy.yml workflow that invokes a “local” reusable workflow and then invokes the centrally managed Deploy workflow for each environment.\n\nReusable Code Scan Workflow\n\nThis is just a stock CodeQL workflow that the Compliance Team will create in the Compliance repo. Obviously it must be reusable:\n\n\n# file: 'COMPLIANCE-REPO/.github/workflows/codeql-scan.yml'\nname: \"CodeQL\"\n\non: \n  workflow_call:\n    inputs:\n      languages:\n        description: Languages to scan, in the format of JSON array, e.g. '[\"csharp\", \"typescript\"]'\n        required: true\n        type: string\n  \njobs:\n  analyze:\n    name: Analyze\n    runs-on: ubuntu-latest\n\n    permissions:\n      actions: read\n      contents: read\n      security-events: write\n\n    strategy:\n      fail-fast: false\n      matrix:\n        language: ${{ fromJSON(inputs.languages) }}\n\n    steps:\n    - name: Checkout repository\n      uses: actions/checkout@v2\n\n    - name: Initialize CodeQL\n      uses: github/codeql-action/init@v1\n      with:\n        languages: ${{ matrix.language }}\n    \n    - name: Autobuild\n      uses: github/codeql-action/autobuild@v1\n\n    - name: Perform CodeQL Analysis\n      uses: github/codeql-action/analyze@v1\n\n\n\ncodeql-scan.yml workflow that runs CodeQL scanning.\n\nNotes:\n\n  The workflow_call makes this workflow reusable.\n  We pass in a JSON string of languages (since we can’t pass arrays to reusable workflows).\n  We specify minimal permissions for the workflow.\n  We create a matrix that spawns a job for each language: checkout, initialize, autobuild and then analyze.\n\n\nReusable Deploy Workflow\n\nThis example shows how to download the build artifact and then dumps the secret to show that it’s getting a value. The actual deployment steps would be inserted into this workflow in real life.\n\n\n# file: 'COMPLIANCE-REPO/.github/workflows/deploy-app.yml'\nname: Build dotnet application\n\non:\n  workflow_call:\n    inputs:\n      runs-on:\n        description: Platform to execute on\n        type: string\n        default: ubuntu-latest\n        \n      artifact-name:\n        description: Name of the artifact to deploy\n        type: string\n        default: drop\n      \n      environment-name:\n        description: Name of environment\n        type: string\n        required: true\n      \n      environment-url:\n        description: URL of environment\n        type: string\n        required: true\n    \n    secrets:\n      PASSWORD:\n        required: true\n\njobs:\n  deploy:\n    name: Deploy app\n    \n    runs-on: ${{ inputs.runs-on }}\n    \n    environment:\n      name: ${{ inputs.environment-name }}\n      url: ${{ inputs.environment-url }}\n    \n    steps:\n      - uses: actions/checkout@v2\n\n      - uses: actions/download-artifact@v2\n        with:\n          name: ${{ inputs.artifact-name }}\n\n      - name: Display structure of downloaded files\n        run: ls -R\n\n      - name: Password\n        run: echo \"JUST FOR TESTING: Password is ${{ secrets.PASSWORD }}\"\n\n      ## INSERT deployment steps here\n\n\n\ndeploy-app.yml workflow that downloads an artifact and has a placeholder for “real” deployment steps.\n\nNotes:\n\n  The workflow_call makes this workflow reusable.\n  We specify the input args and secrets that are required for deployment steps.\n  The runs-on and environment settings are configurable.\n  The workflow downloads the artifact, but doesn’t actually do anything with it - the real deployment steps would go at the bottom of the job.\n\n\nCODEOWNERS File\n\nThe Compliance Team now creates a CODEOWNERS file in the .github folder of the App repo. This tells the repo that any changes to the files specified require review by the Compliance Team:\n\n\n# Changes to `compliant-` workflows requires @compliance-team approval\n/.github/workflows/compliant-* @colinsalmcorner/compliance-team\n\n# Changes to `CODEOWNERS` requires @compliance-team approval\n/.github/CODEOWNERS @colinsalmcorner/compliance-team\n\n\n\nCODEOWNERS file to enforce Compliance Team approvals for changes to compliant-* workflows.\n\n\n  Note: The compliant- prefix is an arbitrary prefix. It can be anything you want, but in this case I wanted to distinguish between workflows the App Team can mess with and those that they can’t. All workflows must be in the .github/workflows folder, so adding a prefix was the only way this would work.\n\n\nBranch Protection Policy\n\nNow that we have the scaffolding in place, we need to ensure that no-one who doesn’t have permissions overwrites or changes files that they shouldn’t! We can do that using branch protection policies.\n\nIn the App Repo, navigate to Settings-&gt;Branches and apply the following settings to the main branch (or whatever your protected branch is called):\n\n\n\nConfiguring Branch Protection.\n\nNotes:\n\n  We enable Require a pull request before merging and Require approvals - these should be default on any repo, regardless of compliance level.\n  We enable Require review from Code Owners to ensure the Compliance Team is notified of changes to the compliant- workflows.\n  To ensure that Code Scanning is performed for all deployable code, we enable Require status checks to pass before merging and then select the Code Scan workflow. We also enable Require branches be up to date before merging as a good practice.\n\n\nEnvironments\n\nThe final bit of configuration is performed on the Environments. Let’s look at the settings for the prod environment:\n\n\n\nConfiguring the Prod Environment.\n\nNotes:\n\n  We add appropriate manual approvals.\n  Under Deployent branches we configure only Protected branches may be deployed (currently only main)\n\n\nSecrets\n\nAs mentioned, we’ll have to create environment-prefixed repo (or org) secrets since reusable workflows don’t support environment secrets. The Compliance Team can create DEV_PASSWORD and PROD_PASSWORD in this example.\n\nWorking Like a Charm\n\nUpdates to main and PRs to main now trigger the code scanning workflow:\n\n\n\nCode Scan running.\n\nIf the Deploy pipeline is triggered in the Actions tab, the pipeline executes as expected. First, the build job builds, tests and packages the application. Then the dev deployment job is triggered to deploy to the dev environment. Finally, the prod job is triggered, but only from the main branch and with the pause for manual approval:\n\n\n\nDeploy Pipeline running.\n\nCan It Be Bypassed?\n\nNow that we have things configured, let’s see if we can bypass anything! I created an account called faux-colin that is a contributor on the App Repo - he’s part of the App Team. Let’s imagine he’s nefarious too!\n\nAttempt to Inject Bad Code\n\nfaux-colin’s first attack vector might be the code itself. So he tries to change the code on main. Cloning locally, changing the code, and pushing fails. In the UI, there’s no way to change code other than creating a branch and submitting a PR:\n\n\n\nTrying to inject bad code into main.\n\nLooks like faux-colin will have to submit a PR. He can’t just sneak bad code into the codebase - at least, not into the main branch, without a code review.\n\nAttempt to Deploy a Bad Branch to Prod\n\nfaux-colin then tries his second attack vector: he’ll inject bad code into a branch and then deploy that to production! He commits bad code to an innoucously named branch: faux-colin-patch-2 for example. No PR - wouldn’t want anyone blocking the bad code, now would we? Now to sneak that code into production, he goes to the Actions tab and queues the Deployment Pipeline, selecting his malicious branch as the source:\n\n\n\nQueuing a deployment containing malicious code.\n\nHa! Bad code on its way…\n\nExcept that the branch protection policy kicks in and prevents the deployment to prod!\n\n\n\nBranch protection policy rejecting prod deployment.\n\nAt worst, the malicious code is now in the dev environment. You could technically add approvals to the dev environment too, but if you’ve walled off your dev/prod environments correctly, the risk of malicious code in dev should be minimal. You have to trust your developers at some stage of the process, otherwise people will be totally bogged down in red tape. At least you can rest assured that prod environments are still protected.\n\nAttempt to Change Deployment Steps\n\nfaux-colin then decides to change the workflows. Perhaps he doesn’t want code scanning to uncover the vulnerability he’s introducing, so he’ll just bypass the code scanning workflow. So he opens up the .github/workflows/compliant-scan.yml and removes the call to the reusable workflow:\n\n\n# file: 'APP-REPO/.github/workflows/compliant-scan.yml'\nname: Scan app\n\non:\n  workflow_dispatch:\n  pull_request:\n    branches: [ main ]\n  push:\n    branches: [ main ]\n\njobs:\n  scan:\n    #name: Code scan\n    #uses: colinsalmcorner/super-approved-workflows/.github/workflows/codeql-scan.yml@main\n    #with:\n    #  languages: '[\"csharp\"]'\n    runs-on: ubuntu-latest\n    steps:\n    - run: echo Code is secure\n\n\n\nAttempting to modify the compliant-scan.yml file.\n\nOnce again, he can’t do this on main so he has to create a branch and a PR. Interestingly, GitHub is smart enought to know that even though the workflow executed on the PR branch is the required workflow, it still required a check from the workflow in the main branch. In the screenshot below, you can see that the “bad” workflow (Scan app / scan (pull_request)) is passing, but the check is still blocked because the “good” workflow (Code scan / Analyze (csharp)) hasn’t been run:\n\n\n\nUnable to bypass the Code Scan workflow requirement.\n\nAttempt to Change CODEOWNERS\n\nfaux-colin then decides to see if he can jimmy the CODEOWNERS file. He tries to add himself as a code owner:\n\n\n# Changes to `compliant-` workflows requires @compliance-team approval\n/.github/workflows/compliant-* @colinsalmcorner/compliance-team @faux-colin\n\n# Changes to `CODEOWNERS` requires @compliance-team approval\n/.github/CODEOWNERS @colinsalmcorner/compliance-team @faux-colin\n\n\n\nAttempt to modify the CODEOWNERS file to add faux-colin as a code owner for the workflows.\n\nOnce again our hacker is foiled! A PR now contains his attempted modifications and merging would require approvals from the Compliance Team:\n\n\n\nPR blocks unapproved changes to the CODEOWNERS file.\n\nCaring About Culture\n\nIt seems that faux-colin has not been able to inject malicious code into the application. Of course, this gives the Compliance Team peace of mind: after all, malicious developers are probably few and far between. However, if a malicious developer can’t bypass the process, then there’s little chance that a developer will mistakenly do something bad. There are enough checks and balances in the configuration.\n\nThat means that the Compliance Team have done their job and can get out of the way and let the App Team do what they do best - code, and hopefully innovate! Remember, DevOps (or DevSecOps if you really prefer) is cultural too. Here we have a good balance of process adherence and compliance without developers being overburdened with red tape or the Compliance Team becoming a bottleneck because they have to enforce draconian policies manually.\n\nCaring about the culture your team works under is critical to success today. After all, Talent Management is one of the four top Developer Velocity business performance indicators. If you can ensure your code is safe and compliant and foster a positive culture, you’re winning. And using GitHub, you can!\n\nConclusion\nUsing a combination of branch protection policies, permission management, reusable workflows, environment approvals and CODEOWNERS file, teams can achieve a good combination of autonomy and enterprise alignment. Compliance Teams can rest assured that the process is being enforced without becoming blockers. Developers are happy, Compliance is happy - and business will boom.\n\nHappy complying!\n",
      "categories": [],
      "tags": ["build","security"],
      
      "collection": "posts",
      "url": "/enforcing-reusable-workflows-for-standardization/"
    },{
      "image": "/assets/images/2021/11/oidc/Data_security_27.jpg",
      "title": "GitHub Actions: Authenticate to Azure Without a Secret using OIDC",
      "date": "2021-11-09 01:22:01 +0000",
      "description": "Authenticating to Azure in GitHub Actions requires a secret for a Service Principal. However, at Universe, GitHub released a new OIDC-based authentication mechanism that eliminates the need for secrets in secure deployments.\n",
      "content": "\n  Update: 11/17/2021\n  Problem Statement\n  OIDC\n  Sample Action using OIDC\n  Azure Configuration    \n      Creating a Service Principal (App Registration)        \n          Authorize the SPN\n        \n      \n    \n  \n  GitHub Configuration\n  The Main Workflow    \n      The Composite Workflow\n    \n  \n  Running the Workflow    \n      Dev Logs\n      Prod Logs\n      Bad-prod Logs\n    \n  \n  Limitations\n  Conclusion\n\n\n\n  Image from www.freepik.com\n\n\nUpdate: 11/17/2021\n\nIn the original version of this post, I extracted the Azure login task to a Composite Action because you need a beta version of the az cli for the OIDC to work. As of today, you can use the @v1 tag of azure/login (which has been updated to include OIDC logic) and you do not have to install the beta az cli. This makes the Composite Action obsolete - all you have to do now is call the azure/login task as before, not passing the secret (assuming you configure the federated credential on the SPN in Azure).\n\nProblem Statement\n\nDeploying to Azure or other cloud providers from Actions requires that you authenticate to the provider. Not only do you have to authenticate, but the credential you use needs authorization to perform tasks in the cloud platform.\n\nFor Azure, this is accomplished by creating a Service Principal (SPN) and then saving the credentials for that SPN to your GitHub repo (or organization) as a secret. The secret is then consumed by the actions/login task to authenticate to Azure before you perform any other steps:\n\n\n{\n  \"clientId\": \"GUID\",\n  \"clientSecret\": \"some value\",\n  \"tenantId\": \"GUID\",\n  \"subscriptionId\": \"GUID\"\n}\n\n\n\nThe format of the AZURE_CREDENTIAL secret.\n\nThe problem with this secret is that it has to be maintained in GitHub. What happens when you rotate the client secret? Deployments will start to fail unless you update the secret. And if you’re using the credential across multiple repos, you’ll have to update the secret in each place it’s used.\n\nOIDC\n\nTo help solve this problem, GitHub has been working with cloud providers to implement OpenID Connect (OIDC) authentication. This is an established, standard protocol that allows systems to request and receive information about authenticated users. You can read more about the supported providers in the official documentation.\n\nThe idea is that you configure your cloud provider to issue a short-lived token to a specific GitHub repo (optionally scoping to a branch, environment, tag or “any PR”). When you run a workflow, you configure the cloud provider authentication step to request a token for a given security context (SPN for Azure, ARN for AWS for example). If the provider is happy with the request, it issues a token that the step then uses to authenticate the session and the rest of the steps proceed as usual.\n\nFor Azure, this means that we can eliminate the secret part of the Azure credential - we still need to know the tenant, subscription and client ID. Since these values are useless without the secret (which you don’t need) you don’t have to rotate them, or even store them secretly, though storing them as secrets is convenient.\n\nSample Action using OIDC\n\nYou can take a look at this repo which contains the code for this post.\n\nFor this sample, I wanted to configure two service Principals (mona-oidc-dev and mon-oidc-prod) that are given access to oidc-dev and oidc-prod resource groups respectively. I used dev and prod environments in the repo, and configured the OIDC scoped to the environment, as we’ll see later.\n\nI then set up a workflow that used the correct clients for dev and prod, expecting those to work as advertised. Finally I added a “bad” job that hard-coded the mona-oidc-dev client ID and attempted to authenticate to the prod environment, just to make sure that the authentication fails.\n\nAzure Configuration\n\nFor the Azure side, I started by creating two service Principals. Nothing special about these, apart from the fact that I have created a federated credential that enables the OIDC connection.\n\nCreating a Service Principal (App Registration)\n\nNavigate to the Active Directory blade in the Azure Portal and click +Add -&gt; App registration. Type in the name and URL - these just have to be unique, but can be any value:\n\n\n\nCreate a new SPN.\n\nOnce created, click on Certificates &amp; Secrets and then on Federated credentials. Click + Add Credential to add a new federated credential.\n\n\n\nConfigure the OIDC settings.\n\nEnter in the org, repo, entity type and additional entity filters if applicable, as well as the name and description for the credential. You can see at the bottom how Azure constructs the Subject identifier for you based on the values you select. Under the hood, when Actions requests a token, it will send a sub parameter, and if this matched, Azure will issue a token.\n\nFinally, go back to the Overview tab of the app registration and note the Application (client) ID and Directory (tenant) ID. You’ll also need the ID of the subscription you plan to target.\n\nI repeated these steps to create a new SPN (mona-oidc-prod) and configured the federated credentials the same way as mona-oidc-dev except that I set the environment value to prod instead of dev.\n\nAuthorize the SPN\n\nDon’t forget that you have to grant the SPN roles within your subscription(s). If you plan to create resource groups, then assign the SPN the Contributor role on your subscription(s). If you want to scope the SPN a little more narrowly, then add it as Contributor on one or more resource groups. You can of course use whatever custom roles you need as well.\n\nIn my case, I created two resource groups, oidc-dev and oidc-prod and assigned mona-oidc-dev and mona-oidc-prod respectively as Contributors. This means the mona-oidc-dev can only “see” the oidc-dev resource group and that mona-oidc-prod can only “see” the oidc-prod resource group.\n\nGitHub Configuration\n\nNow we can configure the GitHub side. In Settings -&gt; Environments I create two environments: dev and prod. I then create the following secrets:\n\n\n  \n    \n      Name\n      Scope\n      Value\n    \n  \n  \n    \n      AZURE_TENANT_ID\n      Repo\n      ID of the AAD tenant\n    \n    \n      AZURE_SUBSCRIPTION_ID\n      Repo\n      ID of the target subscription\n    \n    \n      AZURE_CLIENT_ID\n      dev environment\n      App (client) ID for mona-oidc-dev\n    \n    \n      AZURE_CLIENT_ID\n      prod environment\n      App (client) ID for mona-oidc-prod\n    \n  \n\n\n\n  Note: I could have specified the AZURE_SUBSCRIPTION_ID and AZURE_TENANT_ID at environment level if they differed. Most organizations will have separate dev and prod subscriptions, but usually share a single tenant.\n\n\n\n\nConfigure environments and secrets.\n\n\n  Note: none of these values constitutes a “secret” value. We have not added a secret for the client secret anywhere - just enough information to tell Azure which context we are going to request a token for.\n\n\nOf course you can add other environment configuration like approvals and deployment branches. I’ve left those as empty for this sample.\n\nThe Main Workflow\n\nHere is the workflow that we’ll be using:\n\n\n# file: '.github/workflows/azure.yml'\nname: OIDC Demo\n\non:\n  workflow_dispatch:\n\npermissions:\n  id-token: write\n\njobs:\n  dev:\n    name: Deploy to Dev\n    runs-on: ubuntu-latest\n    environment: dev\n    steps:\n    - uses: actions/checkout@v2\n    - name: Azure Login using OIDC\n      uses: ./.github/workflows/composite/azure-oidc-login\n      with:\n        tenant_id: ${{ secrets.AZURE_TENANT_ID }}\n        client_id: ${{ secrets.AZURE_CLIENT_ID }}\n        subscription_id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\n    - name: 'Run az commands'\n      run: |\n        az account show\n        az group list\n        pwd\n\n  prod:\n    name: Deploy to Prod\n    runs-on: ubuntu-latest\n    environment: prod\n    steps:\n    - uses: actions/checkout@v2\n    - name: Azure Login using OIDC\n      uses: ./.github/workflows/composite/azure-oidc-login\n      with:\n        tenant_id: ${{ secrets.AZURE_TENANT_ID }}\n        client_id: ${{ secrets.AZURE_CLIENT_ID }}\n        subscription_id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\n    - name: 'Run az commands'\n      run: |\n        az account show\n        az group list\n        pwd\n        \n  bad-prod:\n    name: Deploy to Prod using dev\n    runs-on: ubuntu-latest\n    environment: prod\n    steps:\n    - uses: actions/checkout@v2\n    - name: Azure Login using OIDC\n      uses: ./.github/workflows/composite/azure-oidc-login\n      with:\n        tenant_id: ${{ secrets.AZURE_TENANT_ID }}\n        client_id: 84b86cb5-5fbd-4950-88fa-1ab04be41de5\n        subscription_id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\n    - name: 'Run az commands'\n      run: |\n        az account show\n        az group list\n        pwd\n\n\n\nThe main workflow file.\n\nNotes:\n\n  We have to specify the id-token: write permission for this workflow to be able to retrieve and use an OIDC token.\n  There are three jobs: dev (which authenticates to dev), prod (which authenticates to prod) and bad-prod which uses the dev client ID and attempts to get a token for the prod environment.\n  Each job uses a composite action (./.github/workflows/composite/azure-oidc-login) to authenticate to Azure, passing in the tenant_id, client_id and subscription_id. The values that get passed are the values from the corresponding environment that is specified in each job.\n  After the authentication, az cli commands are executed, in this case just showing the current subscription (account) and listing the resource groups.\n  The bad-prod job is targeting the prod environment, so can’t use secrets.AZURE_CLIENT_ID otherwise it would get the ID for mona-oidc-prod. Just for demonstration, I have hard-coded the client ID of mona-oidc-dev.\n\n\nThe Composite Workflow\n\nWhy is there a composite action at all? This is because in order to authenticate to Azure using OIDC, we have to use a beta version of the az cli. Rather than install this each time, I extracted this script and the azure/login step into a composite that can be reused.\n\nHere is the composite action code:\n\n\n# file: '.github/workflows/composite/azure-oidc-login/action.yml\nname: OIDC Azure Login\n\ninputs:\n  tenant_id:\n    description: Azure AAD tenant ID\n    require: true\n  subscription_id:\n    description: Azure subscription ID\n    require: true\n  client_id:\n    description: Azure client ID that has been federated to repo/env/branch/tag\n    require: true\n\nruns:\n  using: composite\n  steps:\n  - name: Installing CLI-beta for OpenID Connect\n    shell: bash\n    run: |\n      cd ../..\n      CWD=\"$(pwd)\"\n      python3 -m venv oidc-venv\n      . oidc-venv/bin/activate\n      echo \"activated environment\"\n      python3 -m pip install -q --upgrade pip\n      echo \"started installing cli beta\"\n      pip install -q --extra-index-url https://azcliprod.blob.core.windows.net/beta/simple/ azure-cli\n      echo \"***************installed cli beta*******************\"\n      echo \"$CWD/oidc-venv/bin\" &gt;&gt; $GITHUB_PATH\n  - uses: azure/login@v1.4.0\n    name: Log in using OIDC\n    with:\n      tenant-id: ${{ inputs.tenant_id }}\n      client-id: ${{ inputs.client_id }}\n      subscription-id: ${{ inputs.subscription_id }}\n\n\n\nThe composite action to login using OIDC.\n\nNotes:\n\n  The composite requires three inputs - the tenant_id, subscription_id and client_id that we need for the authentication.\n  The first step executes a script to install the beta az cli. When the az cli is updated to include OIDC features, this step (and the composite action) will no longer be necessary.\n  The second step uses azure/login@v1.4.0 - this is the release that supports OIDC. It uses the inputs to request a token, and if successful, uses the token to authenticate the remainder of this session (job).\n\n\nRunning the Workflow\n\nWhen we run the workflow, it works as expected. The dev and prod jobs both work correctly. However, the bad-prod job fails since the environment does not match (remember, this was an attempt to authenticate to the prod environment using mona-oidc-dev credentials):\n\n\n\nA workflow run.\n\nDev Logs\n\nLooking at the logs for dev we can see a successful authentication, and we can see that the context only has access to the oidc-dev resource group:\n\n\n\nDev logs.\n\nProd Logs\n\nSimilarly, the prod logs show a successful authentication, and we the context only has access to the oidc-prod resource group:\n\n\n\nProd logs.\n\nBad-prod Logs\n\nFinally, we see that the authentication failed for bad-prod since the environment was prod but the client ID was for mona-oidc-dev, which would have failed the sub match in AAD:\n\n\n\nBad prod logs.\n\nLimitations\n\nAt present, you cannot specify individual users for the OIDC credential - only repo-level entities.\n\nAlso, if you were thinking you could use a reusable workflow and request tokens from consuming workflows this way, you’re out of luck. Even if the azure/login step is in a reusable workflow, the sub will contain the child repo context. This means that if you use the same reusable workflow in 10 repos, you will have to add all 10 repos as federated identities in the SPN Federated Credentials settings in Azure AAD.\n\nConclusion\nAuthenticating to cloud providers without secrets using OIDC is arguably more secure than having to store secrets. Tokens issues are short-lived, and because teams don’t have to store secrets, there is no need to rotate keys. Using OIDC to Azure is fairly simple and does not require a large change to existing workflows.\n\nHappy federating!\n",
      "categories": [],
      "tags": ["build","security"],
      
      "collection": "posts",
      "url": "/actions-authenticate-to-azure-without-a-secret/"
    }
  ]
}

